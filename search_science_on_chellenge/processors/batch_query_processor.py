"""
ë°°ì¹˜ ì¿¼ë¦¬ ì²˜ë¦¬ê¸° ëª¨ë“ˆ
- ì—¬ëŸ¬ ì§ˆë¬¸ì— ëŒ€í•œ ë°°ì¹˜ ì²˜ë¦¬
- ì§„í–‰ë¥  í‘œì‹œ ë° í†µê³„
- ê²°ê³¼ ì €ì¥ ë° ë³€í™˜
"""

import logging
import pandas as pd
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

from processors.single_query_processor import SingleQueryProcessor
from utils.file_manager import FileManager
from utils.result_converter import ResultConverter

class BatchQueryProcessor:
    """ë°°ì¹˜ ì¿¼ë¦¬ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, single_processor: SingleQueryProcessor, file_manager: FileManager, 
                 result_converter: ResultConverter):
        """
        ë°°ì¹˜ ì¿¼ë¦¬ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            single_processor: ë‹¨ì¼ ì¿¼ë¦¬ ì²˜ë¦¬ê¸°
            file_manager: íŒŒì¼ ê´€ë¦¬ì
            result_converter: ê²°ê³¼ ë³€í™˜ê¸°
        """
        self.single_processor = single_processor
        self.file_manager = file_manager
        self.result_converter = result_converter
        
    def process_queries_from_csv(self, csv_path: str, target_documents: int = 50, 
                                max_queries: Optional[int] = None) -> Dict[str, Any]:
        """
        CSV íŒŒì¼ì—ì„œ ì¿¼ë¦¬ë¥¼ ì½ì–´ ë°°ì¹˜ ì²˜ë¦¬
        
        Args:
            csv_path: CSV íŒŒì¼ ê²½ë¡œ
            target_documents: ì§ˆë¬¸ë‹¹ ëª©í‘œ ë¬¸ì„œ ìˆ˜
            max_queries: ìµœëŒ€ ì²˜ë¦¬í•  ì§ˆë¬¸ ìˆ˜ (Noneì´ë©´ ì „ì²´)
            
        Returns:
            ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            # CSV íŒŒì¼ ì½ê¸°
            queries = self._load_queries_from_csv(csv_path, max_queries)
            
            if not queries:
                logging.error("ì²˜ë¦¬í•  ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤")
                return self._create_batch_error_result("ì²˜ë¦¬í•  ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤")
            
            logging.info(f"CSVì—ì„œ {len(queries)}ê°œì˜ ì§ˆë¬¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {csv_path}")
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
            return self.process_queries(queries, target_documents)
            
        except Exception as e:
            logging.error(f"CSV ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_batch_error_result(str(e))
    
    def process_queries(self, queries: List[str], target_documents: int = 50) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•œ ë°°ì¹˜ ì²˜ë¦¬
        
        Args:
            queries: ì²˜ë¦¬í•  ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            target_documents: ì§ˆë¬¸ë‹¹ ëª©í‘œ ë¬¸ì„œ ìˆ˜
            
        Returns:
            ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
        """
        start_time = datetime.now()
        results = []
        successful_queries = 0
        failed_queries = 0
        total_documents = 0
        
        logging.info(f"ë°°ì¹˜ ì‹¤í–‰ ëª¨ë“œ: {len(queries)}ê°œ ì¿¼ë¦¬")
        logging.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(queries)}ê°œ ì¿¼ë¦¬")
        
        for i, query in enumerate(queries, 1):
            try:
                logging.info(f"  ì§„í–‰ë¥ : {i}/{len(queries)} - {query[:30]}...")
                logging.info(f"ì§„í–‰ë¥ : {i}/{len(queries)}")
                
                # ë‹¨ì¼ ì¿¼ë¦¬ ì²˜ë¦¬
                result = self.single_processor.process_query(query, target_documents)
                results.append(result)
                
                if result.get("status") == "success":
                    successful_queries += 1
                    total_documents += result.get("document_count", 0)
                else:
                    failed_queries += 1
                    
            except Exception as e:
                logging.error(f"ì§ˆë¬¸ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                failed_queries += 1
                results.append({
                    "query": query,
                    "status": "error",
                    "error_message": str(e),
                    "documents": [],
                    "document_count": 0,
                    "timestamp": datetime.now().isoformat()
                })
        
        # ë°°ì¹˜ ê²°ê³¼ ìƒì„±
        batch_result = self._create_batch_result(
            results=results,
            successful_queries=successful_queries,
            failed_queries=failed_queries,
            total_documents=total_documents,
            processing_time=datetime.now() - start_time
        )
        
        # ê²°ê³¼ ì €ì¥
        self._save_batch_results(batch_result)
        
        # ìë™ ë³€í™˜
        self._auto_convert_results(batch_result)
        
        logging.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {successful_queries}/{len(queries)} ì„±ê³µ")
        
        return batch_result
    
    def _load_queries_from_csv(self, csv_path: str, max_queries: Optional[int] = None) -> List[str]:
        """CSV íŒŒì¼ì—ì„œ ì§ˆë¬¸ ë¡œë“œ"""
        try:
            df = pd.read_csv(csv_path)
            
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ì§ˆë¬¸ìœ¼ë¡œ ì‚¬ìš©
            queries = df.iloc[:, 0].dropna().tolist()
            
            if max_queries:
                queries = queries[:max_queries]
            
            return queries
            
        except Exception as e:
            logging.error(f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return []
    
    def _create_batch_result(self, results: List[Dict[str, Any]], successful_queries: int, 
                           failed_queries: int, total_documents: int, processing_time) -> Dict[str, Any]:
        """ë°°ì¹˜ ê²°ê³¼ ìƒì„± (ê¸°ì¡´ í˜•ì‹ì— ë§ì¶¤)"""
        # í‚¤ì›Œë“œ í†µê³„ ê³„ì‚°
        total_korean_keywords = sum(len(r.get("keywords", {}).get("korean", [])) for r in results if r.get("status") == "success")
        total_english_keywords = sum(len(r.get("keywords", {}).get("english", [])) for r in results if r.get("status") == "success")
        total_search_queries = sum(len(r.get("search_terms", [])) for r in results if r.get("status") == "success")
        
        return {
            "batch_statistics": {
                "total_queries": len(results),
                "successful_queries": successful_queries,
                "failed_queries": failed_queries,
                "success_rate": (successful_queries / len(results)) * 100 if results else 0,
                "total_documents_found": total_documents,
                "avg_documents_per_query": total_documents / successful_queries if successful_queries > 0 else 0,
                "total_korean_keywords": total_korean_keywords,
                "total_english_keywords": total_english_keywords,
                "avg_korean_keywords_per_query": total_korean_keywords / successful_queries if successful_queries > 0 else 0,
                "avg_english_keywords_per_query": total_english_keywords / successful_queries if successful_queries > 0 else 0,
                "total_search_queries_generated": total_search_queries,
                "avg_search_queries_per_query": total_search_queries / successful_queries if successful_queries > 0 else 0,
                "processing_timestamp": datetime.now().isoformat()
            },
            "results": results,
            "execution_mode": "batch",
            "batch_timestamp": datetime.now().isoformat()
        }
    
    def _create_batch_error_result(self, error_message: str) -> Dict[str, Any]:
        """ë°°ì¹˜ ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            "batch_info": {
                "total_queries": 0,
                "successful_queries": 0,
                "failed_queries": 0,
                "success_rate": 0,
                "total_documents": 0,
                "average_documents_per_query": 0,
                "processing_time_seconds": 0,
                "timestamp": datetime.now().isoformat(),
                "error": error_message
            },
            "results": []
        }
    
    def _save_batch_results(self, batch_result: Dict[str, Any]):
        """ë°°ì¹˜ ê²°ê³¼ ì €ì¥"""
        try:
            output_file = self.file_manager.save_json_results(batch_result)
            logging.info(f"ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"   ê²°ê³¼ íŒŒì¼: {output_file}")
        except Exception as e:
            logging.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _auto_convert_results(self, batch_result: Dict[str, Any]):
        """ê²°ê³¼ ìë™ ë³€í™˜"""
        try:
            # CSV ë³€í™˜
            print("\nğŸ”„ ìë™ìœ¼ë¡œ CSV ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            csv_file = self.result_converter.convert_to_csv(batch_result)
            print(f"âœ… CSV ë³€í™˜ ì™„ë£Œ: {csv_file}")
            
            # JSONL ë³€í™˜
            print("ğŸ”„ ìë™ìœ¼ë¡œ JSONL ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            jsonl_file = self.result_converter.convert_to_jsonl(batch_result)
            print(f"âœ… JSONL ë³€í™˜ ì™„ë£Œ: {jsonl_file}")
            
        except Exception as e:
            logging.error(f"ìë™ ë³€í™˜ ì‹¤íŒ¨: {e}")
    
    def print_batch_summary(self, batch_result: Dict[str, Any]):
        """ë°°ì¹˜ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        batch_info = batch_result.get("batch_statistics", {})
        
        print(f"\nğŸ“Š ë°°ì¹˜ ì‹¤í–‰ ê²°ê³¼:")
        print(f"   ì´ ì²˜ë¦¬ëœ ì§ˆë¬¸: {batch_info.get('total_queries', 0)}ê°œ")
        print(f"   ì„±ê³µí•œ ì§ˆë¬¸: {batch_info.get('successful_queries', 0)}ê°œ")
        print(f"   ì‹¤íŒ¨í•œ ì§ˆë¬¸: {batch_info.get('failed_queries', 0)}ê°œ")
        print(f"   ì„±ê³µë¥ : {batch_info.get('success_rate', 0):.1f}%")
        print(f"   ì´ ì°¾ì€ ë¬¸ì„œ: {batch_info.get('total_documents_found', 0)}ê°œ")
        print(f"   í‰ê·  ë¬¸ì„œ/ì§ˆë¬¸: {batch_info.get('avg_documents_per_query', 0):.1f}ê°œ")
