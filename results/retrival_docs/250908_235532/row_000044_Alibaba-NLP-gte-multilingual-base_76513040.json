{
  "id": "row_000044",
  "model_name": "Alibaba-NLP/gte-multilingual-base",
  "timestamp_kst": "2025-09-08T23:55:38.353340+09:00",
  "trial_id": "76513040",
  "queries": [
    {
      "query": "How does the audiovisual speech recognition approach integrate hidden Markov models and neural networks to achieve robustness in noisy environments?",
      "query_meta": {
        "type": "original"
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8063604235649109,
          "doc_id": "DIKO0011019580",
          "title": "시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합",
          "abstract": "음성인식은 인간과 기계의 인터페이스 서비스를 위한 중요한 기술이다. 현재까지 개발된 많은 음성인식 시스템은 제어된 환경에서는 높은 인식율을 보이지만, 잡음 환경에서는 성능이 크게 저하되는 한계가 있다. 이러한 한계를 극복하기 위한 방법의 하나인 시청각 음성인식은 잡음이 존재하는 환경에서 강인한 인식을 위해 마이크로 녹음한 음성신호 (목소리)와 카메라로 기록한 영상신호(입술의 움직임)를 모두 이용하여 음성을 인식하는 기술이다. 영상신호를 이용한 인식은 잡음이 적은 상황에서는 기존의 음성인식에 비해 낮은 인식 성능을 보이지만 소리잡음에 영향을 받지 않기 때문에 잡음 환경에서 음성인식을 보완하는 유용한 방법이 된다. 본 논문에서는 시청각 음성인식 시스템을 구성하는 청각신호를 이용한 인식, 시각정보를 이용한 인식, 그리고 두 정보의 통합 등의 세 부분을 고려하여 각 부분의 성능을 향상시킴으로써 잡음환경에서의 강인함을 향상시키고자 한다. 첫째, 시각정보를 이용한 인식의 성능을 향상시키기 위해 인식기인 은닉 마르코프 모델(hidden Markov model)의 확률적인 최적화 알고리즘을 제안한다. 알고리즘의 성능과 수렴속도를 개선하기 위해 확률적인 최적화 알고리즘의 하나인 모의 담금질 기법(simulated annealing)과 지역 탐색 연산자를 결합한 하이브리드 모의 담금질 기법을 개발한다. 기존의 은닉 마르코프 모델의 학습 알고리즘인 기대-최대(expectation-maximization) 알고리즘은 가능도 (likelihood) 함수에 대해 지역 최적화만을 수행하는 반면, 제안하는 알고리즘은 전 영역에서 탐색을 수행하며 결과적으로 은닉 마르코프 모델의 인식 성능을 높인다. 제안하는 알고리즘이 전역최적해에 확률로써 수렴하는 것을 수학적으로 증명한다. 둘째, 청각정보를 이용한 인식을 강인하게 하기 위해 은닉 마르코프 모델에서 관측 프레임간의 상관관계를 모델링하는 기법을 개발한다. 음성의 동적 특성은 사람이 잡음환경에서 강인한 인식 성능을 보이는데 도움을 주는 것으로 알려져 있으나 기존의 은닉 마르코프 모델을 이용한 음성 모델링에서는 충분히 고려되지 않고 있다. 본 논문에서는 가우시안 혼합 모델(Gaussian mixture model)로 서로 다른 프레임의 결합 확률분포를 모델링하여 프레임간의 조건부 의존관계를 다루는 기법을 제안한다. 또한, 기대-최대 알고리즘에 기반하여 제안하는 모델의 학습 알고리즘을 개발한다. 제안하는 모델을 이용함으로써 청각 정보를 이용한 인식에서 잡음에 더욱 강인한 성능을 얻도록 한다. 셋째, 시각정보와 청각정보의 상호보완성을 효과적으로 이용하여 잡음에 강인한 최종 인식결과를 얻기 위해 정보 통합 단계에서 신경회로망을 이용하는 기법을 제안한다. 정보 통합 단계에서는 시각정보와 청각정보를 각각 따로 인식한 결과를 가중치 기법을 통해 최종 결과를 얻는데, 학습된 신경회로망은 잡음의 종류와 수준을 알 수 없는 주어진 시청각 데이터에 대해 적절한 가중치를 출력함으로써 최적의 인식결과를 얻도록 한다. 이를 통해 통합 인식 결과가 시각 또는 청각정보만을 이용한 인식결과보다 최소한 같거나 좋을 뿐 아니라 두 정보에 의한 시너지 효과를 최대화하도록 한다. 제안하는 시스템을 화자독립 고립단어 인식문제에 적용하여 그 성능을 보인다. 실험 결과 제안하는 시스템이 기존의 시스템에 비해 다양한 잡음 환경에서 더욱 강인한 성능을 보이는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011019580&target=NART&cn=DIKO0011019580",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 음성인식은 인간과 기계의 인터페이스 서비스를 위한 중요한 기술이다. 현재까지 개발된 많은 음성인식 시스템은 제어된 환경에서는 높은 인식율을 보이지만, 잡음 환경에서는 성능이 크게 저하되는 한계가 있다. 이러한 한계를 극복하기 위한 방법의 하나인 시청각 음성인식은 잡음이 존재하는 환경에서 강인한 인식을 위해 마이크로 녹음한 음성신호 (목소리)와 카메라로 기록한 영상신호(입술의 움직임)를 모두 이용하여 음성을 인식하는 기술이다. 영상신호를 이용한 인식은 잡음이 적은 상황에서는 기존의 음성인식에 비해 낮은 인식 성능을 보이지만 소리잡음에 영향을 받지 않기 때문에 잡음 환경에서 음성인식을 보완하는 유용한 방법이 된다. 본 논문에서는 시청각 음성인식 시스템을 구성하는 청각신호를 이용한 인식, 시각정보를 이용한 인식, 그리고 두 정보의 통합 등의 세 부분을 고려하여 각 부분의 성능을 향상시킴으로써 잡음환경에서의 강인함을 향상시키고자 한다. 첫째, 시각정보를 이용한 인식의 성능을 향상시키기 위해 인식기인 은닉 마르코프 모델(hidden Markov model)의 확률적인 최적화 알고리즘을 제안한다. 알고리즘의 성능과 수렴속도를 개선하기 위해 확률적인 최적화 알고리즘의 하나인 모의 담금질 기법(simulated annealing)과 지역 탐색 연산자를 결합한 하이브리드 모의 담금질 기법을 개발한다. 기존의 은닉 마르코프 모델의 학습 알고리즘인 기대-최대(expectation-maximization) 알고리즘은 가능도 (likelihood) 함수에 대해 지역 최적화만을 수행하는 반면, 제안하는 알고리즘은 전 영역에서 탐색을 수행하며 결과적으로 은닉 마르코프 모델의 인식 성능을 높인다. 제안하는 알고리즘이 전역최적해에 확률로써 수렴하는 것을 수학적으로 증명한다. 둘째, 청각정보를 이용한 인식을 강인하게 하기 위해 은닉 마르코프 모델에서 관측 프레임간의 상관관계를 모델링하는 기법을 개발한다. 음성의 동적 특성은 사람이 잡음환경에서 강인한 인식 성능을 보이는데 도움을 주는 것으로 알려져 있으나 기존의 은닉 마르코프 모델을 이용한 음성 모델링에서는 충분히 고려되지 않고 있다. 본 논문에서는 가우시안 혼합 모델(Gaussian mixture model)로 서로 다른 프레임의 결합 확률분포를 모델링하여 프레임간의 조건부 의존관계를 다루는 기법을 제안한다. 또한, 기대-최대 알고리즘에 기반하여 제안하는 모델의 학습 알고리즘을 개발한다. 제안하는 모델을 이용함으로써 청각 정보를 이용한 인식에서 잡음에 더욱 강인한 성능을 얻도록 한다. 셋째, 시각정보와 청각정보의 상호보완성을 효과적으로 이용하여 잡음에 강인한 최종 인식결과를 얻기 위해 정보 통합 단계에서 신경회로망을 이용하는 기법을 제안한다. 정보 통합 단계에서는 시각정보와 청각정보를 각각 따로 인식한 결과를 가중치 기법을 통해 최종 결과를 얻는데, 학습된 신경회로망은 잡음의 종류와 수준을 알 수 없는 주어진 시청각 데이터에 대해 적절한 가중치를 출력함으로써 최적의 인식결과를 얻도록 한다. 이를 통해 통합 인식 결과가 시각 또는 청각정보만을 이용한 인식결과보다 최소한 같거나 좋을 뿐 아니라 두 정보에 의한 시너지 효과를 최대화하도록 한다. 제안하는 시스템을 화자독립 고립단어 인식문제에 적용하여 그 성능을 보인다. 실험 결과 제안하는 시스템이 기존의 시스템에 비해 다양한 잡음 환경에서 더욱 강인한 성능을 보이는 것을 확인한다."
        },
        {
          "rank": 2,
          "score": 0.7979225516319275,
          "doc_id": "JAKO201403359905324",
          "title": "가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원",
          "abstract": "This paper describes a robust speech recognition technique by reconstructing spectral components mismatched with a training environment. Although the cluster-based reconstruction method can compensate the unreliable components from reliable components in the same spectral vector by assuming an independent, identically distributed Gaussian-mixture process of training spectral vectors, the presented method exploits the temporal dependency of speech to reconstruct the components by introducing a hidden-Markov-model prior which incorporates an internal state transition plausible for an observed spectral vector sequence. The experimental results indicate that the described method can provide temporally consistent reconstruction and further improve recognition performance on average compared to the conventional method.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201403359905324&target=NART&cn=JAKO201403359905324",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 This paper describes a robust speech recognition technique by reconstructing spectral components mismatched with a training environment. Although the cluster-based reconstruction method can compensate the unreliable components from reliable components in the same spectral vector by assuming an independent, identically distributed Gaussian-mixture process of training spectral vectors, the presented method exploits the temporal dependency of speech to reconstruct the components by introducing a hidden-Markov-model prior which incorporates an internal state transition plausible for an observed spectral vector sequence. The experimental results indicate that the described method can provide temporally consistent reconstruction and further improve recognition performance on average compared to the conventional method."
        },
        {
          "rank": 3,
          "score": 0.789067268371582,
          "doc_id": "JAKO200411922338894",
          "title": "신경망 기반 음성, 영상 및 문맥 통합 음성인식",
          "abstract": "최근 잡음환경에서 신뢰도 높은 음성인식을 위해 음성정보와 영상정보를 융합하는 방법이 활발히 연구되고 있다. 본 논문에서는 이절적인 정보의 융합에 적합한 신경망 모델을 기반으로 음성, 영상 및 문맥 정보 등 다양한 정보를 융합하여 잡음 환경에서 고려단어를 인식하는 음성인식 기법에 대하여 기술한다. 음성과 영상 특징을 이용한 이중 모드 신경망 BMNN(BiModal Neural Network)을 제안한다. BMM은 4개 층으로 이루어진 다층퍼셉트론의 구조를 가지며 각 층은 입력 특징의 추상화 기능을 수행한다. BMNN에서는 제 3층이 잡음에 의한 음성 정보의 손실을 보상하기 위하여 음성과 영상 특징을 통합하는 기능을 수행한다. 또한, 잡음환경에서 음성 인식률을 향상시키기 위해 사용자가 말한 단어들의 순차 패턴을 나타내는 문맥정보를 이용한 후처리 방법을 제안한다. 잡음환경에서 BMNN은 단순히 음성만을 사용한 것 보다 높은 성능을 보임으로써 그 타당성을 확인할 수 있을 뿐 아니라, 특히 문맥을 이용한 후처리를 하였을 경우 잡음 환경에서 90%이상의 인식률을 달성하였다 본 연구는 잡음환경에서 강인한 음성인식을 위해 다양한 추가 정보를 사용함으로써 성능을 향상시킬 수 있음을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200411922338894&target=NART&cn=JAKO200411922338894",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망 기반 음성, 영상 및 문맥 통합 음성인식 신경망 기반 음성, 영상 및 문맥 통합 음성인식 신경망 기반 음성, 영상 및 문맥 통합 음성인식 최근 잡음환경에서 신뢰도 높은 음성인식을 위해 음성정보와 영상정보를 융합하는 방법이 활발히 연구되고 있다. 본 논문에서는 이절적인 정보의 융합에 적합한 신경망 모델을 기반으로 음성, 영상 및 문맥 정보 등 다양한 정보를 융합하여 잡음 환경에서 고려단어를 인식하는 음성인식 기법에 대하여 기술한다. 음성과 영상 특징을 이용한 이중 모드 신경망 BMNN(BiModal Neural Network)을 제안한다. BMM은 4개 층으로 이루어진 다층퍼셉트론의 구조를 가지며 각 층은 입력 특징의 추상화 기능을 수행한다. BMNN에서는 제 3층이 잡음에 의한 음성 정보의 손실을 보상하기 위하여 음성과 영상 특징을 통합하는 기능을 수행한다. 또한, 잡음환경에서 음성 인식률을 향상시키기 위해 사용자가 말한 단어들의 순차 패턴을 나타내는 문맥정보를 이용한 후처리 방법을 제안한다. 잡음환경에서 BMNN은 단순히 음성만을 사용한 것 보다 높은 성능을 보임으로써 그 타당성을 확인할 수 있을 뿐 아니라, 특히 문맥을 이용한 후처리를 하였을 경우 잡음 환경에서 90%이상의 인식률을 달성하였다 본 연구는 잡음환경에서 강인한 음성인식을 위해 다양한 추가 정보를 사용함으로써 성능을 향상시킬 수 있음을 제시한다."
        },
        {
          "rank": 4,
          "score": 0.7866977453231812,
          "doc_id": "NART16453920",
          "title": "Neural-network-based HMM adaptation for noisy speech recognition.",
          "abstract": "<P>This paper proposes a new method, using neural networks, of adapting phone HMMs to noisy speech. The neural networks are designed to map clean speech HMMs to noise-adapted HMMs, using noise HMMs and signal-to-noise ratios (SNRs) as inputs. The neural network is trained by minimizing the mean square error between the output HMMs and the target noise-adapted HMMs. In an evaluation, the proposed method was used to recognize noisy broadcast-news speech in speaker-dependent and speaker-independent modes. The trained networks were found to be effective in recognizing new speakers under new noise and various SNR conditions.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART16453920&target=NART&cn=NART16453920",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural-network-based HMM adaptation for noisy speech recognition. Neural-network-based HMM adaptation for noisy speech recognition. Neural-network-based HMM adaptation for noisy speech recognition. <P>This paper proposes a new method, using neural networks, of adapting phone HMMs to noisy speech. The neural networks are designed to map clean speech HMMs to noise-adapted HMMs, using noise HMMs and signal-to-noise ratios (SNRs) as inputs. The neural network is trained by minimizing the mean square error between the output HMMs and the target noise-adapted HMMs. In an evaluation, the proposed method was used to recognize noisy broadcast-news speech in speaker-dependent and speaker-independent modes. The trained networks were found to be effective in recognizing new speakers under new noise and various SNR conditions.</P>"
        },
        {
          "rank": 5,
          "score": 0.7806785106658936,
          "doc_id": "NART37979687",
          "title": "Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network",
          "abstract": "<P>A new method for noisy speech recognition based on a hybrid model of hidden Markov models (HMM) and wavelet neural network (WNN) is presented. The HMM was employed to compute the Viterbi output score. Then the score was used as the input of WNN to acquire the classification information. The result of recognition was made by these two kinds of recognition information. Recognition experiment shows that this hybrid model has higher performance than hidden Markov model in noisy speech recognition.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART37979687&target=NART&cn=NART37979687",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network <P>A new method for noisy speech recognition based on a hybrid model of hidden Markov models (HMM) and wavelet neural network (WNN) is presented. The HMM was employed to compute the Viterbi output score. Then the score was used as the input of WNN to acquire the classification information. The result of recognition was made by these two kinds of recognition information. Recognition experiment shows that this hybrid model has higher performance than hidden Markov model in noisy speech recognition.</P>"
        },
        {
          "rank": 6,
          "score": 0.7754343152046204,
          "doc_id": "NART18014750",
          "title": "Neural nets and hidden Markov models: Review and generalizations",
          "abstract": "Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs).",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART18014750&target=NART&cn=NART18014750",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural nets and hidden Markov models: Review and generalizations Neural nets and hidden Markov models: Review and generalizations Neural nets and hidden Markov models: Review and generalizations Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs)."
        },
        {
          "rank": 7,
          "score": 0.7740074992179871,
          "doc_id": "NART20682074",
          "title": "Robust combination of neural networks and hidden Markov models for speech recognition",
          "abstract": "Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the 'divergence problem'-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20682074&target=NART&cn=NART20682074",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Robust combination of neural networks and hidden Markov models for speech recognition Robust combination of neural networks and hidden Markov models for speech recognition Robust combination of neural networks and hidden Markov models for speech recognition Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the 'divergence problem'-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs."
        },
        {
          "rank": 8,
          "score": 0.7710180282592773,
          "doc_id": "DIKO0011930560",
          "title": "시청각 음성인식을 위한 새로운 통합방법",
          "abstract": "The automatic speech recognition (ASR) is one of the most interesting problems applied to human computer interaction applications; for example, spoken digit recognition for mobile environments. One of the challenges of this problem is that the accuracy of speech recognition will be decrease much if the speaker talks under noisy place such as: restaurant, subway, street… The invention of lip-reading opens a potential chance to improve the performance of recognition. Indeed, human perception considers both auditory and visual nature of speech. The speech recognition system will be more intelligible if the lip motion of speaker is available together with acoustic signal. The combined audio visual speech recognition has been proved to be able enhance the overall performance of recognition, especially under noisy environment. In general, if the two streams are available for speech recognition, they can be integrated by two ways: early integration and late integration. The early integration approach combines the features of two streams into one concatenated feature vector, and uses single classifier for recognition. The late integration approach combines the results of two separate classifiers for recognition in which the reliability of modalities is applied to summation based fusion. The late integration method shows the better performance actually through many experiments. There are several factors are considered to measure reliability including word confusability, SNR level of acoustic signal, the noise type, and illumination change in visual stream rather than the only SNR level based confidence of conventional audio visual speech recognition. In this study, we propose an effective fusion scheme for audio visual speech recognition (AVSR) in which the appropriate combination weights are measured by using an integrated reliability. The significant idea of integrated reliability is the combination of not only acoustic noise but also model confusability for audio visual reliability measurement The experimental results using Samsung AVSR database shows the improved performance of our approach compared to conventional ones. This demonstrates the effectiveness and feasibility of this invention for real speech recognition applications.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011930560&target=NART&cn=DIKO0011930560",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시청각 음성인식을 위한 새로운 통합방법 시청각 음성인식을 위한 새로운 통합방법 시청각 음성인식을 위한 새로운 통합방법 The automatic speech recognition (ASR) is one of the most interesting problems applied to human computer interaction applications; for example, spoken digit recognition for mobile environments. One of the challenges of this problem is that the accuracy of speech recognition will be decrease much if the speaker talks under noisy place such as: restaurant, subway, street… The invention of lip-reading opens a potential chance to improve the performance of recognition. Indeed, human perception considers both auditory and visual nature of speech. The speech recognition system will be more intelligible if the lip motion of speaker is available together with acoustic signal. The combined audio visual speech recognition has been proved to be able enhance the overall performance of recognition, especially under noisy environment. In general, if the two streams are available for speech recognition, they can be integrated by two ways: early integration and late integration. The early integration approach combines the features of two streams into one concatenated feature vector, and uses single classifier for recognition. The late integration approach combines the results of two separate classifiers for recognition in which the reliability of modalities is applied to summation based fusion. The late integration method shows the better performance actually through many experiments. There are several factors are considered to measure reliability including word confusability, SNR level of acoustic signal, the noise type, and illumination change in visual stream rather than the only SNR level based confidence of conventional audio visual speech recognition. In this study, we propose an effective fusion scheme for audio visual speech recognition (AVSR) in which the appropriate combination weights are measured by using an integrated reliability. The significant idea of integrated reliability is the combination of not only acoustic noise but also model confusability for audio visual reliability measurement The experimental results using Samsung AVSR database shows the improved performance of our approach compared to conventional ones. This demonstrates the effectiveness and feasibility of this invention for real speech recognition applications."
        },
        {
          "rank": 9,
          "score": 0.7681554555892944,
          "doc_id": "NART70632792",
          "title": "Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis",
          "abstract": "<P>This paper investigates joint speaker-dependent audiovisual Hidden Semi-Markov Models (HSMM) where the visual models produce a sequence of 3D motion tracking data that is used to animate a talking head and the acoustic models are used for speech synthesis. Different acoustic, visual, and joint audiovisual models for four different Austrian German speakers were trained and we show that the joint models perform better compared to other approaches in terms of synchronization quality of the synthesized visual speech. In addition, a detailed analysis of the acoustic and visual alignment is provided for the different models. Importantly, the joint audiovisual modeling does not decrease the acoustic synthetic speech quality compared to acoustic-only modeling so that there is a clear advantage in the common duration model of the joint audiovisual modeling approach that is used for synchronizing acoustic and visual parameter sequences. Finally, it provides a model that integrates the visual and acoustic speech dynamics.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART70632792&target=NART&cn=NART70632792",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis <P>This paper investigates joint speaker-dependent audiovisual Hidden Semi-Markov Models (HSMM) where the visual models produce a sequence of 3D motion tracking data that is used to animate a talking head and the acoustic models are used for speech synthesis. Different acoustic, visual, and joint audiovisual models for four different Austrian German speakers were trained and we show that the joint models perform better compared to other approaches in terms of synchronization quality of the synthesized visual speech. In addition, a detailed analysis of the acoustic and visual alignment is provided for the different models. Importantly, the joint audiovisual modeling does not decrease the acoustic synthetic speech quality compared to acoustic-only modeling so that there is a clear advantage in the common duration model of the joint audiovisual modeling approach that is used for synchronizing acoustic and visual parameter sequences. Finally, it provides a model that integrates the visual and acoustic speech dynamics.</P>"
        },
        {
          "rank": 10,
          "score": 0.76596599817276,
          "doc_id": "NPAP12270893",
          "title": "Speech Recognition in Noisy Environments with Convolutional Neural Networks",
          "abstract": "<P>One of the biggest challenges in speech recognition today is its use on a daily basis, in which distortion and noise in the environment are present and hinder the recognition task. In the last thirty years, hundreds of methods for noise-robust recognition were proposed, each with its own advantages and disadvantages. In this paper, the use of convolutional neural networks (CNN) as acoustic models in automatic speech recognition systems (ASR) is proposed as an alternative to the classical recognition methods based on HMM without any noise-robust method applied. The experiment showed that the presented method reduces the equal error rate in word recognition tasks with additive noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12270893&target=NART&cn=NPAP12270893",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech Recognition in Noisy Environments with Convolutional Neural Networks Speech Recognition in Noisy Environments with Convolutional Neural Networks Speech Recognition in Noisy Environments with Convolutional Neural Networks <P>One of the biggest challenges in speech recognition today is its use on a daily basis, in which distortion and noise in the environment are present and hinder the recognition task. In the last thirty years, hundreds of methods for noise-robust recognition were proposed, each with its own advantages and disadvantages. In this paper, the use of convolutional neural networks (CNN) as acoustic models in automatic speech recognition systems (ASR) is proposed as an alternative to the classical recognition methods based on HMM without any noise-robust method applied. The experiment showed that the presented method reduces the equal error rate in word recognition tasks with additive noise.</P>"
        },
        {
          "rank": 11,
          "score": 0.7627544403076172,
          "doc_id": "DIKO0007842188",
          "title": "신경망 예측 HMM을 이용한 음성인식에 관한 연구",
          "abstract": "음성은 인간의 가장 자연스러운 의사소통의 수단이며 이러한 음성에 의한 인간-기계 인터페이스는 속도가 빠르고 특별한 훈련이 없어도 이루어진다. 또한 컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식 기술은 중요한 연구 과제가 되고 있다. 이러한 음성인식에 관한 연구는 HMM과 신경망에 의한 방법들이 활발히 진행되고 있다. 그러나 HMM은 모델구조의 결정에 어려움이 있으며, 음성의 과도적 정보를 경시하는 경향이 있기 때문에 시간적 상관 표현력이 부족한 결점이 있다. 따라서 이러한 단점을 극복하기 위하여 음성의 동적특성을 표현할 수 있는 회귀계수와 지속시간 확률등을 파라메터로 추가하거나, 언어학적 제약을 가하여 최적의 단어별을 탐색하는 언어적 모델을 결합한 음성이해 시스템으로 발전하고 있다. 또한 신경망의 경우 그 구조나 가중치에 시간적인 것이 고려되지 않으므로 패턴이 정적인 경우 인식률이 높으나 음성과 같은 시계열 패턴의 비선형 신축을 취급하기 어렵다. 따라서 이러한 단점을 보완하기 위하여 회귀신경망, 예측형 신경망등이 제안되었다. 본 논문에서는 이러한 점들을 고려하여 HMM의 패턴에 대한 시간적 상관표현력을 개선시킬 수 있는 신경망 예측 HMM을 제안한다. 신경망 예측 HMM은 HMM과 신경망의 장점을 함께 사용할 수 있는 하이브리드형 네트워크로 예측형 신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하는 것이다. 따라서 예측형 신경망은 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가지므로 HMM에 동적 특성을 부여할 수 있다. 실험에서는 단독 숫자음과 100음절 데이터를 이용하여 HMM과 신경망 그리고 본 논문에서 제안한 신경망 예측 HMM의 인식성능을 비교·검토하였다. 신경망 예측 HMM은 MLP 예측 HMM, Elman망 예측 HMM, Jordan망 예측 HMM의 3가지이다. 실험결과 평가용 데이터를 기준으로 단독 숫자음에 대해서는 MLP 예측 HMM이 99.5%로 가장 우수한 결과를, 100음절 데이터에 대해서는 HMM이 87.7%로 가장 우수한 결과를 보였다. Elman망 예측 HMM과 Jordan망 예측 HMM의 경우 회귀구조임에도 불구하고 MLP 예측 HMM의 인식성능을 상회하지는 못하였다. 따라서 신경망 예측 HMM의 인식성능을 향상시키기 위해서는 데이터에 따른 효과의 검토와 음성패턴의 시간적 상관관계를 보다 더 잘 표현 할 수 있는 신경망의 구조에 대한 연구가 요구되며, 신경망과 HMM의 학습알고리즘에 대한 연구가 필요하다고 판단된다. 본 논문에서 제안한 신경망 예측 HMM은 HMM과 신경망의 결합이라고 하는 향후 가장 유효한 방법의 하나로 사료되며 연속음성 인식시스템으로 확장할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0007842188&target=NART&cn=DIKO0007842188",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망 예측 HMM을 이용한 음성인식에 관한 연구 신경망 예측 HMM을 이용한 음성인식에 관한 연구 신경망 예측 HMM을 이용한 음성인식에 관한 연구 음성은 인간의 가장 자연스러운 의사소통의 수단이며 이러한 음성에 의한 인간-기계 인터페이스는 속도가 빠르고 특별한 훈련이 없어도 이루어진다. 또한 컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식 기술은 중요한 연구 과제가 되고 있다. 이러한 음성인식에 관한 연구는 HMM과 신경망에 의한 방법들이 활발히 진행되고 있다. 그러나 HMM은 모델구조의 결정에 어려움이 있으며, 음성의 과도적 정보를 경시하는 경향이 있기 때문에 시간적 상관 표현력이 부족한 결점이 있다. 따라서 이러한 단점을 극복하기 위하여 음성의 동적특성을 표현할 수 있는 회귀계수와 지속시간 확률등을 파라메터로 추가하거나, 언어학적 제약을 가하여 최적의 단어별을 탐색하는 언어적 모델을 결합한 음성이해 시스템으로 발전하고 있다. 또한 신경망의 경우 그 구조나 가중치에 시간적인 것이 고려되지 않으므로 패턴이 정적인 경우 인식률이 높으나 음성과 같은 시계열 패턴의 비선형 신축을 취급하기 어렵다. 따라서 이러한 단점을 보완하기 위하여 회귀신경망, 예측형 신경망등이 제안되었다. 본 논문에서는 이러한 점들을 고려하여 HMM의 패턴에 대한 시간적 상관표현력을 개선시킬 수 있는 신경망 예측 HMM을 제안한다. 신경망 예측 HMM은 HMM과 신경망의 장점을 함께 사용할 수 있는 하이브리드형 네트워크로 예측형 신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하는 것이다. 따라서 예측형 신경망은 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가지므로 HMM에 동적 특성을 부여할 수 있다. 실험에서는 단독 숫자음과 100음절 데이터를 이용하여 HMM과 신경망 그리고 본 논문에서 제안한 신경망 예측 HMM의 인식성능을 비교·검토하였다. 신경망 예측 HMM은 MLP 예측 HMM, Elman망 예측 HMM, Jordan망 예측 HMM의 3가지이다. 실험결과 평가용 데이터를 기준으로 단독 숫자음에 대해서는 MLP 예측 HMM이 99.5%로 가장 우수한 결과를, 100음절 데이터에 대해서는 HMM이 87.7%로 가장 우수한 결과를 보였다. Elman망 예측 HMM과 Jordan망 예측 HMM의 경우 회귀구조임에도 불구하고 MLP 예측 HMM의 인식성능을 상회하지는 못하였다. 따라서 신경망 예측 HMM의 인식성능을 향상시키기 위해서는 데이터에 따른 효과의 검토와 음성패턴의 시간적 상관관계를 보다 더 잘 표현 할 수 있는 신경망의 구조에 대한 연구가 요구되며, 신경망과 HMM의 학습알고리즘에 대한 연구가 필요하다고 판단된다. 본 논문에서 제안한 신경망 예측 HMM은 HMM과 신경망의 결합이라고 하는 향후 가장 유효한 방법의 하나로 사료되며 연속음성 인식시스템으로 확장할 수 있을 것으로 기대된다."
        },
        {
          "rank": 12,
          "score": 0.7603362798690796,
          "doc_id": "NART30128358",
          "title": "Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer",
          "abstract": "<P><B>Abstract</B></P><P>Spoken human&ndash;machine interaction in real-world environments requires acoustic models that are robust to changes in acoustic conditions, e.g. presence of noise. Unfortunately, the popular hidden Markov models (HMM) are not noise tolerant. One way to increase recognition performance is to acquire a small adaptation set of noisy utterances, which is used to estimate a normalization mapping between noisy and clean features to be fed into the acoustic model. This paper proposes an unsupervised maximum-likelihood gradient-ascent training algorithm (instead of the usual least squares regression) for a neural feature adaptation module, properly combined with a hybrid connectionist/HMM speech recognizer. The algorithm is inspired by the so-called &ldquo;inversion principle&rdquo;, that prescribes the optimization of the input features instead of the model parameters. Simulation results on a real-world speaker-independent continuous speech corpus of connected Italian digits, corrupted by noise, validate the approach. A small neural net (13 hidden neurons) trained over a single adaptation utterance for one iteration yields a 18.79% relative word error rate (WER) reduction over the bare hybrid, and a 65.10% relative WER reduction over the Gaussian-based HMM.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART30128358&target=NART&cn=NART30128358",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer <P><B>Abstract</B></P><P>Spoken human&ndash;machine interaction in real-world environments requires acoustic models that are robust to changes in acoustic conditions, e.g. presence of noise. Unfortunately, the popular hidden Markov models (HMM) are not noise tolerant. One way to increase recognition performance is to acquire a small adaptation set of noisy utterances, which is used to estimate a normalization mapping between noisy and clean features to be fed into the acoustic model. This paper proposes an unsupervised maximum-likelihood gradient-ascent training algorithm (instead of the usual least squares regression) for a neural feature adaptation module, properly combined with a hybrid connectionist/HMM speech recognizer. The algorithm is inspired by the so-called &ldquo;inversion principle&rdquo;, that prescribes the optimization of the input features instead of the model parameters. Simulation results on a real-world speaker-independent continuous speech corpus of connected Italian digits, corrupted by noise, validate the approach. A small neural net (13 hidden neurons) trained over a single adaptation utterance for one iteration yields a 18.79% relative word error rate (WER) reduction over the bare hybrid, and a 65.10% relative WER reduction over the Gaussian-based HMM.</P>"
        },
        {
          "rank": 13,
          "score": 0.759933590888977,
          "doc_id": "JAKO200011920774657",
          "title": "은닉 마코프 모델 기반 병렬음성인식 시스템",
          "abstract": "본 논문의 병렬음성인식 모델은 연속 은닉 마코프 모델(HMM; hidden Markov model)에 기반한 병렬 음소인식모듈과 계층구조의 지식베이스에 기반한 병렬 문장인식모듈로 구성된다. 병렬 음소인식 모듈은 수천개의 HMM을 병렬 프로세서에 분산시킨 수, 할당된 HMM에 대한 출력확률 계산과 Viterbi 알고리즘을 담당한다. 지식베이스 기반 병렬 문장인식모듈은 음소모듈에서 공급되는 음소열과 지안하는 병렬 음성인식 알고리즘은 분산메모리 MIMD 구조의 다중 트랜스퓨터와 Parsytec CC 상에 구현되었다. 실험결과, 병렬 음소인식모듈을 통한 실행시간 향상과 병렬 문장인식모듈을 통한 인식률 향상을 얻을 수 있었으며 병렬 음성인식 시스템의 실시간 구현 가능성을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200011920774657&target=NART&cn=JAKO200011920774657",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마코프 모델 기반 병렬음성인식 시스템 은닉 마코프 모델 기반 병렬음성인식 시스템 은닉 마코프 모델 기반 병렬음성인식 시스템 본 논문의 병렬음성인식 모델은 연속 은닉 마코프 모델(HMM; hidden Markov model)에 기반한 병렬 음소인식모듈과 계층구조의 지식베이스에 기반한 병렬 문장인식모듈로 구성된다. 병렬 음소인식 모듈은 수천개의 HMM을 병렬 프로세서에 분산시킨 수, 할당된 HMM에 대한 출력확률 계산과 Viterbi 알고리즘을 담당한다. 지식베이스 기반 병렬 문장인식모듈은 음소모듈에서 공급되는 음소열과 지안하는 병렬 음성인식 알고리즘은 분산메모리 MIMD 구조의 다중 트랜스퓨터와 Parsytec CC 상에 구현되었다. 실험결과, 병렬 음소인식모듈을 통한 실행시간 향상과 병렬 문장인식모듈을 통한 인식률 향상을 얻을 수 있었으며 병렬 음성인식 시스템의 실시간 구현 가능성을 확인하였다."
        },
        {
          "rank": 14,
          "score": 0.7592312097549438,
          "doc_id": "NART00705015",
          "title": "Speaker recognition using HMM composition in noisy environments",
          "abstract": "<P><B>Abstract</B></P><P>This paper investigates a speaker recognition method that is robust against background noise. In noisy environments, one important issue is how to create a model for each speaker so as to compensate for noise. The method described here is based on hidden Markov model (HMM) composition, which combines a speaker HMM and a noise-source HMM into a noise-added speaker HMM with a particular signal-to-noise ratio (SNR). Since it is difficult to measure the SNR of input speech with non-stationary noise exactly, this method creates several noise-added speaker HMMs with various SNRs. The HMM that has the highest likelihood value for the input speech is selected, and a speaker decision is made using this likelihood value. Experimental application of this method to text-independent speaker identification and verification in various kinds of noisy environments demonstrated considerable improvement in speaker recognition for speech utterances of male speakers.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART00705015&target=NART&cn=NART00705015",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speaker recognition using HMM composition in noisy environments Speaker recognition using HMM composition in noisy environments Speaker recognition using HMM composition in noisy environments <P><B>Abstract</B></P><P>This paper investigates a speaker recognition method that is robust against background noise. In noisy environments, one important issue is how to create a model for each speaker so as to compensate for noise. The method described here is based on hidden Markov model (HMM) composition, which combines a speaker HMM and a noise-source HMM into a noise-added speaker HMM with a particular signal-to-noise ratio (SNR). Since it is difficult to measure the SNR of input speech with non-stationary noise exactly, this method creates several noise-added speaker HMMs with various SNRs. The HMM that has the highest likelihood value for the input speech is selected, and a speaker decision is made using this likelihood value. Experimental application of this method to text-independent speaker identification and verification in various kinds of noisy environments demonstrated considerable improvement in speaker recognition for speech utterances of male speakers.</P>"
        },
        {
          "rank": 15,
          "score": 0.7579963207244873,
          "doc_id": "NPAP00072266",
          "title": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models",
          "abstract": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP00072266&target=NART&cn=NPAP00072266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error."
        },
        {
          "rank": 16,
          "score": 0.7571561336517334,
          "doc_id": "NART17510385",
          "title": "Hidden-articulator Markov models for speech recognition",
          "abstract": "<P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART17510385&target=NART&cn=NART17510385",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition <P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>"
        },
        {
          "rank": 17,
          "score": 0.7526035308837891,
          "doc_id": "NART95825020",
          "title": "End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition",
          "abstract": "<P><B>Abstract</B></P>  <P>In hidden Markov model (HMM) based automatic speech recognition (ASR) system, modeling the statistical relationship between the acoustic speech signal and the HMM states that represent linguistically motivated subword units such as phonemes is a crucial step. This is typically achieved by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then training a classifier such as artificial neural networks (ANN), Gaussian mixture model that estimates the emission probabilities of the HMM states. This paper investigates an end-to-end acoustic modeling approach using convolutional neural networks (CNNs), where the CNN takes as input raw speech signal and estimates the HMM states class conditional probabilities at the output. Alternately, as opposed to a divide and conquer strategy (i.e., separating feature extraction and statistical modeling steps), in the proposed acoustic modeling approach the relevant features and the classifier are jointly learned from the raw speech signal. Through ASR studies and analyses on multiple languages and multiple tasks, we show that: (a) the proposed approach yields consistently a better system with fewer parameters when compared to the conventional approach of cepstral feature extraction followed by ANN training, (b) unlike conventional method of speech processing, in the proposed approach the relevant feature representations are learned by first processing the input raw speech at the sub-segmental level ( &asymp; 2 ms). Specifically, through an analysis we show that the filters in the first convolution layer automatically learn &ldquo;in-parts&rdquo; formant-like information present in the sub-segmental speech, and (c) the intermediate feature representations obtained by subsequent filtering of the first convolution layer output are more discriminative compared to standard cepstral features and could be transferred across languages and domains.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Novel CNN-based end-to-end acoustic modeling approach is proposed. </LI> <LI>  Relevant features are automatically learned from the signal by discriminating phones. </LI> <LI>  Learned features are more discriminative than cepstral-based features. </LI> <LI>  Learned features are somewhat invariant to languages and domains. </LI> <LI>  Proposed approach leads to better ASR systems. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART95825020&target=NART&cn=NART95825020",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition <P><B>Abstract</B></P>  <P>In hidden Markov model (HMM) based automatic speech recognition (ASR) system, modeling the statistical relationship between the acoustic speech signal and the HMM states that represent linguistically motivated subword units such as phonemes is a crucial step. This is typically achieved by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then training a classifier such as artificial neural networks (ANN), Gaussian mixture model that estimates the emission probabilities of the HMM states. This paper investigates an end-to-end acoustic modeling approach using convolutional neural networks (CNNs), where the CNN takes as input raw speech signal and estimates the HMM states class conditional probabilities at the output. Alternately, as opposed to a divide and conquer strategy (i.e., separating feature extraction and statistical modeling steps), in the proposed acoustic modeling approach the relevant features and the classifier are jointly learned from the raw speech signal. Through ASR studies and analyses on multiple languages and multiple tasks, we show that: (a) the proposed approach yields consistently a better system with fewer parameters when compared to the conventional approach of cepstral feature extraction followed by ANN training, (b) unlike conventional method of speech processing, in the proposed approach the relevant feature representations are learned by first processing the input raw speech at the sub-segmental level ( &asymp; 2 ms). Specifically, through an analysis we show that the filters in the first convolution layer automatically learn &ldquo;in-parts&rdquo; formant-like information present in the sub-segmental speech, and (c) the intermediate feature representations obtained by subsequent filtering of the first convolution layer output are more discriminative compared to standard cepstral features and could be transferred across languages and domains.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Novel CNN-based end-to-end acoustic modeling approach is proposed. </LI> <LI>  Relevant features are automatically learned from the signal by discriminating phones. </LI> <LI>  Learned features are more discriminative than cepstral-based features. </LI> <LI>  Learned features are somewhat invariant to languages and domains. </LI> <LI>  Proposed approach leads to better ASR systems. </LI> </UL> </P>"
        },
        {
          "rank": 18,
          "score": 0.7486822605133057,
          "doc_id": "JAKO200311922043899",
          "title": "Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구",
          "abstract": "본 논문에서는 한국어 음성 데이터를 대상으로 HM-Net(Hidden Markov Network) 음성인식 시스템의 성능평가를 수행하였다. 음향모델 작성은 음성인식에서 널리 사용되고 있는 통계적인 모델링 방법인 HMM(Hidden Markov Model)을 개량한 HM-Net을 도입하였다. HM-Net은 기존의 SSS(Successive State Splitting) 알고리즘을 개량한 PDT(Phonetic Decision Tree)-SSS 알고리즘에 의해 문맥방향과 시간방향의 상태분할을 수행하여 생성되는데, 특히 문맥방향 상태분할의 경우 학습 음성데이터에 출현하지 않는 문맥정보를 효과적으로 표현하기 위해 음소결정트리를 채용하고 있으며, 시간방향 상태분할의 경우 학습 음성데이터에서 각 음소별 지속시간 정보를 효과적으로 표현하기 위한 상태분할을 수행하며, 마지막으로 파라미터의 공유를 통해 triphone 형태의 최적인 모델 네트워크를 작성하게 된다. 인식에 사용된 알고리즘은 음소 및 단어인식의 경우에는 One-Pass Viterbi 빔 탐색을 사용하며 트리 구조 형태의 사전과 phone/word-pair 문법을 채용하고 있다. 연속음성인식의 경우에는 단어 bigram과 단어 trigram 언어모델과 목구조 형태의 사전을 채용한 Multi-Pass 빔 탐색을 사용하고 있다. 전체적으로 본 논문에서는 다양한 조건에서 HM-Net 음성인식 시스템의 성능평가를 수행하였으며, 지금까지 소개된 음성인식 시스템과 비교하여 매우 우수한 인식성능을 보임을 실험을 통해 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200311922043899&target=NART&cn=JAKO200311922043899",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 본 논문에서는 한국어 음성 데이터를 대상으로 HM-Net(Hidden Markov Network) 음성인식 시스템의 성능평가를 수행하였다. 음향모델 작성은 음성인식에서 널리 사용되고 있는 통계적인 모델링 방법인 HMM(Hidden Markov Model)을 개량한 HM-Net을 도입하였다. HM-Net은 기존의 SSS(Successive State Splitting) 알고리즘을 개량한 PDT(Phonetic Decision Tree)-SSS 알고리즘에 의해 문맥방향과 시간방향의 상태분할을 수행하여 생성되는데, 특히 문맥방향 상태분할의 경우 학습 음성데이터에 출현하지 않는 문맥정보를 효과적으로 표현하기 위해 음소결정트리를 채용하고 있으며, 시간방향 상태분할의 경우 학습 음성데이터에서 각 음소별 지속시간 정보를 효과적으로 표현하기 위한 상태분할을 수행하며, 마지막으로 파라미터의 공유를 통해 triphone 형태의 최적인 모델 네트워크를 작성하게 된다. 인식에 사용된 알고리즘은 음소 및 단어인식의 경우에는 One-Pass Viterbi 빔 탐색을 사용하며 트리 구조 형태의 사전과 phone/word-pair 문법을 채용하고 있다. 연속음성인식의 경우에는 단어 bigram과 단어 trigram 언어모델과 목구조 형태의 사전을 채용한 Multi-Pass 빔 탐색을 사용하고 있다. 전체적으로 본 논문에서는 다양한 조건에서 HM-Net 음성인식 시스템의 성능평가를 수행하였으며, 지금까지 소개된 음성인식 시스템과 비교하여 매우 우수한 인식성능을 보임을 실험을 통해 확인할 수 있었다."
        },
        {
          "rank": 19,
          "score": 0.7483258247375488,
          "doc_id": "NART48832461",
          "title": "Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments",
          "abstract": "In this paper, we propose a model for the incorporation of voicing information into a speech recognition system in noisy environments. The employed voicing information is estimated by a novel method that can provide this information for each filter-bank channel and does not require information about the fundamental frequency. The voicing information is modelled by employing the Bernoulli distribution. The voicing model is obtained for each HMM state and mixture by a Viterbi-style training procedure. The proposed voicing incorporation is evaluated both within a standard model and two other models that had compensated for the noise effect, the missing-feature and the multi-conditional training model. Experiments are first performed on noisy speech data from the Aurora 2 database. Significant performance improvements are achieved when the voicing information is incorporated within the standard model as well as the noise-compensated models. The employment of voicing information is also demonstrated on a phoneme recognition task on the noise-corrupted TIMIT database and considerable improvements are observed.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART48832461&target=NART&cn=NART48832461",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments In this paper, we propose a model for the incorporation of voicing information into a speech recognition system in noisy environments. The employed voicing information is estimated by a novel method that can provide this information for each filter-bank channel and does not require information about the fundamental frequency. The voicing information is modelled by employing the Bernoulli distribution. The voicing model is obtained for each HMM state and mixture by a Viterbi-style training procedure. The proposed voicing incorporation is evaluated both within a standard model and two other models that had compensated for the noise effect, the missing-feature and the multi-conditional training model. Experiments are first performed on noisy speech data from the Aurora 2 database. Significant performance improvements are achieved when the voicing information is incorporated within the standard model as well as the noise-compensated models. The employment of voicing information is also demonstrated on a phoneme recognition task on the noise-corrupted TIMIT database and considerable improvements are observed."
        },
        {
          "rank": 20,
          "score": 0.7481551766395569,
          "doc_id": "JAKO201415642601987",
          "title": "SNR 매핑을 이용한 환경적응 기반 음성인식",
          "abstract": "다 모델 기반의 음성인식기는 음성인식에서 매우 성공적임이 알려져 있다. 그것은 다양한 신호-대-잡음비(SNR)와 잡음종류에 해당하는 다수의 HMM을 사용함으로서 선택된 음향모델이 인식잡음음성에 매우 근접한 일치성을 가질 수 있기 때문이다. 그러나 실제 사용시에 HMM의 개수가 제한됨에 따라서 음향모델의 불일치는 여전히 문제로 남아 있다. 본 논문에서는 인식잡음음성과 HMM 간의 SNR 불일치를 줄이고자 이들 간의 최적의 SNR 매핑 (mapping)을 실험적으로 결정하였다. 인식잡음음성으로 부터 추정된 SNR 값을 사용하는 대신 제안된 SNR 매핑을 사용함으로서 향상된 인식결과를 얻을 수 있었다. 다 모델 기반인식기에 제안된 방법을 적용하여 Aurora 2 데이터베이스에 대해서 인식 실험한 결과 기존의 MTR 이나 다 모델 기반 음성인식기에 비해서 6.3%와 9.4%의 상대적 단어 오인식율 감소를 이룰 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201415642601987&target=NART&cn=JAKO201415642601987",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SNR 매핑을 이용한 환경적응 기반 음성인식 SNR 매핑을 이용한 환경적응 기반 음성인식 SNR 매핑을 이용한 환경적응 기반 음성인식 다 모델 기반의 음성인식기는 음성인식에서 매우 성공적임이 알려져 있다. 그것은 다양한 신호-대-잡음비(SNR)와 잡음종류에 해당하는 다수의 HMM을 사용함으로서 선택된 음향모델이 인식잡음음성에 매우 근접한 일치성을 가질 수 있기 때문이다. 그러나 실제 사용시에 HMM의 개수가 제한됨에 따라서 음향모델의 불일치는 여전히 문제로 남아 있다. 본 논문에서는 인식잡음음성과 HMM 간의 SNR 불일치를 줄이고자 이들 간의 최적의 SNR 매핑 (mapping)을 실험적으로 결정하였다. 인식잡음음성으로 부터 추정된 SNR 값을 사용하는 대신 제안된 SNR 매핑을 사용함으로서 향상된 인식결과를 얻을 수 있었다. 다 모델 기반인식기에 제안된 방법을 적용하여 Aurora 2 데이터베이스에 대해서 인식 실험한 결과 기존의 MTR 이나 다 모델 기반 음성인식기에 비해서 6.3%와 9.4%의 상대적 단어 오인식율 감소를 이룰 수 있었다."
        },
        {
          "rank": 21,
          "score": 0.747666597366333,
          "doc_id": "NART13642943",
          "title": "Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments",
          "abstract": "<P>Handling background noise or echo (reverberation) etc. is very important for having an automated robot etc. recognize remote speech in a real environment. As effective schemes for handling this problem, noise reducing schemes such as model adaptation schemes including HMM decomposition and composition or microphone array (beamformer) signal processing, spectral subtraction, etc. have been proposed. In particular, a model adaptation scheme is very effective for speech recognition in a noisy environment and its recognition performance increases in proportion to the signal-to-noise ratio (SNR). In this paper, improving the recognition performance in a low-SNR environment by receiving speech at a high SNR using a microphone array before HMM decomposition and composition is attempted. The results of speech recognition experiments conducted in a noisy environment in an acoustic laboratory show an improvement in the recognition rate of about 25% by the proposed method for the case in which the SNR in a single microphone is 0 dB, as compared with the cases of using microphone array signal processing, HMM decomposition and composition alone. In addition, the proposed method shows recognition performance comparable to the case of using cepstrum mean normalization and spectral subtraction performed with an optimal coefficient given to the speech after microphone array processing. &copy; 2002 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 85(9): 13&ndash;22, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ecjb.10068</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART13642943&target=NART&cn=NART13642943",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments <P>Handling background noise or echo (reverberation) etc. is very important for having an automated robot etc. recognize remote speech in a real environment. As effective schemes for handling this problem, noise reducing schemes such as model adaptation schemes including HMM decomposition and composition or microphone array (beamformer) signal processing, spectral subtraction, etc. have been proposed. In particular, a model adaptation scheme is very effective for speech recognition in a noisy environment and its recognition performance increases in proportion to the signal-to-noise ratio (SNR). In this paper, improving the recognition performance in a low-SNR environment by receiving speech at a high SNR using a microphone array before HMM decomposition and composition is attempted. The results of speech recognition experiments conducted in a noisy environment in an acoustic laboratory show an improvement in the recognition rate of about 25% by the proposed method for the case in which the SNR in a single microphone is 0 dB, as compared with the cases of using microphone array signal processing, HMM decomposition and composition alone. In addition, the proposed method shows recognition performance comparable to the case of using cepstrum mean normalization and spectral subtraction performed with an optimal coefficient given to the speech after microphone array processing. &copy; 2002 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 85(9): 13&ndash;22, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ecjb.10068</P>"
        },
        {
          "rank": 22,
          "score": 0.7468360662460327,
          "doc_id": "NART13081886",
          "title": "Speech recognition under noisy environments using segmental unit input HMM",
          "abstract": "<P>In this paper, we apply a segmental unit input HMM to noisy speech recognition. In this modeling, several successive frames are combined and treated as an input vector. We expect that the segmental unit input HMM will be effective for noisy speech recognition because segmental statistics considering correlation between frames reduce noise effects when the correlation of noise between frames is assumed to be small. In recognition experiments, we compared the segmental unit input HMM with a conventional frame-based HMM and found the segmental unit input HMM to be superior. We also compared the segmental unit input HMM with dynamic cepstral coefficients, which have both static and dynamic features, and found that the segmental unit input HMM is more effective than the dynamic cepstrum. We also combined the segmental unit input HMM with a spectral subtraction method and confirmed the effectiveness of the method. Additionally, in experiments using acoustic models trained with noisy speech, the segmental unit input HMM outperformed the conventional HMM. From these results, we propose PMC for the segmental unit input HMM. Experimental results showed the PMC for segmental unit input HMM offered better recognition performance than the original PMC. &copy; 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(8): 111&ndash;120, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1151</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART13081886&target=NART&cn=NART13081886",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition under noisy environments using segmental unit input HMM Speech recognition under noisy environments using segmental unit input HMM Speech recognition under noisy environments using segmental unit input HMM <P>In this paper, we apply a segmental unit input HMM to noisy speech recognition. In this modeling, several successive frames are combined and treated as an input vector. We expect that the segmental unit input HMM will be effective for noisy speech recognition because segmental statistics considering correlation between frames reduce noise effects when the correlation of noise between frames is assumed to be small. In recognition experiments, we compared the segmental unit input HMM with a conventional frame-based HMM and found the segmental unit input HMM to be superior. We also compared the segmental unit input HMM with dynamic cepstral coefficients, which have both static and dynamic features, and found that the segmental unit input HMM is more effective than the dynamic cepstrum. We also combined the segmental unit input HMM with a spectral subtraction method and confirmed the effectiveness of the method. Additionally, in experiments using acoustic models trained with noisy speech, the segmental unit input HMM outperformed the conventional HMM. From these results, we propose PMC for the segmental unit input HMM. Experimental results showed the PMC for segmental unit input HMM offered better recognition performance than the original PMC. &copy; 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(8): 111&ndash;120, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1151</P>"
        },
        {
          "rank": 23,
          "score": 0.7420048713684082,
          "doc_id": "NART18015173",
          "title": "Speech recognition using hidden Markov models: A CMU perspective",
          "abstract": "Hidden Markov Models (HMMs) have become the predominant approach for speech recognition systems. One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU. In this paper, we introduce Hidden Markov Modelling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used in SPHINX.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART18015173&target=NART&cn=NART18015173",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition using hidden Markov models: A CMU perspective Speech recognition using hidden Markov models: A CMU perspective Speech recognition using hidden Markov models: A CMU perspective Hidden Markov Models (HMMs) have become the predominant approach for speech recognition systems. One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU. In this paper, we introduce Hidden Markov Modelling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used in SPHINX."
        },
        {
          "rank": 24,
          "score": 0.7409117221832275,
          "doc_id": "NART06155061",
          "title": "Speech enhancement based on neural predictive hidden Markov model",
          "abstract": "<P><B>Abstract</B></P><P>In this paper, we describe a new approach to speech enhancement by modeling directly the statistical characteristics of the speech waveform. To represent the nonlinear and nonstationary nature of speech, it is assumed that speech is the output of a neural predictive hidden Markov model (NPHMM). The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a Markov chain. Given some speech data, the parameter of NPHMM is estimated by a learning algorithm based on the combination of Baum&#x2013;Welch algorithm and a neural network learning algorithm using the well known back propagation technique. Given the parameters of NPHMM, a recursive estimation method using multiple Kalman filters, governed by a Markov state chain according to the transition probabilities is developed for enhancing speech signals degraded by statistically independent additive noise characteristics assumed to be white and Gaussian. Under various input signal-to-noise ratios (SNRs), the proposed recursive speech enhancement method achieves an improvement over the method based on hidden filter model (Lee and Shirai, 1996) of about 0.8&#x2013;1.2dB in terms of the measured output SNR.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART06155061&target=NART&cn=NART06155061",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech enhancement based on neural predictive hidden Markov model Speech enhancement based on neural predictive hidden Markov model Speech enhancement based on neural predictive hidden Markov model <P><B>Abstract</B></P><P>In this paper, we describe a new approach to speech enhancement by modeling directly the statistical characteristics of the speech waveform. To represent the nonlinear and nonstationary nature of speech, it is assumed that speech is the output of a neural predictive hidden Markov model (NPHMM). The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a Markov chain. Given some speech data, the parameter of NPHMM is estimated by a learning algorithm based on the combination of Baum&#x2013;Welch algorithm and a neural network learning algorithm using the well known back propagation technique. Given the parameters of NPHMM, a recursive estimation method using multiple Kalman filters, governed by a Markov state chain according to the transition probabilities is developed for enhancing speech signals degraded by statistically independent additive noise characteristics assumed to be white and Gaussian. Under various input signal-to-noise ratios (SNRs), the proposed recursive speech enhancement method achieves an improvement over the method based on hidden filter model (Lee and Shirai, 1996) of about 0.8&#x2013;1.2dB in terms of the measured output SNR.</P>"
        },
        {
          "rank": 25,
          "score": 0.735366940498352,
          "doc_id": "NART20042187",
          "title": "Neural networks with hidden Markov process",
          "abstract": "Observed data contain global structural information and local statistical fluctuation.  To catch both global and local information, neural network models with hidden Markov process are presented. Hidden Markov model(HMM) is a rather general framework which can effectively deal with sequential structure based on stochastic process theory.  Neural networks have strong learning ability which makes it easier to free statistical fluctuation.  By introducing the dependence of weights of neural networks on the states of the systems, neural networks combined with HMM are realized.  The process underlying the transitions of the states is a Markov process. Dependent observations are generated by the neural networks corresponding to the states.  A modified maximum likelihood procedure are employed to estimate both transition probability of Markov process and weights of neural networks.  Applications of neural networks with HMM to econometrics, pattern matching, speech recognition and image process are discussed.(Author abstract)",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20042187&target=NART&cn=NART20042187",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural networks with hidden Markov process Neural networks with hidden Markov process Neural networks with hidden Markov process Observed data contain global structural information and local statistical fluctuation.  To catch both global and local information, neural network models with hidden Markov process are presented. Hidden Markov model(HMM) is a rather general framework which can effectively deal with sequential structure based on stochastic process theory.  Neural networks have strong learning ability which makes it easier to free statistical fluctuation.  By introducing the dependence of weights of neural networks on the states of the systems, neural networks combined with HMM are realized.  The process underlying the transitions of the states is a Markov process. Dependent observations are generated by the neural networks corresponding to the states.  A modified maximum likelihood procedure are employed to estimate both transition probability of Markov process and weights of neural networks.  Applications of neural networks with HMM to econometrics, pattern matching, speech recognition and image process are discussed.(Author abstract)"
        },
        {
          "rank": 26,
          "score": 0.7350937724113464,
          "doc_id": "NART80772700",
          "title": "Noisy training for deep neural networks in speech recognition",
          "abstract": "<P><B>Abstract</B><P>Deep neural networks (DNNs) have gained remarkable success in speech recognition, partially attributed to the flexibility of DNN models in learning complex patterns of speech signals. This flexibility, however, may lead to serious over-fitting and hence miserable performance degradation in adverse acoustic conditions such as those with high ambient noises. We propose a noisy training approach to tackle this problem: by injecting moderate noises into the training data intentionally and randomly, more generalizable DNN models can be learned. This &lsquo;noise injection&rsquo; technique, although known to the neural computation community already, has not been studied with DNNs which involve a highly complex objective function. The experiments presented in this paper confirm that the noisy training approach works well for the DNN model and can provide substantial performance improvement for DNN-based speech recognition.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART80772700&target=NART&cn=NART80772700",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Noisy training for deep neural networks in speech recognition Noisy training for deep neural networks in speech recognition Noisy training for deep neural networks in speech recognition <P><B>Abstract</B><P>Deep neural networks (DNNs) have gained remarkable success in speech recognition, partially attributed to the flexibility of DNN models in learning complex patterns of speech signals. This flexibility, however, may lead to serious over-fitting and hence miserable performance degradation in adverse acoustic conditions such as those with high ambient noises. We propose a noisy training approach to tackle this problem: by injecting moderate noises into the training data intentionally and randomly, more generalizable DNN models can be learned. This &lsquo;noise injection&rsquo; technique, although known to the neural computation community already, has not been studied with DNNs which involve a highly complex objective function. The experiments presented in this paper confirm that the noisy training approach works well for the DNN model and can provide substantial performance improvement for DNN-based speech recognition.</P></P>"
        },
        {
          "rank": 27,
          "score": 0.7342357635498047,
          "doc_id": "JAKO200428635215914",
          "title": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구",
          "abstract": "본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200428635215914&target=NART&cn=JAKO200428635215914",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다."
        },
        {
          "rank": 28,
          "score": 0.7312272787094116,
          "doc_id": "JAKO200819858103682",
          "title": "A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments",
          "abstract": "In this paper, a MSS-NOVO method that combines the HMM composition method with a noise reduction method is proposed for speech recognition in noisy environments. This combined method starts with noise reduction with modified spectral subtraction (MSS) to enhance the input noisy speech, then the noise and voice composition (NOVO) method is applied for making noise adapted models by using the noise in the non-utterance regions of the enhanced noisy speech. In order to evaluate the effectiveness of our proposed method, we compare MSS-NOVO method with other methods, i.e., SS-NOVO, MWF-NOVO. To set up the noisy speech for test, we add White noise to KLE 452 database with different SNRs range from 0dB to 15dB, at 5dB intervals. From the tests, MSS-NOVO method shows average improvement of 66.5% and 13.6% compared with the existing SS-NOVO method and MWF-NOVO method, respectively. Especially our proposed MSS-NOVO method shows a big improvement at low SNRs.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200819858103682&target=NART&cn=JAKO200819858103682",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments In this paper, a MSS-NOVO method that combines the HMM composition method with a noise reduction method is proposed for speech recognition in noisy environments. This combined method starts with noise reduction with modified spectral subtraction (MSS) to enhance the input noisy speech, then the noise and voice composition (NOVO) method is applied for making noise adapted models by using the noise in the non-utterance regions of the enhanced noisy speech. In order to evaluate the effectiveness of our proposed method, we compare MSS-NOVO method with other methods, i.e., SS-NOVO, MWF-NOVO. To set up the noisy speech for test, we add White noise to KLE 452 database with different SNRs range from 0dB to 15dB, at 5dB intervals. From the tests, MSS-NOVO method shows average improvement of 66.5% and 13.6% compared with the existing SS-NOVO method and MWF-NOVO method, respectively. Especially our proposed MSS-NOVO method shows a big improvement at low SNRs."
        },
        {
          "rank": 29,
          "score": 0.7295683026313782,
          "doc_id": "JAKO200416642157049",
          "title": "Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식",
          "abstract": "In this paper, a new noise compensation method based on the eigenvoice framework in feature space is proposed to reduce the mismatch between training and testing environments. The difference between clean and noisy environments is represented by the linear combination of K eigenvectors that represent the variation among environments. In the proposed method, the performance improvement of speech recognition systems is largely affected by how to construct the noisy models and the bias vector set. In this paper, two methods, the one based on MAP adaptation method and the other using stereo DB, are proposed to construct the noisy models. In experiments using Aurora 2 DB, we obtained 44.86% relative improvement with eigen-environment method in comparison with baseline system. Especially, in clean condition training mode, our proposed method yielded 66.74% relative improvement, which is better performance than several methods previously proposed in Aurora project.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200416642157049&target=NART&cn=JAKO200416642157049",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식 Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식 Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식 In this paper, a new noise compensation method based on the eigenvoice framework in feature space is proposed to reduce the mismatch between training and testing environments. The difference between clean and noisy environments is represented by the linear combination of K eigenvectors that represent the variation among environments. In the proposed method, the performance improvement of speech recognition systems is largely affected by how to construct the noisy models and the bias vector set. In this paper, two methods, the one based on MAP adaptation method and the other using stereo DB, are proposed to construct the noisy models. In experiments using Aurora 2 DB, we obtained 44.86% relative improvement with eigen-environment method in comparison with baseline system. Especially, in clean condition training mode, our proposed method yielded 66.74% relative improvement, which is better performance than several methods previously proposed in Aurora project."
        },
        {
          "rank": 30,
          "score": 0.7285274267196655,
          "doc_id": "JAKO200211921444549",
          "title": "2층 구조의 입체 시각형 신경망 기반 음소인식",
          "abstract": "본 연구는 입체 시각을 위한 신경망에 대한 연구 결과로서 인간의 음성을 인식하는데 적용된다. 입체 시각신경망(SVNN)에 기반한 음성인식에서, 먼저 입력된 음성 신호를 표준 모델과 비교함으로써 유사성이 얻어진다. 이 값들은 다이나믹한 처리 과정으로 주어지고 이웃한 신경소자들 사이에서 경쟁적이고 협력적인 처리를 거치게 된다. 이러한 다이나믹한 처리과정을 통해 단 하나의 가장 우수한 신경세포(winner neuron)만이 최후에 검출된다. 비교연구에서 2층 구조의 SVNN은 HMM 인식기보다 인식정확도 측면에서 7.7% 더 높았다. 평가 결과. SVNN은 기손리 HMM 인식기 성능을 능가하는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200211921444549&target=NART&cn=JAKO200211921444549",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "2층 구조의 입체 시각형 신경망 기반 음소인식 2층 구조의 입체 시각형 신경망 기반 음소인식 2층 구조의 입체 시각형 신경망 기반 음소인식 본 연구는 입체 시각을 위한 신경망에 대한 연구 결과로서 인간의 음성을 인식하는데 적용된다. 입체 시각신경망(SVNN)에 기반한 음성인식에서, 먼저 입력된 음성 신호를 표준 모델과 비교함으로써 유사성이 얻어진다. 이 값들은 다이나믹한 처리 과정으로 주어지고 이웃한 신경소자들 사이에서 경쟁적이고 협력적인 처리를 거치게 된다. 이러한 다이나믹한 처리과정을 통해 단 하나의 가장 우수한 신경세포(winner neuron)만이 최후에 검출된다. 비교연구에서 2층 구조의 SVNN은 HMM 인식기보다 인식정확도 측면에서 7.7% 더 높았다. 평가 결과. SVNN은 기손리 HMM 인식기 성능을 능가하는 것으로 나타났다."
        },
        {
          "rank": 31,
          "score": 0.728499710559845,
          "doc_id": "NART20482871",
          "title": "Piecewise-linear transformation-based HMM adaptation for noisy speech",
          "abstract": "<P><B>Abstract</B></P><P>This paper proposes a new method using piecewise-linear transformation for adapting phone HMMs to noisy speech. Various noises are clustered according to their spectral property, and a noisy speech HMM corresponding to each clustered noise and SNR condition is made. Based on the likelihood maximization criterion, an HMM that best matches an input noisy speech is selected and further adapted using linear transformation. The proposed method is evaluated by its ability to recognize noisy broadcast-news speech. It is confirmed that the proposed method is effective in recognizing numerically noise-added speech and actual noisy speech under various noise conditions. The proposed method minimizes mismatches between noisy input speech and the HMM&#x2019;s, sentence by sentence, without requiring online noise spectrum/model estimation. The proposed method is therefore easily applicable to real world conditions with frequently changing noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20482871&target=NART&cn=NART20482871",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Piecewise-linear transformation-based HMM adaptation for noisy speech Piecewise-linear transformation-based HMM adaptation for noisy speech Piecewise-linear transformation-based HMM adaptation for noisy speech <P><B>Abstract</B></P><P>This paper proposes a new method using piecewise-linear transformation for adapting phone HMMs to noisy speech. Various noises are clustered according to their spectral property, and a noisy speech HMM corresponding to each clustered noise and SNR condition is made. Based on the likelihood maximization criterion, an HMM that best matches an input noisy speech is selected and further adapted using linear transformation. The proposed method is evaluated by its ability to recognize noisy broadcast-news speech. It is confirmed that the proposed method is effective in recognizing numerically noise-added speech and actual noisy speech under various noise conditions. The proposed method minimizes mismatches between noisy input speech and the HMM&#x2019;s, sentence by sentence, without requiring online noise spectrum/model estimation. The proposed method is therefore easily applicable to real world conditions with frequently changing noise.</P>"
        },
        {
          "rank": 32,
          "score": 0.7252019643783569,
          "doc_id": "JAKO202029462558904",
          "title": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식",
          "abstract": "특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202029462558904&target=NART&cn=JAKO202029462558904",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다."
        },
        {
          "rank": 33,
          "score": 0.7240059971809387,
          "doc_id": "NPAP07942137",
          "title": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP07942137&target=NART&cn=NPAP07942137",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 34,
          "score": 0.7238528728485107,
          "doc_id": "JAKO201115537947340",
          "title": "멀티밴드 스펙트럼 차감법과 엔트로피 하모닉을 이용한 잡음환경에 강인한 분산음성인식",
          "abstract": "음성인식의 실용화에 가장 저해되는 요소는 배경잡음과 채널에 의한 왜곡이다. 일반적으로 잡음은 음성인식 시스템의 성능을 저하시키고 이로 인해 사용 장소의 제약을 많이 받고 있다. DSR(Distributed Speech Recognition) 기반의 음성인식 역시 이 같은 문제로 성능 향상에 어려움을 겪고 있다. 이 논문은 잡음환경에서 DSR기반의 음성인식률 향상을 위해 정확한 음성구간을 검출하고, 잡음을 제거하여 잡음에 강인한 특징추출을 하도록 설계하였다. 제안된 방법은 엔트로피와 음성의 하모닉을 이용해 음성구간을 검출하며 멀티밴드 스펙트럼 차감법을 이용하여 잡음을 제거한다. 음성의 스펙트럼 에너지에 대한 엔트로피를 사용하여 음성검출을 하게 되면 비교적 높은 SNR 환경 (SNR 15dB) 에서는 성능이 우수하나 잡음환경의 변화에 따라 음성과 비음성의 문턱 값이 변화하여 낮은 SNR환경(SNR 0dB)에시는 정확한 음성 검출이 어렵다. 이 논문은 낮은 SNR 환경(0dB)에서도 정확한 음성을 검출할 수 있도록 음성의 스펙트럴 엔트로피와 하모닉 성분을 이용하였으며 정확한 음성 구간 검출에 따라 잡음을 제거하여 잡음에 강인한 특정을 추출하도록 하였다. 실험결과 잡음환경에 따른 인식조건에서 개선된 인식성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201115537947340&target=NART&cn=JAKO201115537947340",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "멀티밴드 스펙트럼 차감법과 엔트로피 하모닉을 이용한 잡음환경에 강인한 분산음성인식 멀티밴드 스펙트럼 차감법과 엔트로피 하모닉을 이용한 잡음환경에 강인한 분산음성인식 멀티밴드 스펙트럼 차감법과 엔트로피 하모닉을 이용한 잡음환경에 강인한 분산음성인식 음성인식의 실용화에 가장 저해되는 요소는 배경잡음과 채널에 의한 왜곡이다. 일반적으로 잡음은 음성인식 시스템의 성능을 저하시키고 이로 인해 사용 장소의 제약을 많이 받고 있다. DSR(Distributed Speech Recognition) 기반의 음성인식 역시 이 같은 문제로 성능 향상에 어려움을 겪고 있다. 이 논문은 잡음환경에서 DSR기반의 음성인식률 향상을 위해 정확한 음성구간을 검출하고, 잡음을 제거하여 잡음에 강인한 특징추출을 하도록 설계하였다. 제안된 방법은 엔트로피와 음성의 하모닉을 이용해 음성구간을 검출하며 멀티밴드 스펙트럼 차감법을 이용하여 잡음을 제거한다. 음성의 스펙트럼 에너지에 대한 엔트로피를 사용하여 음성검출을 하게 되면 비교적 높은 SNR 환경 (SNR 15dB) 에서는 성능이 우수하나 잡음환경의 변화에 따라 음성과 비음성의 문턱 값이 변화하여 낮은 SNR환경(SNR 0dB)에시는 정확한 음성 검출이 어렵다. 이 논문은 낮은 SNR 환경(0dB)에서도 정확한 음성을 검출할 수 있도록 음성의 스펙트럴 엔트로피와 하모닉 성분을 이용하였으며 정확한 음성 구간 검출에 따라 잡음을 제거하여 잡음에 강인한 특정을 추출하도록 하였다. 실험결과 잡음환경에 따른 인식조건에서 개선된 인식성능을 보였다."
        },
        {
          "rank": 35,
          "score": 0.7214258909225464,
          "doc_id": "JAKO201630932328344",
          "title": "가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법",
          "abstract": "실세계 환경의 원거리에서 녹음된 음성은 가산 잡음이나 반향 성분으로 왜곡되기 때문에 음성인식 성능이 현저히 떨어진다. 따라서 음성 전처리 과정은 실세계 환경에서 강인한 음성인식을 위한 필수과정이다. 모델 기반 특징 향상 방법은 전처리 방법 중 하나로 특징 영역 데이터의 적절한 동적 범위(dynamic range)와 차원 수로 인하여 실시간 처리가 가능하고 깨끗한 음성의 선험적 정보를 모델링하기에 용이하다. 또, 인식을 위한 최종 특징 입력에 가까운 단계에서 데이터를 처리하므로 인식에 밀접한 영향을 준다는 장점이 있다. 그러나 대략적인 왜곡 요인 관련 파라미터 추정 때문에 음성인식 성능이 하락되는 단점이 있다. 최근에 기존 모델 기반 특징 향상의 단점을 개선하여 가산 잡음이나 반향 환경에 적합한 방법이 제안되었다. 이글에서는 특징 향상 방법을 소개하고 개선된 방법의 음성인식 강인성을 알아보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201630932328344&target=NART&cn=JAKO201630932328344",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법 가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법 가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법 실세계 환경의 원거리에서 녹음된 음성은 가산 잡음이나 반향 성분으로 왜곡되기 때문에 음성인식 성능이 현저히 떨어진다. 따라서 음성 전처리 과정은 실세계 환경에서 강인한 음성인식을 위한 필수과정이다. 모델 기반 특징 향상 방법은 전처리 방법 중 하나로 특징 영역 데이터의 적절한 동적 범위(dynamic range)와 차원 수로 인하여 실시간 처리가 가능하고 깨끗한 음성의 선험적 정보를 모델링하기에 용이하다. 또, 인식을 위한 최종 특징 입력에 가까운 단계에서 데이터를 처리하므로 인식에 밀접한 영향을 준다는 장점이 있다. 그러나 대략적인 왜곡 요인 관련 파라미터 추정 때문에 음성인식 성능이 하락되는 단점이 있다. 최근에 기존 모델 기반 특징 향상의 단점을 개선하여 가산 잡음이나 반향 환경에 적합한 방법이 제안되었다. 이글에서는 특징 향상 방법을 소개하고 개선된 방법의 음성인식 강인성을 알아보고자 한다."
        },
        {
          "rank": 36,
          "score": 0.7138190269470215,
          "doc_id": "NART17510378",
          "title": "A spatio-temporal speech enhancement scheme for robust speech recognition in noisy environments",
          "abstract": "<P><B>Abstract</B></P><P>A new speech enhancement scheme is presented integrating spatial and temporal signal processing methods for robust speech recognition in noisy environments. The scheme first separates spatially localized point sources from noisy speech signals recorded by two microphones. Blind source separation algorithms assuming no a priori knowledge about the sources involved are applied in this spatial processing stage. Then denoising of distributed background noise is achieved in a combined spatial/temporal processing approach. The desired speaker signal is first processed along with an artificially constructed noise signal in a supplementary blind source separation step. It is further denoised by exploiting differences in temporal speech and noise statistics in a wavelet filterbank. The scheme&#x2019;s performance is illustrated by speech recognition experiments on real recordings in a noisy car environment. In comparison to a common multi-microphone technique like beamforming with spectral subtraction, the scheme is shown to enable more accurate speech recognition in the presence of a highly interfering point source and strong background noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART17510378&target=NART&cn=NART17510378",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A spatio-temporal speech enhancement scheme for robust speech recognition in noisy environments A spatio-temporal speech enhancement scheme for robust speech recognition in noisy environments A spatio-temporal speech enhancement scheme for robust speech recognition in noisy environments <P><B>Abstract</B></P><P>A new speech enhancement scheme is presented integrating spatial and temporal signal processing methods for robust speech recognition in noisy environments. The scheme first separates spatially localized point sources from noisy speech signals recorded by two microphones. Blind source separation algorithms assuming no a priori knowledge about the sources involved are applied in this spatial processing stage. Then denoising of distributed background noise is achieved in a combined spatial/temporal processing approach. The desired speaker signal is first processed along with an artificially constructed noise signal in a supplementary blind source separation step. It is further denoised by exploiting differences in temporal speech and noise statistics in a wavelet filterbank. The scheme&#x2019;s performance is illustrated by speech recognition experiments on real recordings in a noisy car environment. In comparison to a common multi-microphone technique like beamforming with spectral subtraction, the scheme is shown to enable more accurate speech recognition in the presence of a highly interfering point source and strong background noise.</P>"
        },
        {
          "rank": 37,
          "score": 0.7125575542449951,
          "doc_id": "JAKO200606140790765",
          "title": "Discrimination of Pathological Speech Using Hidden Markov Models",
          "abstract": "Diagnosis of pathological voice is one of the important issues in biomedical applications of speech technology. This study focuses on the discrimination of voice disorder using HMM (Hidden Markov Model) for automatic detection between normal voice and vocal fold disorder voice. This is a non-intrusive, non-expensive and fully automated method using only a speech sample of the subject. Speech data from normal people and patients were collected. Mel-frequency filter cepstral coefficients (MFCCs) were modeled by HMM classifier. Different states (3 states, 5 states and 7 states), 3 mixtures and left to right HMMs were formed. This method gives an accuracy of 93.8% for train data and 91.7% for test data in the discrimination of normal and vocal fold disorder voice for sustained /a/.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200606140790765&target=NART&cn=JAKO200606140790765",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Discrimination of Pathological Speech Using Hidden Markov Models Discrimination of Pathological Speech Using Hidden Markov Models Discrimination of Pathological Speech Using Hidden Markov Models Diagnosis of pathological voice is one of the important issues in biomedical applications of speech technology. This study focuses on the discrimination of voice disorder using HMM (Hidden Markov Model) for automatic detection between normal voice and vocal fold disorder voice. This is a non-intrusive, non-expensive and fully automated method using only a speech sample of the subject. Speech data from normal people and patients were collected. Mel-frequency filter cepstral coefficients (MFCCs) were modeled by HMM classifier. Different states (3 states, 5 states and 7 states), 3 mixtures and left to right HMMs were formed. This method gives an accuracy of 93.8% for train data and 91.7% for test data in the discrimination of normal and vocal fold disorder voice for sustained /a/."
        },
        {
          "rank": 38,
          "score": 0.7101362943649292,
          "doc_id": "JAKO200727500236879",
          "title": "자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식",
          "abstract": "In this paper, we carried out recognition experiments for noisy speech having various levels of car noise and output of an audio system using the speech interface. The speech interface consists of three parts: pre-processing, acoustic echo canceller, post-processing. First, a high pass filter is employed as a pre-processing part to remove some engine noises. Then, an echo canceller implemented by using an FIR-type filter with an NLMS adaptive algorithm is used to remove the music or speech coming from the audio system in a car. As a last part, the MMSE-STSA based speech enhancement method is applied to the out of the echo canceller to remove the residual noise further. For recognition experiments, we generated test signals by adding music to the car noisy speech from Aurora 2 database. The HTK-based continuous HMM system is constructed for a recognition system. Experimental results show that the proposed speech interface is very promising for robust speech recognition in a noisy car environment.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200727500236879&target=NART&cn=JAKO200727500236879",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 In this paper, we carried out recognition experiments for noisy speech having various levels of car noise and output of an audio system using the speech interface. The speech interface consists of three parts: pre-processing, acoustic echo canceller, post-processing. First, a high pass filter is employed as a pre-processing part to remove some engine noises. Then, an echo canceller implemented by using an FIR-type filter with an NLMS adaptive algorithm is used to remove the music or speech coming from the audio system in a car. As a last part, the MMSE-STSA based speech enhancement method is applied to the out of the echo canceller to remove the residual noise further. For recognition experiments, we generated test signals by adding music to the car noisy speech from Aurora 2 database. The HTK-based continuous HMM system is constructed for a recognition system. Experimental results show that the proposed speech interface is very promising for robust speech recognition in a noisy car environment."
        },
        {
          "rank": 39,
          "score": 0.7078783512115479,
          "doc_id": "NART06964546",
          "title": "Low resolution, degraded document recognition using neural networks and hidden Markov models",
          "abstract": "<P><B>Abstract</B></P><P>We collected a large, real world database, containing degraded, old and faxed documents and present a comparison between two leading edge commercial software packages and human reading performance which shows quantitatively the huge performance gap between humans and machines, even on random character documents where no context can be used. This indicates room for possible improvements. We implemented an integrated segmentation and recognition algorithm using neural networks and hidden Markov models trained on the database and present results which show the superior performance of the algorithm.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART06964546&target=NART&cn=NART06964546",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Low resolution, degraded document recognition using neural networks and hidden Markov models Low resolution, degraded document recognition using neural networks and hidden Markov models Low resolution, degraded document recognition using neural networks and hidden Markov models <P><B>Abstract</B></P><P>We collected a large, real world database, containing degraded, old and faxed documents and present a comparison between two leading edge commercial software packages and human reading performance which shows quantitatively the huge performance gap between humans and machines, even on random character documents where no context can be used. This indicates room for possible improvements. We implemented an integrated segmentation and recognition algorithm using neural networks and hidden Markov models trained on the database and present results which show the superior performance of the algorithm.</P>"
        },
        {
          "rank": 40,
          "score": 0.7076500058174133,
          "doc_id": "JAKO200111921140843",
          "title": "회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM (Hidden Markov Model)의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경 망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용 데이터에 대하여 Elman망 예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 98.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200111921140843&target=NART&cn=JAKO200111921140843",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM (Hidden Markov Model)의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경 망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용 데이터에 대하여 Elman망 예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 98.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 41,
          "score": 0.7045892477035522,
          "doc_id": "JAKO201935164467523",
          "title": "심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구",
          "abstract": "본 논문에서는 구개인두부전증(VeloPharyngeal Insufficiency, VPI) 환자의 음성을 효과적으로 인식하기 위해 컨볼루션 신경망 (Convolutional Neural Network, CNN), 장단기 모델(Long Short Term Memory, LSTM) 구조 신경망을 은닉 마르코프 모델(Hidden Markov Model, HMM)과 결합한 하이브리드 구조의 음성 인식 시스템을 구축하고 모델 적응 기법을 적용하여, 기존 Gaussian Mixture Model(GMM-HMM), 완전 연결형 Deep Neural Network(DNN-HMM) 기반의 음성 인식 시스템과 성능을 비교한다. 정상인 화자가 PBW452단어를 발화한 데이터를 이용하여 초기 모델을 학습하고 정상인 화자의 VPI 모의 음성을 이용하여 화자 적응의 사전 모델을 생성한 후에 VPI 환자들의 음성으로 추가 적응 학습을 진행한다. VPI환자의 화자 적응 시에 CNN-HMM 기반 모델에서는 일부층만 적응 학습하고, LSTM-HMM 기반 모델의 경우에는 드롭 아웃 규제기법을 적용하여 성능을 관찰한 결과 기존 완전 연결형 DNN-HMM 인식기보다 3.68 % 향상된 음성 인식 성능을 나타낸다. 이러한 결과는 본 논문에서 제안하는 LSTM-HMM 기반의 하이브리드 음성 인식 기법이 많은 데이터를 확보하기 어려운 VPI 환자 음성에 대해 보다 향상된 인식률의 음성 인식 시스템을 구축하는데 효과적임을 입증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201935164467523&target=NART&cn=JAKO201935164467523",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 본 논문에서는 구개인두부전증(VeloPharyngeal Insufficiency, VPI) 환자의 음성을 효과적으로 인식하기 위해 컨볼루션 신경망 (Convolutional Neural Network, CNN), 장단기 모델(Long Short Term Memory, LSTM) 구조 신경망을 은닉 마르코프 모델(Hidden Markov Model, HMM)과 결합한 하이브리드 구조의 음성 인식 시스템을 구축하고 모델 적응 기법을 적용하여, 기존 Gaussian Mixture Model(GMM-HMM), 완전 연결형 Deep Neural Network(DNN-HMM) 기반의 음성 인식 시스템과 성능을 비교한다. 정상인 화자가 PBW452단어를 발화한 데이터를 이용하여 초기 모델을 학습하고 정상인 화자의 VPI 모의 음성을 이용하여 화자 적응의 사전 모델을 생성한 후에 VPI 환자들의 음성으로 추가 적응 학습을 진행한다. VPI환자의 화자 적응 시에 CNN-HMM 기반 모델에서는 일부층만 적응 학습하고, LSTM-HMM 기반 모델의 경우에는 드롭 아웃 규제기법을 적용하여 성능을 관찰한 결과 기존 완전 연결형 DNN-HMM 인식기보다 3.68 % 향상된 음성 인식 성능을 나타낸다. 이러한 결과는 본 논문에서 제안하는 LSTM-HMM 기반의 하이브리드 음성 인식 기법이 많은 데이터를 확보하기 어려운 VPI 환자 음성에 대해 보다 향상된 인식률의 음성 인식 시스템을 구축하는데 효과적임을 입증한다."
        },
        {
          "rank": 42,
          "score": 0.7021834850311279,
          "doc_id": "JAKO201911338887557",
          "title": "잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법",
          "abstract": "본 논문에서는 잡음 환경에서 효과적인 음성 인식을 위하여 DNN(Deep Neural Network) 기반의 잡음 오염 함수 예측을 이용한 음향 모델 적응 기법을 제안한다. 깨끗한 음성과 잡음 정보를 입력으로 하고 오염된 음성에 대한 특징 벡터를 출력으로 하는 DNN을 학습하여 비선형 관계를 갖는 잡음 오염 함수를 예측한다. 예측된 잡음 오염 함수를 음향모델의 평균 벡터에 적용하여 잡음 환경에 적응된 음향 모델을 생성한다. Aurora 2.0 데이터를 이용한 음성 인식 성능 평가에서 본 논문에서 제안한 모델 적응 기법이 기존의 전처리, 모델 적응 기법에 비해 일치, 불일치 잡음 환경에서 모두 평균적으로 우수한 성능을 나타낸다. 특히 불일치 잡음 환경에서 평균 오류율이 15.87 %의 상대 향상률을 나타낸다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201911338887557&target=NART&cn=JAKO201911338887557",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 본 논문에서는 잡음 환경에서 효과적인 음성 인식을 위하여 DNN(Deep Neural Network) 기반의 잡음 오염 함수 예측을 이용한 음향 모델 적응 기법을 제안한다. 깨끗한 음성과 잡음 정보를 입력으로 하고 오염된 음성에 대한 특징 벡터를 출력으로 하는 DNN을 학습하여 비선형 관계를 갖는 잡음 오염 함수를 예측한다. 예측된 잡음 오염 함수를 음향모델의 평균 벡터에 적용하여 잡음 환경에 적응된 음향 모델을 생성한다. Aurora 2.0 데이터를 이용한 음성 인식 성능 평가에서 본 논문에서 제안한 모델 적응 기법이 기존의 전처리, 모델 적응 기법에 비해 일치, 불일치 잡음 환경에서 모두 평균적으로 우수한 성능을 나타낸다. 특히 불일치 잡음 환경에서 평균 오류율이 15.87 %의 상대 향상률을 나타낸다."
        },
        {
          "rank": 43,
          "score": 0.7013199329376221,
          "doc_id": "JAKO201016450104831",
          "title": "잡음환경에서 음성인식 성능향상을 위한 바이너리 마스크를 이용한 스펙트럼 향상 방법",
          "abstract": "음성인식의 실용화에 가장 저해되는 요소는 배경잡음과 채널잡음에 의한 왜곡이다. 일반적으로 배경잡음은 음성인식 시스템의 성능을 저하시키고 이로 인해 사용 장소의 제약을 받게 한다. DSR (Distributed Speech Recognition) 기반의 음성인식 역시 이와 같은 문제로 성능 향상에 어려움을 겪고 있다. 이러한 문제를 해결하기 위해 다양한 잡음제거 알고리듬이 사용되고 있으나 낮은 SNR환경에서 부정확한 잡음추정으로 발생하는 스펙트럼 손상과 잔존 잡음은 음성인식기의 인식환경과 학습 환경의 불일치를 만들게 되어 인식률을 저하시키는 원인이 된다. 본 논문에서는 이와 같은 문제를 해결하기 위해 잡음제거 알고리듬으로 MMSE-STSA 방법을 사용하였고 손상된 스펙트럼을 보상하기 위해 Ideal Binary Mask를 이용하였다. 잡음환경 (SNR 15 ~ 0 dB)에 따른 실험결과 제안된 방법을 사용했을 때 향상된 스펙트럼을 얻을 수 있었고 향상된 인식성능을 확인했다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201016450104831&target=NART&cn=JAKO201016450104831",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "잡음환경에서 음성인식 성능향상을 위한 바이너리 마스크를 이용한 스펙트럼 향상 방법 잡음환경에서 음성인식 성능향상을 위한 바이너리 마스크를 이용한 스펙트럼 향상 방법 잡음환경에서 음성인식 성능향상을 위한 바이너리 마스크를 이용한 스펙트럼 향상 방법 음성인식의 실용화에 가장 저해되는 요소는 배경잡음과 채널잡음에 의한 왜곡이다. 일반적으로 배경잡음은 음성인식 시스템의 성능을 저하시키고 이로 인해 사용 장소의 제약을 받게 한다. DSR (Distributed Speech Recognition) 기반의 음성인식 역시 이와 같은 문제로 성능 향상에 어려움을 겪고 있다. 이러한 문제를 해결하기 위해 다양한 잡음제거 알고리듬이 사용되고 있으나 낮은 SNR환경에서 부정확한 잡음추정으로 발생하는 스펙트럼 손상과 잔존 잡음은 음성인식기의 인식환경과 학습 환경의 불일치를 만들게 되어 인식률을 저하시키는 원인이 된다. 본 논문에서는 이와 같은 문제를 해결하기 위해 잡음제거 알고리듬으로 MMSE-STSA 방법을 사용하였고 손상된 스펙트럼을 보상하기 위해 Ideal Binary Mask를 이용하였다. 잡음환경 (SNR 15 ~ 0 dB)에 따른 실험결과 제안된 방법을 사용했을 때 향상된 스펙트럼을 얻을 수 있었고 향상된 인식성능을 확인했다."
        },
        {
          "rank": 44,
          "score": 0.7007626891136169,
          "doc_id": "JAKO201120661418238",
          "title": "음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거",
          "abstract": "본 논문에서는 먼저 신경회로망의 학습에 오차역전파 학습 알고리즘을 사용하여 각 프레임에서의 음성 및 잡음 구간의 검출에 의한 음성인식 알고리즘을 제안한다. 그리고 신경회로망에 의하여 음성 및 잡음 구간의 검출에 따라서 각 프레임에서 잡음을 제거하는 스펙트럼 차감법을 제안한다. 본 실험에서는 제안한 음성인식알고리즘의 성능을 원음성에 백색잡음 및 자동차 잡음을 부가하여 인식율을 평가한다. 또한 인식시스템에 의하여 검출된 음성 및 잡음 구간을 이용하여 각 프레임에서의 스펙트럼 차감법에 의한 잡음제거의 실험결과를 나타낸다. 잡음에 의하여 오염된 음성에 대하여 신호대잡음비를 사용하여 본 알고리즘이 유효하다는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201120661418238&target=NART&cn=JAKO201120661418238",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 본 논문에서는 먼저 신경회로망의 학습에 오차역전파 학습 알고리즘을 사용하여 각 프레임에서의 음성 및 잡음 구간의 검출에 의한 음성인식 알고리즘을 제안한다. 그리고 신경회로망에 의하여 음성 및 잡음 구간의 검출에 따라서 각 프레임에서 잡음을 제거하는 스펙트럼 차감법을 제안한다. 본 실험에서는 제안한 음성인식알고리즘의 성능을 원음성에 백색잡음 및 자동차 잡음을 부가하여 인식율을 평가한다. 또한 인식시스템에 의하여 검출된 음성 및 잡음 구간을 이용하여 각 프레임에서의 스펙트럼 차감법에 의한 잡음제거의 실험결과를 나타낸다. 잡음에 의하여 오염된 음성에 대하여 신호대잡음비를 사용하여 본 알고리즘이 유효하다는 것을 확인한다."
        },
        {
          "rank": 45,
          "score": 0.7000831365585327,
          "doc_id": "JAKO201719951669089",
          "title": "원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링",
          "abstract": "This paper proposes a new method to train Deep Neural Network (DNN)-based acoustic models for speech recognition of native and foreign speakers. The proposed method consists of determining multi-set state clusters with various acoustic properties, training a DNN-based acoustic model, and recognizing speech based on the model. In the proposed method, hidden nodes of DNN are shared, but output nodes are separated to accommodate different acoustic properties for native and foreign speech. In an English speech recognition task for speakers of Korean and English respectively, the proposed method is shown to slightly improve recognition accuracy compared to the conventional multi-condition training method.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201719951669089&target=NART&cn=JAKO201719951669089",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 This paper proposes a new method to train Deep Neural Network (DNN)-based acoustic models for speech recognition of native and foreign speakers. The proposed method consists of determining multi-set state clusters with various acoustic properties, training a DNN-based acoustic model, and recognizing speech based on the model. In the proposed method, hidden nodes of DNN are shared, but output nodes are separated to accommodate different acoustic properties for native and foreign speech. In an English speech recognition task for speakers of Korean and English respectively, the proposed method is shown to slightly improve recognition accuracy compared to the conventional multi-condition training method."
        },
        {
          "rank": 46,
          "score": 0.6968162059783936,
          "doc_id": "JAKO199811921284763",
          "title": "은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식",
          "abstract": "한국어 연속 음성에서 발생하는 조음결합문제를 해결하기 위하여 단어를 기본 인식 단위로 사용할 경우 각 단어의 효율적인 표현 방법, 연속된 단어로 이루어진 여러 문장의 표현 방법 그리고 입력된 연속음성을 연속된 여러 단어로의 정합 방법에 관한 연구가 선행되어야 한다. 본 논문에서는 은닉 마르코프 모델과 레벨빌딩 알고리즘을 이용한 한국어 연속 음성 인식 시스템을 제안한다. 각 단어는 은닉 마르코프 모델로 표현하고 문장을 표현하기 위하여 단어 모델을 연결한 형태인 인식 네트워크를 구성한다. 인식네트워크의 탐색 알고리즘으로는 레벨 빌딩 알고리즘을 사용한다. 제안한 방법은 항공기 예약 시스템에 적용한 실험에서 인식율과 인식속도면에서 실용적이었으며 또한 비교적 적은 저장공간으로 전체 문장을 표현하고 쉽게 확장할 수 있다는 장점을 가지고 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199811921284763&target=NART&cn=JAKO199811921284763",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식 은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식 은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식 한국어 연속 음성에서 발생하는 조음결합문제를 해결하기 위하여 단어를 기본 인식 단위로 사용할 경우 각 단어의 효율적인 표현 방법, 연속된 단어로 이루어진 여러 문장의 표현 방법 그리고 입력된 연속음성을 연속된 여러 단어로의 정합 방법에 관한 연구가 선행되어야 한다. 본 논문에서는 은닉 마르코프 모델과 레벨빌딩 알고리즘을 이용한 한국어 연속 음성 인식 시스템을 제안한다. 각 단어는 은닉 마르코프 모델로 표현하고 문장을 표현하기 위하여 단어 모델을 연결한 형태인 인식 네트워크를 구성한다. 인식네트워크의 탐색 알고리즘으로는 레벨 빌딩 알고리즘을 사용한다. 제안한 방법은 항공기 예약 시스템에 적용한 실험에서 인식율과 인식속도면에서 실용적이었으며 또한 비교적 적은 저장공간으로 전체 문장을 표현하고 쉽게 확장할 수 있다는 장점을 가지고 있다."
        },
        {
          "rank": 47,
          "score": 0.6960288286209106,
          "doc_id": "NART133898062",
          "title": "Hidden Markov Neural Networks",
          "abstract": "<P>We define an evolving in-time Bayesian neural network called a Hidden Markov Neural Network, which addresses the crucial challenge in time-series forecasting and continual learning: striking a balance between adapting to new data and appropriately forgetting outdated information. This is achieved by modelling the weights of a neural network as the hidden states of a Hidden Markov model, with the observed process defined by the available data. A filtering algorithm is employed to learn a variational approximation of the evolving-in-time posterior distribution over the weights. By leveraging a sequential variant of Bayes by Backprop, enriched with a stronger regularization technique called variational DropConnect, Hidden Markov Neural Networks achieve robust regularization and scalable inference. Experiments on MNIST, dynamic classification tasks, and next-frame forecasting in videos demonstrate that Hidden Markov Neural Networks provide strong predictive performance while enabling effective uncertainty quantification.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART133898062&target=NART&cn=NART133898062",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden Markov Neural Networks Hidden Markov Neural Networks Hidden Markov Neural Networks <P>We define an evolving in-time Bayesian neural network called a Hidden Markov Neural Network, which addresses the crucial challenge in time-series forecasting and continual learning: striking a balance between adapting to new data and appropriately forgetting outdated information. This is achieved by modelling the weights of a neural network as the hidden states of a Hidden Markov model, with the observed process defined by the available data. A filtering algorithm is employed to learn a variational approximation of the evolving-in-time posterior distribution over the weights. By leveraging a sequential variant of Bayes by Backprop, enriched with a stronger regularization technique called variational DropConnect, Hidden Markov Neural Networks achieve robust regularization and scalable inference. Experiments on MNIST, dynamic classification tasks, and next-frame forecasting in videos demonstrate that Hidden Markov Neural Networks provide strong predictive performance while enabling effective uncertainty quantification.</P>"
        },
        {
          "rank": 48,
          "score": 0.6934654712677002,
          "doc_id": "NART01349148",
          "title": "Automatic segmentation and labeling of speech based on Hidden Markov Models",
          "abstract": "An accurate database documentation at phonetic level is very important for speech research: however, manual segmentation and labeling is a time consuming and error prone task. This article describes an automatic procedure for the segmentation of speech: given either the linguistic or the phonetic content of a speech utterance, the system provides phone boundaries. The technique is based on the use of an acoustic-phonetic unit Hidden Markov Model (HMM) recognizer: both the recognizer and the segmentation system have been designed exploiting the DARPA-TIMIT acoustic-phonetic continuous speech database of American English. Segmentation and labeling experiments have been conducted in different conditions to check the reliability of the resulting system. Satisfactory results have been obtained, especially when the system is trained with some manually presegmented material. The size of this material is a crucial factor; system performance has been evaluated with respect to this parameter. It turns out that the system provides 88.3% correct boundary location, given a tolerance of 20 ms, when only 256 phonetically balanced sentences are used for its training.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART01349148&target=NART&cn=NART01349148",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Automatic segmentation and labeling of speech based on Hidden Markov Models Automatic segmentation and labeling of speech based on Hidden Markov Models Automatic segmentation and labeling of speech based on Hidden Markov Models An accurate database documentation at phonetic level is very important for speech research: however, manual segmentation and labeling is a time consuming and error prone task. This article describes an automatic procedure for the segmentation of speech: given either the linguistic or the phonetic content of a speech utterance, the system provides phone boundaries. The technique is based on the use of an acoustic-phonetic unit Hidden Markov Model (HMM) recognizer: both the recognizer and the segmentation system have been designed exploiting the DARPA-TIMIT acoustic-phonetic continuous speech database of American English. Segmentation and labeling experiments have been conducted in different conditions to check the reliability of the resulting system. Satisfactory results have been obtained, especially when the system is trained with some manually presegmented material. The size of this material is a crucial factor; system performance has been evaluated with respect to this parameter. It turns out that the system provides 88.3% correct boundary location, given a tolerance of 20 ms, when only 256 phonetically balanced sentences are used for its training."
        },
        {
          "rank": 49,
          "score": 0.6931610107421875,
          "doc_id": "JAKO201707851605473",
          "title": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표",
          "abstract": "본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201707851605473&target=NART&cn=JAKO201707851605473",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다."
        },
        {
          "rank": 50,
          "score": 0.6918690204620361,
          "doc_id": "NART56157676",
          "title": "온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합",
          "abstract": "<P> 최근에 음성인식 분야에서 널리 사용되고 있는 은닉 마르코프 모델(HMM)을 이용하여 필기문자를 인식하고자 하는 연구가 활발히 진행되고 있다. 하지만, HMM은 시간에 따라서 변하는 입력특성을 잘 처리하는 장점이 있는 반면에, 각 모델을 독립적으로 학습시키는 경우에 각 패턴 사이의 분별력이 다소 떨어지는 문제가 있다. 본 논문에서는 HMM을 통해서 얻어진 각 모델의 내부 출력값을 이용하여 신경망 분류기로 추가적인 분류작업을 수행하는 방법을 제시한다. 또, 온라인 필기 데이타로 숫자와 영문자 대소문자를 인식하는 실험을 통해서 제시된 방법의 유용성을 입증한다.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART56157676&target=NART&cn=NART56157676",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 <P> 최근에 음성인식 분야에서 널리 사용되고 있는 은닉 마르코프 모델(HMM)을 이용하여 필기문자를 인식하고자 하는 연구가 활발히 진행되고 있다. 하지만, HMM은 시간에 따라서 변하는 입력특성을 잘 처리하는 장점이 있는 반면에, 각 모델을 독립적으로 학습시키는 경우에 각 패턴 사이의 분별력이 다소 떨어지는 문제가 있다. 본 논문에서는 HMM을 통해서 얻어진 각 모델의 내부 출력값을 이용하여 신경망 분류기로 추가적인 분류작업을 수행하는 방법을 제시한다. 또, 온라인 필기 데이타로 숫자와 영문자 대소문자를 인식하는 실험을 통해서 제시된 방법의 유용성을 입증한다.</P>"
        }
      ]
    },
    {
      "query": "What is the function of Hidden Markov Models (HMMs) in audiovisual speech recognition?",
      "query_meta": {
        "type": "single_hop",
        "index": 0
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.7824913859367371,
          "doc_id": "NART70632792",
          "title": "Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis",
          "abstract": "<P>This paper investigates joint speaker-dependent audiovisual Hidden Semi-Markov Models (HSMM) where the visual models produce a sequence of 3D motion tracking data that is used to animate a talking head and the acoustic models are used for speech synthesis. Different acoustic, visual, and joint audiovisual models for four different Austrian German speakers were trained and we show that the joint models perform better compared to other approaches in terms of synchronization quality of the synthesized visual speech. In addition, a detailed analysis of the acoustic and visual alignment is provided for the different models. Importantly, the joint audiovisual modeling does not decrease the acoustic synthetic speech quality compared to acoustic-only modeling so that there is a clear advantage in the common duration model of the joint audiovisual modeling approach that is used for synchronizing acoustic and visual parameter sequences. Finally, it provides a model that integrates the visual and acoustic speech dynamics.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART70632792&target=NART&cn=NART70632792",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis <P>This paper investigates joint speaker-dependent audiovisual Hidden Semi-Markov Models (HSMM) where the visual models produce a sequence of 3D motion tracking data that is used to animate a talking head and the acoustic models are used for speech synthesis. Different acoustic, visual, and joint audiovisual models for four different Austrian German speakers were trained and we show that the joint models perform better compared to other approaches in terms of synchronization quality of the synthesized visual speech. In addition, a detailed analysis of the acoustic and visual alignment is provided for the different models. Importantly, the joint audiovisual modeling does not decrease the acoustic synthetic speech quality compared to acoustic-only modeling so that there is a clear advantage in the common duration model of the joint audiovisual modeling approach that is used for synchronizing acoustic and visual parameter sequences. Finally, it provides a model that integrates the visual and acoustic speech dynamics.</P>"
        },
        {
          "rank": 2,
          "score": 0.7734706401824951,
          "doc_id": "NART18015173",
          "title": "Speech recognition using hidden Markov models: A CMU perspective",
          "abstract": "Hidden Markov Models (HMMs) have become the predominant approach for speech recognition systems. One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU. In this paper, we introduce Hidden Markov Modelling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used in SPHINX.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART18015173&target=NART&cn=NART18015173",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition using hidden Markov models: A CMU perspective Speech recognition using hidden Markov models: A CMU perspective Speech recognition using hidden Markov models: A CMU perspective Hidden Markov Models (HMMs) have become the predominant approach for speech recognition systems. One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU. In this paper, we introduce Hidden Markov Modelling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used in SPHINX."
        },
        {
          "rank": 3,
          "score": 0.7620739936828613,
          "doc_id": "NART17510385",
          "title": "Hidden-articulator Markov models for speech recognition",
          "abstract": "<P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART17510385&target=NART&cn=NART17510385",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition <P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>"
        },
        {
          "rank": 4,
          "score": 0.7483683228492737,
          "doc_id": "DIKO0007842188",
          "title": "신경망 예측 HMM을 이용한 음성인식에 관한 연구",
          "abstract": "음성은 인간의 가장 자연스러운 의사소통의 수단이며 이러한 음성에 의한 인간-기계 인터페이스는 속도가 빠르고 특별한 훈련이 없어도 이루어진다. 또한 컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식 기술은 중요한 연구 과제가 되고 있다. 이러한 음성인식에 관한 연구는 HMM과 신경망에 의한 방법들이 활발히 진행되고 있다. 그러나 HMM은 모델구조의 결정에 어려움이 있으며, 음성의 과도적 정보를 경시하는 경향이 있기 때문에 시간적 상관 표현력이 부족한 결점이 있다. 따라서 이러한 단점을 극복하기 위하여 음성의 동적특성을 표현할 수 있는 회귀계수와 지속시간 확률등을 파라메터로 추가하거나, 언어학적 제약을 가하여 최적의 단어별을 탐색하는 언어적 모델을 결합한 음성이해 시스템으로 발전하고 있다. 또한 신경망의 경우 그 구조나 가중치에 시간적인 것이 고려되지 않으므로 패턴이 정적인 경우 인식률이 높으나 음성과 같은 시계열 패턴의 비선형 신축을 취급하기 어렵다. 따라서 이러한 단점을 보완하기 위하여 회귀신경망, 예측형 신경망등이 제안되었다. 본 논문에서는 이러한 점들을 고려하여 HMM의 패턴에 대한 시간적 상관표현력을 개선시킬 수 있는 신경망 예측 HMM을 제안한다. 신경망 예측 HMM은 HMM과 신경망의 장점을 함께 사용할 수 있는 하이브리드형 네트워크로 예측형 신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하는 것이다. 따라서 예측형 신경망은 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가지므로 HMM에 동적 특성을 부여할 수 있다. 실험에서는 단독 숫자음과 100음절 데이터를 이용하여 HMM과 신경망 그리고 본 논문에서 제안한 신경망 예측 HMM의 인식성능을 비교·검토하였다. 신경망 예측 HMM은 MLP 예측 HMM, Elman망 예측 HMM, Jordan망 예측 HMM의 3가지이다. 실험결과 평가용 데이터를 기준으로 단독 숫자음에 대해서는 MLP 예측 HMM이 99.5%로 가장 우수한 결과를, 100음절 데이터에 대해서는 HMM이 87.7%로 가장 우수한 결과를 보였다. Elman망 예측 HMM과 Jordan망 예측 HMM의 경우 회귀구조임에도 불구하고 MLP 예측 HMM의 인식성능을 상회하지는 못하였다. 따라서 신경망 예측 HMM의 인식성능을 향상시키기 위해서는 데이터에 따른 효과의 검토와 음성패턴의 시간적 상관관계를 보다 더 잘 표현 할 수 있는 신경망의 구조에 대한 연구가 요구되며, 신경망과 HMM의 학습알고리즘에 대한 연구가 필요하다고 판단된다. 본 논문에서 제안한 신경망 예측 HMM은 HMM과 신경망의 결합이라고 하는 향후 가장 유효한 방법의 하나로 사료되며 연속음성 인식시스템으로 확장할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0007842188&target=NART&cn=DIKO0007842188",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망 예측 HMM을 이용한 음성인식에 관한 연구 신경망 예측 HMM을 이용한 음성인식에 관한 연구 신경망 예측 HMM을 이용한 음성인식에 관한 연구 음성은 인간의 가장 자연스러운 의사소통의 수단이며 이러한 음성에 의한 인간-기계 인터페이스는 속도가 빠르고 특별한 훈련이 없어도 이루어진다. 또한 컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식 기술은 중요한 연구 과제가 되고 있다. 이러한 음성인식에 관한 연구는 HMM과 신경망에 의한 방법들이 활발히 진행되고 있다. 그러나 HMM은 모델구조의 결정에 어려움이 있으며, 음성의 과도적 정보를 경시하는 경향이 있기 때문에 시간적 상관 표현력이 부족한 결점이 있다. 따라서 이러한 단점을 극복하기 위하여 음성의 동적특성을 표현할 수 있는 회귀계수와 지속시간 확률등을 파라메터로 추가하거나, 언어학적 제약을 가하여 최적의 단어별을 탐색하는 언어적 모델을 결합한 음성이해 시스템으로 발전하고 있다. 또한 신경망의 경우 그 구조나 가중치에 시간적인 것이 고려되지 않으므로 패턴이 정적인 경우 인식률이 높으나 음성과 같은 시계열 패턴의 비선형 신축을 취급하기 어렵다. 따라서 이러한 단점을 보완하기 위하여 회귀신경망, 예측형 신경망등이 제안되었다. 본 논문에서는 이러한 점들을 고려하여 HMM의 패턴에 대한 시간적 상관표현력을 개선시킬 수 있는 신경망 예측 HMM을 제안한다. 신경망 예측 HMM은 HMM과 신경망의 장점을 함께 사용할 수 있는 하이브리드형 네트워크로 예측형 신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하는 것이다. 따라서 예측형 신경망은 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가지므로 HMM에 동적 특성을 부여할 수 있다. 실험에서는 단독 숫자음과 100음절 데이터를 이용하여 HMM과 신경망 그리고 본 논문에서 제안한 신경망 예측 HMM의 인식성능을 비교·검토하였다. 신경망 예측 HMM은 MLP 예측 HMM, Elman망 예측 HMM, Jordan망 예측 HMM의 3가지이다. 실험결과 평가용 데이터를 기준으로 단독 숫자음에 대해서는 MLP 예측 HMM이 99.5%로 가장 우수한 결과를, 100음절 데이터에 대해서는 HMM이 87.7%로 가장 우수한 결과를 보였다. Elman망 예측 HMM과 Jordan망 예측 HMM의 경우 회귀구조임에도 불구하고 MLP 예측 HMM의 인식성능을 상회하지는 못하였다. 따라서 신경망 예측 HMM의 인식성능을 향상시키기 위해서는 데이터에 따른 효과의 검토와 음성패턴의 시간적 상관관계를 보다 더 잘 표현 할 수 있는 신경망의 구조에 대한 연구가 요구되며, 신경망과 HMM의 학습알고리즘에 대한 연구가 필요하다고 판단된다. 본 논문에서 제안한 신경망 예측 HMM은 HMM과 신경망의 결합이라고 하는 향후 가장 유효한 방법의 하나로 사료되며 연속음성 인식시스템으로 확장할 수 있을 것으로 기대된다."
        },
        {
          "rank": 5,
          "score": 0.7453805208206177,
          "doc_id": "JAKO200606140790765",
          "title": "Discrimination of Pathological Speech Using Hidden Markov Models",
          "abstract": "Diagnosis of pathological voice is one of the important issues in biomedical applications of speech technology. This study focuses on the discrimination of voice disorder using HMM (Hidden Markov Model) for automatic detection between normal voice and vocal fold disorder voice. This is a non-intrusive, non-expensive and fully automated method using only a speech sample of the subject. Speech data from normal people and patients were collected. Mel-frequency filter cepstral coefficients (MFCCs) were modeled by HMM classifier. Different states (3 states, 5 states and 7 states), 3 mixtures and left to right HMMs were formed. This method gives an accuracy of 93.8% for train data and 91.7% for test data in the discrimination of normal and vocal fold disorder voice for sustained /a/.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200606140790765&target=NART&cn=JAKO200606140790765",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Discrimination of Pathological Speech Using Hidden Markov Models Discrimination of Pathological Speech Using Hidden Markov Models Discrimination of Pathological Speech Using Hidden Markov Models Diagnosis of pathological voice is one of the important issues in biomedical applications of speech technology. This study focuses on the discrimination of voice disorder using HMM (Hidden Markov Model) for automatic detection between normal voice and vocal fold disorder voice. This is a non-intrusive, non-expensive and fully automated method using only a speech sample of the subject. Speech data from normal people and patients were collected. Mel-frequency filter cepstral coefficients (MFCCs) were modeled by HMM classifier. Different states (3 states, 5 states and 7 states), 3 mixtures and left to right HMMs were formed. This method gives an accuracy of 93.8% for train data and 91.7% for test data in the discrimination of normal and vocal fold disorder voice for sustained /a/."
        },
        {
          "rank": 6,
          "score": 0.7293279767036438,
          "doc_id": "NPAP07942137",
          "title": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP07942137&target=NART&cn=NPAP07942137",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 7,
          "score": 0.7276300191879272,
          "doc_id": "JAKO200111921140843",
          "title": "회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM (Hidden Markov Model)의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경 망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용 데이터에 대하여 Elman망 예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 98.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200111921140843&target=NART&cn=JAKO200111921140843",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM (Hidden Markov Model)의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경 망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용 데이터에 대하여 Elman망 예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 98.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 8,
          "score": 0.7274945974349976,
          "doc_id": "JAKO201403359905324",
          "title": "가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원",
          "abstract": "This paper describes a robust speech recognition technique by reconstructing spectral components mismatched with a training environment. Although the cluster-based reconstruction method can compensate the unreliable components from reliable components in the same spectral vector by assuming an independent, identically distributed Gaussian-mixture process of training spectral vectors, the presented method exploits the temporal dependency of speech to reconstruct the components by introducing a hidden-Markov-model prior which incorporates an internal state transition plausible for an observed spectral vector sequence. The experimental results indicate that the described method can provide temporally consistent reconstruction and further improve recognition performance on average compared to the conventional method.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201403359905324&target=NART&cn=JAKO201403359905324",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 This paper describes a robust speech recognition technique by reconstructing spectral components mismatched with a training environment. Although the cluster-based reconstruction method can compensate the unreliable components from reliable components in the same spectral vector by assuming an independent, identically distributed Gaussian-mixture process of training spectral vectors, the presented method exploits the temporal dependency of speech to reconstruct the components by introducing a hidden-Markov-model prior which incorporates an internal state transition plausible for an observed spectral vector sequence. The experimental results indicate that the described method can provide temporally consistent reconstruction and further improve recognition performance on average compared to the conventional method."
        },
        {
          "rank": 9,
          "score": 0.7249640226364136,
          "doc_id": "JAKO200311922043899",
          "title": "Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구",
          "abstract": "본 논문에서는 한국어 음성 데이터를 대상으로 HM-Net(Hidden Markov Network) 음성인식 시스템의 성능평가를 수행하였다. 음향모델 작성은 음성인식에서 널리 사용되고 있는 통계적인 모델링 방법인 HMM(Hidden Markov Model)을 개량한 HM-Net을 도입하였다. HM-Net은 기존의 SSS(Successive State Splitting) 알고리즘을 개량한 PDT(Phonetic Decision Tree)-SSS 알고리즘에 의해 문맥방향과 시간방향의 상태분할을 수행하여 생성되는데, 특히 문맥방향 상태분할의 경우 학습 음성데이터에 출현하지 않는 문맥정보를 효과적으로 표현하기 위해 음소결정트리를 채용하고 있으며, 시간방향 상태분할의 경우 학습 음성데이터에서 각 음소별 지속시간 정보를 효과적으로 표현하기 위한 상태분할을 수행하며, 마지막으로 파라미터의 공유를 통해 triphone 형태의 최적인 모델 네트워크를 작성하게 된다. 인식에 사용된 알고리즘은 음소 및 단어인식의 경우에는 One-Pass Viterbi 빔 탐색을 사용하며 트리 구조 형태의 사전과 phone/word-pair 문법을 채용하고 있다. 연속음성인식의 경우에는 단어 bigram과 단어 trigram 언어모델과 목구조 형태의 사전을 채용한 Multi-Pass 빔 탐색을 사용하고 있다. 전체적으로 본 논문에서는 다양한 조건에서 HM-Net 음성인식 시스템의 성능평가를 수행하였으며, 지금까지 소개된 음성인식 시스템과 비교하여 매우 우수한 인식성능을 보임을 실험을 통해 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200311922043899&target=NART&cn=JAKO200311922043899",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 본 논문에서는 한국어 음성 데이터를 대상으로 HM-Net(Hidden Markov Network) 음성인식 시스템의 성능평가를 수행하였다. 음향모델 작성은 음성인식에서 널리 사용되고 있는 통계적인 모델링 방법인 HMM(Hidden Markov Model)을 개량한 HM-Net을 도입하였다. HM-Net은 기존의 SSS(Successive State Splitting) 알고리즘을 개량한 PDT(Phonetic Decision Tree)-SSS 알고리즘에 의해 문맥방향과 시간방향의 상태분할을 수행하여 생성되는데, 특히 문맥방향 상태분할의 경우 학습 음성데이터에 출현하지 않는 문맥정보를 효과적으로 표현하기 위해 음소결정트리를 채용하고 있으며, 시간방향 상태분할의 경우 학습 음성데이터에서 각 음소별 지속시간 정보를 효과적으로 표현하기 위한 상태분할을 수행하며, 마지막으로 파라미터의 공유를 통해 triphone 형태의 최적인 모델 네트워크를 작성하게 된다. 인식에 사용된 알고리즘은 음소 및 단어인식의 경우에는 One-Pass Viterbi 빔 탐색을 사용하며 트리 구조 형태의 사전과 phone/word-pair 문법을 채용하고 있다. 연속음성인식의 경우에는 단어 bigram과 단어 trigram 언어모델과 목구조 형태의 사전을 채용한 Multi-Pass 빔 탐색을 사용하고 있다. 전체적으로 본 논문에서는 다양한 조건에서 HM-Net 음성인식 시스템의 성능평가를 수행하였으며, 지금까지 소개된 음성인식 시스템과 비교하여 매우 우수한 인식성능을 보임을 실험을 통해 확인할 수 있었다."
        },
        {
          "rank": 10,
          "score": 0.7190186977386475,
          "doc_id": "NART13081886",
          "title": "Speech recognition under noisy environments using segmental unit input HMM",
          "abstract": "<P>In this paper, we apply a segmental unit input HMM to noisy speech recognition. In this modeling, several successive frames are combined and treated as an input vector. We expect that the segmental unit input HMM will be effective for noisy speech recognition because segmental statistics considering correlation between frames reduce noise effects when the correlation of noise between frames is assumed to be small. In recognition experiments, we compared the segmental unit input HMM with a conventional frame-based HMM and found the segmental unit input HMM to be superior. We also compared the segmental unit input HMM with dynamic cepstral coefficients, which have both static and dynamic features, and found that the segmental unit input HMM is more effective than the dynamic cepstrum. We also combined the segmental unit input HMM with a spectral subtraction method and confirmed the effectiveness of the method. Additionally, in experiments using acoustic models trained with noisy speech, the segmental unit input HMM outperformed the conventional HMM. From these results, we propose PMC for the segmental unit input HMM. Experimental results showed the PMC for segmental unit input HMM offered better recognition performance than the original PMC. &copy; 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(8): 111&ndash;120, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1151</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART13081886&target=NART&cn=NART13081886",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition under noisy environments using segmental unit input HMM Speech recognition under noisy environments using segmental unit input HMM Speech recognition under noisy environments using segmental unit input HMM <P>In this paper, we apply a segmental unit input HMM to noisy speech recognition. In this modeling, several successive frames are combined and treated as an input vector. We expect that the segmental unit input HMM will be effective for noisy speech recognition because segmental statistics considering correlation between frames reduce noise effects when the correlation of noise between frames is assumed to be small. In recognition experiments, we compared the segmental unit input HMM with a conventional frame-based HMM and found the segmental unit input HMM to be superior. We also compared the segmental unit input HMM with dynamic cepstral coefficients, which have both static and dynamic features, and found that the segmental unit input HMM is more effective than the dynamic cepstrum. We also combined the segmental unit input HMM with a spectral subtraction method and confirmed the effectiveness of the method. Additionally, in experiments using acoustic models trained with noisy speech, the segmental unit input HMM outperformed the conventional HMM. From these results, we propose PMC for the segmental unit input HMM. Experimental results showed the PMC for segmental unit input HMM offered better recognition performance than the original PMC. &copy; 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(8): 111&ndash;120, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1151</P>"
        },
        {
          "rank": 11,
          "score": 0.709287166595459,
          "doc_id": "NART18014750",
          "title": "Neural nets and hidden Markov models: Review and generalizations",
          "abstract": "Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs).",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART18014750&target=NART&cn=NART18014750",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural nets and hidden Markov models: Review and generalizations Neural nets and hidden Markov models: Review and generalizations Neural nets and hidden Markov models: Review and generalizations Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs)."
        },
        {
          "rank": 12,
          "score": 0.7066406011581421,
          "doc_id": "NART16453920",
          "title": "Neural-network-based HMM adaptation for noisy speech recognition.",
          "abstract": "<P>This paper proposes a new method, using neural networks, of adapting phone HMMs to noisy speech. The neural networks are designed to map clean speech HMMs to noise-adapted HMMs, using noise HMMs and signal-to-noise ratios (SNRs) as inputs. The neural network is trained by minimizing the mean square error between the output HMMs and the target noise-adapted HMMs. In an evaluation, the proposed method was used to recognize noisy broadcast-news speech in speaker-dependent and speaker-independent modes. The trained networks were found to be effective in recognizing new speakers under new noise and various SNR conditions.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART16453920&target=NART&cn=NART16453920",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural-network-based HMM adaptation for noisy speech recognition. Neural-network-based HMM adaptation for noisy speech recognition. Neural-network-based HMM adaptation for noisy speech recognition. <P>This paper proposes a new method, using neural networks, of adapting phone HMMs to noisy speech. The neural networks are designed to map clean speech HMMs to noise-adapted HMMs, using noise HMMs and signal-to-noise ratios (SNRs) as inputs. The neural network is trained by minimizing the mean square error between the output HMMs and the target noise-adapted HMMs. In an evaluation, the proposed method was used to recognize noisy broadcast-news speech in speaker-dependent and speaker-independent modes. The trained networks were found to be effective in recognizing new speakers under new noise and various SNR conditions.</P>"
        },
        {
          "rank": 13,
          "score": 0.7060606479644775,
          "doc_id": "JAKO200428635215914",
          "title": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구",
          "abstract": "본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200428635215914&target=NART&cn=JAKO200428635215914",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다."
        },
        {
          "rank": 14,
          "score": 0.7058100700378418,
          "doc_id": "JAKO200011920774657",
          "title": "은닉 마코프 모델 기반 병렬음성인식 시스템",
          "abstract": "본 논문의 병렬음성인식 모델은 연속 은닉 마코프 모델(HMM; hidden Markov model)에 기반한 병렬 음소인식모듈과 계층구조의 지식베이스에 기반한 병렬 문장인식모듈로 구성된다. 병렬 음소인식 모듈은 수천개의 HMM을 병렬 프로세서에 분산시킨 수, 할당된 HMM에 대한 출력확률 계산과 Viterbi 알고리즘을 담당한다. 지식베이스 기반 병렬 문장인식모듈은 음소모듈에서 공급되는 음소열과 지안하는 병렬 음성인식 알고리즘은 분산메모리 MIMD 구조의 다중 트랜스퓨터와 Parsytec CC 상에 구현되었다. 실험결과, 병렬 음소인식모듈을 통한 실행시간 향상과 병렬 문장인식모듈을 통한 인식률 향상을 얻을 수 있었으며 병렬 음성인식 시스템의 실시간 구현 가능성을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200011920774657&target=NART&cn=JAKO200011920774657",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마코프 모델 기반 병렬음성인식 시스템 은닉 마코프 모델 기반 병렬음성인식 시스템 은닉 마코프 모델 기반 병렬음성인식 시스템 본 논문의 병렬음성인식 모델은 연속 은닉 마코프 모델(HMM; hidden Markov model)에 기반한 병렬 음소인식모듈과 계층구조의 지식베이스에 기반한 병렬 문장인식모듈로 구성된다. 병렬 음소인식 모듈은 수천개의 HMM을 병렬 프로세서에 분산시킨 수, 할당된 HMM에 대한 출력확률 계산과 Viterbi 알고리즘을 담당한다. 지식베이스 기반 병렬 문장인식모듈은 음소모듈에서 공급되는 음소열과 지안하는 병렬 음성인식 알고리즘은 분산메모리 MIMD 구조의 다중 트랜스퓨터와 Parsytec CC 상에 구현되었다. 실험결과, 병렬 음소인식모듈을 통한 실행시간 향상과 병렬 문장인식모듈을 통한 인식률 향상을 얻을 수 있었으며 병렬 음성인식 시스템의 실시간 구현 가능성을 확인하였다."
        },
        {
          "rank": 15,
          "score": 0.704127848148346,
          "doc_id": "NART20042187",
          "title": "Neural networks with hidden Markov process",
          "abstract": "Observed data contain global structural information and local statistical fluctuation.  To catch both global and local information, neural network models with hidden Markov process are presented. Hidden Markov model(HMM) is a rather general framework which can effectively deal with sequential structure based on stochastic process theory.  Neural networks have strong learning ability which makes it easier to free statistical fluctuation.  By introducing the dependence of weights of neural networks on the states of the systems, neural networks combined with HMM are realized.  The process underlying the transitions of the states is a Markov process. Dependent observations are generated by the neural networks corresponding to the states.  A modified maximum likelihood procedure are employed to estimate both transition probability of Markov process and weights of neural networks.  Applications of neural networks with HMM to econometrics, pattern matching, speech recognition and image process are discussed.(Author abstract)",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20042187&target=NART&cn=NART20042187",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural networks with hidden Markov process Neural networks with hidden Markov process Neural networks with hidden Markov process Observed data contain global structural information and local statistical fluctuation.  To catch both global and local information, neural network models with hidden Markov process are presented. Hidden Markov model(HMM) is a rather general framework which can effectively deal with sequential structure based on stochastic process theory.  Neural networks have strong learning ability which makes it easier to free statistical fluctuation.  By introducing the dependence of weights of neural networks on the states of the systems, neural networks combined with HMM are realized.  The process underlying the transitions of the states is a Markov process. Dependent observations are generated by the neural networks corresponding to the states.  A modified maximum likelihood procedure are employed to estimate both transition probability of Markov process and weights of neural networks.  Applications of neural networks with HMM to econometrics, pattern matching, speech recognition and image process are discussed.(Author abstract)"
        },
        {
          "rank": 16,
          "score": 0.703572154045105,
          "doc_id": "DIKO0011019580",
          "title": "시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합",
          "abstract": "음성인식은 인간과 기계의 인터페이스 서비스를 위한 중요한 기술이다. 현재까지 개발된 많은 음성인식 시스템은 제어된 환경에서는 높은 인식율을 보이지만, 잡음 환경에서는 성능이 크게 저하되는 한계가 있다. 이러한 한계를 극복하기 위한 방법의 하나인 시청각 음성인식은 잡음이 존재하는 환경에서 강인한 인식을 위해 마이크로 녹음한 음성신호 (목소리)와 카메라로 기록한 영상신호(입술의 움직임)를 모두 이용하여 음성을 인식하는 기술이다. 영상신호를 이용한 인식은 잡음이 적은 상황에서는 기존의 음성인식에 비해 낮은 인식 성능을 보이지만 소리잡음에 영향을 받지 않기 때문에 잡음 환경에서 음성인식을 보완하는 유용한 방법이 된다. 본 논문에서는 시청각 음성인식 시스템을 구성하는 청각신호를 이용한 인식, 시각정보를 이용한 인식, 그리고 두 정보의 통합 등의 세 부분을 고려하여 각 부분의 성능을 향상시킴으로써 잡음환경에서의 강인함을 향상시키고자 한다. 첫째, 시각정보를 이용한 인식의 성능을 향상시키기 위해 인식기인 은닉 마르코프 모델(hidden Markov model)의 확률적인 최적화 알고리즘을 제안한다. 알고리즘의 성능과 수렴속도를 개선하기 위해 확률적인 최적화 알고리즘의 하나인 모의 담금질 기법(simulated annealing)과 지역 탐색 연산자를 결합한 하이브리드 모의 담금질 기법을 개발한다. 기존의 은닉 마르코프 모델의 학습 알고리즘인 기대-최대(expectation-maximization) 알고리즘은 가능도 (likelihood) 함수에 대해 지역 최적화만을 수행하는 반면, 제안하는 알고리즘은 전 영역에서 탐색을 수행하며 결과적으로 은닉 마르코프 모델의 인식 성능을 높인다. 제안하는 알고리즘이 전역최적해에 확률로써 수렴하는 것을 수학적으로 증명한다. 둘째, 청각정보를 이용한 인식을 강인하게 하기 위해 은닉 마르코프 모델에서 관측 프레임간의 상관관계를 모델링하는 기법을 개발한다. 음성의 동적 특성은 사람이 잡음환경에서 강인한 인식 성능을 보이는데 도움을 주는 것으로 알려져 있으나 기존의 은닉 마르코프 모델을 이용한 음성 모델링에서는 충분히 고려되지 않고 있다. 본 논문에서는 가우시안 혼합 모델(Gaussian mixture model)로 서로 다른 프레임의 결합 확률분포를 모델링하여 프레임간의 조건부 의존관계를 다루는 기법을 제안한다. 또한, 기대-최대 알고리즘에 기반하여 제안하는 모델의 학습 알고리즘을 개발한다. 제안하는 모델을 이용함으로써 청각 정보를 이용한 인식에서 잡음에 더욱 강인한 성능을 얻도록 한다. 셋째, 시각정보와 청각정보의 상호보완성을 효과적으로 이용하여 잡음에 강인한 최종 인식결과를 얻기 위해 정보 통합 단계에서 신경회로망을 이용하는 기법을 제안한다. 정보 통합 단계에서는 시각정보와 청각정보를 각각 따로 인식한 결과를 가중치 기법을 통해 최종 결과를 얻는데, 학습된 신경회로망은 잡음의 종류와 수준을 알 수 없는 주어진 시청각 데이터에 대해 적절한 가중치를 출력함으로써 최적의 인식결과를 얻도록 한다. 이를 통해 통합 인식 결과가 시각 또는 청각정보만을 이용한 인식결과보다 최소한 같거나 좋을 뿐 아니라 두 정보에 의한 시너지 효과를 최대화하도록 한다. 제안하는 시스템을 화자독립 고립단어 인식문제에 적용하여 그 성능을 보인다. 실험 결과 제안하는 시스템이 기존의 시스템에 비해 다양한 잡음 환경에서 더욱 강인한 성능을 보이는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011019580&target=NART&cn=DIKO0011019580",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 음성인식은 인간과 기계의 인터페이스 서비스를 위한 중요한 기술이다. 현재까지 개발된 많은 음성인식 시스템은 제어된 환경에서는 높은 인식율을 보이지만, 잡음 환경에서는 성능이 크게 저하되는 한계가 있다. 이러한 한계를 극복하기 위한 방법의 하나인 시청각 음성인식은 잡음이 존재하는 환경에서 강인한 인식을 위해 마이크로 녹음한 음성신호 (목소리)와 카메라로 기록한 영상신호(입술의 움직임)를 모두 이용하여 음성을 인식하는 기술이다. 영상신호를 이용한 인식은 잡음이 적은 상황에서는 기존의 음성인식에 비해 낮은 인식 성능을 보이지만 소리잡음에 영향을 받지 않기 때문에 잡음 환경에서 음성인식을 보완하는 유용한 방법이 된다. 본 논문에서는 시청각 음성인식 시스템을 구성하는 청각신호를 이용한 인식, 시각정보를 이용한 인식, 그리고 두 정보의 통합 등의 세 부분을 고려하여 각 부분의 성능을 향상시킴으로써 잡음환경에서의 강인함을 향상시키고자 한다. 첫째, 시각정보를 이용한 인식의 성능을 향상시키기 위해 인식기인 은닉 마르코프 모델(hidden Markov model)의 확률적인 최적화 알고리즘을 제안한다. 알고리즘의 성능과 수렴속도를 개선하기 위해 확률적인 최적화 알고리즘의 하나인 모의 담금질 기법(simulated annealing)과 지역 탐색 연산자를 결합한 하이브리드 모의 담금질 기법을 개발한다. 기존의 은닉 마르코프 모델의 학습 알고리즘인 기대-최대(expectation-maximization) 알고리즘은 가능도 (likelihood) 함수에 대해 지역 최적화만을 수행하는 반면, 제안하는 알고리즘은 전 영역에서 탐색을 수행하며 결과적으로 은닉 마르코프 모델의 인식 성능을 높인다. 제안하는 알고리즘이 전역최적해에 확률로써 수렴하는 것을 수학적으로 증명한다. 둘째, 청각정보를 이용한 인식을 강인하게 하기 위해 은닉 마르코프 모델에서 관측 프레임간의 상관관계를 모델링하는 기법을 개발한다. 음성의 동적 특성은 사람이 잡음환경에서 강인한 인식 성능을 보이는데 도움을 주는 것으로 알려져 있으나 기존의 은닉 마르코프 모델을 이용한 음성 모델링에서는 충분히 고려되지 않고 있다. 본 논문에서는 가우시안 혼합 모델(Gaussian mixture model)로 서로 다른 프레임의 결합 확률분포를 모델링하여 프레임간의 조건부 의존관계를 다루는 기법을 제안한다. 또한, 기대-최대 알고리즘에 기반하여 제안하는 모델의 학습 알고리즘을 개발한다. 제안하는 모델을 이용함으로써 청각 정보를 이용한 인식에서 잡음에 더욱 강인한 성능을 얻도록 한다. 셋째, 시각정보와 청각정보의 상호보완성을 효과적으로 이용하여 잡음에 강인한 최종 인식결과를 얻기 위해 정보 통합 단계에서 신경회로망을 이용하는 기법을 제안한다. 정보 통합 단계에서는 시각정보와 청각정보를 각각 따로 인식한 결과를 가중치 기법을 통해 최종 결과를 얻는데, 학습된 신경회로망은 잡음의 종류와 수준을 알 수 없는 주어진 시청각 데이터에 대해 적절한 가중치를 출력함으로써 최적의 인식결과를 얻도록 한다. 이를 통해 통합 인식 결과가 시각 또는 청각정보만을 이용한 인식결과보다 최소한 같거나 좋을 뿐 아니라 두 정보에 의한 시너지 효과를 최대화하도록 한다. 제안하는 시스템을 화자독립 고립단어 인식문제에 적용하여 그 성능을 보인다. 실험 결과 제안하는 시스템이 기존의 시스템에 비해 다양한 잡음 환경에서 더욱 강인한 성능을 보이는 것을 확인한다."
        },
        {
          "rank": 17,
          "score": 0.7031691074371338,
          "doc_id": "NART00705015",
          "title": "Speaker recognition using HMM composition in noisy environments",
          "abstract": "<P><B>Abstract</B></P><P>This paper investigates a speaker recognition method that is robust against background noise. In noisy environments, one important issue is how to create a model for each speaker so as to compensate for noise. The method described here is based on hidden Markov model (HMM) composition, which combines a speaker HMM and a noise-source HMM into a noise-added speaker HMM with a particular signal-to-noise ratio (SNR). Since it is difficult to measure the SNR of input speech with non-stationary noise exactly, this method creates several noise-added speaker HMMs with various SNRs. The HMM that has the highest likelihood value for the input speech is selected, and a speaker decision is made using this likelihood value. Experimental application of this method to text-independent speaker identification and verification in various kinds of noisy environments demonstrated considerable improvement in speaker recognition for speech utterances of male speakers.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART00705015&target=NART&cn=NART00705015",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speaker recognition using HMM composition in noisy environments Speaker recognition using HMM composition in noisy environments Speaker recognition using HMM composition in noisy environments <P><B>Abstract</B></P><P>This paper investigates a speaker recognition method that is robust against background noise. In noisy environments, one important issue is how to create a model for each speaker so as to compensate for noise. The method described here is based on hidden Markov model (HMM) composition, which combines a speaker HMM and a noise-source HMM into a noise-added speaker HMM with a particular signal-to-noise ratio (SNR). Since it is difficult to measure the SNR of input speech with non-stationary noise exactly, this method creates several noise-added speaker HMMs with various SNRs. The HMM that has the highest likelihood value for the input speech is selected, and a speaker decision is made using this likelihood value. Experimental application of this method to text-independent speaker identification and verification in various kinds of noisy environments demonstrated considerable improvement in speaker recognition for speech utterances of male speakers.</P>"
        },
        {
          "rank": 18,
          "score": 0.700208842754364,
          "doc_id": "NART37979687",
          "title": "Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network",
          "abstract": "<P>A new method for noisy speech recognition based on a hybrid model of hidden Markov models (HMM) and wavelet neural network (WNN) is presented. The HMM was employed to compute the Viterbi output score. Then the score was used as the input of WNN to acquire the classification information. The result of recognition was made by these two kinds of recognition information. Recognition experiment shows that this hybrid model has higher performance than hidden Markov model in noisy speech recognition.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART37979687&target=NART&cn=NART37979687",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network <P>A new method for noisy speech recognition based on a hybrid model of hidden Markov models (HMM) and wavelet neural network (WNN) is presented. The HMM was employed to compute the Viterbi output score. Then the score was used as the input of WNN to acquire the classification information. The result of recognition was made by these two kinds of recognition information. Recognition experiment shows that this hybrid model has higher performance than hidden Markov model in noisy speech recognition.</P>"
        },
        {
          "rank": 19,
          "score": 0.6995185613632202,
          "doc_id": "NART01349148",
          "title": "Automatic segmentation and labeling of speech based on Hidden Markov Models",
          "abstract": "An accurate database documentation at phonetic level is very important for speech research: however, manual segmentation and labeling is a time consuming and error prone task. This article describes an automatic procedure for the segmentation of speech: given either the linguistic or the phonetic content of a speech utterance, the system provides phone boundaries. The technique is based on the use of an acoustic-phonetic unit Hidden Markov Model (HMM) recognizer: both the recognizer and the segmentation system have been designed exploiting the DARPA-TIMIT acoustic-phonetic continuous speech database of American English. Segmentation and labeling experiments have been conducted in different conditions to check the reliability of the resulting system. Satisfactory results have been obtained, especially when the system is trained with some manually presegmented material. The size of this material is a crucial factor; system performance has been evaluated with respect to this parameter. It turns out that the system provides 88.3% correct boundary location, given a tolerance of 20 ms, when only 256 phonetically balanced sentences are used for its training.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART01349148&target=NART&cn=NART01349148",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Automatic segmentation and labeling of speech based on Hidden Markov Models Automatic segmentation and labeling of speech based on Hidden Markov Models Automatic segmentation and labeling of speech based on Hidden Markov Models An accurate database documentation at phonetic level is very important for speech research: however, manual segmentation and labeling is a time consuming and error prone task. This article describes an automatic procedure for the segmentation of speech: given either the linguistic or the phonetic content of a speech utterance, the system provides phone boundaries. The technique is based on the use of an acoustic-phonetic unit Hidden Markov Model (HMM) recognizer: both the recognizer and the segmentation system have been designed exploiting the DARPA-TIMIT acoustic-phonetic continuous speech database of American English. Segmentation and labeling experiments have been conducted in different conditions to check the reliability of the resulting system. Satisfactory results have been obtained, especially when the system is trained with some manually presegmented material. The size of this material is a crucial factor; system performance has been evaluated with respect to this parameter. It turns out that the system provides 88.3% correct boundary location, given a tolerance of 20 ms, when only 256 phonetically balanced sentences are used for its training."
        },
        {
          "rank": 20,
          "score": 0.6987847685813904,
          "doc_id": "DIKO0011930560",
          "title": "시청각 음성인식을 위한 새로운 통합방법",
          "abstract": "The automatic speech recognition (ASR) is one of the most interesting problems applied to human computer interaction applications; for example, spoken digit recognition for mobile environments. One of the challenges of this problem is that the accuracy of speech recognition will be decrease much if the speaker talks under noisy place such as: restaurant, subway, street… The invention of lip-reading opens a potential chance to improve the performance of recognition. Indeed, human perception considers both auditory and visual nature of speech. The speech recognition system will be more intelligible if the lip motion of speaker is available together with acoustic signal. The combined audio visual speech recognition has been proved to be able enhance the overall performance of recognition, especially under noisy environment. In general, if the two streams are available for speech recognition, they can be integrated by two ways: early integration and late integration. The early integration approach combines the features of two streams into one concatenated feature vector, and uses single classifier for recognition. The late integration approach combines the results of two separate classifiers for recognition in which the reliability of modalities is applied to summation based fusion. The late integration method shows the better performance actually through many experiments. There are several factors are considered to measure reliability including word confusability, SNR level of acoustic signal, the noise type, and illumination change in visual stream rather than the only SNR level based confidence of conventional audio visual speech recognition. In this study, we propose an effective fusion scheme for audio visual speech recognition (AVSR) in which the appropriate combination weights are measured by using an integrated reliability. The significant idea of integrated reliability is the combination of not only acoustic noise but also model confusability for audio visual reliability measurement The experimental results using Samsung AVSR database shows the improved performance of our approach compared to conventional ones. This demonstrates the effectiveness and feasibility of this invention for real speech recognition applications.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011930560&target=NART&cn=DIKO0011930560",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시청각 음성인식을 위한 새로운 통합방법 시청각 음성인식을 위한 새로운 통합방법 시청각 음성인식을 위한 새로운 통합방법 The automatic speech recognition (ASR) is one of the most interesting problems applied to human computer interaction applications; for example, spoken digit recognition for mobile environments. One of the challenges of this problem is that the accuracy of speech recognition will be decrease much if the speaker talks under noisy place such as: restaurant, subway, street… The invention of lip-reading opens a potential chance to improve the performance of recognition. Indeed, human perception considers both auditory and visual nature of speech. The speech recognition system will be more intelligible if the lip motion of speaker is available together with acoustic signal. The combined audio visual speech recognition has been proved to be able enhance the overall performance of recognition, especially under noisy environment. In general, if the two streams are available for speech recognition, they can be integrated by two ways: early integration and late integration. The early integration approach combines the features of two streams into one concatenated feature vector, and uses single classifier for recognition. The late integration approach combines the results of two separate classifiers for recognition in which the reliability of modalities is applied to summation based fusion. The late integration method shows the better performance actually through many experiments. There are several factors are considered to measure reliability including word confusability, SNR level of acoustic signal, the noise type, and illumination change in visual stream rather than the only SNR level based confidence of conventional audio visual speech recognition. In this study, we propose an effective fusion scheme for audio visual speech recognition (AVSR) in which the appropriate combination weights are measured by using an integrated reliability. The significant idea of integrated reliability is the combination of not only acoustic noise but also model confusability for audio visual reliability measurement The experimental results using Samsung AVSR database shows the improved performance of our approach compared to conventional ones. This demonstrates the effectiveness and feasibility of this invention for real speech recognition applications."
        },
        {
          "rank": 21,
          "score": 0.6975263953208923,
          "doc_id": "NPAP00072266",
          "title": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models",
          "abstract": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP00072266&target=NART&cn=NPAP00072266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error."
        },
        {
          "rank": 22,
          "score": 0.6957216858863831,
          "doc_id": "NART48832461",
          "title": "Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments",
          "abstract": "In this paper, we propose a model for the incorporation of voicing information into a speech recognition system in noisy environments. The employed voicing information is estimated by a novel method that can provide this information for each filter-bank channel and does not require information about the fundamental frequency. The voicing information is modelled by employing the Bernoulli distribution. The voicing model is obtained for each HMM state and mixture by a Viterbi-style training procedure. The proposed voicing incorporation is evaluated both within a standard model and two other models that had compensated for the noise effect, the missing-feature and the multi-conditional training model. Experiments are first performed on noisy speech data from the Aurora 2 database. Significant performance improvements are achieved when the voicing information is incorporated within the standard model as well as the noise-compensated models. The employment of voicing information is also demonstrated on a phoneme recognition task on the noise-corrupted TIMIT database and considerable improvements are observed.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART48832461&target=NART&cn=NART48832461",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments In this paper, we propose a model for the incorporation of voicing information into a speech recognition system in noisy environments. The employed voicing information is estimated by a novel method that can provide this information for each filter-bank channel and does not require information about the fundamental frequency. The voicing information is modelled by employing the Bernoulli distribution. The voicing model is obtained for each HMM state and mixture by a Viterbi-style training procedure. The proposed voicing incorporation is evaluated both within a standard model and two other models that had compensated for the noise effect, the missing-feature and the multi-conditional training model. Experiments are first performed on noisy speech data from the Aurora 2 database. Significant performance improvements are achieved when the voicing information is incorporated within the standard model as well as the noise-compensated models. The employment of voicing information is also demonstrated on a phoneme recognition task on the noise-corrupted TIMIT database and considerable improvements are observed."
        },
        {
          "rank": 23,
          "score": 0.6912095546722412,
          "doc_id": "NART06155061",
          "title": "Speech enhancement based on neural predictive hidden Markov model",
          "abstract": "<P><B>Abstract</B></P><P>In this paper, we describe a new approach to speech enhancement by modeling directly the statistical characteristics of the speech waveform. To represent the nonlinear and nonstationary nature of speech, it is assumed that speech is the output of a neural predictive hidden Markov model (NPHMM). The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a Markov chain. Given some speech data, the parameter of NPHMM is estimated by a learning algorithm based on the combination of Baum&#x2013;Welch algorithm and a neural network learning algorithm using the well known back propagation technique. Given the parameters of NPHMM, a recursive estimation method using multiple Kalman filters, governed by a Markov state chain according to the transition probabilities is developed for enhancing speech signals degraded by statistically independent additive noise characteristics assumed to be white and Gaussian. Under various input signal-to-noise ratios (SNRs), the proposed recursive speech enhancement method achieves an improvement over the method based on hidden filter model (Lee and Shirai, 1996) of about 0.8&#x2013;1.2dB in terms of the measured output SNR.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART06155061&target=NART&cn=NART06155061",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech enhancement based on neural predictive hidden Markov model Speech enhancement based on neural predictive hidden Markov model Speech enhancement based on neural predictive hidden Markov model <P><B>Abstract</B></P><P>In this paper, we describe a new approach to speech enhancement by modeling directly the statistical characteristics of the speech waveform. To represent the nonlinear and nonstationary nature of speech, it is assumed that speech is the output of a neural predictive hidden Markov model (NPHMM). The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a Markov chain. Given some speech data, the parameter of NPHMM is estimated by a learning algorithm based on the combination of Baum&#x2013;Welch algorithm and a neural network learning algorithm using the well known back propagation technique. Given the parameters of NPHMM, a recursive estimation method using multiple Kalman filters, governed by a Markov state chain according to the transition probabilities is developed for enhancing speech signals degraded by statistically independent additive noise characteristics assumed to be white and Gaussian. Under various input signal-to-noise ratios (SNRs), the proposed recursive speech enhancement method achieves an improvement over the method based on hidden filter model (Lee and Shirai, 1996) of about 0.8&#x2013;1.2dB in terms of the measured output SNR.</P>"
        },
        {
          "rank": 24,
          "score": 0.6866422891616821,
          "doc_id": "JAKO199615875841474",
          "title": "천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식",
          "abstract": "본 논문에서는 상태간의 천이가 특정한 시간 구간에서만 발생하도록 하는 천이 제한(transition constrained) HMM를 제안하고 잡음 환경에서의 성능을 평가하였다. 천이 제한 HMM는 상태 지속을 제한하고 음성 신호의 시간적 변화를 단순하고 효과적으로 표현할 수 있다. 제안된 천이 제한 HMM은 기존 HMM 보다 성능이 우수할 뿐만아니라 계산량도 매우 감소한다.  제안된 방법의 성능을 평가하기 위하여 반연속(semi-continuous) HMM을 이용하여 잡음이 SNR 20, 10, 0 dB로 첨가된 음성에 화자독립 단독음 인식실험을 수행하였다. 실험 결과에서 제안된 방법은 잡음에 강인한 특성을 나타내었다. 두 가지 종류의 잡음을 SNR 10dB로 첨가하여 사용한 경우, 천이제한 HMM의 인식률은 기존 HMM의 단어 인식률 81.08%와 75.36%에 비하여 각각 7.31%와 10.35% 향상되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199615875841474&target=NART&cn=JAKO199615875841474",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식 천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식 천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식 본 논문에서는 상태간의 천이가 특정한 시간 구간에서만 발생하도록 하는 천이 제한(transition constrained) HMM를 제안하고 잡음 환경에서의 성능을 평가하였다. 천이 제한 HMM는 상태 지속을 제한하고 음성 신호의 시간적 변화를 단순하고 효과적으로 표현할 수 있다. 제안된 천이 제한 HMM은 기존 HMM 보다 성능이 우수할 뿐만아니라 계산량도 매우 감소한다.  제안된 방법의 성능을 평가하기 위하여 반연속(semi-continuous) HMM을 이용하여 잡음이 SNR 20, 10, 0 dB로 첨가된 음성에 화자독립 단독음 인식실험을 수행하였다. 실험 결과에서 제안된 방법은 잡음에 강인한 특성을 나타내었다. 두 가지 종류의 잡음을 SNR 10dB로 첨가하여 사용한 경우, 천이제한 HMM의 인식률은 기존 HMM의 단어 인식률 81.08%와 75.36%에 비하여 각각 7.31%와 10.35% 향상되었다."
        },
        {
          "rank": 25,
          "score": 0.6825109720230103,
          "doc_id": "NART95825020",
          "title": "End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition",
          "abstract": "<P><B>Abstract</B></P>  <P>In hidden Markov model (HMM) based automatic speech recognition (ASR) system, modeling the statistical relationship between the acoustic speech signal and the HMM states that represent linguistically motivated subword units such as phonemes is a crucial step. This is typically achieved by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then training a classifier such as artificial neural networks (ANN), Gaussian mixture model that estimates the emission probabilities of the HMM states. This paper investigates an end-to-end acoustic modeling approach using convolutional neural networks (CNNs), where the CNN takes as input raw speech signal and estimates the HMM states class conditional probabilities at the output. Alternately, as opposed to a divide and conquer strategy (i.e., separating feature extraction and statistical modeling steps), in the proposed acoustic modeling approach the relevant features and the classifier are jointly learned from the raw speech signal. Through ASR studies and analyses on multiple languages and multiple tasks, we show that: (a) the proposed approach yields consistently a better system with fewer parameters when compared to the conventional approach of cepstral feature extraction followed by ANN training, (b) unlike conventional method of speech processing, in the proposed approach the relevant feature representations are learned by first processing the input raw speech at the sub-segmental level ( &asymp; 2 ms). Specifically, through an analysis we show that the filters in the first convolution layer automatically learn &ldquo;in-parts&rdquo; formant-like information present in the sub-segmental speech, and (c) the intermediate feature representations obtained by subsequent filtering of the first convolution layer output are more discriminative compared to standard cepstral features and could be transferred across languages and domains.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Novel CNN-based end-to-end acoustic modeling approach is proposed. </LI> <LI>  Relevant features are automatically learned from the signal by discriminating phones. </LI> <LI>  Learned features are more discriminative than cepstral-based features. </LI> <LI>  Learned features are somewhat invariant to languages and domains. </LI> <LI>  Proposed approach leads to better ASR systems. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART95825020&target=NART&cn=NART95825020",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition <P><B>Abstract</B></P>  <P>In hidden Markov model (HMM) based automatic speech recognition (ASR) system, modeling the statistical relationship between the acoustic speech signal and the HMM states that represent linguistically motivated subword units such as phonemes is a crucial step. This is typically achieved by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then training a classifier such as artificial neural networks (ANN), Gaussian mixture model that estimates the emission probabilities of the HMM states. This paper investigates an end-to-end acoustic modeling approach using convolutional neural networks (CNNs), where the CNN takes as input raw speech signal and estimates the HMM states class conditional probabilities at the output. Alternately, as opposed to a divide and conquer strategy (i.e., separating feature extraction and statistical modeling steps), in the proposed acoustic modeling approach the relevant features and the classifier are jointly learned from the raw speech signal. Through ASR studies and analyses on multiple languages and multiple tasks, we show that: (a) the proposed approach yields consistently a better system with fewer parameters when compared to the conventional approach of cepstral feature extraction followed by ANN training, (b) unlike conventional method of speech processing, in the proposed approach the relevant feature representations are learned by first processing the input raw speech at the sub-segmental level ( &asymp; 2 ms). Specifically, through an analysis we show that the filters in the first convolution layer automatically learn &ldquo;in-parts&rdquo; formant-like information present in the sub-segmental speech, and (c) the intermediate feature representations obtained by subsequent filtering of the first convolution layer output are more discriminative compared to standard cepstral features and could be transferred across languages and domains.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Novel CNN-based end-to-end acoustic modeling approach is proposed. </LI> <LI>  Relevant features are automatically learned from the signal by discriminating phones. </LI> <LI>  Learned features are more discriminative than cepstral-based features. </LI> <LI>  Learned features are somewhat invariant to languages and domains. </LI> <LI>  Proposed approach leads to better ASR systems. </LI> </UL> </P>"
        },
        {
          "rank": 26,
          "score": 0.6806807518005371,
          "doc_id": "NART20482871",
          "title": "Piecewise-linear transformation-based HMM adaptation for noisy speech",
          "abstract": "<P><B>Abstract</B></P><P>This paper proposes a new method using piecewise-linear transformation for adapting phone HMMs to noisy speech. Various noises are clustered according to their spectral property, and a noisy speech HMM corresponding to each clustered noise and SNR condition is made. Based on the likelihood maximization criterion, an HMM that best matches an input noisy speech is selected and further adapted using linear transformation. The proposed method is evaluated by its ability to recognize noisy broadcast-news speech. It is confirmed that the proposed method is effective in recognizing numerically noise-added speech and actual noisy speech under various noise conditions. The proposed method minimizes mismatches between noisy input speech and the HMM&#x2019;s, sentence by sentence, without requiring online noise spectrum/model estimation. The proposed method is therefore easily applicable to real world conditions with frequently changing noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20482871&target=NART&cn=NART20482871",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Piecewise-linear transformation-based HMM adaptation for noisy speech Piecewise-linear transformation-based HMM adaptation for noisy speech Piecewise-linear transformation-based HMM adaptation for noisy speech <P><B>Abstract</B></P><P>This paper proposes a new method using piecewise-linear transformation for adapting phone HMMs to noisy speech. Various noises are clustered according to their spectral property, and a noisy speech HMM corresponding to each clustered noise and SNR condition is made. Based on the likelihood maximization criterion, an HMM that best matches an input noisy speech is selected and further adapted using linear transformation. The proposed method is evaluated by its ability to recognize noisy broadcast-news speech. It is confirmed that the proposed method is effective in recognizing numerically noise-added speech and actual noisy speech under various noise conditions. The proposed method minimizes mismatches between noisy input speech and the HMM&#x2019;s, sentence by sentence, without requiring online noise spectrum/model estimation. The proposed method is therefore easily applicable to real world conditions with frequently changing noise.</P>"
        },
        {
          "rank": 27,
          "score": 0.6783497333526611,
          "doc_id": "JAKO200411922338894",
          "title": "신경망 기반 음성, 영상 및 문맥 통합 음성인식",
          "abstract": "최근 잡음환경에서 신뢰도 높은 음성인식을 위해 음성정보와 영상정보를 융합하는 방법이 활발히 연구되고 있다. 본 논문에서는 이절적인 정보의 융합에 적합한 신경망 모델을 기반으로 음성, 영상 및 문맥 정보 등 다양한 정보를 융합하여 잡음 환경에서 고려단어를 인식하는 음성인식 기법에 대하여 기술한다. 음성과 영상 특징을 이용한 이중 모드 신경망 BMNN(BiModal Neural Network)을 제안한다. BMM은 4개 층으로 이루어진 다층퍼셉트론의 구조를 가지며 각 층은 입력 특징의 추상화 기능을 수행한다. BMNN에서는 제 3층이 잡음에 의한 음성 정보의 손실을 보상하기 위하여 음성과 영상 특징을 통합하는 기능을 수행한다. 또한, 잡음환경에서 음성 인식률을 향상시키기 위해 사용자가 말한 단어들의 순차 패턴을 나타내는 문맥정보를 이용한 후처리 방법을 제안한다. 잡음환경에서 BMNN은 단순히 음성만을 사용한 것 보다 높은 성능을 보임으로써 그 타당성을 확인할 수 있을 뿐 아니라, 특히 문맥을 이용한 후처리를 하였을 경우 잡음 환경에서 90%이상의 인식률을 달성하였다 본 연구는 잡음환경에서 강인한 음성인식을 위해 다양한 추가 정보를 사용함으로써 성능을 향상시킬 수 있음을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200411922338894&target=NART&cn=JAKO200411922338894",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망 기반 음성, 영상 및 문맥 통합 음성인식 신경망 기반 음성, 영상 및 문맥 통합 음성인식 신경망 기반 음성, 영상 및 문맥 통합 음성인식 최근 잡음환경에서 신뢰도 높은 음성인식을 위해 음성정보와 영상정보를 융합하는 방법이 활발히 연구되고 있다. 본 논문에서는 이절적인 정보의 융합에 적합한 신경망 모델을 기반으로 음성, 영상 및 문맥 정보 등 다양한 정보를 융합하여 잡음 환경에서 고려단어를 인식하는 음성인식 기법에 대하여 기술한다. 음성과 영상 특징을 이용한 이중 모드 신경망 BMNN(BiModal Neural Network)을 제안한다. BMM은 4개 층으로 이루어진 다층퍼셉트론의 구조를 가지며 각 층은 입력 특징의 추상화 기능을 수행한다. BMNN에서는 제 3층이 잡음에 의한 음성 정보의 손실을 보상하기 위하여 음성과 영상 특징을 통합하는 기능을 수행한다. 또한, 잡음환경에서 음성 인식률을 향상시키기 위해 사용자가 말한 단어들의 순차 패턴을 나타내는 문맥정보를 이용한 후처리 방법을 제안한다. 잡음환경에서 BMNN은 단순히 음성만을 사용한 것 보다 높은 성능을 보임으로써 그 타당성을 확인할 수 있을 뿐 아니라, 특히 문맥을 이용한 후처리를 하였을 경우 잡음 환경에서 90%이상의 인식률을 달성하였다 본 연구는 잡음환경에서 강인한 음성인식을 위해 다양한 추가 정보를 사용함으로써 성능을 향상시킬 수 있음을 제시한다."
        },
        {
          "rank": 28,
          "score": 0.6750868558883667,
          "doc_id": "NART13642943",
          "title": "Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments",
          "abstract": "<P>Handling background noise or echo (reverberation) etc. is very important for having an automated robot etc. recognize remote speech in a real environment. As effective schemes for handling this problem, noise reducing schemes such as model adaptation schemes including HMM decomposition and composition or microphone array (beamformer) signal processing, spectral subtraction, etc. have been proposed. In particular, a model adaptation scheme is very effective for speech recognition in a noisy environment and its recognition performance increases in proportion to the signal-to-noise ratio (SNR). In this paper, improving the recognition performance in a low-SNR environment by receiving speech at a high SNR using a microphone array before HMM decomposition and composition is attempted. The results of speech recognition experiments conducted in a noisy environment in an acoustic laboratory show an improvement in the recognition rate of about 25% by the proposed method for the case in which the SNR in a single microphone is 0 dB, as compared with the cases of using microphone array signal processing, HMM decomposition and composition alone. In addition, the proposed method shows recognition performance comparable to the case of using cepstrum mean normalization and spectral subtraction performed with an optimal coefficient given to the speech after microphone array processing. &copy; 2002 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 85(9): 13&ndash;22, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ecjb.10068</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART13642943&target=NART&cn=NART13642943",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments <P>Handling background noise or echo (reverberation) etc. is very important for having an automated robot etc. recognize remote speech in a real environment. As effective schemes for handling this problem, noise reducing schemes such as model adaptation schemes including HMM decomposition and composition or microphone array (beamformer) signal processing, spectral subtraction, etc. have been proposed. In particular, a model adaptation scheme is very effective for speech recognition in a noisy environment and its recognition performance increases in proportion to the signal-to-noise ratio (SNR). In this paper, improving the recognition performance in a low-SNR environment by receiving speech at a high SNR using a microphone array before HMM decomposition and composition is attempted. The results of speech recognition experiments conducted in a noisy environment in an acoustic laboratory show an improvement in the recognition rate of about 25% by the proposed method for the case in which the SNR in a single microphone is 0 dB, as compared with the cases of using microphone array signal processing, HMM decomposition and composition alone. In addition, the proposed method shows recognition performance comparable to the case of using cepstrum mean normalization and spectral subtraction performed with an optimal coefficient given to the speech after microphone array processing. &copy; 2002 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 85(9): 13&ndash;22, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ecjb.10068</P>"
        },
        {
          "rank": 29,
          "score": 0.6734315752983093,
          "doc_id": "JAKO200211921444549",
          "title": "2층 구조의 입체 시각형 신경망 기반 음소인식",
          "abstract": "본 연구는 입체 시각을 위한 신경망에 대한 연구 결과로서 인간의 음성을 인식하는데 적용된다. 입체 시각신경망(SVNN)에 기반한 음성인식에서, 먼저 입력된 음성 신호를 표준 모델과 비교함으로써 유사성이 얻어진다. 이 값들은 다이나믹한 처리 과정으로 주어지고 이웃한 신경소자들 사이에서 경쟁적이고 협력적인 처리를 거치게 된다. 이러한 다이나믹한 처리과정을 통해 단 하나의 가장 우수한 신경세포(winner neuron)만이 최후에 검출된다. 비교연구에서 2층 구조의 SVNN은 HMM 인식기보다 인식정확도 측면에서 7.7% 더 높았다. 평가 결과. SVNN은 기손리 HMM 인식기 성능을 능가하는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200211921444549&target=NART&cn=JAKO200211921444549",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "2층 구조의 입체 시각형 신경망 기반 음소인식 2층 구조의 입체 시각형 신경망 기반 음소인식 2층 구조의 입체 시각형 신경망 기반 음소인식 본 연구는 입체 시각을 위한 신경망에 대한 연구 결과로서 인간의 음성을 인식하는데 적용된다. 입체 시각신경망(SVNN)에 기반한 음성인식에서, 먼저 입력된 음성 신호를 표준 모델과 비교함으로써 유사성이 얻어진다. 이 값들은 다이나믹한 처리 과정으로 주어지고 이웃한 신경소자들 사이에서 경쟁적이고 협력적인 처리를 거치게 된다. 이러한 다이나믹한 처리과정을 통해 단 하나의 가장 우수한 신경세포(winner neuron)만이 최후에 검출된다. 비교연구에서 2층 구조의 SVNN은 HMM 인식기보다 인식정확도 측면에서 7.7% 더 높았다. 평가 결과. SVNN은 기손리 HMM 인식기 성능을 능가하는 것으로 나타났다."
        },
        {
          "rank": 30,
          "score": 0.6649751663208008,
          "doc_id": "JAKO200612842592571",
          "title": "제스처 인식을 위한 은닉 마르코프 모델",
          "abstract": "본 논문에서는 은닉 마르코프 모델 (HMM: hidden Markov model)을 이용한 제스처 인식 방법을 제안하고, 이를 게임 시스템의 인터페이스로 적용한 사례를 소개한다. 제안된 방법은 다음의 두 가지 특징을 가진다. 첫 번째는 사전에 분할된 데이터 열을 입력으로 사용하는 기존의 방법과는 달리, 제안된 방법은 카메라로부터 입력되는 비디오 스트림을 HMM의 입력으로 사용한다는 것이다. 두 번째는 제안된 HMM은 제스처의 분할과 인식을 동시에 수행한다는 것이다. 제안된 방법에서 사용자의 제스처는 13개의 제스처들을 인식하는 13개의 specific-HMM들을 결합하는 하나의 통합된 HMM을 통해 인식된다. 제안된 HMM은 사용자의 머리와 양손의 2D-위치 좌표로 구성된 포즈 심볼들의 열을 입력받는다. 그리고 새로운 포즈가 입력될 때마다, HMM의 상태 확률 값을 갱신한다. 그때, 만약 특정 상태의 확률 값이 미리 정해둔 임계치보다 큰 경우, 그 특정 상태를 포함하고 있는 제스처로 인식한다 제안된 방법의 정당성을 입증하기 위하여, 제안된 방법은 Quake II라는 컴퓨터 게임에 적용되었다. 실험결과는 제안된 방법이 높은 인식 정확률과, 계산 시간을 확연하게 감소시킬 수 있었음을 보여주었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200612842592571&target=NART&cn=JAKO200612842592571",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "제스처 인식을 위한 은닉 마르코프 모델 제스처 인식을 위한 은닉 마르코프 모델 제스처 인식을 위한 은닉 마르코프 모델 본 논문에서는 은닉 마르코프 모델 (HMM: hidden Markov model)을 이용한 제스처 인식 방법을 제안하고, 이를 게임 시스템의 인터페이스로 적용한 사례를 소개한다. 제안된 방법은 다음의 두 가지 특징을 가진다. 첫 번째는 사전에 분할된 데이터 열을 입력으로 사용하는 기존의 방법과는 달리, 제안된 방법은 카메라로부터 입력되는 비디오 스트림을 HMM의 입력으로 사용한다는 것이다. 두 번째는 제안된 HMM은 제스처의 분할과 인식을 동시에 수행한다는 것이다. 제안된 방법에서 사용자의 제스처는 13개의 제스처들을 인식하는 13개의 specific-HMM들을 결합하는 하나의 통합된 HMM을 통해 인식된다. 제안된 HMM은 사용자의 머리와 양손의 2D-위치 좌표로 구성된 포즈 심볼들의 열을 입력받는다. 그리고 새로운 포즈가 입력될 때마다, HMM의 상태 확률 값을 갱신한다. 그때, 만약 특정 상태의 확률 값이 미리 정해둔 임계치보다 큰 경우, 그 특정 상태를 포함하고 있는 제스처로 인식한다 제안된 방법의 정당성을 입증하기 위하여, 제안된 방법은 Quake II라는 컴퓨터 게임에 적용되었다. 실험결과는 제안된 방법이 높은 인식 정확률과, 계산 시간을 확연하게 감소시킬 수 있었음을 보여주었다."
        },
        {
          "rank": 31,
          "score": 0.6645939946174622,
          "doc_id": "JAKO200727500236879",
          "title": "자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식",
          "abstract": "In this paper, we carried out recognition experiments for noisy speech having various levels of car noise and output of an audio system using the speech interface. The speech interface consists of three parts: pre-processing, acoustic echo canceller, post-processing. First, a high pass filter is employed as a pre-processing part to remove some engine noises. Then, an echo canceller implemented by using an FIR-type filter with an NLMS adaptive algorithm is used to remove the music or speech coming from the audio system in a car. As a last part, the MMSE-STSA based speech enhancement method is applied to the out of the echo canceller to remove the residual noise further. For recognition experiments, we generated test signals by adding music to the car noisy speech from Aurora 2 database. The HTK-based continuous HMM system is constructed for a recognition system. Experimental results show that the proposed speech interface is very promising for robust speech recognition in a noisy car environment.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200727500236879&target=NART&cn=JAKO200727500236879",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 In this paper, we carried out recognition experiments for noisy speech having various levels of car noise and output of an audio system using the speech interface. The speech interface consists of three parts: pre-processing, acoustic echo canceller, post-processing. First, a high pass filter is employed as a pre-processing part to remove some engine noises. Then, an echo canceller implemented by using an FIR-type filter with an NLMS adaptive algorithm is used to remove the music or speech coming from the audio system in a car. As a last part, the MMSE-STSA based speech enhancement method is applied to the out of the echo canceller to remove the residual noise further. For recognition experiments, we generated test signals by adding music to the car noisy speech from Aurora 2 database. The HTK-based continuous HMM system is constructed for a recognition system. Experimental results show that the proposed speech interface is very promising for robust speech recognition in a noisy car environment."
        },
        {
          "rank": 32,
          "score": 0.6622277498245239,
          "doc_id": "NART56158762",
          "title": "오프라인 글씨 인식을 위한 은닉 마르코프 메쉬 랜덤 필드 모델",
          "abstract": "<P> 2차원 영상을 다루는데 있어 1차원 은닉 마르코프 모델(Hidden Markov Model : HMM)의 한계가 지적됨에 따라 새로이 등장한 은닉 마르코프 메쉬 랜덤 필드(Hidden Markov Mesh Random Field HMMRF) 모델은 영상의 모델링과 인식을 위한 새로운 통계적 모델이다. 이 모델은 영상이 마르코프 메쉬 랜덤 필드(Malkov Mesh Random Field MMRF)에 의해 표현될 수 있다는 가정하에서 제안된 모델로서, 본 논문에서는 2차원 통계적 모델인 HMMRF 모델을 기반으로 하여 오프라인 글씨 인식을 위한 새로운 방법론을 제안한다. 제안된 HMMRF 모델이 글씨 인식의 문제에 적용되기 위해서는 디코팅(혹은 레이블링) 단계와 훈련 단계가 필요하며 본 논문에서는 관측값들에 포함되어 있는 정보를 기반으로 화소의 상태를 결정하는 디코딩 문제를 중점적으로 다루고자 한다. 이 문제에 대한 해결 방안은 HMMRF 모델에 대하여 최대 주변 사후 확률 분포 기준에 기반을 둔 &quot;look-ahead&quot; 기법으로 부터 유도될 수 있다.  오프라인 글씨 인식을 위한 2차원 HMMRF 모델의 유용성을 입증하기 위하여 1차원 모델과의 성능 비교가 이루어졌으며, 실험 결과 HMMRF 모델이 기존의 1차원 모델이 갖는 한계를 극복할 수 있다는 점에서 오프라인 글씨 인식 분야에서 대표적인 통계적 모델로서 확립될 수 있으리라 판단된다.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART56158762&target=NART&cn=NART56158762",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오프라인 글씨 인식을 위한 은닉 마르코프 메쉬 랜덤 필드 모델 오프라인 글씨 인식을 위한 은닉 마르코프 메쉬 랜덤 필드 모델 오프라인 글씨 인식을 위한 은닉 마르코프 메쉬 랜덤 필드 모델 <P> 2차원 영상을 다루는데 있어 1차원 은닉 마르코프 모델(Hidden Markov Model : HMM)의 한계가 지적됨에 따라 새로이 등장한 은닉 마르코프 메쉬 랜덤 필드(Hidden Markov Mesh Random Field HMMRF) 모델은 영상의 모델링과 인식을 위한 새로운 통계적 모델이다. 이 모델은 영상이 마르코프 메쉬 랜덤 필드(Malkov Mesh Random Field MMRF)에 의해 표현될 수 있다는 가정하에서 제안된 모델로서, 본 논문에서는 2차원 통계적 모델인 HMMRF 모델을 기반으로 하여 오프라인 글씨 인식을 위한 새로운 방법론을 제안한다. 제안된 HMMRF 모델이 글씨 인식의 문제에 적용되기 위해서는 디코팅(혹은 레이블링) 단계와 훈련 단계가 필요하며 본 논문에서는 관측값들에 포함되어 있는 정보를 기반으로 화소의 상태를 결정하는 디코딩 문제를 중점적으로 다루고자 한다. 이 문제에 대한 해결 방안은 HMMRF 모델에 대하여 최대 주변 사후 확률 분포 기준에 기반을 둔 &quot;look-ahead&quot; 기법으로 부터 유도될 수 있다.  오프라인 글씨 인식을 위한 2차원 HMMRF 모델의 유용성을 입증하기 위하여 1차원 모델과의 성능 비교가 이루어졌으며, 실험 결과 HMMRF 모델이 기존의 1차원 모델이 갖는 한계를 극복할 수 있다는 점에서 오프라인 글씨 인식 분야에서 대표적인 통계적 모델로서 확립될 수 있으리라 판단된다.</P>"
        },
        {
          "rank": 33,
          "score": 0.6598086357116699,
          "doc_id": "JAKO201935164467523",
          "title": "심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구",
          "abstract": "본 논문에서는 구개인두부전증(VeloPharyngeal Insufficiency, VPI) 환자의 음성을 효과적으로 인식하기 위해 컨볼루션 신경망 (Convolutional Neural Network, CNN), 장단기 모델(Long Short Term Memory, LSTM) 구조 신경망을 은닉 마르코프 모델(Hidden Markov Model, HMM)과 결합한 하이브리드 구조의 음성 인식 시스템을 구축하고 모델 적응 기법을 적용하여, 기존 Gaussian Mixture Model(GMM-HMM), 완전 연결형 Deep Neural Network(DNN-HMM) 기반의 음성 인식 시스템과 성능을 비교한다. 정상인 화자가 PBW452단어를 발화한 데이터를 이용하여 초기 모델을 학습하고 정상인 화자의 VPI 모의 음성을 이용하여 화자 적응의 사전 모델을 생성한 후에 VPI 환자들의 음성으로 추가 적응 학습을 진행한다. VPI환자의 화자 적응 시에 CNN-HMM 기반 모델에서는 일부층만 적응 학습하고, LSTM-HMM 기반 모델의 경우에는 드롭 아웃 규제기법을 적용하여 성능을 관찰한 결과 기존 완전 연결형 DNN-HMM 인식기보다 3.68 % 향상된 음성 인식 성능을 나타낸다. 이러한 결과는 본 논문에서 제안하는 LSTM-HMM 기반의 하이브리드 음성 인식 기법이 많은 데이터를 확보하기 어려운 VPI 환자 음성에 대해 보다 향상된 인식률의 음성 인식 시스템을 구축하는데 효과적임을 입증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201935164467523&target=NART&cn=JAKO201935164467523",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 본 논문에서는 구개인두부전증(VeloPharyngeal Insufficiency, VPI) 환자의 음성을 효과적으로 인식하기 위해 컨볼루션 신경망 (Convolutional Neural Network, CNN), 장단기 모델(Long Short Term Memory, LSTM) 구조 신경망을 은닉 마르코프 모델(Hidden Markov Model, HMM)과 결합한 하이브리드 구조의 음성 인식 시스템을 구축하고 모델 적응 기법을 적용하여, 기존 Gaussian Mixture Model(GMM-HMM), 완전 연결형 Deep Neural Network(DNN-HMM) 기반의 음성 인식 시스템과 성능을 비교한다. 정상인 화자가 PBW452단어를 발화한 데이터를 이용하여 초기 모델을 학습하고 정상인 화자의 VPI 모의 음성을 이용하여 화자 적응의 사전 모델을 생성한 후에 VPI 환자들의 음성으로 추가 적응 학습을 진행한다. VPI환자의 화자 적응 시에 CNN-HMM 기반 모델에서는 일부층만 적응 학습하고, LSTM-HMM 기반 모델의 경우에는 드롭 아웃 규제기법을 적용하여 성능을 관찰한 결과 기존 완전 연결형 DNN-HMM 인식기보다 3.68 % 향상된 음성 인식 성능을 나타낸다. 이러한 결과는 본 논문에서 제안하는 LSTM-HMM 기반의 하이브리드 음성 인식 기법이 많은 데이터를 확보하기 어려운 VPI 환자 음성에 대해 보다 향상된 인식률의 음성 인식 시스템을 구축하는데 효과적임을 입증한다."
        },
        {
          "rank": 34,
          "score": 0.6594056487083435,
          "doc_id": "JAKO201415642601987",
          "title": "SNR 매핑을 이용한 환경적응 기반 음성인식",
          "abstract": "다 모델 기반의 음성인식기는 음성인식에서 매우 성공적임이 알려져 있다. 그것은 다양한 신호-대-잡음비(SNR)와 잡음종류에 해당하는 다수의 HMM을 사용함으로서 선택된 음향모델이 인식잡음음성에 매우 근접한 일치성을 가질 수 있기 때문이다. 그러나 실제 사용시에 HMM의 개수가 제한됨에 따라서 음향모델의 불일치는 여전히 문제로 남아 있다. 본 논문에서는 인식잡음음성과 HMM 간의 SNR 불일치를 줄이고자 이들 간의 최적의 SNR 매핑 (mapping)을 실험적으로 결정하였다. 인식잡음음성으로 부터 추정된 SNR 값을 사용하는 대신 제안된 SNR 매핑을 사용함으로서 향상된 인식결과를 얻을 수 있었다. 다 모델 기반인식기에 제안된 방법을 적용하여 Aurora 2 데이터베이스에 대해서 인식 실험한 결과 기존의 MTR 이나 다 모델 기반 음성인식기에 비해서 6.3%와 9.4%의 상대적 단어 오인식율 감소를 이룰 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201415642601987&target=NART&cn=JAKO201415642601987",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SNR 매핑을 이용한 환경적응 기반 음성인식 SNR 매핑을 이용한 환경적응 기반 음성인식 SNR 매핑을 이용한 환경적응 기반 음성인식 다 모델 기반의 음성인식기는 음성인식에서 매우 성공적임이 알려져 있다. 그것은 다양한 신호-대-잡음비(SNR)와 잡음종류에 해당하는 다수의 HMM을 사용함으로서 선택된 음향모델이 인식잡음음성에 매우 근접한 일치성을 가질 수 있기 때문이다. 그러나 실제 사용시에 HMM의 개수가 제한됨에 따라서 음향모델의 불일치는 여전히 문제로 남아 있다. 본 논문에서는 인식잡음음성과 HMM 간의 SNR 불일치를 줄이고자 이들 간의 최적의 SNR 매핑 (mapping)을 실험적으로 결정하였다. 인식잡음음성으로 부터 추정된 SNR 값을 사용하는 대신 제안된 SNR 매핑을 사용함으로서 향상된 인식결과를 얻을 수 있었다. 다 모델 기반인식기에 제안된 방법을 적용하여 Aurora 2 데이터베이스에 대해서 인식 실험한 결과 기존의 MTR 이나 다 모델 기반 음성인식기에 비해서 6.3%와 9.4%의 상대적 단어 오인식율 감소를 이룰 수 있었다."
        },
        {
          "rank": 35,
          "score": 0.6583361625671387,
          "doc_id": "NART19307168",
          "title": "Speech emotion recognition using hidden Markov models",
          "abstract": "<P><B>Abstract</B></P><P>In emotion classification of speech signals, the popular features employed are statistics of fundamental frequency, energy contour, duration of silence and voice quality. However, the performance of systems employing these features degrades substantially when more than two categories of emotion are to be classified. In this paper, a text independent method of emotion classification of speech is proposed. The proposed method makes use of short time log frequency power coefficients (LFPC) to represent the speech signals and a discrete hidden Markov model (HMM) as the classifier. The emotions are classified into six categories. The category labels used are, the archetypal emotions of Anger, Disgust, Fear, Joy, Sadness and Surprise. A database consisting of 60 emotional utterances, each from twelve speakers is constructed and used to train and test the proposed system. Performance of the LFPC feature parameters is compared with that of the linear prediction Cepstral coefficients (LPCC) and mel-frequency Cepstral coefficients (MFCC) feature parameters commonly used in speech recognition systems. Results show that the proposed system yields an average accuracy of 78% and the best accuracy of 96% in the classification of six emotions. This is beyond the 17% chances by a random hit for a sample set of 6 categories. Results also reveal that LFPC is a better choice as feature parameters for emotion classification than the traditional feature parameters.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART19307168&target=NART&cn=NART19307168",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech emotion recognition using hidden Markov models Speech emotion recognition using hidden Markov models Speech emotion recognition using hidden Markov models <P><B>Abstract</B></P><P>In emotion classification of speech signals, the popular features employed are statistics of fundamental frequency, energy contour, duration of silence and voice quality. However, the performance of systems employing these features degrades substantially when more than two categories of emotion are to be classified. In this paper, a text independent method of emotion classification of speech is proposed. The proposed method makes use of short time log frequency power coefficients (LFPC) to represent the speech signals and a discrete hidden Markov model (HMM) as the classifier. The emotions are classified into six categories. The category labels used are, the archetypal emotions of Anger, Disgust, Fear, Joy, Sadness and Surprise. A database consisting of 60 emotional utterances, each from twelve speakers is constructed and used to train and test the proposed system. Performance of the LFPC feature parameters is compared with that of the linear prediction Cepstral coefficients (LPCC) and mel-frequency Cepstral coefficients (MFCC) feature parameters commonly used in speech recognition systems. Results show that the proposed system yields an average accuracy of 78% and the best accuracy of 96% in the classification of six emotions. This is beyond the 17% chances by a random hit for a sample set of 6 categories. Results also reveal that LFPC is a better choice as feature parameters for emotion classification than the traditional feature parameters.</P>"
        },
        {
          "rank": 36,
          "score": 0.65777987241745,
          "doc_id": "JAKO201326952134359",
          "title": "은닉 마르코프 모델을 이용한 동영상 기반 낙상 인식 알고리듬",
          "abstract": "동영상에서 추출한 변수값을 은닉 마르코프 모델(Hidden Markov Model; HMM)에 적용한 새로운 낙상 인식 알고리듬을 제안한다. 개인간 낙상 양식의 차이나 유사 낙상을 실제 낙상과 구분하기 위한 기계 학습 방법으로 HMM알고리듬을 사용하였다. 비디오의 낙상 특징 변수를 얻기 위해 동영상의 광류를 구한 후 이를 주성분 분석 방식에 적용하여 움직임을 정량화하였다. 주성분 분석으로 얻어진 전체 움직임 벡터의 각도, 장단축의 비, 속도등의 조합으로 새로운 여러 종류의 낙상 특징 변수를 정의한 후 이를 HMM에 적용하여 결과를 비교, 분석하였다. 이들 변수들 중에 각도에 의해 얻어진 변수가 가장 좋은 결과를 보여 본 실험에서 91.5%의 민감도(성공 감지율)와 88.01% 의 특이도(실패 감지율)를 나타내었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201326952134359&target=NART&cn=JAKO201326952134359",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마르코프 모델을 이용한 동영상 기반 낙상 인식 알고리듬 은닉 마르코프 모델을 이용한 동영상 기반 낙상 인식 알고리듬 은닉 마르코프 모델을 이용한 동영상 기반 낙상 인식 알고리듬 동영상에서 추출한 변수값을 은닉 마르코프 모델(Hidden Markov Model; HMM)에 적용한 새로운 낙상 인식 알고리듬을 제안한다. 개인간 낙상 양식의 차이나 유사 낙상을 실제 낙상과 구분하기 위한 기계 학습 방법으로 HMM알고리듬을 사용하였다. 비디오의 낙상 특징 변수를 얻기 위해 동영상의 광류를 구한 후 이를 주성분 분석 방식에 적용하여 움직임을 정량화하였다. 주성분 분석으로 얻어진 전체 움직임 벡터의 각도, 장단축의 비, 속도등의 조합으로 새로운 여러 종류의 낙상 특징 변수를 정의한 후 이를 HMM에 적용하여 결과를 비교, 분석하였다. 이들 변수들 중에 각도에 의해 얻어진 변수가 가장 좋은 결과를 보여 본 실험에서 91.5%의 민감도(성공 감지율)와 88.01% 의 특이도(실패 감지율)를 나타내었다."
        },
        {
          "rank": 37,
          "score": 0.6557321548461914,
          "doc_id": "NART20682074",
          "title": "Robust combination of neural networks and hidden Markov models for speech recognition",
          "abstract": "Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the 'divergence problem'-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20682074&target=NART&cn=NART20682074",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Robust combination of neural networks and hidden Markov models for speech recognition Robust combination of neural networks and hidden Markov models for speech recognition Robust combination of neural networks and hidden Markov models for speech recognition Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the 'divergence problem'-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs."
        },
        {
          "rank": 38,
          "score": 0.6528236865997314,
          "doc_id": "ART002455771",
          "title": "Assessment of Dysarthria Using   One-Word Speech Recognition with  Hidden Markov Models",
          "abstract": "Background: The gold standard in dysarthria assessment involves subjective analysis by a speech–language pathologist (SLP). We aimed to investigate the feasibility of dysarthria assessment using automatic speech recognition.Methods: We developed an automatic speech recognition based software to assess dysarthria severity using hidden Markov models (HMMs). Word-specific HMMs were trained using the utterances from one hundred healthy individuals. Twenty-eight patients with dysarthria caused by neurological disorders, including stroke, traumatic brain injury, and Parkinson's disease were participated and their utterances were recorded. The utterances of 37 words from the Assessment of Phonology and Articulation for Children test were recorded in a quiet control booth in both groups. Patients were asked to repeat the recordings for evaluating the test–retest reliability. Patients' utterances were evaluated by two experienced SLPs, and the consonant production accuracy was calculated as a measure of dysarthria severity. The trained HMMs were also employed to evaluate the patients' utterances by calculating the averaged log likelihood (aLL) as the fitness of the spoken word to the word-specific HMM.Results: The consonant production accuracy reported by the SLPs strongly correlated (r = 0.808) with the aLL, and the aLL showed excellent test–retest reliability (intraclass correlation coefficient, 0.964).Conclusion: This leads to the conclusion that dysarthria assessment using a one-word speech recognition system based on word-specific HMMs is feasible in neurological disorders.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002455771&target=NART&cn=ART002455771",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Assessment of Dysarthria Using   One-Word Speech Recognition with  Hidden Markov Models Assessment of Dysarthria Using   One-Word Speech Recognition with  Hidden Markov Models Assessment of Dysarthria Using   One-Word Speech Recognition with  Hidden Markov Models Background: The gold standard in dysarthria assessment involves subjective analysis by a speech–language pathologist (SLP). We aimed to investigate the feasibility of dysarthria assessment using automatic speech recognition.Methods: We developed an automatic speech recognition based software to assess dysarthria severity using hidden Markov models (HMMs). Word-specific HMMs were trained using the utterances from one hundred healthy individuals. Twenty-eight patients with dysarthria caused by neurological disorders, including stroke, traumatic brain injury, and Parkinson's disease were participated and their utterances were recorded. The utterances of 37 words from the Assessment of Phonology and Articulation for Children test were recorded in a quiet control booth in both groups. Patients were asked to repeat the recordings for evaluating the test–retest reliability. Patients' utterances were evaluated by two experienced SLPs, and the consonant production accuracy was calculated as a measure of dysarthria severity. The trained HMMs were also employed to evaluate the patients' utterances by calculating the averaged log likelihood (aLL) as the fitness of the spoken word to the word-specific HMM.Results: The consonant production accuracy reported by the SLPs strongly correlated (r = 0.808) with the aLL, and the aLL showed excellent test–retest reliability (intraclass correlation coefficient, 0.964).Conclusion: This leads to the conclusion that dysarthria assessment using a one-word speech recognition system based on word-specific HMMs is feasible in neurological disorders."
        },
        {
          "rank": 39,
          "score": 0.6517530679702759,
          "doc_id": "NART56157676",
          "title": "온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합",
          "abstract": "<P> 최근에 음성인식 분야에서 널리 사용되고 있는 은닉 마르코프 모델(HMM)을 이용하여 필기문자를 인식하고자 하는 연구가 활발히 진행되고 있다. 하지만, HMM은 시간에 따라서 변하는 입력특성을 잘 처리하는 장점이 있는 반면에, 각 모델을 독립적으로 학습시키는 경우에 각 패턴 사이의 분별력이 다소 떨어지는 문제가 있다. 본 논문에서는 HMM을 통해서 얻어진 각 모델의 내부 출력값을 이용하여 신경망 분류기로 추가적인 분류작업을 수행하는 방법을 제시한다. 또, 온라인 필기 데이타로 숫자와 영문자 대소문자를 인식하는 실험을 통해서 제시된 방법의 유용성을 입증한다.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART56157676&target=NART&cn=NART56157676",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 <P> 최근에 음성인식 분야에서 널리 사용되고 있는 은닉 마르코프 모델(HMM)을 이용하여 필기문자를 인식하고자 하는 연구가 활발히 진행되고 있다. 하지만, HMM은 시간에 따라서 변하는 입력특성을 잘 처리하는 장점이 있는 반면에, 각 모델을 독립적으로 학습시키는 경우에 각 패턴 사이의 분별력이 다소 떨어지는 문제가 있다. 본 논문에서는 HMM을 통해서 얻어진 각 모델의 내부 출력값을 이용하여 신경망 분류기로 추가적인 분류작업을 수행하는 방법을 제시한다. 또, 온라인 필기 데이타로 숫자와 영문자 대소문자를 인식하는 실험을 통해서 제시된 방법의 유용성을 입증한다.</P>"
        },
        {
          "rank": 40,
          "score": 0.6494389772415161,
          "doc_id": "JAKO202123157167812",
          "title": "은닉 마르코프 모델을 이용한 국가별 주가지수 예측",
          "abstract": "은닉 마르코프 모델(hidden Markov model, HMM)은 은닉된 상태와 관찰 가능한 결과의 두 가지 요소로 이루어진 통계적 모형으로 확률론적 접근이 가능하고, 다양한 수학적인 구조를 가지고 있어 여러 분야에서 활발하게 사용되고 있다. 특히 금융 분야의 시계열 데이터에 응용되어 다양한 연구가 진행되고 있다. 본 연구는 HMM 이론을 국내 KOSPI200 주가지수와 더불어 NIKKEI225, HSI, S&P500, FTSE100과 같은 해외 주가지수 예측에 적용해 보고자 한다. 또한, 최근 인공지능 분야의 발전으로 인해 주식 가격 예측에 빈번하게 사용되는 서포트 벡터 회귀(support vector regression, SVR) 결과와 어떤 차이가 있는지 비교하여 살펴보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202123157167812&target=NART&cn=JAKO202123157167812",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마르코프 모델을 이용한 국가별 주가지수 예측 은닉 마르코프 모델을 이용한 국가별 주가지수 예측 은닉 마르코프 모델을 이용한 국가별 주가지수 예측 은닉 마르코프 모델(hidden Markov model, HMM)은 은닉된 상태와 관찰 가능한 결과의 두 가지 요소로 이루어진 통계적 모형으로 확률론적 접근이 가능하고, 다양한 수학적인 구조를 가지고 있어 여러 분야에서 활발하게 사용되고 있다. 특히 금융 분야의 시계열 데이터에 응용되어 다양한 연구가 진행되고 있다. 본 연구는 HMM 이론을 국내 KOSPI200 주가지수와 더불어 NIKKEI225, HSI, S&P500, FTSE100과 같은 해외 주가지수 예측에 적용해 보고자 한다. 또한, 최근 인공지능 분야의 발전으로 인해 주식 가격 예측에 빈번하게 사용되는 서포트 벡터 회귀(support vector regression, SVR) 결과와 어떤 차이가 있는지 비교하여 살펴보고자 한다."
        },
        {
          "rank": 41,
          "score": 0.6484619379043579,
          "doc_id": "JAKO200819858103682",
          "title": "A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments",
          "abstract": "In this paper, a MSS-NOVO method that combines the HMM composition method with a noise reduction method is proposed for speech recognition in noisy environments. This combined method starts with noise reduction with modified spectral subtraction (MSS) to enhance the input noisy speech, then the noise and voice composition (NOVO) method is applied for making noise adapted models by using the noise in the non-utterance regions of the enhanced noisy speech. In order to evaluate the effectiveness of our proposed method, we compare MSS-NOVO method with other methods, i.e., SS-NOVO, MWF-NOVO. To set up the noisy speech for test, we add White noise to KLE 452 database with different SNRs range from 0dB to 15dB, at 5dB intervals. From the tests, MSS-NOVO method shows average improvement of 66.5% and 13.6% compared with the existing SS-NOVO method and MWF-NOVO method, respectively. Especially our proposed MSS-NOVO method shows a big improvement at low SNRs.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200819858103682&target=NART&cn=JAKO200819858103682",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments In this paper, a MSS-NOVO method that combines the HMM composition method with a noise reduction method is proposed for speech recognition in noisy environments. This combined method starts with noise reduction with modified spectral subtraction (MSS) to enhance the input noisy speech, then the noise and voice composition (NOVO) method is applied for making noise adapted models by using the noise in the non-utterance regions of the enhanced noisy speech. In order to evaluate the effectiveness of our proposed method, we compare MSS-NOVO method with other methods, i.e., SS-NOVO, MWF-NOVO. To set up the noisy speech for test, we add White noise to KLE 452 database with different SNRs range from 0dB to 15dB, at 5dB intervals. From the tests, MSS-NOVO method shows average improvement of 66.5% and 13.6% compared with the existing SS-NOVO method and MWF-NOVO method, respectively. Especially our proposed MSS-NOVO method shows a big improvement at low SNRs."
        },
        {
          "rank": 42,
          "score": 0.6478896737098694,
          "doc_id": "NART53843256",
          "title": "Neural development of networks for audiovisual speech comprehension",
          "abstract": "<P><B>Abstract</B></P><P>Everyday conversation is both an auditory and a visual phenomenon. While visual speech information enhances comprehension for the listener, evidence suggests that the ability to benefit from this information improves with development. A number of brain regions have been implicated in audiovisual speech comprehension, but the extent to which the neurobiological substrate in the child compares to the adult is unknown. In particular, developmental differences in the network for audiovisual speech comprehension could manifest through the incorporation of additional brain regions, or through different patterns of effective connectivity. In the present study we used functional magnetic resonance imaging and structural equation modeling (SEM) to characterize the developmental changes in network interactions for audiovisual speech comprehension. The brain response was recorded while children 8- to 11-years-old and adults passively listened to stories under audiovisual (AV) and auditory-only (A) conditions. Results showed that in children and adults, AV comprehension activated the same fronto-temporo-parietal network of regions known for their contribution to speech production and perception. However, the SEM network analysis revealed age-related differences in the functional interactions among these regions. In particular, the influence of the posterior inferior frontal gyrus/ventral premotor cortex on supramarginal gyrus differed across age groups during AV, but not A speech. This functional pathway might be important for relating motor and sensory information used by the listener to identify speech sounds. Further, its development might reflect changes in the mechanisms that relate visual speech information to articulatory speech representations through experience producing and perceiving speech.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART53843256&target=NART&cn=NART53843256",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural development of networks for audiovisual speech comprehension Neural development of networks for audiovisual speech comprehension Neural development of networks for audiovisual speech comprehension <P><B>Abstract</B></P><P>Everyday conversation is both an auditory and a visual phenomenon. While visual speech information enhances comprehension for the listener, evidence suggests that the ability to benefit from this information improves with development. A number of brain regions have been implicated in audiovisual speech comprehension, but the extent to which the neurobiological substrate in the child compares to the adult is unknown. In particular, developmental differences in the network for audiovisual speech comprehension could manifest through the incorporation of additional brain regions, or through different patterns of effective connectivity. In the present study we used functional magnetic resonance imaging and structural equation modeling (SEM) to characterize the developmental changes in network interactions for audiovisual speech comprehension. The brain response was recorded while children 8- to 11-years-old and adults passively listened to stories under audiovisual (AV) and auditory-only (A) conditions. Results showed that in children and adults, AV comprehension activated the same fronto-temporo-parietal network of regions known for their contribution to speech production and perception. However, the SEM network analysis revealed age-related differences in the functional interactions among these regions. In particular, the influence of the posterior inferior frontal gyrus/ventral premotor cortex on supramarginal gyrus differed across age groups during AV, but not A speech. This functional pathway might be important for relating motor and sensory information used by the listener to identify speech sounds. Further, its development might reflect changes in the mechanisms that relate visual speech information to articulatory speech representations through experience producing and perceiving speech.</P>"
        },
        {
          "rank": 43,
          "score": 0.6472734212875366,
          "doc_id": "NART30128358",
          "title": "Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer",
          "abstract": "<P><B>Abstract</B></P><P>Spoken human&ndash;machine interaction in real-world environments requires acoustic models that are robust to changes in acoustic conditions, e.g. presence of noise. Unfortunately, the popular hidden Markov models (HMM) are not noise tolerant. One way to increase recognition performance is to acquire a small adaptation set of noisy utterances, which is used to estimate a normalization mapping between noisy and clean features to be fed into the acoustic model. This paper proposes an unsupervised maximum-likelihood gradient-ascent training algorithm (instead of the usual least squares regression) for a neural feature adaptation module, properly combined with a hybrid connectionist/HMM speech recognizer. The algorithm is inspired by the so-called &ldquo;inversion principle&rdquo;, that prescribes the optimization of the input features instead of the model parameters. Simulation results on a real-world speaker-independent continuous speech corpus of connected Italian digits, corrupted by noise, validate the approach. A small neural net (13 hidden neurons) trained over a single adaptation utterance for one iteration yields a 18.79% relative word error rate (WER) reduction over the bare hybrid, and a 65.10% relative WER reduction over the Gaussian-based HMM.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART30128358&target=NART&cn=NART30128358",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer <P><B>Abstract</B></P><P>Spoken human&ndash;machine interaction in real-world environments requires acoustic models that are robust to changes in acoustic conditions, e.g. presence of noise. Unfortunately, the popular hidden Markov models (HMM) are not noise tolerant. One way to increase recognition performance is to acquire a small adaptation set of noisy utterances, which is used to estimate a normalization mapping between noisy and clean features to be fed into the acoustic model. This paper proposes an unsupervised maximum-likelihood gradient-ascent training algorithm (instead of the usual least squares regression) for a neural feature adaptation module, properly combined with a hybrid connectionist/HMM speech recognizer. The algorithm is inspired by the so-called &ldquo;inversion principle&rdquo;, that prescribes the optimization of the input features instead of the model parameters. Simulation results on a real-world speaker-independent continuous speech corpus of connected Italian digits, corrupted by noise, validate the approach. A small neural net (13 hidden neurons) trained over a single adaptation utterance for one iteration yields a 18.79% relative word error rate (WER) reduction over the bare hybrid, and a 65.10% relative WER reduction over the Gaussian-based HMM.</P>"
        },
        {
          "rank": 44,
          "score": 0.6438335180282593,
          "doc_id": "NPAP12270893",
          "title": "Speech Recognition in Noisy Environments with Convolutional Neural Networks",
          "abstract": "<P>One of the biggest challenges in speech recognition today is its use on a daily basis, in which distortion and noise in the environment are present and hinder the recognition task. In the last thirty years, hundreds of methods for noise-robust recognition were proposed, each with its own advantages and disadvantages. In this paper, the use of convolutional neural networks (CNN) as acoustic models in automatic speech recognition systems (ASR) is proposed as an alternative to the classical recognition methods based on HMM without any noise-robust method applied. The experiment showed that the presented method reduces the equal error rate in word recognition tasks with additive noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12270893&target=NART&cn=NPAP12270893",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech Recognition in Noisy Environments with Convolutional Neural Networks Speech Recognition in Noisy Environments with Convolutional Neural Networks Speech Recognition in Noisy Environments with Convolutional Neural Networks <P>One of the biggest challenges in speech recognition today is its use on a daily basis, in which distortion and noise in the environment are present and hinder the recognition task. In the last thirty years, hundreds of methods for noise-robust recognition were proposed, each with its own advantages and disadvantages. In this paper, the use of convolutional neural networks (CNN) as acoustic models in automatic speech recognition systems (ASR) is proposed as an alternative to the classical recognition methods based on HMM without any noise-robust method applied. The experiment showed that the presented method reduces the equal error rate in word recognition tasks with additive noise.</P>"
        },
        {
          "rank": 45,
          "score": 0.6425334215164185,
          "doc_id": "JAKO201630932328344",
          "title": "가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법",
          "abstract": "실세계 환경의 원거리에서 녹음된 음성은 가산 잡음이나 반향 성분으로 왜곡되기 때문에 음성인식 성능이 현저히 떨어진다. 따라서 음성 전처리 과정은 실세계 환경에서 강인한 음성인식을 위한 필수과정이다. 모델 기반 특징 향상 방법은 전처리 방법 중 하나로 특징 영역 데이터의 적절한 동적 범위(dynamic range)와 차원 수로 인하여 실시간 처리가 가능하고 깨끗한 음성의 선험적 정보를 모델링하기에 용이하다. 또, 인식을 위한 최종 특징 입력에 가까운 단계에서 데이터를 처리하므로 인식에 밀접한 영향을 준다는 장점이 있다. 그러나 대략적인 왜곡 요인 관련 파라미터 추정 때문에 음성인식 성능이 하락되는 단점이 있다. 최근에 기존 모델 기반 특징 향상의 단점을 개선하여 가산 잡음이나 반향 환경에 적합한 방법이 제안되었다. 이글에서는 특징 향상 방법을 소개하고 개선된 방법의 음성인식 강인성을 알아보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201630932328344&target=NART&cn=JAKO201630932328344",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법 가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법 가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법 실세계 환경의 원거리에서 녹음된 음성은 가산 잡음이나 반향 성분으로 왜곡되기 때문에 음성인식 성능이 현저히 떨어진다. 따라서 음성 전처리 과정은 실세계 환경에서 강인한 음성인식을 위한 필수과정이다. 모델 기반 특징 향상 방법은 전처리 방법 중 하나로 특징 영역 데이터의 적절한 동적 범위(dynamic range)와 차원 수로 인하여 실시간 처리가 가능하고 깨끗한 음성의 선험적 정보를 모델링하기에 용이하다. 또, 인식을 위한 최종 특징 입력에 가까운 단계에서 데이터를 처리하므로 인식에 밀접한 영향을 준다는 장점이 있다. 그러나 대략적인 왜곡 요인 관련 파라미터 추정 때문에 음성인식 성능이 하락되는 단점이 있다. 최근에 기존 모델 기반 특징 향상의 단점을 개선하여 가산 잡음이나 반향 환경에 적합한 방법이 제안되었다. 이글에서는 특징 향상 방법을 소개하고 개선된 방법의 음성인식 강인성을 알아보고자 한다."
        },
        {
          "rank": 46,
          "score": 0.6407412886619568,
          "doc_id": "NART02614282",
          "title": "Probabilistic Independence Networks for Hidden Markov Probability Models",
          "abstract": "<P>Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas, including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper presents a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART02614282&target=NART&cn=NART02614282",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Probabilistic Independence Networks for Hidden Markov Probability Models Probabilistic Independence Networks for Hidden Markov Probability Models Probabilistic Independence Networks for Hidden Markov Probability Models <P>Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas, including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper presents a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach.</P>"
        },
        {
          "rank": 47,
          "score": 0.6390783786773682,
          "doc_id": "NART56157876",
          "title": "한국어 단어범주예측을 위한 은닉마르코프 모델과 신경망의 결합",
          "abstract": "<P> 본 논문에서는 일반적인 한국어 텍스트 문장에서 단어범주를 예측하기 위하여 HMM(Hidden Markov Model)과 신경망을 결합한 모델을 제안하였다.  한국어의 단어범주를 품사와 조사의 격 및 형에 따라 33개로 분류하였으며, 분류된 단어범주를 이용하여 국민학교 교과서를 대상으로 텍스트 데이타베이스를 구성하였다. 기존의 단어범주예측을 위해 제안된 NETgram은 동적, 정적 특징을 모두 신경망으로 표현하지만 본 논문에서는 시간에 따른 동적 변화를 잘 표현해주는 HMM을 신경망과 결합하는 방법을 제안하였다. 임의의 한국어 텍스트 문장으로 실험한 결과 4-gram 예측에서 종료상태가 고정되지 않고 6개의 관찰확률을 가지며 입력관찰열을 2번 순환반복으로 훈련시킨 HMM 모델과 신경망을 결합한 모델이 가장 우수하여 단어범주예측률이 20.22%를 차지하였다. 이것은 기존의 NETgram을 이용한 방식에 비하여 2.14% 향상된 것이다.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART56157876&target=NART&cn=NART56157876",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "한국어 단어범주예측을 위한 은닉마르코프 모델과 신경망의 결합 한국어 단어범주예측을 위한 은닉마르코프 모델과 신경망의 결합 한국어 단어범주예측을 위한 은닉마르코프 모델과 신경망의 결합 <P> 본 논문에서는 일반적인 한국어 텍스트 문장에서 단어범주를 예측하기 위하여 HMM(Hidden Markov Model)과 신경망을 결합한 모델을 제안하였다.  한국어의 단어범주를 품사와 조사의 격 및 형에 따라 33개로 분류하였으며, 분류된 단어범주를 이용하여 국민학교 교과서를 대상으로 텍스트 데이타베이스를 구성하였다. 기존의 단어범주예측을 위해 제안된 NETgram은 동적, 정적 특징을 모두 신경망으로 표현하지만 본 논문에서는 시간에 따른 동적 변화를 잘 표현해주는 HMM을 신경망과 결합하는 방법을 제안하였다. 임의의 한국어 텍스트 문장으로 실험한 결과 4-gram 예측에서 종료상태가 고정되지 않고 6개의 관찰확률을 가지며 입력관찰열을 2번 순환반복으로 훈련시킨 HMM 모델과 신경망을 결합한 모델이 가장 우수하여 단어범주예측률이 20.22%를 차지하였다. 이것은 기존의 NETgram을 이용한 방식에 비하여 2.14% 향상된 것이다.</P>"
        },
        {
          "rank": 48,
          "score": 0.6389330625534058,
          "doc_id": "JAKO202029462558904",
          "title": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식",
          "abstract": "특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202029462558904&target=NART&cn=JAKO202029462558904",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다."
        },
        {
          "rank": 49,
          "score": 0.6378833055496216,
          "doc_id": "NART56158825",
          "title": "은닉 마르코프 메쉬 랜덤 필드 모델을 위한 모수 추정기법",
          "abstract": "<P> 최근들어, 1차원 은닉 마르코프 모델(Hidden Markov Model: HMM)을 2차원 모델로 확장 하려는 연구가 시도되고 있다. 그러나, 이러한 연구 노력은 안정된 2차원 모델을 확립하는데 있어서의 어려움과 모델 자체가 갖는 계산 복잡도로 인해 완전 연결된 진정한 2차원 모델이 아닌 축소된 형태의 연결 구조를 갖는 의사 2차원 모델로 확장하는데 그치고 있다. 본 논문에서는 영상이 3차 마르코프 메쉬 랜덤 필드(Markov Mesh Random Field: MMRF)에 의해 표현될 수 있다는 가정하에 영상의 모델링과 인식을 위한 새로운 통계적 모델인 은닉 마르코프 메쉬 랜덤 필드(Hidden Markov Mesh Random Field HMMRF) 모델을 제안하고, 이를 위한 효과적인 모수 추정 기법을 개발한다.  제안된 모수 추정 기법은 HMMRF 모델에 대한 최대 주변 사후 확률(a maximum, marginal a posteriori probability) 기준에 기반을 둔 &quot;look-ahead&quot; 기법을 확장, 이용함으로써 모수를 추정하는 재귀적 기법이다. HMMRF 모델에 대한 제안된 모수 추정 기법이 실세계 문제에 적용 가능한가를 입중하기 위하여 오프라인 글씨 인식 문제에 실험한 결과, 많은 변형을 갖는 글씨 데이타의 모델링 및 인식에 유용하게 사용될 수 있음을 확인할 수 있었다.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART56158825&target=NART&cn=NART56158825",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마르코프 메쉬 랜덤 필드 모델을 위한 모수 추정기법 은닉 마르코프 메쉬 랜덤 필드 모델을 위한 모수 추정기법 은닉 마르코프 메쉬 랜덤 필드 모델을 위한 모수 추정기법 <P> 최근들어, 1차원 은닉 마르코프 모델(Hidden Markov Model: HMM)을 2차원 모델로 확장 하려는 연구가 시도되고 있다. 그러나, 이러한 연구 노력은 안정된 2차원 모델을 확립하는데 있어서의 어려움과 모델 자체가 갖는 계산 복잡도로 인해 완전 연결된 진정한 2차원 모델이 아닌 축소된 형태의 연결 구조를 갖는 의사 2차원 모델로 확장하는데 그치고 있다. 본 논문에서는 영상이 3차 마르코프 메쉬 랜덤 필드(Markov Mesh Random Field: MMRF)에 의해 표현될 수 있다는 가정하에 영상의 모델링과 인식을 위한 새로운 통계적 모델인 은닉 마르코프 메쉬 랜덤 필드(Hidden Markov Mesh Random Field HMMRF) 모델을 제안하고, 이를 위한 효과적인 모수 추정 기법을 개발한다.  제안된 모수 추정 기법은 HMMRF 모델에 대한 최대 주변 사후 확률(a maximum, marginal a posteriori probability) 기준에 기반을 둔 &quot;look-ahead&quot; 기법을 확장, 이용함으로써 모수를 추정하는 재귀적 기법이다. HMMRF 모델에 대한 제안된 모수 추정 기법이 실세계 문제에 적용 가능한가를 입중하기 위하여 오프라인 글씨 인식 문제에 실험한 결과, 많은 변형을 갖는 글씨 데이타의 모델링 및 인식에 유용하게 사용될 수 있음을 확인할 수 있었다.</P>"
        },
        {
          "rank": 50,
          "score": 0.6365941166877747,
          "doc_id": "ATN0026947402",
          "title": "서포트 벡터 머신과 퍼지 클러스터링 기법을 이용한 오디오 분할 및 분류",
          "abstract": "The rapid increase of information imposes new demands of content management. The purpose of automatic audio segmentation and classification is to meet the rising need for efficient content management. With this reason, this paper proposes a high-accuracy algorithm that segments audio signals and classifies them into different classes such as speech, music, silence, and environment sounds. The proposed algorithm utilizes support vector machine (SVM) to detect audio-cuts, which are boundaries between different kinds of sounds using the parameter sequence. We then extract feature vectors that are composed of statistical data and they are used as an input of fuzzy c-means (FCM) classifier to partition audio-segments into different classes. To evaluate segmentation and classification performance of the proposed SVM-FCM based algorithm, we consider precision and recall rates for segmentation and classification accuracy for classification. Furthermore, we compare the proposed algorithm with other methods including binary and FCM classifiers in terms of segmentation performance. Experimental results show that the proposed algorithm outperforms other methods in both precision and recall rates.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0026947402&target=NART&cn=ATN0026947402",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "서포트 벡터 머신과 퍼지 클러스터링 기법을 이용한 오디오 분할 및 분류 서포트 벡터 머신과 퍼지 클러스터링 기법을 이용한 오디오 분할 및 분류 서포트 벡터 머신과 퍼지 클러스터링 기법을 이용한 오디오 분할 및 분류 The rapid increase of information imposes new demands of content management. The purpose of automatic audio segmentation and classification is to meet the rising need for efficient content management. With this reason, this paper proposes a high-accuracy algorithm that segments audio signals and classifies them into different classes such as speech, music, silence, and environment sounds. The proposed algorithm utilizes support vector machine (SVM) to detect audio-cuts, which are boundaries between different kinds of sounds using the parameter sequence. We then extract feature vectors that are composed of statistical data and they are used as an input of fuzzy c-means (FCM) classifier to partition audio-segments into different classes. To evaluate segmentation and classification performance of the proposed SVM-FCM based algorithm, we consider precision and recall rates for segmentation and classification accuracy for classification. Furthermore, we compare the proposed algorithm with other methods including binary and FCM classifiers in terms of segmentation performance. Experimental results show that the proposed algorithm outperforms other methods in both precision and recall rates."
        }
      ]
    },
    {
      "query": "What is the function of neural networks (NNs) in audiovisual speech recognition?",
      "query_meta": {
        "type": "single_hop",
        "index": 1
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.7729944586753845,
          "doc_id": "JAKO200411922338894",
          "title": "신경망 기반 음성, 영상 및 문맥 통합 음성인식",
          "abstract": "최근 잡음환경에서 신뢰도 높은 음성인식을 위해 음성정보와 영상정보를 융합하는 방법이 활발히 연구되고 있다. 본 논문에서는 이절적인 정보의 융합에 적합한 신경망 모델을 기반으로 음성, 영상 및 문맥 정보 등 다양한 정보를 융합하여 잡음 환경에서 고려단어를 인식하는 음성인식 기법에 대하여 기술한다. 음성과 영상 특징을 이용한 이중 모드 신경망 BMNN(BiModal Neural Network)을 제안한다. BMM은 4개 층으로 이루어진 다층퍼셉트론의 구조를 가지며 각 층은 입력 특징의 추상화 기능을 수행한다. BMNN에서는 제 3층이 잡음에 의한 음성 정보의 손실을 보상하기 위하여 음성과 영상 특징을 통합하는 기능을 수행한다. 또한, 잡음환경에서 음성 인식률을 향상시키기 위해 사용자가 말한 단어들의 순차 패턴을 나타내는 문맥정보를 이용한 후처리 방법을 제안한다. 잡음환경에서 BMNN은 단순히 음성만을 사용한 것 보다 높은 성능을 보임으로써 그 타당성을 확인할 수 있을 뿐 아니라, 특히 문맥을 이용한 후처리를 하였을 경우 잡음 환경에서 90%이상의 인식률을 달성하였다 본 연구는 잡음환경에서 강인한 음성인식을 위해 다양한 추가 정보를 사용함으로써 성능을 향상시킬 수 있음을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200411922338894&target=NART&cn=JAKO200411922338894",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망 기반 음성, 영상 및 문맥 통합 음성인식 신경망 기반 음성, 영상 및 문맥 통합 음성인식 신경망 기반 음성, 영상 및 문맥 통합 음성인식 최근 잡음환경에서 신뢰도 높은 음성인식을 위해 음성정보와 영상정보를 융합하는 방법이 활발히 연구되고 있다. 본 논문에서는 이절적인 정보의 융합에 적합한 신경망 모델을 기반으로 음성, 영상 및 문맥 정보 등 다양한 정보를 융합하여 잡음 환경에서 고려단어를 인식하는 음성인식 기법에 대하여 기술한다. 음성과 영상 특징을 이용한 이중 모드 신경망 BMNN(BiModal Neural Network)을 제안한다. BMM은 4개 층으로 이루어진 다층퍼셉트론의 구조를 가지며 각 층은 입력 특징의 추상화 기능을 수행한다. BMNN에서는 제 3층이 잡음에 의한 음성 정보의 손실을 보상하기 위하여 음성과 영상 특징을 통합하는 기능을 수행한다. 또한, 잡음환경에서 음성 인식률을 향상시키기 위해 사용자가 말한 단어들의 순차 패턴을 나타내는 문맥정보를 이용한 후처리 방법을 제안한다. 잡음환경에서 BMNN은 단순히 음성만을 사용한 것 보다 높은 성능을 보임으로써 그 타당성을 확인할 수 있을 뿐 아니라, 특히 문맥을 이용한 후처리를 하였을 경우 잡음 환경에서 90%이상의 인식률을 달성하였다 본 연구는 잡음환경에서 강인한 음성인식을 위해 다양한 추가 정보를 사용함으로써 성능을 향상시킬 수 있음을 제시한다."
        },
        {
          "rank": 2,
          "score": 0.7560111284255981,
          "doc_id": "NART16453920",
          "title": "Neural-network-based HMM adaptation for noisy speech recognition.",
          "abstract": "<P>This paper proposes a new method, using neural networks, of adapting phone HMMs to noisy speech. The neural networks are designed to map clean speech HMMs to noise-adapted HMMs, using noise HMMs and signal-to-noise ratios (SNRs) as inputs. The neural network is trained by minimizing the mean square error between the output HMMs and the target noise-adapted HMMs. In an evaluation, the proposed method was used to recognize noisy broadcast-news speech in speaker-dependent and speaker-independent modes. The trained networks were found to be effective in recognizing new speakers under new noise and various SNR conditions.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART16453920&target=NART&cn=NART16453920",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural-network-based HMM adaptation for noisy speech recognition. Neural-network-based HMM adaptation for noisy speech recognition. Neural-network-based HMM adaptation for noisy speech recognition. <P>This paper proposes a new method, using neural networks, of adapting phone HMMs to noisy speech. The neural networks are designed to map clean speech HMMs to noise-adapted HMMs, using noise HMMs and signal-to-noise ratios (SNRs) as inputs. The neural network is trained by minimizing the mean square error between the output HMMs and the target noise-adapted HMMs. In an evaluation, the proposed method was used to recognize noisy broadcast-news speech in speaker-dependent and speaker-independent modes. The trained networks were found to be effective in recognizing new speakers under new noise and various SNR conditions.</P>"
        },
        {
          "rank": 3,
          "score": 0.7531344294548035,
          "doc_id": "JAKO200211921444549",
          "title": "2층 구조의 입체 시각형 신경망 기반 음소인식",
          "abstract": "본 연구는 입체 시각을 위한 신경망에 대한 연구 결과로서 인간의 음성을 인식하는데 적용된다. 입체 시각신경망(SVNN)에 기반한 음성인식에서, 먼저 입력된 음성 신호를 표준 모델과 비교함으로써 유사성이 얻어진다. 이 값들은 다이나믹한 처리 과정으로 주어지고 이웃한 신경소자들 사이에서 경쟁적이고 협력적인 처리를 거치게 된다. 이러한 다이나믹한 처리과정을 통해 단 하나의 가장 우수한 신경세포(winner neuron)만이 최후에 검출된다. 비교연구에서 2층 구조의 SVNN은 HMM 인식기보다 인식정확도 측면에서 7.7% 더 높았다. 평가 결과. SVNN은 기손리 HMM 인식기 성능을 능가하는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200211921444549&target=NART&cn=JAKO200211921444549",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "2층 구조의 입체 시각형 신경망 기반 음소인식 2층 구조의 입체 시각형 신경망 기반 음소인식 2층 구조의 입체 시각형 신경망 기반 음소인식 본 연구는 입체 시각을 위한 신경망에 대한 연구 결과로서 인간의 음성을 인식하는데 적용된다. 입체 시각신경망(SVNN)에 기반한 음성인식에서, 먼저 입력된 음성 신호를 표준 모델과 비교함으로써 유사성이 얻어진다. 이 값들은 다이나믹한 처리 과정으로 주어지고 이웃한 신경소자들 사이에서 경쟁적이고 협력적인 처리를 거치게 된다. 이러한 다이나믹한 처리과정을 통해 단 하나의 가장 우수한 신경세포(winner neuron)만이 최후에 검출된다. 비교연구에서 2층 구조의 SVNN은 HMM 인식기보다 인식정확도 측면에서 7.7% 더 높았다. 평가 결과. SVNN은 기손리 HMM 인식기 성능을 능가하는 것으로 나타났다."
        },
        {
          "rank": 4,
          "score": 0.7369182109832764,
          "doc_id": "NPAP00072266",
          "title": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models",
          "abstract": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP00072266&target=NART&cn=NPAP00072266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error."
        },
        {
          "rank": 5,
          "score": 0.7333389520645142,
          "doc_id": "NPAP12270893",
          "title": "Speech Recognition in Noisy Environments with Convolutional Neural Networks",
          "abstract": "<P>One of the biggest challenges in speech recognition today is its use on a daily basis, in which distortion and noise in the environment are present and hinder the recognition task. In the last thirty years, hundreds of methods for noise-robust recognition were proposed, each with its own advantages and disadvantages. In this paper, the use of convolutional neural networks (CNN) as acoustic models in automatic speech recognition systems (ASR) is proposed as an alternative to the classical recognition methods based on HMM without any noise-robust method applied. The experiment showed that the presented method reduces the equal error rate in word recognition tasks with additive noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12270893&target=NART&cn=NPAP12270893",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech Recognition in Noisy Environments with Convolutional Neural Networks Speech Recognition in Noisy Environments with Convolutional Neural Networks Speech Recognition in Noisy Environments with Convolutional Neural Networks <P>One of the biggest challenges in speech recognition today is its use on a daily basis, in which distortion and noise in the environment are present and hinder the recognition task. In the last thirty years, hundreds of methods for noise-robust recognition were proposed, each with its own advantages and disadvantages. In this paper, the use of convolutional neural networks (CNN) as acoustic models in automatic speech recognition systems (ASR) is proposed as an alternative to the classical recognition methods based on HMM without any noise-robust method applied. The experiment showed that the presented method reduces the equal error rate in word recognition tasks with additive noise.</P>"
        },
        {
          "rank": 6,
          "score": 0.7254548668861389,
          "doc_id": "DIKO0011930560",
          "title": "시청각 음성인식을 위한 새로운 통합방법",
          "abstract": "The automatic speech recognition (ASR) is one of the most interesting problems applied to human computer interaction applications; for example, spoken digit recognition for mobile environments. One of the challenges of this problem is that the accuracy of speech recognition will be decrease much if the speaker talks under noisy place such as: restaurant, subway, street… The invention of lip-reading opens a potential chance to improve the performance of recognition. Indeed, human perception considers both auditory and visual nature of speech. The speech recognition system will be more intelligible if the lip motion of speaker is available together with acoustic signal. The combined audio visual speech recognition has been proved to be able enhance the overall performance of recognition, especially under noisy environment. In general, if the two streams are available for speech recognition, they can be integrated by two ways: early integration and late integration. The early integration approach combines the features of two streams into one concatenated feature vector, and uses single classifier for recognition. The late integration approach combines the results of two separate classifiers for recognition in which the reliability of modalities is applied to summation based fusion. The late integration method shows the better performance actually through many experiments. There are several factors are considered to measure reliability including word confusability, SNR level of acoustic signal, the noise type, and illumination change in visual stream rather than the only SNR level based confidence of conventional audio visual speech recognition. In this study, we propose an effective fusion scheme for audio visual speech recognition (AVSR) in which the appropriate combination weights are measured by using an integrated reliability. The significant idea of integrated reliability is the combination of not only acoustic noise but also model confusability for audio visual reliability measurement The experimental results using Samsung AVSR database shows the improved performance of our approach compared to conventional ones. This demonstrates the effectiveness and feasibility of this invention for real speech recognition applications.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011930560&target=NART&cn=DIKO0011930560",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시청각 음성인식을 위한 새로운 통합방법 시청각 음성인식을 위한 새로운 통합방법 시청각 음성인식을 위한 새로운 통합방법 The automatic speech recognition (ASR) is one of the most interesting problems applied to human computer interaction applications; for example, spoken digit recognition for mobile environments. One of the challenges of this problem is that the accuracy of speech recognition will be decrease much if the speaker talks under noisy place such as: restaurant, subway, street… The invention of lip-reading opens a potential chance to improve the performance of recognition. Indeed, human perception considers both auditory and visual nature of speech. The speech recognition system will be more intelligible if the lip motion of speaker is available together with acoustic signal. The combined audio visual speech recognition has been proved to be able enhance the overall performance of recognition, especially under noisy environment. In general, if the two streams are available for speech recognition, they can be integrated by two ways: early integration and late integration. The early integration approach combines the features of two streams into one concatenated feature vector, and uses single classifier for recognition. The late integration approach combines the results of two separate classifiers for recognition in which the reliability of modalities is applied to summation based fusion. The late integration method shows the better performance actually through many experiments. There are several factors are considered to measure reliability including word confusability, SNR level of acoustic signal, the noise type, and illumination change in visual stream rather than the only SNR level based confidence of conventional audio visual speech recognition. In this study, we propose an effective fusion scheme for audio visual speech recognition (AVSR) in which the appropriate combination weights are measured by using an integrated reliability. The significant idea of integrated reliability is the combination of not only acoustic noise but also model confusability for audio visual reliability measurement The experimental results using Samsung AVSR database shows the improved performance of our approach compared to conventional ones. This demonstrates the effectiveness and feasibility of this invention for real speech recognition applications."
        },
        {
          "rank": 7,
          "score": 0.7237687110900879,
          "doc_id": "NART18014750",
          "title": "Neural nets and hidden Markov models: Review and generalizations",
          "abstract": "Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs).",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART18014750&target=NART&cn=NART18014750",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural nets and hidden Markov models: Review and generalizations Neural nets and hidden Markov models: Review and generalizations Neural nets and hidden Markov models: Review and generalizations Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs)."
        },
        {
          "rank": 8,
          "score": 0.7173045873641968,
          "doc_id": "NART53843256",
          "title": "Neural development of networks for audiovisual speech comprehension",
          "abstract": "<P><B>Abstract</B></P><P>Everyday conversation is both an auditory and a visual phenomenon. While visual speech information enhances comprehension for the listener, evidence suggests that the ability to benefit from this information improves with development. A number of brain regions have been implicated in audiovisual speech comprehension, but the extent to which the neurobiological substrate in the child compares to the adult is unknown. In particular, developmental differences in the network for audiovisual speech comprehension could manifest through the incorporation of additional brain regions, or through different patterns of effective connectivity. In the present study we used functional magnetic resonance imaging and structural equation modeling (SEM) to characterize the developmental changes in network interactions for audiovisual speech comprehension. The brain response was recorded while children 8- to 11-years-old and adults passively listened to stories under audiovisual (AV) and auditory-only (A) conditions. Results showed that in children and adults, AV comprehension activated the same fronto-temporo-parietal network of regions known for their contribution to speech production and perception. However, the SEM network analysis revealed age-related differences in the functional interactions among these regions. In particular, the influence of the posterior inferior frontal gyrus/ventral premotor cortex on supramarginal gyrus differed across age groups during AV, but not A speech. This functional pathway might be important for relating motor and sensory information used by the listener to identify speech sounds. Further, its development might reflect changes in the mechanisms that relate visual speech information to articulatory speech representations through experience producing and perceiving speech.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART53843256&target=NART&cn=NART53843256",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural development of networks for audiovisual speech comprehension Neural development of networks for audiovisual speech comprehension Neural development of networks for audiovisual speech comprehension <P><B>Abstract</B></P><P>Everyday conversation is both an auditory and a visual phenomenon. While visual speech information enhances comprehension for the listener, evidence suggests that the ability to benefit from this information improves with development. A number of brain regions have been implicated in audiovisual speech comprehension, but the extent to which the neurobiological substrate in the child compares to the adult is unknown. In particular, developmental differences in the network for audiovisual speech comprehension could manifest through the incorporation of additional brain regions, or through different patterns of effective connectivity. In the present study we used functional magnetic resonance imaging and structural equation modeling (SEM) to characterize the developmental changes in network interactions for audiovisual speech comprehension. The brain response was recorded while children 8- to 11-years-old and adults passively listened to stories under audiovisual (AV) and auditory-only (A) conditions. Results showed that in children and adults, AV comprehension activated the same fronto-temporo-parietal network of regions known for their contribution to speech production and perception. However, the SEM network analysis revealed age-related differences in the functional interactions among these regions. In particular, the influence of the posterior inferior frontal gyrus/ventral premotor cortex on supramarginal gyrus differed across age groups during AV, but not A speech. This functional pathway might be important for relating motor and sensory information used by the listener to identify speech sounds. Further, its development might reflect changes in the mechanisms that relate visual speech information to articulatory speech representations through experience producing and perceiving speech.</P>"
        },
        {
          "rank": 9,
          "score": 0.7151015996932983,
          "doc_id": "DIKO0011019580",
          "title": "시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합",
          "abstract": "음성인식은 인간과 기계의 인터페이스 서비스를 위한 중요한 기술이다. 현재까지 개발된 많은 음성인식 시스템은 제어된 환경에서는 높은 인식율을 보이지만, 잡음 환경에서는 성능이 크게 저하되는 한계가 있다. 이러한 한계를 극복하기 위한 방법의 하나인 시청각 음성인식은 잡음이 존재하는 환경에서 강인한 인식을 위해 마이크로 녹음한 음성신호 (목소리)와 카메라로 기록한 영상신호(입술의 움직임)를 모두 이용하여 음성을 인식하는 기술이다. 영상신호를 이용한 인식은 잡음이 적은 상황에서는 기존의 음성인식에 비해 낮은 인식 성능을 보이지만 소리잡음에 영향을 받지 않기 때문에 잡음 환경에서 음성인식을 보완하는 유용한 방법이 된다. 본 논문에서는 시청각 음성인식 시스템을 구성하는 청각신호를 이용한 인식, 시각정보를 이용한 인식, 그리고 두 정보의 통합 등의 세 부분을 고려하여 각 부분의 성능을 향상시킴으로써 잡음환경에서의 강인함을 향상시키고자 한다. 첫째, 시각정보를 이용한 인식의 성능을 향상시키기 위해 인식기인 은닉 마르코프 모델(hidden Markov model)의 확률적인 최적화 알고리즘을 제안한다. 알고리즘의 성능과 수렴속도를 개선하기 위해 확률적인 최적화 알고리즘의 하나인 모의 담금질 기법(simulated annealing)과 지역 탐색 연산자를 결합한 하이브리드 모의 담금질 기법을 개발한다. 기존의 은닉 마르코프 모델의 학습 알고리즘인 기대-최대(expectation-maximization) 알고리즘은 가능도 (likelihood) 함수에 대해 지역 최적화만을 수행하는 반면, 제안하는 알고리즘은 전 영역에서 탐색을 수행하며 결과적으로 은닉 마르코프 모델의 인식 성능을 높인다. 제안하는 알고리즘이 전역최적해에 확률로써 수렴하는 것을 수학적으로 증명한다. 둘째, 청각정보를 이용한 인식을 강인하게 하기 위해 은닉 마르코프 모델에서 관측 프레임간의 상관관계를 모델링하는 기법을 개발한다. 음성의 동적 특성은 사람이 잡음환경에서 강인한 인식 성능을 보이는데 도움을 주는 것으로 알려져 있으나 기존의 은닉 마르코프 모델을 이용한 음성 모델링에서는 충분히 고려되지 않고 있다. 본 논문에서는 가우시안 혼합 모델(Gaussian mixture model)로 서로 다른 프레임의 결합 확률분포를 모델링하여 프레임간의 조건부 의존관계를 다루는 기법을 제안한다. 또한, 기대-최대 알고리즘에 기반하여 제안하는 모델의 학습 알고리즘을 개발한다. 제안하는 모델을 이용함으로써 청각 정보를 이용한 인식에서 잡음에 더욱 강인한 성능을 얻도록 한다. 셋째, 시각정보와 청각정보의 상호보완성을 효과적으로 이용하여 잡음에 강인한 최종 인식결과를 얻기 위해 정보 통합 단계에서 신경회로망을 이용하는 기법을 제안한다. 정보 통합 단계에서는 시각정보와 청각정보를 각각 따로 인식한 결과를 가중치 기법을 통해 최종 결과를 얻는데, 학습된 신경회로망은 잡음의 종류와 수준을 알 수 없는 주어진 시청각 데이터에 대해 적절한 가중치를 출력함으로써 최적의 인식결과를 얻도록 한다. 이를 통해 통합 인식 결과가 시각 또는 청각정보만을 이용한 인식결과보다 최소한 같거나 좋을 뿐 아니라 두 정보에 의한 시너지 효과를 최대화하도록 한다. 제안하는 시스템을 화자독립 고립단어 인식문제에 적용하여 그 성능을 보인다. 실험 결과 제안하는 시스템이 기존의 시스템에 비해 다양한 잡음 환경에서 더욱 강인한 성능을 보이는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011019580&target=NART&cn=DIKO0011019580",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 음성인식은 인간과 기계의 인터페이스 서비스를 위한 중요한 기술이다. 현재까지 개발된 많은 음성인식 시스템은 제어된 환경에서는 높은 인식율을 보이지만, 잡음 환경에서는 성능이 크게 저하되는 한계가 있다. 이러한 한계를 극복하기 위한 방법의 하나인 시청각 음성인식은 잡음이 존재하는 환경에서 강인한 인식을 위해 마이크로 녹음한 음성신호 (목소리)와 카메라로 기록한 영상신호(입술의 움직임)를 모두 이용하여 음성을 인식하는 기술이다. 영상신호를 이용한 인식은 잡음이 적은 상황에서는 기존의 음성인식에 비해 낮은 인식 성능을 보이지만 소리잡음에 영향을 받지 않기 때문에 잡음 환경에서 음성인식을 보완하는 유용한 방법이 된다. 본 논문에서는 시청각 음성인식 시스템을 구성하는 청각신호를 이용한 인식, 시각정보를 이용한 인식, 그리고 두 정보의 통합 등의 세 부분을 고려하여 각 부분의 성능을 향상시킴으로써 잡음환경에서의 강인함을 향상시키고자 한다. 첫째, 시각정보를 이용한 인식의 성능을 향상시키기 위해 인식기인 은닉 마르코프 모델(hidden Markov model)의 확률적인 최적화 알고리즘을 제안한다. 알고리즘의 성능과 수렴속도를 개선하기 위해 확률적인 최적화 알고리즘의 하나인 모의 담금질 기법(simulated annealing)과 지역 탐색 연산자를 결합한 하이브리드 모의 담금질 기법을 개발한다. 기존의 은닉 마르코프 모델의 학습 알고리즘인 기대-최대(expectation-maximization) 알고리즘은 가능도 (likelihood) 함수에 대해 지역 최적화만을 수행하는 반면, 제안하는 알고리즘은 전 영역에서 탐색을 수행하며 결과적으로 은닉 마르코프 모델의 인식 성능을 높인다. 제안하는 알고리즘이 전역최적해에 확률로써 수렴하는 것을 수학적으로 증명한다. 둘째, 청각정보를 이용한 인식을 강인하게 하기 위해 은닉 마르코프 모델에서 관측 프레임간의 상관관계를 모델링하는 기법을 개발한다. 음성의 동적 특성은 사람이 잡음환경에서 강인한 인식 성능을 보이는데 도움을 주는 것으로 알려져 있으나 기존의 은닉 마르코프 모델을 이용한 음성 모델링에서는 충분히 고려되지 않고 있다. 본 논문에서는 가우시안 혼합 모델(Gaussian mixture model)로 서로 다른 프레임의 결합 확률분포를 모델링하여 프레임간의 조건부 의존관계를 다루는 기법을 제안한다. 또한, 기대-최대 알고리즘에 기반하여 제안하는 모델의 학습 알고리즘을 개발한다. 제안하는 모델을 이용함으로써 청각 정보를 이용한 인식에서 잡음에 더욱 강인한 성능을 얻도록 한다. 셋째, 시각정보와 청각정보의 상호보완성을 효과적으로 이용하여 잡음에 강인한 최종 인식결과를 얻기 위해 정보 통합 단계에서 신경회로망을 이용하는 기법을 제안한다. 정보 통합 단계에서는 시각정보와 청각정보를 각각 따로 인식한 결과를 가중치 기법을 통해 최종 결과를 얻는데, 학습된 신경회로망은 잡음의 종류와 수준을 알 수 없는 주어진 시청각 데이터에 대해 적절한 가중치를 출력함으로써 최적의 인식결과를 얻도록 한다. 이를 통해 통합 인식 결과가 시각 또는 청각정보만을 이용한 인식결과보다 최소한 같거나 좋을 뿐 아니라 두 정보에 의한 시너지 효과를 최대화하도록 한다. 제안하는 시스템을 화자독립 고립단어 인식문제에 적용하여 그 성능을 보인다. 실험 결과 제안하는 시스템이 기존의 시스템에 비해 다양한 잡음 환경에서 더욱 강인한 성능을 보이는 것을 확인한다."
        },
        {
          "rank": 10,
          "score": 0.7034012675285339,
          "doc_id": "ATN0053123228",
          "title": "ARTIFICIAL NEURAL NETWORKS IN MACHINE LINGUISTICS",
          "abstract": "<jats:p>Artificial neural networks (ANN) have revolutionized natural language processing (NLP) and have fundamentally changed the approach to solving linguistic problems. Due to their ability to learn from large amounts of data, ANNs demonstrate high performance</jats:p>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0053123228&target=NART&cn=ATN0053123228",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "ARTIFICIAL NEURAL NETWORKS IN MACHINE LINGUISTICS ARTIFICIAL NEURAL NETWORKS IN MACHINE LINGUISTICS ARTIFICIAL NEURAL NETWORKS IN MACHINE LINGUISTICS <jats:p>Artificial neural networks (ANN) have revolutionized natural language processing (NLP) and have fundamentally changed the approach to solving linguistic problems. Due to their ability to learn from large amounts of data, ANNs demonstrate high performance</jats:p>"
        },
        {
          "rank": 11,
          "score": 0.7033339142799377,
          "doc_id": "DIKO0007842188",
          "title": "신경망 예측 HMM을 이용한 음성인식에 관한 연구",
          "abstract": "음성은 인간의 가장 자연스러운 의사소통의 수단이며 이러한 음성에 의한 인간-기계 인터페이스는 속도가 빠르고 특별한 훈련이 없어도 이루어진다. 또한 컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식 기술은 중요한 연구 과제가 되고 있다. 이러한 음성인식에 관한 연구는 HMM과 신경망에 의한 방법들이 활발히 진행되고 있다. 그러나 HMM은 모델구조의 결정에 어려움이 있으며, 음성의 과도적 정보를 경시하는 경향이 있기 때문에 시간적 상관 표현력이 부족한 결점이 있다. 따라서 이러한 단점을 극복하기 위하여 음성의 동적특성을 표현할 수 있는 회귀계수와 지속시간 확률등을 파라메터로 추가하거나, 언어학적 제약을 가하여 최적의 단어별을 탐색하는 언어적 모델을 결합한 음성이해 시스템으로 발전하고 있다. 또한 신경망의 경우 그 구조나 가중치에 시간적인 것이 고려되지 않으므로 패턴이 정적인 경우 인식률이 높으나 음성과 같은 시계열 패턴의 비선형 신축을 취급하기 어렵다. 따라서 이러한 단점을 보완하기 위하여 회귀신경망, 예측형 신경망등이 제안되었다. 본 논문에서는 이러한 점들을 고려하여 HMM의 패턴에 대한 시간적 상관표현력을 개선시킬 수 있는 신경망 예측 HMM을 제안한다. 신경망 예측 HMM은 HMM과 신경망의 장점을 함께 사용할 수 있는 하이브리드형 네트워크로 예측형 신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하는 것이다. 따라서 예측형 신경망은 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가지므로 HMM에 동적 특성을 부여할 수 있다. 실험에서는 단독 숫자음과 100음절 데이터를 이용하여 HMM과 신경망 그리고 본 논문에서 제안한 신경망 예측 HMM의 인식성능을 비교·검토하였다. 신경망 예측 HMM은 MLP 예측 HMM, Elman망 예측 HMM, Jordan망 예측 HMM의 3가지이다. 실험결과 평가용 데이터를 기준으로 단독 숫자음에 대해서는 MLP 예측 HMM이 99.5%로 가장 우수한 결과를, 100음절 데이터에 대해서는 HMM이 87.7%로 가장 우수한 결과를 보였다. Elman망 예측 HMM과 Jordan망 예측 HMM의 경우 회귀구조임에도 불구하고 MLP 예측 HMM의 인식성능을 상회하지는 못하였다. 따라서 신경망 예측 HMM의 인식성능을 향상시키기 위해서는 데이터에 따른 효과의 검토와 음성패턴의 시간적 상관관계를 보다 더 잘 표현 할 수 있는 신경망의 구조에 대한 연구가 요구되며, 신경망과 HMM의 학습알고리즘에 대한 연구가 필요하다고 판단된다. 본 논문에서 제안한 신경망 예측 HMM은 HMM과 신경망의 결합이라고 하는 향후 가장 유효한 방법의 하나로 사료되며 연속음성 인식시스템으로 확장할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0007842188&target=NART&cn=DIKO0007842188",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망 예측 HMM을 이용한 음성인식에 관한 연구 신경망 예측 HMM을 이용한 음성인식에 관한 연구 신경망 예측 HMM을 이용한 음성인식에 관한 연구 음성은 인간의 가장 자연스러운 의사소통의 수단이며 이러한 음성에 의한 인간-기계 인터페이스는 속도가 빠르고 특별한 훈련이 없어도 이루어진다. 또한 컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식 기술은 중요한 연구 과제가 되고 있다. 이러한 음성인식에 관한 연구는 HMM과 신경망에 의한 방법들이 활발히 진행되고 있다. 그러나 HMM은 모델구조의 결정에 어려움이 있으며, 음성의 과도적 정보를 경시하는 경향이 있기 때문에 시간적 상관 표현력이 부족한 결점이 있다. 따라서 이러한 단점을 극복하기 위하여 음성의 동적특성을 표현할 수 있는 회귀계수와 지속시간 확률등을 파라메터로 추가하거나, 언어학적 제약을 가하여 최적의 단어별을 탐색하는 언어적 모델을 결합한 음성이해 시스템으로 발전하고 있다. 또한 신경망의 경우 그 구조나 가중치에 시간적인 것이 고려되지 않으므로 패턴이 정적인 경우 인식률이 높으나 음성과 같은 시계열 패턴의 비선형 신축을 취급하기 어렵다. 따라서 이러한 단점을 보완하기 위하여 회귀신경망, 예측형 신경망등이 제안되었다. 본 논문에서는 이러한 점들을 고려하여 HMM의 패턴에 대한 시간적 상관표현력을 개선시킬 수 있는 신경망 예측 HMM을 제안한다. 신경망 예측 HMM은 HMM과 신경망의 장점을 함께 사용할 수 있는 하이브리드형 네트워크로 예측형 신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하는 것이다. 따라서 예측형 신경망은 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가지므로 HMM에 동적 특성을 부여할 수 있다. 실험에서는 단독 숫자음과 100음절 데이터를 이용하여 HMM과 신경망 그리고 본 논문에서 제안한 신경망 예측 HMM의 인식성능을 비교·검토하였다. 신경망 예측 HMM은 MLP 예측 HMM, Elman망 예측 HMM, Jordan망 예측 HMM의 3가지이다. 실험결과 평가용 데이터를 기준으로 단독 숫자음에 대해서는 MLP 예측 HMM이 99.5%로 가장 우수한 결과를, 100음절 데이터에 대해서는 HMM이 87.7%로 가장 우수한 결과를 보였다. Elman망 예측 HMM과 Jordan망 예측 HMM의 경우 회귀구조임에도 불구하고 MLP 예측 HMM의 인식성능을 상회하지는 못하였다. 따라서 신경망 예측 HMM의 인식성능을 향상시키기 위해서는 데이터에 따른 효과의 검토와 음성패턴의 시간적 상관관계를 보다 더 잘 표현 할 수 있는 신경망의 구조에 대한 연구가 요구되며, 신경망과 HMM의 학습알고리즘에 대한 연구가 필요하다고 판단된다. 본 논문에서 제안한 신경망 예측 HMM은 HMM과 신경망의 결합이라고 하는 향후 가장 유효한 방법의 하나로 사료되며 연속음성 인식시스템으로 확장할 수 있을 것으로 기대된다."
        },
        {
          "rank": 12,
          "score": 0.6998673677444458,
          "doc_id": "JAKO200428635215914",
          "title": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구",
          "abstract": "본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200428635215914&target=NART&cn=JAKO200428635215914",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다."
        },
        {
          "rank": 13,
          "score": 0.6926463842391968,
          "doc_id": "NART95825020",
          "title": "End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition",
          "abstract": "<P><B>Abstract</B></P>  <P>In hidden Markov model (HMM) based automatic speech recognition (ASR) system, modeling the statistical relationship between the acoustic speech signal and the HMM states that represent linguistically motivated subword units such as phonemes is a crucial step. This is typically achieved by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then training a classifier such as artificial neural networks (ANN), Gaussian mixture model that estimates the emission probabilities of the HMM states. This paper investigates an end-to-end acoustic modeling approach using convolutional neural networks (CNNs), where the CNN takes as input raw speech signal and estimates the HMM states class conditional probabilities at the output. Alternately, as opposed to a divide and conquer strategy (i.e., separating feature extraction and statistical modeling steps), in the proposed acoustic modeling approach the relevant features and the classifier are jointly learned from the raw speech signal. Through ASR studies and analyses on multiple languages and multiple tasks, we show that: (a) the proposed approach yields consistently a better system with fewer parameters when compared to the conventional approach of cepstral feature extraction followed by ANN training, (b) unlike conventional method of speech processing, in the proposed approach the relevant feature representations are learned by first processing the input raw speech at the sub-segmental level ( &asymp; 2 ms). Specifically, through an analysis we show that the filters in the first convolution layer automatically learn &ldquo;in-parts&rdquo; formant-like information present in the sub-segmental speech, and (c) the intermediate feature representations obtained by subsequent filtering of the first convolution layer output are more discriminative compared to standard cepstral features and could be transferred across languages and domains.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Novel CNN-based end-to-end acoustic modeling approach is proposed. </LI> <LI>  Relevant features are automatically learned from the signal by discriminating phones. </LI> <LI>  Learned features are more discriminative than cepstral-based features. </LI> <LI>  Learned features are somewhat invariant to languages and domains. </LI> <LI>  Proposed approach leads to better ASR systems. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART95825020&target=NART&cn=NART95825020",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition <P><B>Abstract</B></P>  <P>In hidden Markov model (HMM) based automatic speech recognition (ASR) system, modeling the statistical relationship between the acoustic speech signal and the HMM states that represent linguistically motivated subword units such as phonemes is a crucial step. This is typically achieved by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then training a classifier such as artificial neural networks (ANN), Gaussian mixture model that estimates the emission probabilities of the HMM states. This paper investigates an end-to-end acoustic modeling approach using convolutional neural networks (CNNs), where the CNN takes as input raw speech signal and estimates the HMM states class conditional probabilities at the output. Alternately, as opposed to a divide and conquer strategy (i.e., separating feature extraction and statistical modeling steps), in the proposed acoustic modeling approach the relevant features and the classifier are jointly learned from the raw speech signal. Through ASR studies and analyses on multiple languages and multiple tasks, we show that: (a) the proposed approach yields consistently a better system with fewer parameters when compared to the conventional approach of cepstral feature extraction followed by ANN training, (b) unlike conventional method of speech processing, in the proposed approach the relevant feature representations are learned by first processing the input raw speech at the sub-segmental level ( &asymp; 2 ms). Specifically, through an analysis we show that the filters in the first convolution layer automatically learn &ldquo;in-parts&rdquo; formant-like information present in the sub-segmental speech, and (c) the intermediate feature representations obtained by subsequent filtering of the first convolution layer output are more discriminative compared to standard cepstral features and could be transferred across languages and domains.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Novel CNN-based end-to-end acoustic modeling approach is proposed. </LI> <LI>  Relevant features are automatically learned from the signal by discriminating phones. </LI> <LI>  Learned features are more discriminative than cepstral-based features. </LI> <LI>  Learned features are somewhat invariant to languages and domains. </LI> <LI>  Proposed approach leads to better ASR systems. </LI> </UL> </P>"
        },
        {
          "rank": 14,
          "score": 0.6920373439788818,
          "doc_id": "NART37979687",
          "title": "Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network",
          "abstract": "<P>A new method for noisy speech recognition based on a hybrid model of hidden Markov models (HMM) and wavelet neural network (WNN) is presented. The HMM was employed to compute the Viterbi output score. Then the score was used as the input of WNN to acquire the classification information. The result of recognition was made by these two kinds of recognition information. Recognition experiment shows that this hybrid model has higher performance than hidden Markov model in noisy speech recognition.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART37979687&target=NART&cn=NART37979687",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network <P>A new method for noisy speech recognition based on a hybrid model of hidden Markov models (HMM) and wavelet neural network (WNN) is presented. The HMM was employed to compute the Viterbi output score. Then the score was used as the input of WNN to acquire the classification information. The result of recognition was made by these two kinds of recognition information. Recognition experiment shows that this hybrid model has higher performance than hidden Markov model in noisy speech recognition.</P>"
        },
        {
          "rank": 15,
          "score": 0.6897963285446167,
          "doc_id": "JAKO201719951669089",
          "title": "원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링",
          "abstract": "This paper proposes a new method to train Deep Neural Network (DNN)-based acoustic models for speech recognition of native and foreign speakers. The proposed method consists of determining multi-set state clusters with various acoustic properties, training a DNN-based acoustic model, and recognizing speech based on the model. In the proposed method, hidden nodes of DNN are shared, but output nodes are separated to accommodate different acoustic properties for native and foreign speech. In an English speech recognition task for speakers of Korean and English respectively, the proposed method is shown to slightly improve recognition accuracy compared to the conventional multi-condition training method.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201719951669089&target=NART&cn=JAKO201719951669089",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 This paper proposes a new method to train Deep Neural Network (DNN)-based acoustic models for speech recognition of native and foreign speakers. The proposed method consists of determining multi-set state clusters with various acoustic properties, training a DNN-based acoustic model, and recognizing speech based on the model. In the proposed method, hidden nodes of DNN are shared, but output nodes are separated to accommodate different acoustic properties for native and foreign speech. In an English speech recognition task for speakers of Korean and English respectively, the proposed method is shown to slightly improve recognition accuracy compared to the conventional multi-condition training method."
        },
        {
          "rank": 16,
          "score": 0.6896539926528931,
          "doc_id": "JAKO201707851605473",
          "title": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표",
          "abstract": "본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201707851605473&target=NART&cn=JAKO201707851605473",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다."
        },
        {
          "rank": 17,
          "score": 0.6893130540847778,
          "doc_id": "JAKO199911921383665",
          "title": "회귀신경망을 이용한 음성인식에 관한 연구",
          "abstract": "본 논문은 회귀신경망을 이용한 음성인식에 관한 연구이다. 예측형 신경망으로 음절단위로 모델링한 후 미지의 입력음성에 대하여 예측오차가 최소가 되는 모델을 인식결과로 한다. 이를 위해서 예측형으로 구성된 신경망에 음성의 시변성을 신경망 내부에 흡수시키기 위해서 회귀구조의 동적인 신경망인 회귀예측신경망을 구성하고 Elman과 Jordan이 제안한 회귀구조에 따라 인식성능을 서로 비교하였다. 음성DB는 ETRI의 샘돌이 음성 데이터를 사용하였다. 그리고, 신경망의 최적모델을 구하기 위하여 예측차수와 은닉층 유니트 수의 변화에 따른 인식률의 변화와 문맥층에서 자기회귀계수를 두어 이전의 값들이 문맥층에서 누적되도록 하였을 경우에 대한 인식률의 변화를 비교하였다. 실험결과, 최적의 예측차수, 은닉층 유니트수, 자기회귀계수는 신경망의 구조에 따라 차이가 나타났으며, 전반적으로 Jordan망이 Elman망보다 인식률이 높았으며, 자기회귀계수에 대한 영향은 신경망의 구조와 계수값에 따라 불규칙하게 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199911921383665&target=NART&cn=JAKO199911921383665",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망을 이용한 음성인식에 관한 연구 회귀신경망을 이용한 음성인식에 관한 연구 회귀신경망을 이용한 음성인식에 관한 연구 본 논문은 회귀신경망을 이용한 음성인식에 관한 연구이다. 예측형 신경망으로 음절단위로 모델링한 후 미지의 입력음성에 대하여 예측오차가 최소가 되는 모델을 인식결과로 한다. 이를 위해서 예측형으로 구성된 신경망에 음성의 시변성을 신경망 내부에 흡수시키기 위해서 회귀구조의 동적인 신경망인 회귀예측신경망을 구성하고 Elman과 Jordan이 제안한 회귀구조에 따라 인식성능을 서로 비교하였다. 음성DB는 ETRI의 샘돌이 음성 데이터를 사용하였다. 그리고, 신경망의 최적모델을 구하기 위하여 예측차수와 은닉층 유니트 수의 변화에 따른 인식률의 변화와 문맥층에서 자기회귀계수를 두어 이전의 값들이 문맥층에서 누적되도록 하였을 경우에 대한 인식률의 변화를 비교하였다. 실험결과, 최적의 예측차수, 은닉층 유니트수, 자기회귀계수는 신경망의 구조에 따라 차이가 나타났으며, 전반적으로 Jordan망이 Elman망보다 인식률이 높았으며, 자기회귀계수에 대한 영향은 신경망의 구조와 계수값에 따라 불규칙하게 나타났다."
        },
        {
          "rank": 18,
          "score": 0.684623122215271,
          "doc_id": "JAKO201403359905324",
          "title": "가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원",
          "abstract": "This paper describes a robust speech recognition technique by reconstructing spectral components mismatched with a training environment. Although the cluster-based reconstruction method can compensate the unreliable components from reliable components in the same spectral vector by assuming an independent, identically distributed Gaussian-mixture process of training spectral vectors, the presented method exploits the temporal dependency of speech to reconstruct the components by introducing a hidden-Markov-model prior which incorporates an internal state transition plausible for an observed spectral vector sequence. The experimental results indicate that the described method can provide temporally consistent reconstruction and further improve recognition performance on average compared to the conventional method.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201403359905324&target=NART&cn=JAKO201403359905324",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 This paper describes a robust speech recognition technique by reconstructing spectral components mismatched with a training environment. Although the cluster-based reconstruction method can compensate the unreliable components from reliable components in the same spectral vector by assuming an independent, identically distributed Gaussian-mixture process of training spectral vectors, the presented method exploits the temporal dependency of speech to reconstruct the components by introducing a hidden-Markov-model prior which incorporates an internal state transition plausible for an observed spectral vector sequence. The experimental results indicate that the described method can provide temporally consistent reconstruction and further improve recognition performance on average compared to the conventional method."
        },
        {
          "rank": 19,
          "score": 0.6845501661300659,
          "doc_id": "JAKO201734964189755",
          "title": "주목 메커니즘 기반의 심층신경망을 이용한 음성 감정인식",
          "abstract": "본 논문에서는 주목 메커니즘 기반의 심층 신경망을 사용한 음성 감정인식 방법을 제안한다. 제안하는 방식은 CNN(Convolution Neural Networks), GRU(Gated Recurrent Unit), DNN(Deep Neural Networks)의 결합으로 이루어진 심층 신경망 구조와 주목 메커니즘으로 구성된다. 음성의 스펙트로그램에는 감정에 따른 특징적인 패턴이 포함되어 있으므로 제안하는 방식에서는 일반적인 CNN에서 컨벌루션 필터를 tuned Gabor 필터로 사용하는 GCNN(Gabor CNN)을 사용하여 패턴을 효과적으로 모델링한다. 또한 CNN과 FC(Fully-Connected)레이어 기반의 주목 메커니즘을 적용하여 추출된 특징의 맥락 정보를 고려한 주목 가중치를 구해 감정인식에 사용한다. 본 논문에서 제안하는 방식의 검증을 위해 6가지 감정에 대해 인식 실험을 진행하였다. 실험 결과, 제안한 방식이 음성 감정인식에서 기존의 방식보다 더 높은 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201734964189755&target=NART&cn=JAKO201734964189755",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "주목 메커니즘 기반의 심층신경망을 이용한 음성 감정인식 주목 메커니즘 기반의 심층신경망을 이용한 음성 감정인식 주목 메커니즘 기반의 심층신경망을 이용한 음성 감정인식 본 논문에서는 주목 메커니즘 기반의 심층 신경망을 사용한 음성 감정인식 방법을 제안한다. 제안하는 방식은 CNN(Convolution Neural Networks), GRU(Gated Recurrent Unit), DNN(Deep Neural Networks)의 결합으로 이루어진 심층 신경망 구조와 주목 메커니즘으로 구성된다. 음성의 스펙트로그램에는 감정에 따른 특징적인 패턴이 포함되어 있으므로 제안하는 방식에서는 일반적인 CNN에서 컨벌루션 필터를 tuned Gabor 필터로 사용하는 GCNN(Gabor CNN)을 사용하여 패턴을 효과적으로 모델링한다. 또한 CNN과 FC(Fully-Connected)레이어 기반의 주목 메커니즘을 적용하여 추출된 특징의 맥락 정보를 고려한 주목 가중치를 구해 감정인식에 사용한다. 본 논문에서 제안하는 방식의 검증을 위해 6가지 감정에 대해 인식 실험을 진행하였다. 실험 결과, 제안한 방식이 음성 감정인식에서 기존의 방식보다 더 높은 성능을 보였다."
        },
        {
          "rank": 20,
          "score": 0.6844441890716553,
          "doc_id": "JAKO201721558887385",
          "title": "벌크 지표의 신경망 학습에 기반한 한국어 모음 'ㅡ'의 음성 인식",
          "abstract": "음성 인식은 HCI 분야에서 널리 사용되는 기술 중 하나이다. 가정 자동화, 자동 통역, 차량 내비게이션 등 음성 인식 기술이 적용될 수 있는 많은 응용들이 현재 개발되고 있다. 또한, 모바일 환경에서 작동 가능한 음성 인식 시스템에 대한 수요도 급속히 증대되고 있다. 본 논문은 한국어 음성 인식 시스템의 일부로서, 한국어 모음 'ㅡ'를 빠르게 인식할 수 있는 방안을 제시한다. 제안하는 방식은 주파수 영역 대신, 시간 영역에서 계산되는 지표인 벌크 지표를 사용하므로, 인식을 위한 계산 비용을 절감할 수 있다. 모음 'ㅡ'의 전형적인 시퀀스 패턴들을 표현하는 벌크 지표들에 대한 신경망 학습을 수행하며, 최종적인 인식을 위해 학습된 신경망을 사용한다. 실험 결과를 통해, 제안하는 방식이 모음 'ㅡ'를 88.7%의 정확도로 인식할 수 있음을 확인하였고, 인식 속도는 어절 당 0.74msec이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201721558887385&target=NART&cn=JAKO201721558887385",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "벌크 지표의 신경망 학습에 기반한 한국어 모음 'ㅡ'의 음성 인식 벌크 지표의 신경망 학습에 기반한 한국어 모음 'ㅡ'의 음성 인식 벌크 지표의 신경망 학습에 기반한 한국어 모음 'ㅡ'의 음성 인식 음성 인식은 HCI 분야에서 널리 사용되는 기술 중 하나이다. 가정 자동화, 자동 통역, 차량 내비게이션 등 음성 인식 기술이 적용될 수 있는 많은 응용들이 현재 개발되고 있다. 또한, 모바일 환경에서 작동 가능한 음성 인식 시스템에 대한 수요도 급속히 증대되고 있다. 본 논문은 한국어 음성 인식 시스템의 일부로서, 한국어 모음 'ㅡ'를 빠르게 인식할 수 있는 방안을 제시한다. 제안하는 방식은 주파수 영역 대신, 시간 영역에서 계산되는 지표인 벌크 지표를 사용하므로, 인식을 위한 계산 비용을 절감할 수 있다. 모음 'ㅡ'의 전형적인 시퀀스 패턴들을 표현하는 벌크 지표들에 대한 신경망 학습을 수행하며, 최종적인 인식을 위해 학습된 신경망을 사용한다. 실험 결과를 통해, 제안하는 방식이 모음 'ㅡ'를 88.7%의 정확도로 인식할 수 있음을 확인하였고, 인식 속도는 어절 당 0.74msec이다."
        },
        {
          "rank": 21,
          "score": 0.6841676235198975,
          "doc_id": "NART20042187",
          "title": "Neural networks with hidden Markov process",
          "abstract": "Observed data contain global structural information and local statistical fluctuation.  To catch both global and local information, neural network models with hidden Markov process are presented. Hidden Markov model(HMM) is a rather general framework which can effectively deal with sequential structure based on stochastic process theory.  Neural networks have strong learning ability which makes it easier to free statistical fluctuation.  By introducing the dependence of weights of neural networks on the states of the systems, neural networks combined with HMM are realized.  The process underlying the transitions of the states is a Markov process. Dependent observations are generated by the neural networks corresponding to the states.  A modified maximum likelihood procedure are employed to estimate both transition probability of Markov process and weights of neural networks.  Applications of neural networks with HMM to econometrics, pattern matching, speech recognition and image process are discussed.(Author abstract)",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20042187&target=NART&cn=NART20042187",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural networks with hidden Markov process Neural networks with hidden Markov process Neural networks with hidden Markov process Observed data contain global structural information and local statistical fluctuation.  To catch both global and local information, neural network models with hidden Markov process are presented. Hidden Markov model(HMM) is a rather general framework which can effectively deal with sequential structure based on stochastic process theory.  Neural networks have strong learning ability which makes it easier to free statistical fluctuation.  By introducing the dependence of weights of neural networks on the states of the systems, neural networks combined with HMM are realized.  The process underlying the transitions of the states is a Markov process. Dependent observations are generated by the neural networks corresponding to the states.  A modified maximum likelihood procedure are employed to estimate both transition probability of Markov process and weights of neural networks.  Applications of neural networks with HMM to econometrics, pattern matching, speech recognition and image process are discussed.(Author abstract)"
        },
        {
          "rank": 22,
          "score": 0.6809001564979553,
          "doc_id": "NART30128358",
          "title": "Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer",
          "abstract": "<P><B>Abstract</B></P><P>Spoken human&ndash;machine interaction in real-world environments requires acoustic models that are robust to changes in acoustic conditions, e.g. presence of noise. Unfortunately, the popular hidden Markov models (HMM) are not noise tolerant. One way to increase recognition performance is to acquire a small adaptation set of noisy utterances, which is used to estimate a normalization mapping between noisy and clean features to be fed into the acoustic model. This paper proposes an unsupervised maximum-likelihood gradient-ascent training algorithm (instead of the usual least squares regression) for a neural feature adaptation module, properly combined with a hybrid connectionist/HMM speech recognizer. The algorithm is inspired by the so-called &ldquo;inversion principle&rdquo;, that prescribes the optimization of the input features instead of the model parameters. Simulation results on a real-world speaker-independent continuous speech corpus of connected Italian digits, corrupted by noise, validate the approach. A small neural net (13 hidden neurons) trained over a single adaptation utterance for one iteration yields a 18.79% relative word error rate (WER) reduction over the bare hybrid, and a 65.10% relative WER reduction over the Gaussian-based HMM.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART30128358&target=NART&cn=NART30128358",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer <P><B>Abstract</B></P><P>Spoken human&ndash;machine interaction in real-world environments requires acoustic models that are robust to changes in acoustic conditions, e.g. presence of noise. Unfortunately, the popular hidden Markov models (HMM) are not noise tolerant. One way to increase recognition performance is to acquire a small adaptation set of noisy utterances, which is used to estimate a normalization mapping between noisy and clean features to be fed into the acoustic model. This paper proposes an unsupervised maximum-likelihood gradient-ascent training algorithm (instead of the usual least squares regression) for a neural feature adaptation module, properly combined with a hybrid connectionist/HMM speech recognizer. The algorithm is inspired by the so-called &ldquo;inversion principle&rdquo;, that prescribes the optimization of the input features instead of the model parameters. Simulation results on a real-world speaker-independent continuous speech corpus of connected Italian digits, corrupted by noise, validate the approach. A small neural net (13 hidden neurons) trained over a single adaptation utterance for one iteration yields a 18.79% relative word error rate (WER) reduction over the bare hybrid, and a 65.10% relative WER reduction over the Gaussian-based HMM.</P>"
        },
        {
          "rank": 23,
          "score": 0.6780325174331665,
          "doc_id": "JAKO200011920774657",
          "title": "은닉 마코프 모델 기반 병렬음성인식 시스템",
          "abstract": "본 논문의 병렬음성인식 모델은 연속 은닉 마코프 모델(HMM; hidden Markov model)에 기반한 병렬 음소인식모듈과 계층구조의 지식베이스에 기반한 병렬 문장인식모듈로 구성된다. 병렬 음소인식 모듈은 수천개의 HMM을 병렬 프로세서에 분산시킨 수, 할당된 HMM에 대한 출력확률 계산과 Viterbi 알고리즘을 담당한다. 지식베이스 기반 병렬 문장인식모듈은 음소모듈에서 공급되는 음소열과 지안하는 병렬 음성인식 알고리즘은 분산메모리 MIMD 구조의 다중 트랜스퓨터와 Parsytec CC 상에 구현되었다. 실험결과, 병렬 음소인식모듈을 통한 실행시간 향상과 병렬 문장인식모듈을 통한 인식률 향상을 얻을 수 있었으며 병렬 음성인식 시스템의 실시간 구현 가능성을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200011920774657&target=NART&cn=JAKO200011920774657",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마코프 모델 기반 병렬음성인식 시스템 은닉 마코프 모델 기반 병렬음성인식 시스템 은닉 마코프 모델 기반 병렬음성인식 시스템 본 논문의 병렬음성인식 모델은 연속 은닉 마코프 모델(HMM; hidden Markov model)에 기반한 병렬 음소인식모듈과 계층구조의 지식베이스에 기반한 병렬 문장인식모듈로 구성된다. 병렬 음소인식 모듈은 수천개의 HMM을 병렬 프로세서에 분산시킨 수, 할당된 HMM에 대한 출력확률 계산과 Viterbi 알고리즘을 담당한다. 지식베이스 기반 병렬 문장인식모듈은 음소모듈에서 공급되는 음소열과 지안하는 병렬 음성인식 알고리즘은 분산메모리 MIMD 구조의 다중 트랜스퓨터와 Parsytec CC 상에 구현되었다. 실험결과, 병렬 음소인식모듈을 통한 실행시간 향상과 병렬 문장인식모듈을 통한 인식률 향상을 얻을 수 있었으며 병렬 음성인식 시스템의 실시간 구현 가능성을 확인하였다."
        },
        {
          "rank": 24,
          "score": 0.6743231415748596,
          "doc_id": "NART70632792",
          "title": "Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis",
          "abstract": "<P>This paper investigates joint speaker-dependent audiovisual Hidden Semi-Markov Models (HSMM) where the visual models produce a sequence of 3D motion tracking data that is used to animate a talking head and the acoustic models are used for speech synthesis. Different acoustic, visual, and joint audiovisual models for four different Austrian German speakers were trained and we show that the joint models perform better compared to other approaches in terms of synchronization quality of the synthesized visual speech. In addition, a detailed analysis of the acoustic and visual alignment is provided for the different models. Importantly, the joint audiovisual modeling does not decrease the acoustic synthetic speech quality compared to acoustic-only modeling so that there is a clear advantage in the common duration model of the joint audiovisual modeling approach that is used for synchronizing acoustic and visual parameter sequences. Finally, it provides a model that integrates the visual and acoustic speech dynamics.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART70632792&target=NART&cn=NART70632792",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis <P>This paper investigates joint speaker-dependent audiovisual Hidden Semi-Markov Models (HSMM) where the visual models produce a sequence of 3D motion tracking data that is used to animate a talking head and the acoustic models are used for speech synthesis. Different acoustic, visual, and joint audiovisual models for four different Austrian German speakers were trained and we show that the joint models perform better compared to other approaches in terms of synchronization quality of the synthesized visual speech. In addition, a detailed analysis of the acoustic and visual alignment is provided for the different models. Importantly, the joint audiovisual modeling does not decrease the acoustic synthetic speech quality compared to acoustic-only modeling so that there is a clear advantage in the common duration model of the joint audiovisual modeling approach that is used for synchronizing acoustic and visual parameter sequences. Finally, it provides a model that integrates the visual and acoustic speech dynamics.</P>"
        },
        {
          "rank": 25,
          "score": 0.6734422445297241,
          "doc_id": "JAKO200311922043899",
          "title": "Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구",
          "abstract": "본 논문에서는 한국어 음성 데이터를 대상으로 HM-Net(Hidden Markov Network) 음성인식 시스템의 성능평가를 수행하였다. 음향모델 작성은 음성인식에서 널리 사용되고 있는 통계적인 모델링 방법인 HMM(Hidden Markov Model)을 개량한 HM-Net을 도입하였다. HM-Net은 기존의 SSS(Successive State Splitting) 알고리즘을 개량한 PDT(Phonetic Decision Tree)-SSS 알고리즘에 의해 문맥방향과 시간방향의 상태분할을 수행하여 생성되는데, 특히 문맥방향 상태분할의 경우 학습 음성데이터에 출현하지 않는 문맥정보를 효과적으로 표현하기 위해 음소결정트리를 채용하고 있으며, 시간방향 상태분할의 경우 학습 음성데이터에서 각 음소별 지속시간 정보를 효과적으로 표현하기 위한 상태분할을 수행하며, 마지막으로 파라미터의 공유를 통해 triphone 형태의 최적인 모델 네트워크를 작성하게 된다. 인식에 사용된 알고리즘은 음소 및 단어인식의 경우에는 One-Pass Viterbi 빔 탐색을 사용하며 트리 구조 형태의 사전과 phone/word-pair 문법을 채용하고 있다. 연속음성인식의 경우에는 단어 bigram과 단어 trigram 언어모델과 목구조 형태의 사전을 채용한 Multi-Pass 빔 탐색을 사용하고 있다. 전체적으로 본 논문에서는 다양한 조건에서 HM-Net 음성인식 시스템의 성능평가를 수행하였으며, 지금까지 소개된 음성인식 시스템과 비교하여 매우 우수한 인식성능을 보임을 실험을 통해 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200311922043899&target=NART&cn=JAKO200311922043899",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 본 논문에서는 한국어 음성 데이터를 대상으로 HM-Net(Hidden Markov Network) 음성인식 시스템의 성능평가를 수행하였다. 음향모델 작성은 음성인식에서 널리 사용되고 있는 통계적인 모델링 방법인 HMM(Hidden Markov Model)을 개량한 HM-Net을 도입하였다. HM-Net은 기존의 SSS(Successive State Splitting) 알고리즘을 개량한 PDT(Phonetic Decision Tree)-SSS 알고리즘에 의해 문맥방향과 시간방향의 상태분할을 수행하여 생성되는데, 특히 문맥방향 상태분할의 경우 학습 음성데이터에 출현하지 않는 문맥정보를 효과적으로 표현하기 위해 음소결정트리를 채용하고 있으며, 시간방향 상태분할의 경우 학습 음성데이터에서 각 음소별 지속시간 정보를 효과적으로 표현하기 위한 상태분할을 수행하며, 마지막으로 파라미터의 공유를 통해 triphone 형태의 최적인 모델 네트워크를 작성하게 된다. 인식에 사용된 알고리즘은 음소 및 단어인식의 경우에는 One-Pass Viterbi 빔 탐색을 사용하며 트리 구조 형태의 사전과 phone/word-pair 문법을 채용하고 있다. 연속음성인식의 경우에는 단어 bigram과 단어 trigram 언어모델과 목구조 형태의 사전을 채용한 Multi-Pass 빔 탐색을 사용하고 있다. 전체적으로 본 논문에서는 다양한 조건에서 HM-Net 음성인식 시스템의 성능평가를 수행하였으며, 지금까지 소개된 음성인식 시스템과 비교하여 매우 우수한 인식성능을 보임을 실험을 통해 확인할 수 있었다."
        },
        {
          "rank": 26,
          "score": 0.6728936433792114,
          "doc_id": "JAKO200727500210663",
          "title": "신경망을 이용한 영역 행위 예측",
          "abstract": "목적 지향 대화에서 사용자의 의도는 화행과 개념열의 쌍으로 구성된 영역행위로 표현될 수 있다. 사용자 발화에 대한 영역행위 예측은 음성 인식 오류를 보정하는데 유용하며, 시스템 발화에 대한 영역행위 예측은 유연한 응답 생성에 유용하다. 본 논문에서는 신경망을 이용하여 영역행위를 예측하는 모델을 제안한다. 제안 모델은 대화 이력 벡터와 현재 영역행위를 신경망의 입력으로 사용하여 다음 영역행위를 예측한다. 실험 결과, 제안 모델은 화행 예측과 개념열 예측에서 각각 80.02%, 82.09%의 정확률을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200727500210663&target=NART&cn=JAKO200727500210663",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망을 이용한 영역 행위 예측 신경망을 이용한 영역 행위 예측 신경망을 이용한 영역 행위 예측 목적 지향 대화에서 사용자의 의도는 화행과 개념열의 쌍으로 구성된 영역행위로 표현될 수 있다. 사용자 발화에 대한 영역행위 예측은 음성 인식 오류를 보정하는데 유용하며, 시스템 발화에 대한 영역행위 예측은 유연한 응답 생성에 유용하다. 본 논문에서는 신경망을 이용하여 영역행위를 예측하는 모델을 제안한다. 제안 모델은 대화 이력 벡터와 현재 영역행위를 신경망의 입력으로 사용하여 다음 영역행위를 예측한다. 실험 결과, 제안 모델은 화행 예측과 개념열 예측에서 각각 80.02%, 82.09%의 정확률을 보였다."
        },
        {
          "rank": 27,
          "score": 0.6726447343826294,
          "doc_id": "NART48832461",
          "title": "Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments",
          "abstract": "In this paper, we propose a model for the incorporation of voicing information into a speech recognition system in noisy environments. The employed voicing information is estimated by a novel method that can provide this information for each filter-bank channel and does not require information about the fundamental frequency. The voicing information is modelled by employing the Bernoulli distribution. The voicing model is obtained for each HMM state and mixture by a Viterbi-style training procedure. The proposed voicing incorporation is evaluated both within a standard model and two other models that had compensated for the noise effect, the missing-feature and the multi-conditional training model. Experiments are first performed on noisy speech data from the Aurora 2 database. Significant performance improvements are achieved when the voicing information is incorporated within the standard model as well as the noise-compensated models. The employment of voicing information is also demonstrated on a phoneme recognition task on the noise-corrupted TIMIT database and considerable improvements are observed.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART48832461&target=NART&cn=NART48832461",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments In this paper, we propose a model for the incorporation of voicing information into a speech recognition system in noisy environments. The employed voicing information is estimated by a novel method that can provide this information for each filter-bank channel and does not require information about the fundamental frequency. The voicing information is modelled by employing the Bernoulli distribution. The voicing model is obtained for each HMM state and mixture by a Viterbi-style training procedure. The proposed voicing incorporation is evaluated both within a standard model and two other models that had compensated for the noise effect, the missing-feature and the multi-conditional training model. Experiments are first performed on noisy speech data from the Aurora 2 database. Significant performance improvements are achieved when the voicing information is incorporated within the standard model as well as the noise-compensated models. The employment of voicing information is also demonstrated on a phoneme recognition task on the noise-corrupted TIMIT database and considerable improvements are observed."
        },
        {
          "rank": 28,
          "score": 0.6724816560745239,
          "doc_id": "NART80772700",
          "title": "Noisy training for deep neural networks in speech recognition",
          "abstract": "<P><B>Abstract</B><P>Deep neural networks (DNNs) have gained remarkable success in speech recognition, partially attributed to the flexibility of DNN models in learning complex patterns of speech signals. This flexibility, however, may lead to serious over-fitting and hence miserable performance degradation in adverse acoustic conditions such as those with high ambient noises. We propose a noisy training approach to tackle this problem: by injecting moderate noises into the training data intentionally and randomly, more generalizable DNN models can be learned. This &lsquo;noise injection&rsquo; technique, although known to the neural computation community already, has not been studied with DNNs which involve a highly complex objective function. The experiments presented in this paper confirm that the noisy training approach works well for the DNN model and can provide substantial performance improvement for DNN-based speech recognition.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART80772700&target=NART&cn=NART80772700",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Noisy training for deep neural networks in speech recognition Noisy training for deep neural networks in speech recognition Noisy training for deep neural networks in speech recognition <P><B>Abstract</B><P>Deep neural networks (DNNs) have gained remarkable success in speech recognition, partially attributed to the flexibility of DNN models in learning complex patterns of speech signals. This flexibility, however, may lead to serious over-fitting and hence miserable performance degradation in adverse acoustic conditions such as those with high ambient noises. We propose a noisy training approach to tackle this problem: by injecting moderate noises into the training data intentionally and randomly, more generalizable DNN models can be learned. This &lsquo;noise injection&rsquo; technique, although known to the neural computation community already, has not been studied with DNNs which involve a highly complex objective function. The experiments presented in this paper confirm that the noisy training approach works well for the DNN model and can provide substantial performance improvement for DNN-based speech recognition.</P></P>"
        },
        {
          "rank": 29,
          "score": 0.6715728044509888,
          "doc_id": "JAKO202029462558904",
          "title": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식",
          "abstract": "특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202029462558904&target=NART&cn=JAKO202029462558904",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다."
        },
        {
          "rank": 30,
          "score": 0.6703542470932007,
          "doc_id": "JAKO201415642601987",
          "title": "SNR 매핑을 이용한 환경적응 기반 음성인식",
          "abstract": "다 모델 기반의 음성인식기는 음성인식에서 매우 성공적임이 알려져 있다. 그것은 다양한 신호-대-잡음비(SNR)와 잡음종류에 해당하는 다수의 HMM을 사용함으로서 선택된 음향모델이 인식잡음음성에 매우 근접한 일치성을 가질 수 있기 때문이다. 그러나 실제 사용시에 HMM의 개수가 제한됨에 따라서 음향모델의 불일치는 여전히 문제로 남아 있다. 본 논문에서는 인식잡음음성과 HMM 간의 SNR 불일치를 줄이고자 이들 간의 최적의 SNR 매핑 (mapping)을 실험적으로 결정하였다. 인식잡음음성으로 부터 추정된 SNR 값을 사용하는 대신 제안된 SNR 매핑을 사용함으로서 향상된 인식결과를 얻을 수 있었다. 다 모델 기반인식기에 제안된 방법을 적용하여 Aurora 2 데이터베이스에 대해서 인식 실험한 결과 기존의 MTR 이나 다 모델 기반 음성인식기에 비해서 6.3%와 9.4%의 상대적 단어 오인식율 감소를 이룰 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201415642601987&target=NART&cn=JAKO201415642601987",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SNR 매핑을 이용한 환경적응 기반 음성인식 SNR 매핑을 이용한 환경적응 기반 음성인식 SNR 매핑을 이용한 환경적응 기반 음성인식 다 모델 기반의 음성인식기는 음성인식에서 매우 성공적임이 알려져 있다. 그것은 다양한 신호-대-잡음비(SNR)와 잡음종류에 해당하는 다수의 HMM을 사용함으로서 선택된 음향모델이 인식잡음음성에 매우 근접한 일치성을 가질 수 있기 때문이다. 그러나 실제 사용시에 HMM의 개수가 제한됨에 따라서 음향모델의 불일치는 여전히 문제로 남아 있다. 본 논문에서는 인식잡음음성과 HMM 간의 SNR 불일치를 줄이고자 이들 간의 최적의 SNR 매핑 (mapping)을 실험적으로 결정하였다. 인식잡음음성으로 부터 추정된 SNR 값을 사용하는 대신 제안된 SNR 매핑을 사용함으로서 향상된 인식결과를 얻을 수 있었다. 다 모델 기반인식기에 제안된 방법을 적용하여 Aurora 2 데이터베이스에 대해서 인식 실험한 결과 기존의 MTR 이나 다 모델 기반 음성인식기에 비해서 6.3%와 9.4%의 상대적 단어 오인식율 감소를 이룰 수 있었다."
        },
        {
          "rank": 31,
          "score": 0.670110285282135,
          "doc_id": "JAKO199215875841266",
          "title": "음성 인식 신경망을 위한 음성 파라키터들의 성능 비교",
          "abstract": "음성 인식에 신경망 모델을 적용하는 많은 연구들이 있었지만, 주된 관심은 음성인식에 적합한 구조와 학습 방법이었다.  그러나 음성인식에 신경망 모델을 적용한 시스템의 효율 향상은 모델 자체의 구조뿐 아니라, 신경망 모델의 입력으로 어떤 음성 파라미터를 사용하는가에 따라서도 큰 영향을 받는다.  본 논문은 기존 음성인식에 신경망 모델을 적용한 많은 연구들에서 사용한 음성 파라미터를 살펴보고, 대표적인 음성 파라미터 6개를 선정하여, 같은 데이타와 같은 신경망 모델 하에서 어떻게 성능이 달라지는지를 분석한다.  인식 실험에 있어서는 한국어 파열음 9개에 대한 8개 데이터 집합과 모음 8개에 대한 18개 데이터 집합을 음성 파라미터로 하고 신경망 모델은 순환 신경망 모델을 사용하여 노드의 수를 일정하게 한뒤 다양한 입력 파라미터의 성능을 비교하였다.  그 결과 선형 예측 계수로부터 얻어진 delta cepstrum의 음성 파라미터가 가장 좋은 성능을 보였으며 이때 인식률은 같은 학습 데이터에 대해 파열음 100.0%, 모음 95.1%이었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199215875841266&target=NART&cn=JAKO199215875841266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식에 신경망 모델을 적용하는 많은 연구들이 있었지만, 주된 관심은 음성인식에 적합한 구조와 학습 방법이었다.  그러나 음성인식에 신경망 모델을 적용한 시스템의 효율 향상은 모델 자체의 구조뿐 아니라, 신경망 모델의 입력으로 어떤 음성 파라미터를 사용하는가에 따라서도 큰 영향을 받는다.  본 논문은 기존 음성인식에 신경망 모델을 적용한 많은 연구들에서 사용한 음성 파라미터를 살펴보고, 대표적인 음성 파라미터 6개를 선정하여, 같은 데이타와 같은 신경망 모델 하에서 어떻게 성능이 달라지는지를 분석한다.  인식 실험에 있어서는 한국어 파열음 9개에 대한 8개 데이터 집합과 모음 8개에 대한 18개 데이터 집합을 음성 파라미터로 하고 신경망 모델은 순환 신경망 모델을 사용하여 노드의 수를 일정하게 한뒤 다양한 입력 파라미터의 성능을 비교하였다.  그 결과 선형 예측 계수로부터 얻어진 delta cepstrum의 음성 파라미터가 가장 좋은 성능을 보였으며 이때 인식률은 같은 학습 데이터에 대해 파열음 100.0%, 모음 95.1%이었다."
        },
        {
          "rank": 32,
          "score": 0.6669787168502808,
          "doc_id": "JAKO201120661418238",
          "title": "음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거",
          "abstract": "본 논문에서는 먼저 신경회로망의 학습에 오차역전파 학습 알고리즘을 사용하여 각 프레임에서의 음성 및 잡음 구간의 검출에 의한 음성인식 알고리즘을 제안한다. 그리고 신경회로망에 의하여 음성 및 잡음 구간의 검출에 따라서 각 프레임에서 잡음을 제거하는 스펙트럼 차감법을 제안한다. 본 실험에서는 제안한 음성인식알고리즘의 성능을 원음성에 백색잡음 및 자동차 잡음을 부가하여 인식율을 평가한다. 또한 인식시스템에 의하여 검출된 음성 및 잡음 구간을 이용하여 각 프레임에서의 스펙트럼 차감법에 의한 잡음제거의 실험결과를 나타낸다. 잡음에 의하여 오염된 음성에 대하여 신호대잡음비를 사용하여 본 알고리즘이 유효하다는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201120661418238&target=NART&cn=JAKO201120661418238",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 본 논문에서는 먼저 신경회로망의 학습에 오차역전파 학습 알고리즘을 사용하여 각 프레임에서의 음성 및 잡음 구간의 검출에 의한 음성인식 알고리즘을 제안한다. 그리고 신경회로망에 의하여 음성 및 잡음 구간의 검출에 따라서 각 프레임에서 잡음을 제거하는 스펙트럼 차감법을 제안한다. 본 실험에서는 제안한 음성인식알고리즘의 성능을 원음성에 백색잡음 및 자동차 잡음을 부가하여 인식율을 평가한다. 또한 인식시스템에 의하여 검출된 음성 및 잡음 구간을 이용하여 각 프레임에서의 스펙트럼 차감법에 의한 잡음제거의 실험결과를 나타낸다. 잡음에 의하여 오염된 음성에 대하여 신호대잡음비를 사용하여 본 알고리즘이 유효하다는 것을 확인한다."
        },
        {
          "rank": 33,
          "score": 0.6663776636123657,
          "doc_id": "JAKO201911338887557",
          "title": "잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법",
          "abstract": "본 논문에서는 잡음 환경에서 효과적인 음성 인식을 위하여 DNN(Deep Neural Network) 기반의 잡음 오염 함수 예측을 이용한 음향 모델 적응 기법을 제안한다. 깨끗한 음성과 잡음 정보를 입력으로 하고 오염된 음성에 대한 특징 벡터를 출력으로 하는 DNN을 학습하여 비선형 관계를 갖는 잡음 오염 함수를 예측한다. 예측된 잡음 오염 함수를 음향모델의 평균 벡터에 적용하여 잡음 환경에 적응된 음향 모델을 생성한다. Aurora 2.0 데이터를 이용한 음성 인식 성능 평가에서 본 논문에서 제안한 모델 적응 기법이 기존의 전처리, 모델 적응 기법에 비해 일치, 불일치 잡음 환경에서 모두 평균적으로 우수한 성능을 나타낸다. 특히 불일치 잡음 환경에서 평균 오류율이 15.87 %의 상대 향상률을 나타낸다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201911338887557&target=NART&cn=JAKO201911338887557",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 본 논문에서는 잡음 환경에서 효과적인 음성 인식을 위하여 DNN(Deep Neural Network) 기반의 잡음 오염 함수 예측을 이용한 음향 모델 적응 기법을 제안한다. 깨끗한 음성과 잡음 정보를 입력으로 하고 오염된 음성에 대한 특징 벡터를 출력으로 하는 DNN을 학습하여 비선형 관계를 갖는 잡음 오염 함수를 예측한다. 예측된 잡음 오염 함수를 음향모델의 평균 벡터에 적용하여 잡음 환경에 적응된 음향 모델을 생성한다. Aurora 2.0 데이터를 이용한 음성 인식 성능 평가에서 본 논문에서 제안한 모델 적응 기법이 기존의 전처리, 모델 적응 기법에 비해 일치, 불일치 잡음 환경에서 모두 평균적으로 우수한 성능을 나타낸다. 특히 불일치 잡음 환경에서 평균 오류율이 15.87 %의 상대 향상률을 나타낸다."
        },
        {
          "rank": 34,
          "score": 0.6633584499359131,
          "doc_id": "ART002391816",
          "title": "Artificial Intelligence, Language Intelligence, and Mathematics",
          "abstract": "Artificial neural networks(ANN) has provided a theoretical framework on the study of human behavior/cognition and artificial intelligence. This article aims to introduce ANN and its mathematical principle to the field of applied linguistics. An ANN consists of input, hidden, and output vectors and the vectors are connected to one another by weight matrices. Mapping from input to output is accounted for by simple matrix multiplication.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002391816&target=NART&cn=ART002391816",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Artificial Intelligence, Language Intelligence, and Mathematics Artificial Intelligence, Language Intelligence, and Mathematics Artificial Intelligence, Language Intelligence, and Mathematics Artificial neural networks(ANN) has provided a theoretical framework on the study of human behavior/cognition and artificial intelligence. This article aims to introduce ANN and its mathematical principle to the field of applied linguistics. An ANN consists of input, hidden, and output vectors and the vectors are connected to one another by weight matrices. Mapping from input to output is accounted for by simple matrix multiplication."
        },
        {
          "rank": 35,
          "score": 0.6632564067840576,
          "doc_id": "JAKO200416642157049",
          "title": "Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식",
          "abstract": "In this paper, a new noise compensation method based on the eigenvoice framework in feature space is proposed to reduce the mismatch between training and testing environments. The difference between clean and noisy environments is represented by the linear combination of K eigenvectors that represent the variation among environments. In the proposed method, the performance improvement of speech recognition systems is largely affected by how to construct the noisy models and the bias vector set. In this paper, two methods, the one based on MAP adaptation method and the other using stereo DB, are proposed to construct the noisy models. In experiments using Aurora 2 DB, we obtained 44.86% relative improvement with eigen-environment method in comparison with baseline system. Especially, in clean condition training mode, our proposed method yielded 66.74% relative improvement, which is better performance than several methods previously proposed in Aurora project.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200416642157049&target=NART&cn=JAKO200416642157049",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식 Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식 Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식 In this paper, a new noise compensation method based on the eigenvoice framework in feature space is proposed to reduce the mismatch between training and testing environments. The difference between clean and noisy environments is represented by the linear combination of K eigenvectors that represent the variation among environments. In the proposed method, the performance improvement of speech recognition systems is largely affected by how to construct the noisy models and the bias vector set. In this paper, two methods, the one based on MAP adaptation method and the other using stereo DB, are proposed to construct the noisy models. In experiments using Aurora 2 DB, we obtained 44.86% relative improvement with eigen-environment method in comparison with baseline system. Especially, in clean condition training mode, our proposed method yielded 66.74% relative improvement, which is better performance than several methods previously proposed in Aurora project."
        },
        {
          "rank": 36,
          "score": 0.663123607635498,
          "doc_id": "JAKO201935164467523",
          "title": "심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구",
          "abstract": "본 논문에서는 구개인두부전증(VeloPharyngeal Insufficiency, VPI) 환자의 음성을 효과적으로 인식하기 위해 컨볼루션 신경망 (Convolutional Neural Network, CNN), 장단기 모델(Long Short Term Memory, LSTM) 구조 신경망을 은닉 마르코프 모델(Hidden Markov Model, HMM)과 결합한 하이브리드 구조의 음성 인식 시스템을 구축하고 모델 적응 기법을 적용하여, 기존 Gaussian Mixture Model(GMM-HMM), 완전 연결형 Deep Neural Network(DNN-HMM) 기반의 음성 인식 시스템과 성능을 비교한다. 정상인 화자가 PBW452단어를 발화한 데이터를 이용하여 초기 모델을 학습하고 정상인 화자의 VPI 모의 음성을 이용하여 화자 적응의 사전 모델을 생성한 후에 VPI 환자들의 음성으로 추가 적응 학습을 진행한다. VPI환자의 화자 적응 시에 CNN-HMM 기반 모델에서는 일부층만 적응 학습하고, LSTM-HMM 기반 모델의 경우에는 드롭 아웃 규제기법을 적용하여 성능을 관찰한 결과 기존 완전 연결형 DNN-HMM 인식기보다 3.68 % 향상된 음성 인식 성능을 나타낸다. 이러한 결과는 본 논문에서 제안하는 LSTM-HMM 기반의 하이브리드 음성 인식 기법이 많은 데이터를 확보하기 어려운 VPI 환자 음성에 대해 보다 향상된 인식률의 음성 인식 시스템을 구축하는데 효과적임을 입증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201935164467523&target=NART&cn=JAKO201935164467523",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 본 논문에서는 구개인두부전증(VeloPharyngeal Insufficiency, VPI) 환자의 음성을 효과적으로 인식하기 위해 컨볼루션 신경망 (Convolutional Neural Network, CNN), 장단기 모델(Long Short Term Memory, LSTM) 구조 신경망을 은닉 마르코프 모델(Hidden Markov Model, HMM)과 결합한 하이브리드 구조의 음성 인식 시스템을 구축하고 모델 적응 기법을 적용하여, 기존 Gaussian Mixture Model(GMM-HMM), 완전 연결형 Deep Neural Network(DNN-HMM) 기반의 음성 인식 시스템과 성능을 비교한다. 정상인 화자가 PBW452단어를 발화한 데이터를 이용하여 초기 모델을 학습하고 정상인 화자의 VPI 모의 음성을 이용하여 화자 적응의 사전 모델을 생성한 후에 VPI 환자들의 음성으로 추가 적응 학습을 진행한다. VPI환자의 화자 적응 시에 CNN-HMM 기반 모델에서는 일부층만 적응 학습하고, LSTM-HMM 기반 모델의 경우에는 드롭 아웃 규제기법을 적용하여 성능을 관찰한 결과 기존 완전 연결형 DNN-HMM 인식기보다 3.68 % 향상된 음성 인식 성능을 나타낸다. 이러한 결과는 본 논문에서 제안하는 LSTM-HMM 기반의 하이브리드 음성 인식 기법이 많은 데이터를 확보하기 어려운 VPI 환자 음성에 대해 보다 향상된 인식률의 음성 인식 시스템을 구축하는데 효과적임을 입증한다."
        },
        {
          "rank": 37,
          "score": 0.6604433059692383,
          "doc_id": "NPAP07942137",
          "title": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP07942137&target=NART&cn=NPAP07942137",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 38,
          "score": 0.6585180163383484,
          "doc_id": "JAKO201809242561431",
          "title": "합성곱 신경망 기반 환경잡음에 강인한 교통 소음 분류 모델",
          "abstract": "도시 유동인구가 증가함에 따라 도시 환경 소음에 관한 연구의 중요성이 증가하고 있다. 본 연구에서는 교통상황에서 발생하는 이상 소음을 최근 환경 소음 분류 연구에서 높은 성능을 보이는 딥러닝 알고리즘을 이용하여 분류한다. 구체적으로는 타이어 제동 마찰음, 자동차 충돌음, 자동차 경적음, 정상 소음 네 개의 클래스에 대하여 합성곱 신경망을 이용하여 분류한다. 또한, 실제 교통 상황에서의 환경잡음에 강인한 분류 성능을 갖기 위해 빗소리, 바람 소리, 군중 소리의 세 가지 환경잡음을 설정하였고 이를 활용하여 분류 모델을 설계하였으며 3 dB SNR(Signal to Noise Ratio) 조건에서 88 % 이상의 분류 성능을 가진다. 제시한 교통 소음에 대하여 기존 선행연구 대비 높은 분류 성능을 보이고, 빗소리, 바람 소리, 군중 소리의 세 가지 환경잡음에 강인한 교통 소음 분류 모델을 제안한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201809242561431&target=NART&cn=JAKO201809242561431",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "합성곱 신경망 기반 환경잡음에 강인한 교통 소음 분류 모델 합성곱 신경망 기반 환경잡음에 강인한 교통 소음 분류 모델 합성곱 신경망 기반 환경잡음에 강인한 교통 소음 분류 모델 도시 유동인구가 증가함에 따라 도시 환경 소음에 관한 연구의 중요성이 증가하고 있다. 본 연구에서는 교통상황에서 발생하는 이상 소음을 최근 환경 소음 분류 연구에서 높은 성능을 보이는 딥러닝 알고리즘을 이용하여 분류한다. 구체적으로는 타이어 제동 마찰음, 자동차 충돌음, 자동차 경적음, 정상 소음 네 개의 클래스에 대하여 합성곱 신경망을 이용하여 분류한다. 또한, 실제 교통 상황에서의 환경잡음에 강인한 분류 성능을 갖기 위해 빗소리, 바람 소리, 군중 소리의 세 가지 환경잡음을 설정하였고 이를 활용하여 분류 모델을 설계하였으며 3 dB SNR(Signal to Noise Ratio) 조건에서 88 % 이상의 분류 성능을 가진다. 제시한 교통 소음에 대하여 기존 선행연구 대비 높은 분류 성능을 보이고, 빗소리, 바람 소리, 군중 소리의 세 가지 환경잡음에 강인한 교통 소음 분류 모델을 제안한다."
        },
        {
          "rank": 39,
          "score": 0.6557653546333313,
          "doc_id": "NART17510385",
          "title": "Hidden-articulator Markov models for speech recognition",
          "abstract": "<P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART17510385&target=NART&cn=NART17510385",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition <P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>"
        },
        {
          "rank": 40,
          "score": 0.6557620763778687,
          "doc_id": "NART20682074",
          "title": "Robust combination of neural networks and hidden Markov models for speech recognition",
          "abstract": "Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the 'divergence problem'-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20682074&target=NART&cn=NART20682074",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Robust combination of neural networks and hidden Markov models for speech recognition Robust combination of neural networks and hidden Markov models for speech recognition Robust combination of neural networks and hidden Markov models for speech recognition Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the 'divergence problem'-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs."
        },
        {
          "rank": 41,
          "score": 0.6547659635543823,
          "doc_id": "JAKO199811921284763",
          "title": "은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식",
          "abstract": "한국어 연속 음성에서 발생하는 조음결합문제를 해결하기 위하여 단어를 기본 인식 단위로 사용할 경우 각 단어의 효율적인 표현 방법, 연속된 단어로 이루어진 여러 문장의 표현 방법 그리고 입력된 연속음성을 연속된 여러 단어로의 정합 방법에 관한 연구가 선행되어야 한다. 본 논문에서는 은닉 마르코프 모델과 레벨빌딩 알고리즘을 이용한 한국어 연속 음성 인식 시스템을 제안한다. 각 단어는 은닉 마르코프 모델로 표현하고 문장을 표현하기 위하여 단어 모델을 연결한 형태인 인식 네트워크를 구성한다. 인식네트워크의 탐색 알고리즘으로는 레벨 빌딩 알고리즘을 사용한다. 제안한 방법은 항공기 예약 시스템에 적용한 실험에서 인식율과 인식속도면에서 실용적이었으며 또한 비교적 적은 저장공간으로 전체 문장을 표현하고 쉽게 확장할 수 있다는 장점을 가지고 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199811921284763&target=NART&cn=JAKO199811921284763",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식 은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식 은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식 한국어 연속 음성에서 발생하는 조음결합문제를 해결하기 위하여 단어를 기본 인식 단위로 사용할 경우 각 단어의 효율적인 표현 방법, 연속된 단어로 이루어진 여러 문장의 표현 방법 그리고 입력된 연속음성을 연속된 여러 단어로의 정합 방법에 관한 연구가 선행되어야 한다. 본 논문에서는 은닉 마르코프 모델과 레벨빌딩 알고리즘을 이용한 한국어 연속 음성 인식 시스템을 제안한다. 각 단어는 은닉 마르코프 모델로 표현하고 문장을 표현하기 위하여 단어 모델을 연결한 형태인 인식 네트워크를 구성한다. 인식네트워크의 탐색 알고리즘으로는 레벨 빌딩 알고리즘을 사용한다. 제안한 방법은 항공기 예약 시스템에 적용한 실험에서 인식율과 인식속도면에서 실용적이었으며 또한 비교적 적은 저장공간으로 전체 문장을 표현하고 쉽게 확장할 수 있다는 장점을 가지고 있다."
        },
        {
          "rank": 42,
          "score": 0.6534910202026367,
          "doc_id": "NART06155061",
          "title": "Speech enhancement based on neural predictive hidden Markov model",
          "abstract": "<P><B>Abstract</B></P><P>In this paper, we describe a new approach to speech enhancement by modeling directly the statistical characteristics of the speech waveform. To represent the nonlinear and nonstationary nature of speech, it is assumed that speech is the output of a neural predictive hidden Markov model (NPHMM). The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a Markov chain. Given some speech data, the parameter of NPHMM is estimated by a learning algorithm based on the combination of Baum&#x2013;Welch algorithm and a neural network learning algorithm using the well known back propagation technique. Given the parameters of NPHMM, a recursive estimation method using multiple Kalman filters, governed by a Markov state chain according to the transition probabilities is developed for enhancing speech signals degraded by statistically independent additive noise characteristics assumed to be white and Gaussian. Under various input signal-to-noise ratios (SNRs), the proposed recursive speech enhancement method achieves an improvement over the method based on hidden filter model (Lee and Shirai, 1996) of about 0.8&#x2013;1.2dB in terms of the measured output SNR.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART06155061&target=NART&cn=NART06155061",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech enhancement based on neural predictive hidden Markov model Speech enhancement based on neural predictive hidden Markov model Speech enhancement based on neural predictive hidden Markov model <P><B>Abstract</B></P><P>In this paper, we describe a new approach to speech enhancement by modeling directly the statistical characteristics of the speech waveform. To represent the nonlinear and nonstationary nature of speech, it is assumed that speech is the output of a neural predictive hidden Markov model (NPHMM). The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a Markov chain. Given some speech data, the parameter of NPHMM is estimated by a learning algorithm based on the combination of Baum&#x2013;Welch algorithm and a neural network learning algorithm using the well known back propagation technique. Given the parameters of NPHMM, a recursive estimation method using multiple Kalman filters, governed by a Markov state chain according to the transition probabilities is developed for enhancing speech signals degraded by statistically independent additive noise characteristics assumed to be white and Gaussian. Under various input signal-to-noise ratios (SNRs), the proposed recursive speech enhancement method achieves an improvement over the method based on hidden filter model (Lee and Shirai, 1996) of about 0.8&#x2013;1.2dB in terms of the measured output SNR.</P>"
        },
        {
          "rank": 43,
          "score": 0.6515600681304932,
          "doc_id": "JAKO200727500236879",
          "title": "자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식",
          "abstract": "In this paper, we carried out recognition experiments for noisy speech having various levels of car noise and output of an audio system using the speech interface. The speech interface consists of three parts: pre-processing, acoustic echo canceller, post-processing. First, a high pass filter is employed as a pre-processing part to remove some engine noises. Then, an echo canceller implemented by using an FIR-type filter with an NLMS adaptive algorithm is used to remove the music or speech coming from the audio system in a car. As a last part, the MMSE-STSA based speech enhancement method is applied to the out of the echo canceller to remove the residual noise further. For recognition experiments, we generated test signals by adding music to the car noisy speech from Aurora 2 database. The HTK-based continuous HMM system is constructed for a recognition system. Experimental results show that the proposed speech interface is very promising for robust speech recognition in a noisy car environment.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200727500236879&target=NART&cn=JAKO200727500236879",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 In this paper, we carried out recognition experiments for noisy speech having various levels of car noise and output of an audio system using the speech interface. The speech interface consists of three parts: pre-processing, acoustic echo canceller, post-processing. First, a high pass filter is employed as a pre-processing part to remove some engine noises. Then, an echo canceller implemented by using an FIR-type filter with an NLMS adaptive algorithm is used to remove the music or speech coming from the audio system in a car. As a last part, the MMSE-STSA based speech enhancement method is applied to the out of the echo canceller to remove the residual noise further. For recognition experiments, we generated test signals by adding music to the car noisy speech from Aurora 2 database. The HTK-based continuous HMM system is constructed for a recognition system. Experimental results show that the proposed speech interface is very promising for robust speech recognition in a noisy car environment."
        },
        {
          "rank": 44,
          "score": 0.6511776447296143,
          "doc_id": "JAKO202011263332681",
          "title": "심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용",
          "abstract": "가우스 혼합 모델-은닉 마코프 모델(Gaussian Mixture Model-Hidden Markov Model, GMM-HMM)을 이용하는 전통적인 음성인식 시스템에서는, 극점 필터링 기반의 켑스트럼 특징 정규화 방식이 잡음 환경에서 짧은 발화의 인식 성능을 향상시키는데 효과적이었다. 본 논문에서는 심층신경망(Deep Neural Network, DNN)을 이용하는 최신의 음성인식 시스템에서도 이 방식의 유용성이 있는지 검토한다. AURORA 2 DB에 대한 실험 결과, 특히 훈련 및 테스트 환경 사이의 불일치가 클 때에, 극점 필터링 기반의 켑스트럼 평균 분산 정규화 방식이 극점 필터링을 사용하지 않는 방식에 비해 매우 짧은 발화의 인식 성능을 개선시킴을 보여 준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202011263332681&target=NART&cn=JAKO202011263332681",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 가우스 혼합 모델-은닉 마코프 모델(Gaussian Mixture Model-Hidden Markov Model, GMM-HMM)을 이용하는 전통적인 음성인식 시스템에서는, 극점 필터링 기반의 켑스트럼 특징 정규화 방식이 잡음 환경에서 짧은 발화의 인식 성능을 향상시키는데 효과적이었다. 본 논문에서는 심층신경망(Deep Neural Network, DNN)을 이용하는 최신의 음성인식 시스템에서도 이 방식의 유용성이 있는지 검토한다. AURORA 2 DB에 대한 실험 결과, 특히 훈련 및 테스트 환경 사이의 불일치가 클 때에, 극점 필터링 기반의 켑스트럼 평균 분산 정규화 방식이 극점 필터링을 사용하지 않는 방식에 비해 매우 짧은 발화의 인식 성능을 개선시킴을 보여 준다."
        },
        {
          "rank": 45,
          "score": 0.6497673988342285,
          "doc_id": "DIKO0015893049",
          "title": "도메인 적대적 신경망을 이용한 종단 간 억양음성인식",
          "abstract": "최근 딥러닝(Deep learning) 기술의 발전은 음성인식 성능 향상에 크게 기여하였다. 이러한 발전에도 불구하고 소음, 감정, 억양 등이 섞인 특정 발화에 대해서는 좋은 성능을 보이지 못하고 있다. 이 가운데 억양이 섞인 발화는 표준 발화와 비교했을 때 언어학적인 차이가 존재하는데, 이러한 차이가 억양이 섞인 발화를 인식하기 어렵게 만든다. 따라서 본 연구에서는 억양이 섞인 발화와 표준 발화 사이에 존재하는 특성의 차이를 줄이고자 도메인 적대적 신경망(Domain Adversarial Neural Network) 기법을 사용하였다. 또한, 종단 간(End-to-end) 기법을 사용하여 음성인식의 과정을 간소화하였다.&amp;#xD; 오래전부터 억양음성인식의 성능을 높이기 위한 연구는 활발히 진행되어 왔다. 2010년대 초반까지는 가우시안 혼합 모델(Gaussian Mixture Model) 기반의 최대 사후 확률(Maximum A Posteriori), 최대 우도 선형 회귀(Maximum Likelihood Linear Regression) 적응 기법이 주로 사용되었다. 하지만 딥러닝 기술이 발전하고 신경망 기반의 모델들이 주목 받기 시작하면서 가우시안 혼합 모델보다는 신경망 모델에 적합한 기법들이 사용되었다. 최근 몇 년간은 음성인식 모델에 억양에 대한 정보를 직접 삽입하는 accent embedding 기법이 많이 사용되었다. Accent embedding 기법은 억양음성인식의 성능을 향상시켰지만 몇 가지 문제점을 가지고 있다. 첫째, 억양을 분류하여 accent embedding 특징들을 만들어내는 모델을 독립적으로 만들어 훈련시켜야 하며, 해당 모델의 억양 분류 정확도가 음성인식 모델의 성능에 큰 영향을 미치기 때문에 모델을 정교하게 만들어야 하는 부담감이 있다. &amp;#xD; 둘째, 억양 분류 모델의 결과를 음성인식 모델의 추가적인 입력 특징(Input feature)으로 사용하기 때문에 음성인식 모델의 매개변수를 증가시키며 계산량 또한 증가한다는 문제가 있다. 따라서 본 연구에서는 추가적인 입력 특징이 필요하지 않고 음성인식에서 기본적으로 사용되는 특징인 주파수 정보(스펙트로그램)만을 이용하여 학습이 가능한 도메인 적대적 신경망을 기법을 제안하였다. &amp;#xD; 도메인 적대적 신경망은 소스 도메인(Source domain) 데이터와 타겟 도메인(Target domain) 데이터가 적대적으로 학습이 되면서 두 도메인 간의 분포 차이를 줄이는 것을 목적으로 한다. 본 연구의 목표인 억양음성인식에서는 표준 발화를 소스 도메인으로, 억양이 섞인 발화를 타겟 도메인으로 정하였다. 도메인 적대적 신경망은 특징 추출기(Feature extractor), 도메인 분류기(Domain classifier), 레이블 예측기(Label predictor) 총 3개의 부분망(sub-network)으로 구성된다. 각각의 부분망은 서로 다른 역할을 수행하기 때문에 신경망의 특성을 고려하여 만들어야 한다. 따라서 본 연구에서는 신경망의 특성을 고려하여 특징 추출기에는 합성곱 신경망(Convolutional Neural Network)을, 도메인 분류기에는 심층 신경망(Deep Neural Network)을, 그리고 레이블 예측기에는 양방향 게이트 순환 유닛(Bidirectional Gated Recurrent Unit)을 이용하여 도메인 적대적 신경망을 구성하였다. 또한, 레이블을 예측할 때 종단 간 기법을 활용하여 입력 데이터를 사전 분할하지 않고, 레이블 예측 이후의 후처리 작업을 없애면서 음성인식 과정을 간소화하였다.&amp;#xD; 본 연구에서 제안한 도메인 적대적 학습 기반의 억양음성인식 기법의 효과를 입증하기 위하여 Baseline 모델과 DANN 모델을 만들어 실험을 진행하였다. 실험 데이터로는 Mozilla의 Common Voice 코퍼스를 사용하였는데, Common Voice 코퍼스는 여러 언어에 대해 막대한 양의 검증된 음성파일을 오픈소스로 제공하기 때문에 음성인식 연구에서 많이 사용된다. 또한, Common Voice 코퍼스는 음성 녹음 파일과 함께 억양 정보도 같이 제공을 하기 때문에 억양음성인식 연구에 효율적으로 사용될 수 있다. &amp;#xD; Common Voice 코퍼스의 영어 데이터셋은 여러 억양의 음성파일들을 가지고 있는데, 본 연구에서는 미국 억양, 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양의 데이터를 실험에 사용하였으며, 미국 억양을 소스 도메인으로 나머지 네 개의 억양을 타겟 도메인으로 정하였다.&amp;#xD; 실험 결과 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양 모두에서 DANN 모델의 성능이 Baseline 모델보다 높은 성능을 보였다. 하지만 억양에 따라 성능 개선의 차이가 있었으며, 캐나다 억양에 비해 잉글랜드 억양과 인도 억양에서 성능이 눈에 띄게 향상되었다. 이 같은 결과는 잉글랜드 억양과 인도 억양이 소스 도메인으로 사용된 미국 억양 데이터와 언어학적으로 큰 차이가 존재하여 baseline 모델에서는 성능이 낮았으나, 도메인 적대적 학습을 통해 생성된 DANN 모델이 타겟 억양의 특성을 반영함으로써 성능이 크게 개선된 것으로 분석된다. 따라서 도메인 적대적 신경망은 소스 도메인과 타겟 도메인 사이의 분포의 차이를 줄임으로서 억양음성인식의 성능을 향상시킬 수 있음이 확인되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015893049&target=NART&cn=DIKO0015893049",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "도메인 적대적 신경망을 이용한 종단 간 억양음성인식 도메인 적대적 신경망을 이용한 종단 간 억양음성인식 도메인 적대적 신경망을 이용한 종단 간 억양음성인식 최근 딥러닝(Deep learning) 기술의 발전은 음성인식 성능 향상에 크게 기여하였다. 이러한 발전에도 불구하고 소음, 감정, 억양 등이 섞인 특정 발화에 대해서는 좋은 성능을 보이지 못하고 있다. 이 가운데 억양이 섞인 발화는 표준 발화와 비교했을 때 언어학적인 차이가 존재하는데, 이러한 차이가 억양이 섞인 발화를 인식하기 어렵게 만든다. 따라서 본 연구에서는 억양이 섞인 발화와 표준 발화 사이에 존재하는 특성의 차이를 줄이고자 도메인 적대적 신경망(Domain Adversarial Neural Network) 기법을 사용하였다. 또한, 종단 간(End-to-end) 기법을 사용하여 음성인식의 과정을 간소화하였다.&amp;#xD; 오래전부터 억양음성인식의 성능을 높이기 위한 연구는 활발히 진행되어 왔다. 2010년대 초반까지는 가우시안 혼합 모델(Gaussian Mixture Model) 기반의 최대 사후 확률(Maximum A Posteriori), 최대 우도 선형 회귀(Maximum Likelihood Linear Regression) 적응 기법이 주로 사용되었다. 하지만 딥러닝 기술이 발전하고 신경망 기반의 모델들이 주목 받기 시작하면서 가우시안 혼합 모델보다는 신경망 모델에 적합한 기법들이 사용되었다. 최근 몇 년간은 음성인식 모델에 억양에 대한 정보를 직접 삽입하는 accent embedding 기법이 많이 사용되었다. Accent embedding 기법은 억양음성인식의 성능을 향상시켰지만 몇 가지 문제점을 가지고 있다. 첫째, 억양을 분류하여 accent embedding 특징들을 만들어내는 모델을 독립적으로 만들어 훈련시켜야 하며, 해당 모델의 억양 분류 정확도가 음성인식 모델의 성능에 큰 영향을 미치기 때문에 모델을 정교하게 만들어야 하는 부담감이 있다. &amp;#xD; 둘째, 억양 분류 모델의 결과를 음성인식 모델의 추가적인 입력 특징(Input feature)으로 사용하기 때문에 음성인식 모델의 매개변수를 증가시키며 계산량 또한 증가한다는 문제가 있다. 따라서 본 연구에서는 추가적인 입력 특징이 필요하지 않고 음성인식에서 기본적으로 사용되는 특징인 주파수 정보(스펙트로그램)만을 이용하여 학습이 가능한 도메인 적대적 신경망을 기법을 제안하였다. &amp;#xD; 도메인 적대적 신경망은 소스 도메인(Source domain) 데이터와 타겟 도메인(Target domain) 데이터가 적대적으로 학습이 되면서 두 도메인 간의 분포 차이를 줄이는 것을 목적으로 한다. 본 연구의 목표인 억양음성인식에서는 표준 발화를 소스 도메인으로, 억양이 섞인 발화를 타겟 도메인으로 정하였다. 도메인 적대적 신경망은 특징 추출기(Feature extractor), 도메인 분류기(Domain classifier), 레이블 예측기(Label predictor) 총 3개의 부분망(sub-network)으로 구성된다. 각각의 부분망은 서로 다른 역할을 수행하기 때문에 신경망의 특성을 고려하여 만들어야 한다. 따라서 본 연구에서는 신경망의 특성을 고려하여 특징 추출기에는 합성곱 신경망(Convolutional Neural Network)을, 도메인 분류기에는 심층 신경망(Deep Neural Network)을, 그리고 레이블 예측기에는 양방향 게이트 순환 유닛(Bidirectional Gated Recurrent Unit)을 이용하여 도메인 적대적 신경망을 구성하였다. 또한, 레이블을 예측할 때 종단 간 기법을 활용하여 입력 데이터를 사전 분할하지 않고, 레이블 예측 이후의 후처리 작업을 없애면서 음성인식 과정을 간소화하였다.&amp;#xD; 본 연구에서 제안한 도메인 적대적 학습 기반의 억양음성인식 기법의 효과를 입증하기 위하여 Baseline 모델과 DANN 모델을 만들어 실험을 진행하였다. 실험 데이터로는 Mozilla의 Common Voice 코퍼스를 사용하였는데, Common Voice 코퍼스는 여러 언어에 대해 막대한 양의 검증된 음성파일을 오픈소스로 제공하기 때문에 음성인식 연구에서 많이 사용된다. 또한, Common Voice 코퍼스는 음성 녹음 파일과 함께 억양 정보도 같이 제공을 하기 때문에 억양음성인식 연구에 효율적으로 사용될 수 있다. &amp;#xD; Common Voice 코퍼스의 영어 데이터셋은 여러 억양의 음성파일들을 가지고 있는데, 본 연구에서는 미국 억양, 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양의 데이터를 실험에 사용하였으며, 미국 억양을 소스 도메인으로 나머지 네 개의 억양을 타겟 도메인으로 정하였다.&amp;#xD; 실험 결과 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양 모두에서 DANN 모델의 성능이 Baseline 모델보다 높은 성능을 보였다. 하지만 억양에 따라 성능 개선의 차이가 있었으며, 캐나다 억양에 비해 잉글랜드 억양과 인도 억양에서 성능이 눈에 띄게 향상되었다. 이 같은 결과는 잉글랜드 억양과 인도 억양이 소스 도메인으로 사용된 미국 억양 데이터와 언어학적으로 큰 차이가 존재하여 baseline 모델에서는 성능이 낮았으나, 도메인 적대적 학습을 통해 생성된 DANN 모델이 타겟 억양의 특성을 반영함으로써 성능이 크게 개선된 것으로 분석된다. 따라서 도메인 적대적 신경망은 소스 도메인과 타겟 도메인 사이의 분포의 차이를 줄임으로서 억양음성인식의 성능을 향상시킬 수 있음이 확인되었다."
        },
        {
          "rank": 46,
          "score": 0.6482424736022949,
          "doc_id": "NART13642943",
          "title": "Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments",
          "abstract": "<P>Handling background noise or echo (reverberation) etc. is very important for having an automated robot etc. recognize remote speech in a real environment. As effective schemes for handling this problem, noise reducing schemes such as model adaptation schemes including HMM decomposition and composition or microphone array (beamformer) signal processing, spectral subtraction, etc. have been proposed. In particular, a model adaptation scheme is very effective for speech recognition in a noisy environment and its recognition performance increases in proportion to the signal-to-noise ratio (SNR). In this paper, improving the recognition performance in a low-SNR environment by receiving speech at a high SNR using a microphone array before HMM decomposition and composition is attempted. The results of speech recognition experiments conducted in a noisy environment in an acoustic laboratory show an improvement in the recognition rate of about 25% by the proposed method for the case in which the SNR in a single microphone is 0 dB, as compared with the cases of using microphone array signal processing, HMM decomposition and composition alone. In addition, the proposed method shows recognition performance comparable to the case of using cepstrum mean normalization and spectral subtraction performed with an optimal coefficient given to the speech after microphone array processing. &copy; 2002 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 85(9): 13&ndash;22, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ecjb.10068</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART13642943&target=NART&cn=NART13642943",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments <P>Handling background noise or echo (reverberation) etc. is very important for having an automated robot etc. recognize remote speech in a real environment. As effective schemes for handling this problem, noise reducing schemes such as model adaptation schemes including HMM decomposition and composition or microphone array (beamformer) signal processing, spectral subtraction, etc. have been proposed. In particular, a model adaptation scheme is very effective for speech recognition in a noisy environment and its recognition performance increases in proportion to the signal-to-noise ratio (SNR). In this paper, improving the recognition performance in a low-SNR environment by receiving speech at a high SNR using a microphone array before HMM decomposition and composition is attempted. The results of speech recognition experiments conducted in a noisy environment in an acoustic laboratory show an improvement in the recognition rate of about 25% by the proposed method for the case in which the SNR in a single microphone is 0 dB, as compared with the cases of using microphone array signal processing, HMM decomposition and composition alone. In addition, the proposed method shows recognition performance comparable to the case of using cepstrum mean normalization and spectral subtraction performed with an optimal coefficient given to the speech after microphone array processing. &copy; 2002 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 85(9): 13&ndash;22, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ecjb.10068</P>"
        },
        {
          "rank": 47,
          "score": 0.6480672359466553,
          "doc_id": "JAKO200614222983757",
          "title": "Missing-Feature 복구를 위한 대역 독립 방식의 베이시안 분류기 기반 마스크 예측 기법",
          "abstract": "본 논문에서는 알려지지 않은 잡음 환경에서 강인한 음성 인식 성능을 위하여 missing-feature복구 기법을 다루며, 베이시안 분류기를 기반으로 하는 마스크 예측 기법의 성능을 향상시킬 수 있는 방법을 제안한다. 기존의 마스크 예측 기법에서는 배경 잡음 종류에 독립적인 성능을 위해 전 주파수 대역을 분할하여 발생시킨 유색 잡음을 마스크 예측기의 훈련에 이용하였으나, 제한된 양의 훈련 데이터베이스 조건에서는 성능의 한계가 불가피하다. 보다 다양한 잡음 스펙트럼을 반영하면서 마스크 예측의 성능을 향상시키기 위해, 서로 다른 주파수 대역에 독립적인 구조를 가지는 베이시안 분류기를 제안하며, 훈련에 사용하는 유색 잡음의 생성 방식을 이에 맞게 수정한다. 각각의 주파수 대역을 분할하여 유색 잡음을 생성함으로써 다양한 잡음 환경을 반영하는 동시에 훈련 데이터베이스 부족 문제를 줄일 수 있다. 제안하는 마스크 예측 기법을 클러스터 기반의 missing-feature 복구 기법과 결합하여 음성 인식기에 적용함으로써 성능을 평가한다. 실험 결과는 제안한 기법이 백색 잡음, 자동차잡음, 배경 음악환경에서 기존의 방법에 비해 향상된 성능을 가짐을 입증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200614222983757&target=NART&cn=JAKO200614222983757",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Missing-Feature 복구를 위한 대역 독립 방식의 베이시안 분류기 기반 마스크 예측 기법 Missing-Feature 복구를 위한 대역 독립 방식의 베이시안 분류기 기반 마스크 예측 기법 Missing-Feature 복구를 위한 대역 독립 방식의 베이시안 분류기 기반 마스크 예측 기법 본 논문에서는 알려지지 않은 잡음 환경에서 강인한 음성 인식 성능을 위하여 missing-feature복구 기법을 다루며, 베이시안 분류기를 기반으로 하는 마스크 예측 기법의 성능을 향상시킬 수 있는 방법을 제안한다. 기존의 마스크 예측 기법에서는 배경 잡음 종류에 독립적인 성능을 위해 전 주파수 대역을 분할하여 발생시킨 유색 잡음을 마스크 예측기의 훈련에 이용하였으나, 제한된 양의 훈련 데이터베이스 조건에서는 성능의 한계가 불가피하다. 보다 다양한 잡음 스펙트럼을 반영하면서 마스크 예측의 성능을 향상시키기 위해, 서로 다른 주파수 대역에 독립적인 구조를 가지는 베이시안 분류기를 제안하며, 훈련에 사용하는 유색 잡음의 생성 방식을 이에 맞게 수정한다. 각각의 주파수 대역을 분할하여 유색 잡음을 생성함으로써 다양한 잡음 환경을 반영하는 동시에 훈련 데이터베이스 부족 문제를 줄일 수 있다. 제안하는 마스크 예측 기법을 클러스터 기반의 missing-feature 복구 기법과 결합하여 음성 인식기에 적용함으로써 성능을 평가한다. 실험 결과는 제안한 기법이 백색 잡음, 자동차잡음, 배경 음악환경에서 기존의 방법에 비해 향상된 성능을 가짐을 입증한다."
        },
        {
          "rank": 48,
          "score": 0.6444741487503052,
          "doc_id": "JAKO200606140790765",
          "title": "Discrimination of Pathological Speech Using Hidden Markov Models",
          "abstract": "Diagnosis of pathological voice is one of the important issues in biomedical applications of speech technology. This study focuses on the discrimination of voice disorder using HMM (Hidden Markov Model) for automatic detection between normal voice and vocal fold disorder voice. This is a non-intrusive, non-expensive and fully automated method using only a speech sample of the subject. Speech data from normal people and patients were collected. Mel-frequency filter cepstral coefficients (MFCCs) were modeled by HMM classifier. Different states (3 states, 5 states and 7 states), 3 mixtures and left to right HMMs were formed. This method gives an accuracy of 93.8% for train data and 91.7% for test data in the discrimination of normal and vocal fold disorder voice for sustained /a/.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200606140790765&target=NART&cn=JAKO200606140790765",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Discrimination of Pathological Speech Using Hidden Markov Models Discrimination of Pathological Speech Using Hidden Markov Models Discrimination of Pathological Speech Using Hidden Markov Models Diagnosis of pathological voice is one of the important issues in biomedical applications of speech technology. This study focuses on the discrimination of voice disorder using HMM (Hidden Markov Model) for automatic detection between normal voice and vocal fold disorder voice. This is a non-intrusive, non-expensive and fully automated method using only a speech sample of the subject. Speech data from normal people and patients were collected. Mel-frequency filter cepstral coefficients (MFCCs) were modeled by HMM classifier. Different states (3 states, 5 states and 7 states), 3 mixtures and left to right HMMs were formed. This method gives an accuracy of 93.8% for train data and 91.7% for test data in the discrimination of normal and vocal fold disorder voice for sustained /a/."
        },
        {
          "rank": 49,
          "score": 0.6438800096511841,
          "doc_id": "NART75359998",
          "title": "Artificial Neural Networks Applied to Image Steganography",
          "abstract": "<P>This paper presents a technique for transmitting information efficiently and securely, hiding confidential messages on seemingly innocent messages using steganography. The insertion technique in the least significant bit is used to insert images into digital pictures or other secret watermark. Artificial Neural Networks are used in the process of withdrawal of encrypted information acting as keys that determine the existence of hidden information.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART75359998&target=NART&cn=NART75359998",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Artificial Neural Networks Applied to Image Steganography Artificial Neural Networks Applied to Image Steganography Artificial Neural Networks Applied to Image Steganography <P>This paper presents a technique for transmitting information efficiently and securely, hiding confidential messages on seemingly innocent messages using steganography. The insertion technique in the least significant bit is used to insert images into digital pictures or other secret watermark. Artificial Neural Networks are used in the process of withdrawal of encrypted information acting as keys that determine the existence of hidden information.</P>"
        },
        {
          "rank": 50,
          "score": 0.6437990069389343,
          "doc_id": "NART13081886",
          "title": "Speech recognition under noisy environments using segmental unit input HMM",
          "abstract": "<P>In this paper, we apply a segmental unit input HMM to noisy speech recognition. In this modeling, several successive frames are combined and treated as an input vector. We expect that the segmental unit input HMM will be effective for noisy speech recognition because segmental statistics considering correlation between frames reduce noise effects when the correlation of noise between frames is assumed to be small. In recognition experiments, we compared the segmental unit input HMM with a conventional frame-based HMM and found the segmental unit input HMM to be superior. We also compared the segmental unit input HMM with dynamic cepstral coefficients, which have both static and dynamic features, and found that the segmental unit input HMM is more effective than the dynamic cepstrum. We also combined the segmental unit input HMM with a spectral subtraction method and confirmed the effectiveness of the method. Additionally, in experiments using acoustic models trained with noisy speech, the segmental unit input HMM outperformed the conventional HMM. From these results, we propose PMC for the segmental unit input HMM. Experimental results showed the PMC for segmental unit input HMM offered better recognition performance than the original PMC. &copy; 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(8): 111&ndash;120, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1151</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART13081886&target=NART&cn=NART13081886",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition under noisy environments using segmental unit input HMM Speech recognition under noisy environments using segmental unit input HMM Speech recognition under noisy environments using segmental unit input HMM <P>In this paper, we apply a segmental unit input HMM to noisy speech recognition. In this modeling, several successive frames are combined and treated as an input vector. We expect that the segmental unit input HMM will be effective for noisy speech recognition because segmental statistics considering correlation between frames reduce noise effects when the correlation of noise between frames is assumed to be small. In recognition experiments, we compared the segmental unit input HMM with a conventional frame-based HMM and found the segmental unit input HMM to be superior. We also compared the segmental unit input HMM with dynamic cepstral coefficients, which have both static and dynamic features, and found that the segmental unit input HMM is more effective than the dynamic cepstrum. We also combined the segmental unit input HMM with a spectral subtraction method and confirmed the effectiveness of the method. Additionally, in experiments using acoustic models trained with noisy speech, the segmental unit input HMM outperformed the conventional HMM. From these results, we propose PMC for the segmental unit input HMM. Experimental results showed the PMC for segmental unit input HMM offered better recognition performance than the original PMC. &copy; 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(8): 111&ndash;120, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1151</P>"
        }
      ]
    },
    {
      "query": "How are Hidden Markov Models (HMMs) and neural networks (NNs) integrated within the audiovisual speech recognition approach?",
      "query_meta": {
        "type": "single_hop",
        "index": 2
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8010904788970947,
          "doc_id": "DIKO0007842188",
          "title": "신경망 예측 HMM을 이용한 음성인식에 관한 연구",
          "abstract": "음성은 인간의 가장 자연스러운 의사소통의 수단이며 이러한 음성에 의한 인간-기계 인터페이스는 속도가 빠르고 특별한 훈련이 없어도 이루어진다. 또한 컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식 기술은 중요한 연구 과제가 되고 있다. 이러한 음성인식에 관한 연구는 HMM과 신경망에 의한 방법들이 활발히 진행되고 있다. 그러나 HMM은 모델구조의 결정에 어려움이 있으며, 음성의 과도적 정보를 경시하는 경향이 있기 때문에 시간적 상관 표현력이 부족한 결점이 있다. 따라서 이러한 단점을 극복하기 위하여 음성의 동적특성을 표현할 수 있는 회귀계수와 지속시간 확률등을 파라메터로 추가하거나, 언어학적 제약을 가하여 최적의 단어별을 탐색하는 언어적 모델을 결합한 음성이해 시스템으로 발전하고 있다. 또한 신경망의 경우 그 구조나 가중치에 시간적인 것이 고려되지 않으므로 패턴이 정적인 경우 인식률이 높으나 음성과 같은 시계열 패턴의 비선형 신축을 취급하기 어렵다. 따라서 이러한 단점을 보완하기 위하여 회귀신경망, 예측형 신경망등이 제안되었다. 본 논문에서는 이러한 점들을 고려하여 HMM의 패턴에 대한 시간적 상관표현력을 개선시킬 수 있는 신경망 예측 HMM을 제안한다. 신경망 예측 HMM은 HMM과 신경망의 장점을 함께 사용할 수 있는 하이브리드형 네트워크로 예측형 신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하는 것이다. 따라서 예측형 신경망은 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가지므로 HMM에 동적 특성을 부여할 수 있다. 실험에서는 단독 숫자음과 100음절 데이터를 이용하여 HMM과 신경망 그리고 본 논문에서 제안한 신경망 예측 HMM의 인식성능을 비교·검토하였다. 신경망 예측 HMM은 MLP 예측 HMM, Elman망 예측 HMM, Jordan망 예측 HMM의 3가지이다. 실험결과 평가용 데이터를 기준으로 단독 숫자음에 대해서는 MLP 예측 HMM이 99.5%로 가장 우수한 결과를, 100음절 데이터에 대해서는 HMM이 87.7%로 가장 우수한 결과를 보였다. Elman망 예측 HMM과 Jordan망 예측 HMM의 경우 회귀구조임에도 불구하고 MLP 예측 HMM의 인식성능을 상회하지는 못하였다. 따라서 신경망 예측 HMM의 인식성능을 향상시키기 위해서는 데이터에 따른 효과의 검토와 음성패턴의 시간적 상관관계를 보다 더 잘 표현 할 수 있는 신경망의 구조에 대한 연구가 요구되며, 신경망과 HMM의 학습알고리즘에 대한 연구가 필요하다고 판단된다. 본 논문에서 제안한 신경망 예측 HMM은 HMM과 신경망의 결합이라고 하는 향후 가장 유효한 방법의 하나로 사료되며 연속음성 인식시스템으로 확장할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0007842188&target=NART&cn=DIKO0007842188",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망 예측 HMM을 이용한 음성인식에 관한 연구 신경망 예측 HMM을 이용한 음성인식에 관한 연구 신경망 예측 HMM을 이용한 음성인식에 관한 연구 음성은 인간의 가장 자연스러운 의사소통의 수단이며 이러한 음성에 의한 인간-기계 인터페이스는 속도가 빠르고 특별한 훈련이 없어도 이루어진다. 또한 컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식 기술은 중요한 연구 과제가 되고 있다. 이러한 음성인식에 관한 연구는 HMM과 신경망에 의한 방법들이 활발히 진행되고 있다. 그러나 HMM은 모델구조의 결정에 어려움이 있으며, 음성의 과도적 정보를 경시하는 경향이 있기 때문에 시간적 상관 표현력이 부족한 결점이 있다. 따라서 이러한 단점을 극복하기 위하여 음성의 동적특성을 표현할 수 있는 회귀계수와 지속시간 확률등을 파라메터로 추가하거나, 언어학적 제약을 가하여 최적의 단어별을 탐색하는 언어적 모델을 결합한 음성이해 시스템으로 발전하고 있다. 또한 신경망의 경우 그 구조나 가중치에 시간적인 것이 고려되지 않으므로 패턴이 정적인 경우 인식률이 높으나 음성과 같은 시계열 패턴의 비선형 신축을 취급하기 어렵다. 따라서 이러한 단점을 보완하기 위하여 회귀신경망, 예측형 신경망등이 제안되었다. 본 논문에서는 이러한 점들을 고려하여 HMM의 패턴에 대한 시간적 상관표현력을 개선시킬 수 있는 신경망 예측 HMM을 제안한다. 신경망 예측 HMM은 HMM과 신경망의 장점을 함께 사용할 수 있는 하이브리드형 네트워크로 예측형 신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하는 것이다. 따라서 예측형 신경망은 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가지므로 HMM에 동적 특성을 부여할 수 있다. 실험에서는 단독 숫자음과 100음절 데이터를 이용하여 HMM과 신경망 그리고 본 논문에서 제안한 신경망 예측 HMM의 인식성능을 비교·검토하였다. 신경망 예측 HMM은 MLP 예측 HMM, Elman망 예측 HMM, Jordan망 예측 HMM의 3가지이다. 실험결과 평가용 데이터를 기준으로 단독 숫자음에 대해서는 MLP 예측 HMM이 99.5%로 가장 우수한 결과를, 100음절 데이터에 대해서는 HMM이 87.7%로 가장 우수한 결과를 보였다. Elman망 예측 HMM과 Jordan망 예측 HMM의 경우 회귀구조임에도 불구하고 MLP 예측 HMM의 인식성능을 상회하지는 못하였다. 따라서 신경망 예측 HMM의 인식성능을 향상시키기 위해서는 데이터에 따른 효과의 검토와 음성패턴의 시간적 상관관계를 보다 더 잘 표현 할 수 있는 신경망의 구조에 대한 연구가 요구되며, 신경망과 HMM의 학습알고리즘에 대한 연구가 필요하다고 판단된다. 본 논문에서 제안한 신경망 예측 HMM은 HMM과 신경망의 결합이라고 하는 향후 가장 유효한 방법의 하나로 사료되며 연속음성 인식시스템으로 확장할 수 있을 것으로 기대된다."
        },
        {
          "rank": 2,
          "score": 0.7940711975097656,
          "doc_id": "NART18014750",
          "title": "Neural nets and hidden Markov models: Review and generalizations",
          "abstract": "Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs).",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART18014750&target=NART&cn=NART18014750",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural nets and hidden Markov models: Review and generalizations Neural nets and hidden Markov models: Review and generalizations Neural nets and hidden Markov models: Review and generalizations Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs)."
        },
        {
          "rank": 3,
          "score": 0.7882605791091919,
          "doc_id": "NART16453920",
          "title": "Neural-network-based HMM adaptation for noisy speech recognition.",
          "abstract": "<P>This paper proposes a new method, using neural networks, of adapting phone HMMs to noisy speech. The neural networks are designed to map clean speech HMMs to noise-adapted HMMs, using noise HMMs and signal-to-noise ratios (SNRs) as inputs. The neural network is trained by minimizing the mean square error between the output HMMs and the target noise-adapted HMMs. In an evaluation, the proposed method was used to recognize noisy broadcast-news speech in speaker-dependent and speaker-independent modes. The trained networks were found to be effective in recognizing new speakers under new noise and various SNR conditions.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART16453920&target=NART&cn=NART16453920",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural-network-based HMM adaptation for noisy speech recognition. Neural-network-based HMM adaptation for noisy speech recognition. Neural-network-based HMM adaptation for noisy speech recognition. <P>This paper proposes a new method, using neural networks, of adapting phone HMMs to noisy speech. The neural networks are designed to map clean speech HMMs to noise-adapted HMMs, using noise HMMs and signal-to-noise ratios (SNRs) as inputs. The neural network is trained by minimizing the mean square error between the output HMMs and the target noise-adapted HMMs. In an evaluation, the proposed method was used to recognize noisy broadcast-news speech in speaker-dependent and speaker-independent modes. The trained networks were found to be effective in recognizing new speakers under new noise and various SNR conditions.</P>"
        },
        {
          "rank": 4,
          "score": 0.7761327028274536,
          "doc_id": "JAKO200411922338894",
          "title": "신경망 기반 음성, 영상 및 문맥 통합 음성인식",
          "abstract": "최근 잡음환경에서 신뢰도 높은 음성인식을 위해 음성정보와 영상정보를 융합하는 방법이 활발히 연구되고 있다. 본 논문에서는 이절적인 정보의 융합에 적합한 신경망 모델을 기반으로 음성, 영상 및 문맥 정보 등 다양한 정보를 융합하여 잡음 환경에서 고려단어를 인식하는 음성인식 기법에 대하여 기술한다. 음성과 영상 특징을 이용한 이중 모드 신경망 BMNN(BiModal Neural Network)을 제안한다. BMM은 4개 층으로 이루어진 다층퍼셉트론의 구조를 가지며 각 층은 입력 특징의 추상화 기능을 수행한다. BMNN에서는 제 3층이 잡음에 의한 음성 정보의 손실을 보상하기 위하여 음성과 영상 특징을 통합하는 기능을 수행한다. 또한, 잡음환경에서 음성 인식률을 향상시키기 위해 사용자가 말한 단어들의 순차 패턴을 나타내는 문맥정보를 이용한 후처리 방법을 제안한다. 잡음환경에서 BMNN은 단순히 음성만을 사용한 것 보다 높은 성능을 보임으로써 그 타당성을 확인할 수 있을 뿐 아니라, 특히 문맥을 이용한 후처리를 하였을 경우 잡음 환경에서 90%이상의 인식률을 달성하였다 본 연구는 잡음환경에서 강인한 음성인식을 위해 다양한 추가 정보를 사용함으로써 성능을 향상시킬 수 있음을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200411922338894&target=NART&cn=JAKO200411922338894",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망 기반 음성, 영상 및 문맥 통합 음성인식 신경망 기반 음성, 영상 및 문맥 통합 음성인식 신경망 기반 음성, 영상 및 문맥 통합 음성인식 최근 잡음환경에서 신뢰도 높은 음성인식을 위해 음성정보와 영상정보를 융합하는 방법이 활발히 연구되고 있다. 본 논문에서는 이절적인 정보의 융합에 적합한 신경망 모델을 기반으로 음성, 영상 및 문맥 정보 등 다양한 정보를 융합하여 잡음 환경에서 고려단어를 인식하는 음성인식 기법에 대하여 기술한다. 음성과 영상 특징을 이용한 이중 모드 신경망 BMNN(BiModal Neural Network)을 제안한다. BMM은 4개 층으로 이루어진 다층퍼셉트론의 구조를 가지며 각 층은 입력 특징의 추상화 기능을 수행한다. BMNN에서는 제 3층이 잡음에 의한 음성 정보의 손실을 보상하기 위하여 음성과 영상 특징을 통합하는 기능을 수행한다. 또한, 잡음환경에서 음성 인식률을 향상시키기 위해 사용자가 말한 단어들의 순차 패턴을 나타내는 문맥정보를 이용한 후처리 방법을 제안한다. 잡음환경에서 BMNN은 단순히 음성만을 사용한 것 보다 높은 성능을 보임으로써 그 타당성을 확인할 수 있을 뿐 아니라, 특히 문맥을 이용한 후처리를 하였을 경우 잡음 환경에서 90%이상의 인식률을 달성하였다 본 연구는 잡음환경에서 강인한 음성인식을 위해 다양한 추가 정보를 사용함으로써 성능을 향상시킬 수 있음을 제시한다."
        },
        {
          "rank": 5,
          "score": 0.773412823677063,
          "doc_id": "NART70632792",
          "title": "Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis",
          "abstract": "<P>This paper investigates joint speaker-dependent audiovisual Hidden Semi-Markov Models (HSMM) where the visual models produce a sequence of 3D motion tracking data that is used to animate a talking head and the acoustic models are used for speech synthesis. Different acoustic, visual, and joint audiovisual models for four different Austrian German speakers were trained and we show that the joint models perform better compared to other approaches in terms of synchronization quality of the synthesized visual speech. In addition, a detailed analysis of the acoustic and visual alignment is provided for the different models. Importantly, the joint audiovisual modeling does not decrease the acoustic synthetic speech quality compared to acoustic-only modeling so that there is a clear advantage in the common duration model of the joint audiovisual modeling approach that is used for synchronizing acoustic and visual parameter sequences. Finally, it provides a model that integrates the visual and acoustic speech dynamics.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART70632792&target=NART&cn=NART70632792",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis <P>This paper investigates joint speaker-dependent audiovisual Hidden Semi-Markov Models (HSMM) where the visual models produce a sequence of 3D motion tracking data that is used to animate a talking head and the acoustic models are used for speech synthesis. Different acoustic, visual, and joint audiovisual models for four different Austrian German speakers were trained and we show that the joint models perform better compared to other approaches in terms of synchronization quality of the synthesized visual speech. In addition, a detailed analysis of the acoustic and visual alignment is provided for the different models. Importantly, the joint audiovisual modeling does not decrease the acoustic synthetic speech quality compared to acoustic-only modeling so that there is a clear advantage in the common duration model of the joint audiovisual modeling approach that is used for synchronizing acoustic and visual parameter sequences. Finally, it provides a model that integrates the visual and acoustic speech dynamics.</P>"
        },
        {
          "rank": 6,
          "score": 0.767103374004364,
          "doc_id": "JAKO200428635215914",
          "title": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구",
          "abstract": "본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200428635215914&target=NART&cn=JAKO200428635215914",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다."
        },
        {
          "rank": 7,
          "score": 0.7609192132949829,
          "doc_id": "NPAP00072266",
          "title": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models",
          "abstract": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP00072266&target=NART&cn=NPAP00072266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error."
        },
        {
          "rank": 8,
          "score": 0.7599156498908997,
          "doc_id": "NART37979687",
          "title": "Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network",
          "abstract": "<P>A new method for noisy speech recognition based on a hybrid model of hidden Markov models (HMM) and wavelet neural network (WNN) is presented. The HMM was employed to compute the Viterbi output score. Then the score was used as the input of WNN to acquire the classification information. The result of recognition was made by these two kinds of recognition information. Recognition experiment shows that this hybrid model has higher performance than hidden Markov model in noisy speech recognition.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART37979687&target=NART&cn=NART37979687",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network <P>A new method for noisy speech recognition based on a hybrid model of hidden Markov models (HMM) and wavelet neural network (WNN) is presented. The HMM was employed to compute the Viterbi output score. Then the score was used as the input of WNN to acquire the classification information. The result of recognition was made by these two kinds of recognition information. Recognition experiment shows that this hybrid model has higher performance than hidden Markov model in noisy speech recognition.</P>"
        },
        {
          "rank": 9,
          "score": 0.7584120631217957,
          "doc_id": "NART20042187",
          "title": "Neural networks with hidden Markov process",
          "abstract": "Observed data contain global structural information and local statistical fluctuation.  To catch both global and local information, neural network models with hidden Markov process are presented. Hidden Markov model(HMM) is a rather general framework which can effectively deal with sequential structure based on stochastic process theory.  Neural networks have strong learning ability which makes it easier to free statistical fluctuation.  By introducing the dependence of weights of neural networks on the states of the systems, neural networks combined with HMM are realized.  The process underlying the transitions of the states is a Markov process. Dependent observations are generated by the neural networks corresponding to the states.  A modified maximum likelihood procedure are employed to estimate both transition probability of Markov process and weights of neural networks.  Applications of neural networks with HMM to econometrics, pattern matching, speech recognition and image process are discussed.(Author abstract)",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20042187&target=NART&cn=NART20042187",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural networks with hidden Markov process Neural networks with hidden Markov process Neural networks with hidden Markov process Observed data contain global structural information and local statistical fluctuation.  To catch both global and local information, neural network models with hidden Markov process are presented. Hidden Markov model(HMM) is a rather general framework which can effectively deal with sequential structure based on stochastic process theory.  Neural networks have strong learning ability which makes it easier to free statistical fluctuation.  By introducing the dependence of weights of neural networks on the states of the systems, neural networks combined with HMM are realized.  The process underlying the transitions of the states is a Markov process. Dependent observations are generated by the neural networks corresponding to the states.  A modified maximum likelihood procedure are employed to estimate both transition probability of Markov process and weights of neural networks.  Applications of neural networks with HMM to econometrics, pattern matching, speech recognition and image process are discussed.(Author abstract)"
        },
        {
          "rank": 10,
          "score": 0.7533841133117676,
          "doc_id": "NART95825020",
          "title": "End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition",
          "abstract": "<P><B>Abstract</B></P>  <P>In hidden Markov model (HMM) based automatic speech recognition (ASR) system, modeling the statistical relationship between the acoustic speech signal and the HMM states that represent linguistically motivated subword units such as phonemes is a crucial step. This is typically achieved by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then training a classifier such as artificial neural networks (ANN), Gaussian mixture model that estimates the emission probabilities of the HMM states. This paper investigates an end-to-end acoustic modeling approach using convolutional neural networks (CNNs), where the CNN takes as input raw speech signal and estimates the HMM states class conditional probabilities at the output. Alternately, as opposed to a divide and conquer strategy (i.e., separating feature extraction and statistical modeling steps), in the proposed acoustic modeling approach the relevant features and the classifier are jointly learned from the raw speech signal. Through ASR studies and analyses on multiple languages and multiple tasks, we show that: (a) the proposed approach yields consistently a better system with fewer parameters when compared to the conventional approach of cepstral feature extraction followed by ANN training, (b) unlike conventional method of speech processing, in the proposed approach the relevant feature representations are learned by first processing the input raw speech at the sub-segmental level ( &asymp; 2 ms). Specifically, through an analysis we show that the filters in the first convolution layer automatically learn &ldquo;in-parts&rdquo; formant-like information present in the sub-segmental speech, and (c) the intermediate feature representations obtained by subsequent filtering of the first convolution layer output are more discriminative compared to standard cepstral features and could be transferred across languages and domains.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Novel CNN-based end-to-end acoustic modeling approach is proposed. </LI> <LI>  Relevant features are automatically learned from the signal by discriminating phones. </LI> <LI>  Learned features are more discriminative than cepstral-based features. </LI> <LI>  Learned features are somewhat invariant to languages and domains. </LI> <LI>  Proposed approach leads to better ASR systems. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART95825020&target=NART&cn=NART95825020",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition <P><B>Abstract</B></P>  <P>In hidden Markov model (HMM) based automatic speech recognition (ASR) system, modeling the statistical relationship between the acoustic speech signal and the HMM states that represent linguistically motivated subword units such as phonemes is a crucial step. This is typically achieved by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then training a classifier such as artificial neural networks (ANN), Gaussian mixture model that estimates the emission probabilities of the HMM states. This paper investigates an end-to-end acoustic modeling approach using convolutional neural networks (CNNs), where the CNN takes as input raw speech signal and estimates the HMM states class conditional probabilities at the output. Alternately, as opposed to a divide and conquer strategy (i.e., separating feature extraction and statistical modeling steps), in the proposed acoustic modeling approach the relevant features and the classifier are jointly learned from the raw speech signal. Through ASR studies and analyses on multiple languages and multiple tasks, we show that: (a) the proposed approach yields consistently a better system with fewer parameters when compared to the conventional approach of cepstral feature extraction followed by ANN training, (b) unlike conventional method of speech processing, in the proposed approach the relevant feature representations are learned by first processing the input raw speech at the sub-segmental level ( &asymp; 2 ms). Specifically, through an analysis we show that the filters in the first convolution layer automatically learn &ldquo;in-parts&rdquo; formant-like information present in the sub-segmental speech, and (c) the intermediate feature representations obtained by subsequent filtering of the first convolution layer output are more discriminative compared to standard cepstral features and could be transferred across languages and domains.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Novel CNN-based end-to-end acoustic modeling approach is proposed. </LI> <LI>  Relevant features are automatically learned from the signal by discriminating phones. </LI> <LI>  Learned features are more discriminative than cepstral-based features. </LI> <LI>  Learned features are somewhat invariant to languages and domains. </LI> <LI>  Proposed approach leads to better ASR systems. </LI> </UL> </P>"
        },
        {
          "rank": 11,
          "score": 0.7516436576843262,
          "doc_id": "NPAP07942137",
          "title": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP07942137&target=NART&cn=NPAP07942137",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 12,
          "score": 0.750921905040741,
          "doc_id": "NART20682074",
          "title": "Robust combination of neural networks and hidden Markov models for speech recognition",
          "abstract": "Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the 'divergence problem'-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20682074&target=NART&cn=NART20682074",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Robust combination of neural networks and hidden Markov models for speech recognition Robust combination of neural networks and hidden Markov models for speech recognition Robust combination of neural networks and hidden Markov models for speech recognition Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the 'divergence problem'-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs."
        },
        {
          "rank": 13,
          "score": 0.749033510684967,
          "doc_id": "JAKO200311922043899",
          "title": "Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구",
          "abstract": "본 논문에서는 한국어 음성 데이터를 대상으로 HM-Net(Hidden Markov Network) 음성인식 시스템의 성능평가를 수행하였다. 음향모델 작성은 음성인식에서 널리 사용되고 있는 통계적인 모델링 방법인 HMM(Hidden Markov Model)을 개량한 HM-Net을 도입하였다. HM-Net은 기존의 SSS(Successive State Splitting) 알고리즘을 개량한 PDT(Phonetic Decision Tree)-SSS 알고리즘에 의해 문맥방향과 시간방향의 상태분할을 수행하여 생성되는데, 특히 문맥방향 상태분할의 경우 학습 음성데이터에 출현하지 않는 문맥정보를 효과적으로 표현하기 위해 음소결정트리를 채용하고 있으며, 시간방향 상태분할의 경우 학습 음성데이터에서 각 음소별 지속시간 정보를 효과적으로 표현하기 위한 상태분할을 수행하며, 마지막으로 파라미터의 공유를 통해 triphone 형태의 최적인 모델 네트워크를 작성하게 된다. 인식에 사용된 알고리즘은 음소 및 단어인식의 경우에는 One-Pass Viterbi 빔 탐색을 사용하며 트리 구조 형태의 사전과 phone/word-pair 문법을 채용하고 있다. 연속음성인식의 경우에는 단어 bigram과 단어 trigram 언어모델과 목구조 형태의 사전을 채용한 Multi-Pass 빔 탐색을 사용하고 있다. 전체적으로 본 논문에서는 다양한 조건에서 HM-Net 음성인식 시스템의 성능평가를 수행하였으며, 지금까지 소개된 음성인식 시스템과 비교하여 매우 우수한 인식성능을 보임을 실험을 통해 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200311922043899&target=NART&cn=JAKO200311922043899",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 본 논문에서는 한국어 음성 데이터를 대상으로 HM-Net(Hidden Markov Network) 음성인식 시스템의 성능평가를 수행하였다. 음향모델 작성은 음성인식에서 널리 사용되고 있는 통계적인 모델링 방법인 HMM(Hidden Markov Model)을 개량한 HM-Net을 도입하였다. HM-Net은 기존의 SSS(Successive State Splitting) 알고리즘을 개량한 PDT(Phonetic Decision Tree)-SSS 알고리즘에 의해 문맥방향과 시간방향의 상태분할을 수행하여 생성되는데, 특히 문맥방향 상태분할의 경우 학습 음성데이터에 출현하지 않는 문맥정보를 효과적으로 표현하기 위해 음소결정트리를 채용하고 있으며, 시간방향 상태분할의 경우 학습 음성데이터에서 각 음소별 지속시간 정보를 효과적으로 표현하기 위한 상태분할을 수행하며, 마지막으로 파라미터의 공유를 통해 triphone 형태의 최적인 모델 네트워크를 작성하게 된다. 인식에 사용된 알고리즘은 음소 및 단어인식의 경우에는 One-Pass Viterbi 빔 탐색을 사용하며 트리 구조 형태의 사전과 phone/word-pair 문법을 채용하고 있다. 연속음성인식의 경우에는 단어 bigram과 단어 trigram 언어모델과 목구조 형태의 사전을 채용한 Multi-Pass 빔 탐색을 사용하고 있다. 전체적으로 본 논문에서는 다양한 조건에서 HM-Net 음성인식 시스템의 성능평가를 수행하였으며, 지금까지 소개된 음성인식 시스템과 비교하여 매우 우수한 인식성능을 보임을 실험을 통해 확인할 수 있었다."
        },
        {
          "rank": 14,
          "score": 0.7477172017097473,
          "doc_id": "NART18015173",
          "title": "Speech recognition using hidden Markov models: A CMU perspective",
          "abstract": "Hidden Markov Models (HMMs) have become the predominant approach for speech recognition systems. One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU. In this paper, we introduce Hidden Markov Modelling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used in SPHINX.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART18015173&target=NART&cn=NART18015173",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition using hidden Markov models: A CMU perspective Speech recognition using hidden Markov models: A CMU perspective Speech recognition using hidden Markov models: A CMU perspective Hidden Markov Models (HMMs) have become the predominant approach for speech recognition systems. One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU. In this paper, we introduce Hidden Markov Modelling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used in SPHINX."
        },
        {
          "rank": 15,
          "score": 0.746229350566864,
          "doc_id": "JAKO200011920774657",
          "title": "은닉 마코프 모델 기반 병렬음성인식 시스템",
          "abstract": "본 논문의 병렬음성인식 모델은 연속 은닉 마코프 모델(HMM; hidden Markov model)에 기반한 병렬 음소인식모듈과 계층구조의 지식베이스에 기반한 병렬 문장인식모듈로 구성된다. 병렬 음소인식 모듈은 수천개의 HMM을 병렬 프로세서에 분산시킨 수, 할당된 HMM에 대한 출력확률 계산과 Viterbi 알고리즘을 담당한다. 지식베이스 기반 병렬 문장인식모듈은 음소모듈에서 공급되는 음소열과 지안하는 병렬 음성인식 알고리즘은 분산메모리 MIMD 구조의 다중 트랜스퓨터와 Parsytec CC 상에 구현되었다. 실험결과, 병렬 음소인식모듈을 통한 실행시간 향상과 병렬 문장인식모듈을 통한 인식률 향상을 얻을 수 있었으며 병렬 음성인식 시스템의 실시간 구현 가능성을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200011920774657&target=NART&cn=JAKO200011920774657",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마코프 모델 기반 병렬음성인식 시스템 은닉 마코프 모델 기반 병렬음성인식 시스템 은닉 마코프 모델 기반 병렬음성인식 시스템 본 논문의 병렬음성인식 모델은 연속 은닉 마코프 모델(HMM; hidden Markov model)에 기반한 병렬 음소인식모듈과 계층구조의 지식베이스에 기반한 병렬 문장인식모듈로 구성된다. 병렬 음소인식 모듈은 수천개의 HMM을 병렬 프로세서에 분산시킨 수, 할당된 HMM에 대한 출력확률 계산과 Viterbi 알고리즘을 담당한다. 지식베이스 기반 병렬 문장인식모듈은 음소모듈에서 공급되는 음소열과 지안하는 병렬 음성인식 알고리즘은 분산메모리 MIMD 구조의 다중 트랜스퓨터와 Parsytec CC 상에 구현되었다. 실험결과, 병렬 음소인식모듈을 통한 실행시간 향상과 병렬 문장인식모듈을 통한 인식률 향상을 얻을 수 있었으며 병렬 음성인식 시스템의 실시간 구현 가능성을 확인하였다."
        },
        {
          "rank": 16,
          "score": 0.7462165355682373,
          "doc_id": "DIKO0011019580",
          "title": "시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합",
          "abstract": "음성인식은 인간과 기계의 인터페이스 서비스를 위한 중요한 기술이다. 현재까지 개발된 많은 음성인식 시스템은 제어된 환경에서는 높은 인식율을 보이지만, 잡음 환경에서는 성능이 크게 저하되는 한계가 있다. 이러한 한계를 극복하기 위한 방법의 하나인 시청각 음성인식은 잡음이 존재하는 환경에서 강인한 인식을 위해 마이크로 녹음한 음성신호 (목소리)와 카메라로 기록한 영상신호(입술의 움직임)를 모두 이용하여 음성을 인식하는 기술이다. 영상신호를 이용한 인식은 잡음이 적은 상황에서는 기존의 음성인식에 비해 낮은 인식 성능을 보이지만 소리잡음에 영향을 받지 않기 때문에 잡음 환경에서 음성인식을 보완하는 유용한 방법이 된다. 본 논문에서는 시청각 음성인식 시스템을 구성하는 청각신호를 이용한 인식, 시각정보를 이용한 인식, 그리고 두 정보의 통합 등의 세 부분을 고려하여 각 부분의 성능을 향상시킴으로써 잡음환경에서의 강인함을 향상시키고자 한다. 첫째, 시각정보를 이용한 인식의 성능을 향상시키기 위해 인식기인 은닉 마르코프 모델(hidden Markov model)의 확률적인 최적화 알고리즘을 제안한다. 알고리즘의 성능과 수렴속도를 개선하기 위해 확률적인 최적화 알고리즘의 하나인 모의 담금질 기법(simulated annealing)과 지역 탐색 연산자를 결합한 하이브리드 모의 담금질 기법을 개발한다. 기존의 은닉 마르코프 모델의 학습 알고리즘인 기대-최대(expectation-maximization) 알고리즘은 가능도 (likelihood) 함수에 대해 지역 최적화만을 수행하는 반면, 제안하는 알고리즘은 전 영역에서 탐색을 수행하며 결과적으로 은닉 마르코프 모델의 인식 성능을 높인다. 제안하는 알고리즘이 전역최적해에 확률로써 수렴하는 것을 수학적으로 증명한다. 둘째, 청각정보를 이용한 인식을 강인하게 하기 위해 은닉 마르코프 모델에서 관측 프레임간의 상관관계를 모델링하는 기법을 개발한다. 음성의 동적 특성은 사람이 잡음환경에서 강인한 인식 성능을 보이는데 도움을 주는 것으로 알려져 있으나 기존의 은닉 마르코프 모델을 이용한 음성 모델링에서는 충분히 고려되지 않고 있다. 본 논문에서는 가우시안 혼합 모델(Gaussian mixture model)로 서로 다른 프레임의 결합 확률분포를 모델링하여 프레임간의 조건부 의존관계를 다루는 기법을 제안한다. 또한, 기대-최대 알고리즘에 기반하여 제안하는 모델의 학습 알고리즘을 개발한다. 제안하는 모델을 이용함으로써 청각 정보를 이용한 인식에서 잡음에 더욱 강인한 성능을 얻도록 한다. 셋째, 시각정보와 청각정보의 상호보완성을 효과적으로 이용하여 잡음에 강인한 최종 인식결과를 얻기 위해 정보 통합 단계에서 신경회로망을 이용하는 기법을 제안한다. 정보 통합 단계에서는 시각정보와 청각정보를 각각 따로 인식한 결과를 가중치 기법을 통해 최종 결과를 얻는데, 학습된 신경회로망은 잡음의 종류와 수준을 알 수 없는 주어진 시청각 데이터에 대해 적절한 가중치를 출력함으로써 최적의 인식결과를 얻도록 한다. 이를 통해 통합 인식 결과가 시각 또는 청각정보만을 이용한 인식결과보다 최소한 같거나 좋을 뿐 아니라 두 정보에 의한 시너지 효과를 최대화하도록 한다. 제안하는 시스템을 화자독립 고립단어 인식문제에 적용하여 그 성능을 보인다. 실험 결과 제안하는 시스템이 기존의 시스템에 비해 다양한 잡음 환경에서 더욱 강인한 성능을 보이는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011019580&target=NART&cn=DIKO0011019580",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 음성인식은 인간과 기계의 인터페이스 서비스를 위한 중요한 기술이다. 현재까지 개발된 많은 음성인식 시스템은 제어된 환경에서는 높은 인식율을 보이지만, 잡음 환경에서는 성능이 크게 저하되는 한계가 있다. 이러한 한계를 극복하기 위한 방법의 하나인 시청각 음성인식은 잡음이 존재하는 환경에서 강인한 인식을 위해 마이크로 녹음한 음성신호 (목소리)와 카메라로 기록한 영상신호(입술의 움직임)를 모두 이용하여 음성을 인식하는 기술이다. 영상신호를 이용한 인식은 잡음이 적은 상황에서는 기존의 음성인식에 비해 낮은 인식 성능을 보이지만 소리잡음에 영향을 받지 않기 때문에 잡음 환경에서 음성인식을 보완하는 유용한 방법이 된다. 본 논문에서는 시청각 음성인식 시스템을 구성하는 청각신호를 이용한 인식, 시각정보를 이용한 인식, 그리고 두 정보의 통합 등의 세 부분을 고려하여 각 부분의 성능을 향상시킴으로써 잡음환경에서의 강인함을 향상시키고자 한다. 첫째, 시각정보를 이용한 인식의 성능을 향상시키기 위해 인식기인 은닉 마르코프 모델(hidden Markov model)의 확률적인 최적화 알고리즘을 제안한다. 알고리즘의 성능과 수렴속도를 개선하기 위해 확률적인 최적화 알고리즘의 하나인 모의 담금질 기법(simulated annealing)과 지역 탐색 연산자를 결합한 하이브리드 모의 담금질 기법을 개발한다. 기존의 은닉 마르코프 모델의 학습 알고리즘인 기대-최대(expectation-maximization) 알고리즘은 가능도 (likelihood) 함수에 대해 지역 최적화만을 수행하는 반면, 제안하는 알고리즘은 전 영역에서 탐색을 수행하며 결과적으로 은닉 마르코프 모델의 인식 성능을 높인다. 제안하는 알고리즘이 전역최적해에 확률로써 수렴하는 것을 수학적으로 증명한다. 둘째, 청각정보를 이용한 인식을 강인하게 하기 위해 은닉 마르코프 모델에서 관측 프레임간의 상관관계를 모델링하는 기법을 개발한다. 음성의 동적 특성은 사람이 잡음환경에서 강인한 인식 성능을 보이는데 도움을 주는 것으로 알려져 있으나 기존의 은닉 마르코프 모델을 이용한 음성 모델링에서는 충분히 고려되지 않고 있다. 본 논문에서는 가우시안 혼합 모델(Gaussian mixture model)로 서로 다른 프레임의 결합 확률분포를 모델링하여 프레임간의 조건부 의존관계를 다루는 기법을 제안한다. 또한, 기대-최대 알고리즘에 기반하여 제안하는 모델의 학습 알고리즘을 개발한다. 제안하는 모델을 이용함으로써 청각 정보를 이용한 인식에서 잡음에 더욱 강인한 성능을 얻도록 한다. 셋째, 시각정보와 청각정보의 상호보완성을 효과적으로 이용하여 잡음에 강인한 최종 인식결과를 얻기 위해 정보 통합 단계에서 신경회로망을 이용하는 기법을 제안한다. 정보 통합 단계에서는 시각정보와 청각정보를 각각 따로 인식한 결과를 가중치 기법을 통해 최종 결과를 얻는데, 학습된 신경회로망은 잡음의 종류와 수준을 알 수 없는 주어진 시청각 데이터에 대해 적절한 가중치를 출력함으로써 최적의 인식결과를 얻도록 한다. 이를 통해 통합 인식 결과가 시각 또는 청각정보만을 이용한 인식결과보다 최소한 같거나 좋을 뿐 아니라 두 정보에 의한 시너지 효과를 최대화하도록 한다. 제안하는 시스템을 화자독립 고립단어 인식문제에 적용하여 그 성능을 보인다. 실험 결과 제안하는 시스템이 기존의 시스템에 비해 다양한 잡음 환경에서 더욱 강인한 성능을 보이는 것을 확인한다."
        },
        {
          "rank": 17,
          "score": 0.7453005313873291,
          "doc_id": "DIKO0011930560",
          "title": "시청각 음성인식을 위한 새로운 통합방법",
          "abstract": "The automatic speech recognition (ASR) is one of the most interesting problems applied to human computer interaction applications; for example, spoken digit recognition for mobile environments. One of the challenges of this problem is that the accuracy of speech recognition will be decrease much if the speaker talks under noisy place such as: restaurant, subway, street… The invention of lip-reading opens a potential chance to improve the performance of recognition. Indeed, human perception considers both auditory and visual nature of speech. The speech recognition system will be more intelligible if the lip motion of speaker is available together with acoustic signal. The combined audio visual speech recognition has been proved to be able enhance the overall performance of recognition, especially under noisy environment. In general, if the two streams are available for speech recognition, they can be integrated by two ways: early integration and late integration. The early integration approach combines the features of two streams into one concatenated feature vector, and uses single classifier for recognition. The late integration approach combines the results of two separate classifiers for recognition in which the reliability of modalities is applied to summation based fusion. The late integration method shows the better performance actually through many experiments. There are several factors are considered to measure reliability including word confusability, SNR level of acoustic signal, the noise type, and illumination change in visual stream rather than the only SNR level based confidence of conventional audio visual speech recognition. In this study, we propose an effective fusion scheme for audio visual speech recognition (AVSR) in which the appropriate combination weights are measured by using an integrated reliability. The significant idea of integrated reliability is the combination of not only acoustic noise but also model confusability for audio visual reliability measurement The experimental results using Samsung AVSR database shows the improved performance of our approach compared to conventional ones. This demonstrates the effectiveness and feasibility of this invention for real speech recognition applications.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011930560&target=NART&cn=DIKO0011930560",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시청각 음성인식을 위한 새로운 통합방법 시청각 음성인식을 위한 새로운 통합방법 시청각 음성인식을 위한 새로운 통합방법 The automatic speech recognition (ASR) is one of the most interesting problems applied to human computer interaction applications; for example, spoken digit recognition for mobile environments. One of the challenges of this problem is that the accuracy of speech recognition will be decrease much if the speaker talks under noisy place such as: restaurant, subway, street… The invention of lip-reading opens a potential chance to improve the performance of recognition. Indeed, human perception considers both auditory and visual nature of speech. The speech recognition system will be more intelligible if the lip motion of speaker is available together with acoustic signal. The combined audio visual speech recognition has been proved to be able enhance the overall performance of recognition, especially under noisy environment. In general, if the two streams are available for speech recognition, they can be integrated by two ways: early integration and late integration. The early integration approach combines the features of two streams into one concatenated feature vector, and uses single classifier for recognition. The late integration approach combines the results of two separate classifiers for recognition in which the reliability of modalities is applied to summation based fusion. The late integration method shows the better performance actually through many experiments. There are several factors are considered to measure reliability including word confusability, SNR level of acoustic signal, the noise type, and illumination change in visual stream rather than the only SNR level based confidence of conventional audio visual speech recognition. In this study, we propose an effective fusion scheme for audio visual speech recognition (AVSR) in which the appropriate combination weights are measured by using an integrated reliability. The significant idea of integrated reliability is the combination of not only acoustic noise but also model confusability for audio visual reliability measurement The experimental results using Samsung AVSR database shows the improved performance of our approach compared to conventional ones. This demonstrates the effectiveness and feasibility of this invention for real speech recognition applications."
        },
        {
          "rank": 18,
          "score": 0.7396749258041382,
          "doc_id": "JAKO200211921444549",
          "title": "2층 구조의 입체 시각형 신경망 기반 음소인식",
          "abstract": "본 연구는 입체 시각을 위한 신경망에 대한 연구 결과로서 인간의 음성을 인식하는데 적용된다. 입체 시각신경망(SVNN)에 기반한 음성인식에서, 먼저 입력된 음성 신호를 표준 모델과 비교함으로써 유사성이 얻어진다. 이 값들은 다이나믹한 처리 과정으로 주어지고 이웃한 신경소자들 사이에서 경쟁적이고 협력적인 처리를 거치게 된다. 이러한 다이나믹한 처리과정을 통해 단 하나의 가장 우수한 신경세포(winner neuron)만이 최후에 검출된다. 비교연구에서 2층 구조의 SVNN은 HMM 인식기보다 인식정확도 측면에서 7.7% 더 높았다. 평가 결과. SVNN은 기손리 HMM 인식기 성능을 능가하는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200211921444549&target=NART&cn=JAKO200211921444549",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "2층 구조의 입체 시각형 신경망 기반 음소인식 2층 구조의 입체 시각형 신경망 기반 음소인식 2층 구조의 입체 시각형 신경망 기반 음소인식 본 연구는 입체 시각을 위한 신경망에 대한 연구 결과로서 인간의 음성을 인식하는데 적용된다. 입체 시각신경망(SVNN)에 기반한 음성인식에서, 먼저 입력된 음성 신호를 표준 모델과 비교함으로써 유사성이 얻어진다. 이 값들은 다이나믹한 처리 과정으로 주어지고 이웃한 신경소자들 사이에서 경쟁적이고 협력적인 처리를 거치게 된다. 이러한 다이나믹한 처리과정을 통해 단 하나의 가장 우수한 신경세포(winner neuron)만이 최후에 검출된다. 비교연구에서 2층 구조의 SVNN은 HMM 인식기보다 인식정확도 측면에서 7.7% 더 높았다. 평가 결과. SVNN은 기손리 HMM 인식기 성능을 능가하는 것으로 나타났다."
        },
        {
          "rank": 19,
          "score": 0.737007737159729,
          "doc_id": "NART17510385",
          "title": "Hidden-articulator Markov models for speech recognition",
          "abstract": "<P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART17510385&target=NART&cn=NART17510385",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition <P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>"
        },
        {
          "rank": 20,
          "score": 0.7258217334747314,
          "doc_id": "JAKO200111921140843",
          "title": "회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM (Hidden Markov Model)의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경 망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용 데이터에 대하여 Elman망 예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 98.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200111921140843&target=NART&cn=JAKO200111921140843",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM (Hidden Markov Model)의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경 망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용 데이터에 대하여 Elman망 예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 98.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 21,
          "score": 0.7246606349945068,
          "doc_id": "NART56157676",
          "title": "온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합",
          "abstract": "<P> 최근에 음성인식 분야에서 널리 사용되고 있는 은닉 마르코프 모델(HMM)을 이용하여 필기문자를 인식하고자 하는 연구가 활발히 진행되고 있다. 하지만, HMM은 시간에 따라서 변하는 입력특성을 잘 처리하는 장점이 있는 반면에, 각 모델을 독립적으로 학습시키는 경우에 각 패턴 사이의 분별력이 다소 떨어지는 문제가 있다. 본 논문에서는 HMM을 통해서 얻어진 각 모델의 내부 출력값을 이용하여 신경망 분류기로 추가적인 분류작업을 수행하는 방법을 제시한다. 또, 온라인 필기 데이타로 숫자와 영문자 대소문자를 인식하는 실험을 통해서 제시된 방법의 유용성을 입증한다.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART56157676&target=NART&cn=NART56157676",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 <P> 최근에 음성인식 분야에서 널리 사용되고 있는 은닉 마르코프 모델(HMM)을 이용하여 필기문자를 인식하고자 하는 연구가 활발히 진행되고 있다. 하지만, HMM은 시간에 따라서 변하는 입력특성을 잘 처리하는 장점이 있는 반면에, 각 모델을 독립적으로 학습시키는 경우에 각 패턴 사이의 분별력이 다소 떨어지는 문제가 있다. 본 논문에서는 HMM을 통해서 얻어진 각 모델의 내부 출력값을 이용하여 신경망 분류기로 추가적인 분류작업을 수행하는 방법을 제시한다. 또, 온라인 필기 데이타로 숫자와 영문자 대소문자를 인식하는 실험을 통해서 제시된 방법의 유용성을 입증한다.</P>"
        },
        {
          "rank": 22,
          "score": 0.7211470603942871,
          "doc_id": "JAKO201403359905324",
          "title": "가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원",
          "abstract": "This paper describes a robust speech recognition technique by reconstructing spectral components mismatched with a training environment. Although the cluster-based reconstruction method can compensate the unreliable components from reliable components in the same spectral vector by assuming an independent, identically distributed Gaussian-mixture process of training spectral vectors, the presented method exploits the temporal dependency of speech to reconstruct the components by introducing a hidden-Markov-model prior which incorporates an internal state transition plausible for an observed spectral vector sequence. The experimental results indicate that the described method can provide temporally consistent reconstruction and further improve recognition performance on average compared to the conventional method.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201403359905324&target=NART&cn=JAKO201403359905324",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 This paper describes a robust speech recognition technique by reconstructing spectral components mismatched with a training environment. Although the cluster-based reconstruction method can compensate the unreliable components from reliable components in the same spectral vector by assuming an independent, identically distributed Gaussian-mixture process of training spectral vectors, the presented method exploits the temporal dependency of speech to reconstruct the components by introducing a hidden-Markov-model prior which incorporates an internal state transition plausible for an observed spectral vector sequence. The experimental results indicate that the described method can provide temporally consistent reconstruction and further improve recognition performance on average compared to the conventional method."
        },
        {
          "rank": 23,
          "score": 0.7199625372886658,
          "doc_id": "NPAP12270893",
          "title": "Speech Recognition in Noisy Environments with Convolutional Neural Networks",
          "abstract": "<P>One of the biggest challenges in speech recognition today is its use on a daily basis, in which distortion and noise in the environment are present and hinder the recognition task. In the last thirty years, hundreds of methods for noise-robust recognition were proposed, each with its own advantages and disadvantages. In this paper, the use of convolutional neural networks (CNN) as acoustic models in automatic speech recognition systems (ASR) is proposed as an alternative to the classical recognition methods based on HMM without any noise-robust method applied. The experiment showed that the presented method reduces the equal error rate in word recognition tasks with additive noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12270893&target=NART&cn=NPAP12270893",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech Recognition in Noisy Environments with Convolutional Neural Networks Speech Recognition in Noisy Environments with Convolutional Neural Networks Speech Recognition in Noisy Environments with Convolutional Neural Networks <P>One of the biggest challenges in speech recognition today is its use on a daily basis, in which distortion and noise in the environment are present and hinder the recognition task. In the last thirty years, hundreds of methods for noise-robust recognition were proposed, each with its own advantages and disadvantages. In this paper, the use of convolutional neural networks (CNN) as acoustic models in automatic speech recognition systems (ASR) is proposed as an alternative to the classical recognition methods based on HMM without any noise-robust method applied. The experiment showed that the presented method reduces the equal error rate in word recognition tasks with additive noise.</P>"
        },
        {
          "rank": 24,
          "score": 0.7174750566482544,
          "doc_id": "NART06155061",
          "title": "Speech enhancement based on neural predictive hidden Markov model",
          "abstract": "<P><B>Abstract</B></P><P>In this paper, we describe a new approach to speech enhancement by modeling directly the statistical characteristics of the speech waveform. To represent the nonlinear and nonstationary nature of speech, it is assumed that speech is the output of a neural predictive hidden Markov model (NPHMM). The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a Markov chain. Given some speech data, the parameter of NPHMM is estimated by a learning algorithm based on the combination of Baum&#x2013;Welch algorithm and a neural network learning algorithm using the well known back propagation technique. Given the parameters of NPHMM, a recursive estimation method using multiple Kalman filters, governed by a Markov state chain according to the transition probabilities is developed for enhancing speech signals degraded by statistically independent additive noise characteristics assumed to be white and Gaussian. Under various input signal-to-noise ratios (SNRs), the proposed recursive speech enhancement method achieves an improvement over the method based on hidden filter model (Lee and Shirai, 1996) of about 0.8&#x2013;1.2dB in terms of the measured output SNR.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART06155061&target=NART&cn=NART06155061",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech enhancement based on neural predictive hidden Markov model Speech enhancement based on neural predictive hidden Markov model Speech enhancement based on neural predictive hidden Markov model <P><B>Abstract</B></P><P>In this paper, we describe a new approach to speech enhancement by modeling directly the statistical characteristics of the speech waveform. To represent the nonlinear and nonstationary nature of speech, it is assumed that speech is the output of a neural predictive hidden Markov model (NPHMM). The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a Markov chain. Given some speech data, the parameter of NPHMM is estimated by a learning algorithm based on the combination of Baum&#x2013;Welch algorithm and a neural network learning algorithm using the well known back propagation technique. Given the parameters of NPHMM, a recursive estimation method using multiple Kalman filters, governed by a Markov state chain according to the transition probabilities is developed for enhancing speech signals degraded by statistically independent additive noise characteristics assumed to be white and Gaussian. Under various input signal-to-noise ratios (SNRs), the proposed recursive speech enhancement method achieves an improvement over the method based on hidden filter model (Lee and Shirai, 1996) of about 0.8&#x2013;1.2dB in terms of the measured output SNR.</P>"
        },
        {
          "rank": 25,
          "score": 0.7165242433547974,
          "doc_id": "JAKO201415642601987",
          "title": "SNR 매핑을 이용한 환경적응 기반 음성인식",
          "abstract": "다 모델 기반의 음성인식기는 음성인식에서 매우 성공적임이 알려져 있다. 그것은 다양한 신호-대-잡음비(SNR)와 잡음종류에 해당하는 다수의 HMM을 사용함으로서 선택된 음향모델이 인식잡음음성에 매우 근접한 일치성을 가질 수 있기 때문이다. 그러나 실제 사용시에 HMM의 개수가 제한됨에 따라서 음향모델의 불일치는 여전히 문제로 남아 있다. 본 논문에서는 인식잡음음성과 HMM 간의 SNR 불일치를 줄이고자 이들 간의 최적의 SNR 매핑 (mapping)을 실험적으로 결정하였다. 인식잡음음성으로 부터 추정된 SNR 값을 사용하는 대신 제안된 SNR 매핑을 사용함으로서 향상된 인식결과를 얻을 수 있었다. 다 모델 기반인식기에 제안된 방법을 적용하여 Aurora 2 데이터베이스에 대해서 인식 실험한 결과 기존의 MTR 이나 다 모델 기반 음성인식기에 비해서 6.3%와 9.4%의 상대적 단어 오인식율 감소를 이룰 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201415642601987&target=NART&cn=JAKO201415642601987",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SNR 매핑을 이용한 환경적응 기반 음성인식 SNR 매핑을 이용한 환경적응 기반 음성인식 SNR 매핑을 이용한 환경적응 기반 음성인식 다 모델 기반의 음성인식기는 음성인식에서 매우 성공적임이 알려져 있다. 그것은 다양한 신호-대-잡음비(SNR)와 잡음종류에 해당하는 다수의 HMM을 사용함으로서 선택된 음향모델이 인식잡음음성에 매우 근접한 일치성을 가질 수 있기 때문이다. 그러나 실제 사용시에 HMM의 개수가 제한됨에 따라서 음향모델의 불일치는 여전히 문제로 남아 있다. 본 논문에서는 인식잡음음성과 HMM 간의 SNR 불일치를 줄이고자 이들 간의 최적의 SNR 매핑 (mapping)을 실험적으로 결정하였다. 인식잡음음성으로 부터 추정된 SNR 값을 사용하는 대신 제안된 SNR 매핑을 사용함으로서 향상된 인식결과를 얻을 수 있었다. 다 모델 기반인식기에 제안된 방법을 적용하여 Aurora 2 데이터베이스에 대해서 인식 실험한 결과 기존의 MTR 이나 다 모델 기반 음성인식기에 비해서 6.3%와 9.4%의 상대적 단어 오인식율 감소를 이룰 수 있었다."
        },
        {
          "rank": 26,
          "score": 0.7137480974197388,
          "doc_id": "NART00705015",
          "title": "Speaker recognition using HMM composition in noisy environments",
          "abstract": "<P><B>Abstract</B></P><P>This paper investigates a speaker recognition method that is robust against background noise. In noisy environments, one important issue is how to create a model for each speaker so as to compensate for noise. The method described here is based on hidden Markov model (HMM) composition, which combines a speaker HMM and a noise-source HMM into a noise-added speaker HMM with a particular signal-to-noise ratio (SNR). Since it is difficult to measure the SNR of input speech with non-stationary noise exactly, this method creates several noise-added speaker HMMs with various SNRs. The HMM that has the highest likelihood value for the input speech is selected, and a speaker decision is made using this likelihood value. Experimental application of this method to text-independent speaker identification and verification in various kinds of noisy environments demonstrated considerable improvement in speaker recognition for speech utterances of male speakers.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART00705015&target=NART&cn=NART00705015",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speaker recognition using HMM composition in noisy environments Speaker recognition using HMM composition in noisy environments Speaker recognition using HMM composition in noisy environments <P><B>Abstract</B></P><P>This paper investigates a speaker recognition method that is robust against background noise. In noisy environments, one important issue is how to create a model for each speaker so as to compensate for noise. The method described here is based on hidden Markov model (HMM) composition, which combines a speaker HMM and a noise-source HMM into a noise-added speaker HMM with a particular signal-to-noise ratio (SNR). Since it is difficult to measure the SNR of input speech with non-stationary noise exactly, this method creates several noise-added speaker HMMs with various SNRs. The HMM that has the highest likelihood value for the input speech is selected, and a speaker decision is made using this likelihood value. Experimental application of this method to text-independent speaker identification and verification in various kinds of noisy environments demonstrated considerable improvement in speaker recognition for speech utterances of male speakers.</P>"
        },
        {
          "rank": 27,
          "score": 0.7097820043563843,
          "doc_id": "NART13081886",
          "title": "Speech recognition under noisy environments using segmental unit input HMM",
          "abstract": "<P>In this paper, we apply a segmental unit input HMM to noisy speech recognition. In this modeling, several successive frames are combined and treated as an input vector. We expect that the segmental unit input HMM will be effective for noisy speech recognition because segmental statistics considering correlation between frames reduce noise effects when the correlation of noise between frames is assumed to be small. In recognition experiments, we compared the segmental unit input HMM with a conventional frame-based HMM and found the segmental unit input HMM to be superior. We also compared the segmental unit input HMM with dynamic cepstral coefficients, which have both static and dynamic features, and found that the segmental unit input HMM is more effective than the dynamic cepstrum. We also combined the segmental unit input HMM with a spectral subtraction method and confirmed the effectiveness of the method. Additionally, in experiments using acoustic models trained with noisy speech, the segmental unit input HMM outperformed the conventional HMM. From these results, we propose PMC for the segmental unit input HMM. Experimental results showed the PMC for segmental unit input HMM offered better recognition performance than the original PMC. &copy; 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(8): 111&ndash;120, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1151</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART13081886&target=NART&cn=NART13081886",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition under noisy environments using segmental unit input HMM Speech recognition under noisy environments using segmental unit input HMM Speech recognition under noisy environments using segmental unit input HMM <P>In this paper, we apply a segmental unit input HMM to noisy speech recognition. In this modeling, several successive frames are combined and treated as an input vector. We expect that the segmental unit input HMM will be effective for noisy speech recognition because segmental statistics considering correlation between frames reduce noise effects when the correlation of noise between frames is assumed to be small. In recognition experiments, we compared the segmental unit input HMM with a conventional frame-based HMM and found the segmental unit input HMM to be superior. We also compared the segmental unit input HMM with dynamic cepstral coefficients, which have both static and dynamic features, and found that the segmental unit input HMM is more effective than the dynamic cepstrum. We also combined the segmental unit input HMM with a spectral subtraction method and confirmed the effectiveness of the method. Additionally, in experiments using acoustic models trained with noisy speech, the segmental unit input HMM outperformed the conventional HMM. From these results, we propose PMC for the segmental unit input HMM. Experimental results showed the PMC for segmental unit input HMM offered better recognition performance than the original PMC. &copy; 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(8): 111&ndash;120, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1151</P>"
        },
        {
          "rank": 28,
          "score": 0.7056357860565186,
          "doc_id": "JAKO200606140790765",
          "title": "Discrimination of Pathological Speech Using Hidden Markov Models",
          "abstract": "Diagnosis of pathological voice is one of the important issues in biomedical applications of speech technology. This study focuses on the discrimination of voice disorder using HMM (Hidden Markov Model) for automatic detection between normal voice and vocal fold disorder voice. This is a non-intrusive, non-expensive and fully automated method using only a speech sample of the subject. Speech data from normal people and patients were collected. Mel-frequency filter cepstral coefficients (MFCCs) were modeled by HMM classifier. Different states (3 states, 5 states and 7 states), 3 mixtures and left to right HMMs were formed. This method gives an accuracy of 93.8% for train data and 91.7% for test data in the discrimination of normal and vocal fold disorder voice for sustained /a/.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200606140790765&target=NART&cn=JAKO200606140790765",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Discrimination of Pathological Speech Using Hidden Markov Models Discrimination of Pathological Speech Using Hidden Markov Models Discrimination of Pathological Speech Using Hidden Markov Models Diagnosis of pathological voice is one of the important issues in biomedical applications of speech technology. This study focuses on the discrimination of voice disorder using HMM (Hidden Markov Model) for automatic detection between normal voice and vocal fold disorder voice. This is a non-intrusive, non-expensive and fully automated method using only a speech sample of the subject. Speech data from normal people and patients were collected. Mel-frequency filter cepstral coefficients (MFCCs) were modeled by HMM classifier. Different states (3 states, 5 states and 7 states), 3 mixtures and left to right HMMs were formed. This method gives an accuracy of 93.8% for train data and 91.7% for test data in the discrimination of normal and vocal fold disorder voice for sustained /a/."
        },
        {
          "rank": 29,
          "score": 0.7047439813613892,
          "doc_id": "NART30128358",
          "title": "Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer",
          "abstract": "<P><B>Abstract</B></P><P>Spoken human&ndash;machine interaction in real-world environments requires acoustic models that are robust to changes in acoustic conditions, e.g. presence of noise. Unfortunately, the popular hidden Markov models (HMM) are not noise tolerant. One way to increase recognition performance is to acquire a small adaptation set of noisy utterances, which is used to estimate a normalization mapping between noisy and clean features to be fed into the acoustic model. This paper proposes an unsupervised maximum-likelihood gradient-ascent training algorithm (instead of the usual least squares regression) for a neural feature adaptation module, properly combined with a hybrid connectionist/HMM speech recognizer. The algorithm is inspired by the so-called &ldquo;inversion principle&rdquo;, that prescribes the optimization of the input features instead of the model parameters. Simulation results on a real-world speaker-independent continuous speech corpus of connected Italian digits, corrupted by noise, validate the approach. A small neural net (13 hidden neurons) trained over a single adaptation utterance for one iteration yields a 18.79% relative word error rate (WER) reduction over the bare hybrid, and a 65.10% relative WER reduction over the Gaussian-based HMM.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART30128358&target=NART&cn=NART30128358",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer <P><B>Abstract</B></P><P>Spoken human&ndash;machine interaction in real-world environments requires acoustic models that are robust to changes in acoustic conditions, e.g. presence of noise. Unfortunately, the popular hidden Markov models (HMM) are not noise tolerant. One way to increase recognition performance is to acquire a small adaptation set of noisy utterances, which is used to estimate a normalization mapping between noisy and clean features to be fed into the acoustic model. This paper proposes an unsupervised maximum-likelihood gradient-ascent training algorithm (instead of the usual least squares regression) for a neural feature adaptation module, properly combined with a hybrid connectionist/HMM speech recognizer. The algorithm is inspired by the so-called &ldquo;inversion principle&rdquo;, that prescribes the optimization of the input features instead of the model parameters. Simulation results on a real-world speaker-independent continuous speech corpus of connected Italian digits, corrupted by noise, validate the approach. A small neural net (13 hidden neurons) trained over a single adaptation utterance for one iteration yields a 18.79% relative word error rate (WER) reduction over the bare hybrid, and a 65.10% relative WER reduction over the Gaussian-based HMM.</P>"
        },
        {
          "rank": 30,
          "score": 0.6994912624359131,
          "doc_id": "NART53843256",
          "title": "Neural development of networks for audiovisual speech comprehension",
          "abstract": "<P><B>Abstract</B></P><P>Everyday conversation is both an auditory and a visual phenomenon. While visual speech information enhances comprehension for the listener, evidence suggests that the ability to benefit from this information improves with development. A number of brain regions have been implicated in audiovisual speech comprehension, but the extent to which the neurobiological substrate in the child compares to the adult is unknown. In particular, developmental differences in the network for audiovisual speech comprehension could manifest through the incorporation of additional brain regions, or through different patterns of effective connectivity. In the present study we used functional magnetic resonance imaging and structural equation modeling (SEM) to characterize the developmental changes in network interactions for audiovisual speech comprehension. The brain response was recorded while children 8- to 11-years-old and adults passively listened to stories under audiovisual (AV) and auditory-only (A) conditions. Results showed that in children and adults, AV comprehension activated the same fronto-temporo-parietal network of regions known for their contribution to speech production and perception. However, the SEM network analysis revealed age-related differences in the functional interactions among these regions. In particular, the influence of the posterior inferior frontal gyrus/ventral premotor cortex on supramarginal gyrus differed across age groups during AV, but not A speech. This functional pathway might be important for relating motor and sensory information used by the listener to identify speech sounds. Further, its development might reflect changes in the mechanisms that relate visual speech information to articulatory speech representations through experience producing and perceiving speech.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART53843256&target=NART&cn=NART53843256",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural development of networks for audiovisual speech comprehension Neural development of networks for audiovisual speech comprehension Neural development of networks for audiovisual speech comprehension <P><B>Abstract</B></P><P>Everyday conversation is both an auditory and a visual phenomenon. While visual speech information enhances comprehension for the listener, evidence suggests that the ability to benefit from this information improves with development. A number of brain regions have been implicated in audiovisual speech comprehension, but the extent to which the neurobiological substrate in the child compares to the adult is unknown. In particular, developmental differences in the network for audiovisual speech comprehension could manifest through the incorporation of additional brain regions, or through different patterns of effective connectivity. In the present study we used functional magnetic resonance imaging and structural equation modeling (SEM) to characterize the developmental changes in network interactions for audiovisual speech comprehension. The brain response was recorded while children 8- to 11-years-old and adults passively listened to stories under audiovisual (AV) and auditory-only (A) conditions. Results showed that in children and adults, AV comprehension activated the same fronto-temporo-parietal network of regions known for their contribution to speech production and perception. However, the SEM network analysis revealed age-related differences in the functional interactions among these regions. In particular, the influence of the posterior inferior frontal gyrus/ventral premotor cortex on supramarginal gyrus differed across age groups during AV, but not A speech. This functional pathway might be important for relating motor and sensory information used by the listener to identify speech sounds. Further, its development might reflect changes in the mechanisms that relate visual speech information to articulatory speech representations through experience producing and perceiving speech.</P>"
        },
        {
          "rank": 31,
          "score": 0.6992770433425903,
          "doc_id": "JAKO201935164467523",
          "title": "심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구",
          "abstract": "본 논문에서는 구개인두부전증(VeloPharyngeal Insufficiency, VPI) 환자의 음성을 효과적으로 인식하기 위해 컨볼루션 신경망 (Convolutional Neural Network, CNN), 장단기 모델(Long Short Term Memory, LSTM) 구조 신경망을 은닉 마르코프 모델(Hidden Markov Model, HMM)과 결합한 하이브리드 구조의 음성 인식 시스템을 구축하고 모델 적응 기법을 적용하여, 기존 Gaussian Mixture Model(GMM-HMM), 완전 연결형 Deep Neural Network(DNN-HMM) 기반의 음성 인식 시스템과 성능을 비교한다. 정상인 화자가 PBW452단어를 발화한 데이터를 이용하여 초기 모델을 학습하고 정상인 화자의 VPI 모의 음성을 이용하여 화자 적응의 사전 모델을 생성한 후에 VPI 환자들의 음성으로 추가 적응 학습을 진행한다. VPI환자의 화자 적응 시에 CNN-HMM 기반 모델에서는 일부층만 적응 학습하고, LSTM-HMM 기반 모델의 경우에는 드롭 아웃 규제기법을 적용하여 성능을 관찰한 결과 기존 완전 연결형 DNN-HMM 인식기보다 3.68 % 향상된 음성 인식 성능을 나타낸다. 이러한 결과는 본 논문에서 제안하는 LSTM-HMM 기반의 하이브리드 음성 인식 기법이 많은 데이터를 확보하기 어려운 VPI 환자 음성에 대해 보다 향상된 인식률의 음성 인식 시스템을 구축하는데 효과적임을 입증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201935164467523&target=NART&cn=JAKO201935164467523",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 본 논문에서는 구개인두부전증(VeloPharyngeal Insufficiency, VPI) 환자의 음성을 효과적으로 인식하기 위해 컨볼루션 신경망 (Convolutional Neural Network, CNN), 장단기 모델(Long Short Term Memory, LSTM) 구조 신경망을 은닉 마르코프 모델(Hidden Markov Model, HMM)과 결합한 하이브리드 구조의 음성 인식 시스템을 구축하고 모델 적응 기법을 적용하여, 기존 Gaussian Mixture Model(GMM-HMM), 완전 연결형 Deep Neural Network(DNN-HMM) 기반의 음성 인식 시스템과 성능을 비교한다. 정상인 화자가 PBW452단어를 발화한 데이터를 이용하여 초기 모델을 학습하고 정상인 화자의 VPI 모의 음성을 이용하여 화자 적응의 사전 모델을 생성한 후에 VPI 환자들의 음성으로 추가 적응 학습을 진행한다. VPI환자의 화자 적응 시에 CNN-HMM 기반 모델에서는 일부층만 적응 학습하고, LSTM-HMM 기반 모델의 경우에는 드롭 아웃 규제기법을 적용하여 성능을 관찰한 결과 기존 완전 연결형 DNN-HMM 인식기보다 3.68 % 향상된 음성 인식 성능을 나타낸다. 이러한 결과는 본 논문에서 제안하는 LSTM-HMM 기반의 하이브리드 음성 인식 기법이 많은 데이터를 확보하기 어려운 VPI 환자 음성에 대해 보다 향상된 인식률의 음성 인식 시스템을 구축하는데 효과적임을 입증한다."
        },
        {
          "rank": 32,
          "score": 0.6956678628921509,
          "doc_id": "NART48832461",
          "title": "Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments",
          "abstract": "In this paper, we propose a model for the incorporation of voicing information into a speech recognition system in noisy environments. The employed voicing information is estimated by a novel method that can provide this information for each filter-bank channel and does not require information about the fundamental frequency. The voicing information is modelled by employing the Bernoulli distribution. The voicing model is obtained for each HMM state and mixture by a Viterbi-style training procedure. The proposed voicing incorporation is evaluated both within a standard model and two other models that had compensated for the noise effect, the missing-feature and the multi-conditional training model. Experiments are first performed on noisy speech data from the Aurora 2 database. Significant performance improvements are achieved when the voicing information is incorporated within the standard model as well as the noise-compensated models. The employment of voicing information is also demonstrated on a phoneme recognition task on the noise-corrupted TIMIT database and considerable improvements are observed.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART48832461&target=NART&cn=NART48832461",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments In this paper, we propose a model for the incorporation of voicing information into a speech recognition system in noisy environments. The employed voicing information is estimated by a novel method that can provide this information for each filter-bank channel and does not require information about the fundamental frequency. The voicing information is modelled by employing the Bernoulli distribution. The voicing model is obtained for each HMM state and mixture by a Viterbi-style training procedure. The proposed voicing incorporation is evaluated both within a standard model and two other models that had compensated for the noise effect, the missing-feature and the multi-conditional training model. Experiments are first performed on noisy speech data from the Aurora 2 database. Significant performance improvements are achieved when the voicing information is incorporated within the standard model as well as the noise-compensated models. The employment of voicing information is also demonstrated on a phoneme recognition task on the noise-corrupted TIMIT database and considerable improvements are observed."
        },
        {
          "rank": 33,
          "score": 0.6927005052566528,
          "doc_id": "JAKO202029462558904",
          "title": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식",
          "abstract": "특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202029462558904&target=NART&cn=JAKO202029462558904",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다."
        },
        {
          "rank": 34,
          "score": 0.6907665729522705,
          "doc_id": "NART133898062",
          "title": "Hidden Markov Neural Networks",
          "abstract": "<P>We define an evolving in-time Bayesian neural network called a Hidden Markov Neural Network, which addresses the crucial challenge in time-series forecasting and continual learning: striking a balance between adapting to new data and appropriately forgetting outdated information. This is achieved by modelling the weights of a neural network as the hidden states of a Hidden Markov model, with the observed process defined by the available data. A filtering algorithm is employed to learn a variational approximation of the evolving-in-time posterior distribution over the weights. By leveraging a sequential variant of Bayes by Backprop, enriched with a stronger regularization technique called variational DropConnect, Hidden Markov Neural Networks achieve robust regularization and scalable inference. Experiments on MNIST, dynamic classification tasks, and next-frame forecasting in videos demonstrate that Hidden Markov Neural Networks provide strong predictive performance while enabling effective uncertainty quantification.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART133898062&target=NART&cn=NART133898062",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden Markov Neural Networks Hidden Markov Neural Networks Hidden Markov Neural Networks <P>We define an evolving in-time Bayesian neural network called a Hidden Markov Neural Network, which addresses the crucial challenge in time-series forecasting and continual learning: striking a balance between adapting to new data and appropriately forgetting outdated information. This is achieved by modelling the weights of a neural network as the hidden states of a Hidden Markov model, with the observed process defined by the available data. A filtering algorithm is employed to learn a variational approximation of the evolving-in-time posterior distribution over the weights. By leveraging a sequential variant of Bayes by Backprop, enriched with a stronger regularization technique called variational DropConnect, Hidden Markov Neural Networks achieve robust regularization and scalable inference. Experiments on MNIST, dynamic classification tasks, and next-frame forecasting in videos demonstrate that Hidden Markov Neural Networks provide strong predictive performance while enabling effective uncertainty quantification.</P>"
        },
        {
          "rank": 35,
          "score": 0.6896363496780396,
          "doc_id": "NART06964546",
          "title": "Low resolution, degraded document recognition using neural networks and hidden Markov models",
          "abstract": "<P><B>Abstract</B></P><P>We collected a large, real world database, containing degraded, old and faxed documents and present a comparison between two leading edge commercial software packages and human reading performance which shows quantitatively the huge performance gap between humans and machines, even on random character documents where no context can be used. This indicates room for possible improvements. We implemented an integrated segmentation and recognition algorithm using neural networks and hidden Markov models trained on the database and present results which show the superior performance of the algorithm.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART06964546&target=NART&cn=NART06964546",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Low resolution, degraded document recognition using neural networks and hidden Markov models Low resolution, degraded document recognition using neural networks and hidden Markov models Low resolution, degraded document recognition using neural networks and hidden Markov models <P><B>Abstract</B></P><P>We collected a large, real world database, containing degraded, old and faxed documents and present a comparison between two leading edge commercial software packages and human reading performance which shows quantitatively the huge performance gap between humans and machines, even on random character documents where no context can be used. This indicates room for possible improvements. We implemented an integrated segmentation and recognition algorithm using neural networks and hidden Markov models trained on the database and present results which show the superior performance of the algorithm.</P>"
        },
        {
          "rank": 36,
          "score": 0.688791036605835,
          "doc_id": "JAKO201719951669089",
          "title": "원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링",
          "abstract": "This paper proposes a new method to train Deep Neural Network (DNN)-based acoustic models for speech recognition of native and foreign speakers. The proposed method consists of determining multi-set state clusters with various acoustic properties, training a DNN-based acoustic model, and recognizing speech based on the model. In the proposed method, hidden nodes of DNN are shared, but output nodes are separated to accommodate different acoustic properties for native and foreign speech. In an English speech recognition task for speakers of Korean and English respectively, the proposed method is shown to slightly improve recognition accuracy compared to the conventional multi-condition training method.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201719951669089&target=NART&cn=JAKO201719951669089",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 This paper proposes a new method to train Deep Neural Network (DNN)-based acoustic models for speech recognition of native and foreign speakers. The proposed method consists of determining multi-set state clusters with various acoustic properties, training a DNN-based acoustic model, and recognizing speech based on the model. In the proposed method, hidden nodes of DNN are shared, but output nodes are separated to accommodate different acoustic properties for native and foreign speech. In an English speech recognition task for speakers of Korean and English respectively, the proposed method is shown to slightly improve recognition accuracy compared to the conventional multi-condition training method."
        },
        {
          "rank": 37,
          "score": 0.6869703531265259,
          "doc_id": "JAKO199911921383665",
          "title": "회귀신경망을 이용한 음성인식에 관한 연구",
          "abstract": "본 논문은 회귀신경망을 이용한 음성인식에 관한 연구이다. 예측형 신경망으로 음절단위로 모델링한 후 미지의 입력음성에 대하여 예측오차가 최소가 되는 모델을 인식결과로 한다. 이를 위해서 예측형으로 구성된 신경망에 음성의 시변성을 신경망 내부에 흡수시키기 위해서 회귀구조의 동적인 신경망인 회귀예측신경망을 구성하고 Elman과 Jordan이 제안한 회귀구조에 따라 인식성능을 서로 비교하였다. 음성DB는 ETRI의 샘돌이 음성 데이터를 사용하였다. 그리고, 신경망의 최적모델을 구하기 위하여 예측차수와 은닉층 유니트 수의 변화에 따른 인식률의 변화와 문맥층에서 자기회귀계수를 두어 이전의 값들이 문맥층에서 누적되도록 하였을 경우에 대한 인식률의 변화를 비교하였다. 실험결과, 최적의 예측차수, 은닉층 유니트수, 자기회귀계수는 신경망의 구조에 따라 차이가 나타났으며, 전반적으로 Jordan망이 Elman망보다 인식률이 높았으며, 자기회귀계수에 대한 영향은 신경망의 구조와 계수값에 따라 불규칙하게 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199911921383665&target=NART&cn=JAKO199911921383665",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망을 이용한 음성인식에 관한 연구 회귀신경망을 이용한 음성인식에 관한 연구 회귀신경망을 이용한 음성인식에 관한 연구 본 논문은 회귀신경망을 이용한 음성인식에 관한 연구이다. 예측형 신경망으로 음절단위로 모델링한 후 미지의 입력음성에 대하여 예측오차가 최소가 되는 모델을 인식결과로 한다. 이를 위해서 예측형으로 구성된 신경망에 음성의 시변성을 신경망 내부에 흡수시키기 위해서 회귀구조의 동적인 신경망인 회귀예측신경망을 구성하고 Elman과 Jordan이 제안한 회귀구조에 따라 인식성능을 서로 비교하였다. 음성DB는 ETRI의 샘돌이 음성 데이터를 사용하였다. 그리고, 신경망의 최적모델을 구하기 위하여 예측차수와 은닉층 유니트 수의 변화에 따른 인식률의 변화와 문맥층에서 자기회귀계수를 두어 이전의 값들이 문맥층에서 누적되도록 하였을 경우에 대한 인식률의 변화를 비교하였다. 실험결과, 최적의 예측차수, 은닉층 유니트수, 자기회귀계수는 신경망의 구조에 따라 차이가 나타났으며, 전반적으로 Jordan망이 Elman망보다 인식률이 높았으며, 자기회귀계수에 대한 영향은 신경망의 구조와 계수값에 따라 불규칙하게 나타났다."
        },
        {
          "rank": 38,
          "score": 0.6861984729766846,
          "doc_id": "NART56157876",
          "title": "한국어 단어범주예측을 위한 은닉마르코프 모델과 신경망의 결합",
          "abstract": "<P> 본 논문에서는 일반적인 한국어 텍스트 문장에서 단어범주를 예측하기 위하여 HMM(Hidden Markov Model)과 신경망을 결합한 모델을 제안하였다.  한국어의 단어범주를 품사와 조사의 격 및 형에 따라 33개로 분류하였으며, 분류된 단어범주를 이용하여 국민학교 교과서를 대상으로 텍스트 데이타베이스를 구성하였다. 기존의 단어범주예측을 위해 제안된 NETgram은 동적, 정적 특징을 모두 신경망으로 표현하지만 본 논문에서는 시간에 따른 동적 변화를 잘 표현해주는 HMM을 신경망과 결합하는 방법을 제안하였다. 임의의 한국어 텍스트 문장으로 실험한 결과 4-gram 예측에서 종료상태가 고정되지 않고 6개의 관찰확률을 가지며 입력관찰열을 2번 순환반복으로 훈련시킨 HMM 모델과 신경망을 결합한 모델이 가장 우수하여 단어범주예측률이 20.22%를 차지하였다. 이것은 기존의 NETgram을 이용한 방식에 비하여 2.14% 향상된 것이다.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART56157876&target=NART&cn=NART56157876",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "한국어 단어범주예측을 위한 은닉마르코프 모델과 신경망의 결합 한국어 단어범주예측을 위한 은닉마르코프 모델과 신경망의 결합 한국어 단어범주예측을 위한 은닉마르코프 모델과 신경망의 결합 <P> 본 논문에서는 일반적인 한국어 텍스트 문장에서 단어범주를 예측하기 위하여 HMM(Hidden Markov Model)과 신경망을 결합한 모델을 제안하였다.  한국어의 단어범주를 품사와 조사의 격 및 형에 따라 33개로 분류하였으며, 분류된 단어범주를 이용하여 국민학교 교과서를 대상으로 텍스트 데이타베이스를 구성하였다. 기존의 단어범주예측을 위해 제안된 NETgram은 동적, 정적 특징을 모두 신경망으로 표현하지만 본 논문에서는 시간에 따른 동적 변화를 잘 표현해주는 HMM을 신경망과 결합하는 방법을 제안하였다. 임의의 한국어 텍스트 문장으로 실험한 결과 4-gram 예측에서 종료상태가 고정되지 않고 6개의 관찰확률을 가지며 입력관찰열을 2번 순환반복으로 훈련시킨 HMM 모델과 신경망을 결합한 모델이 가장 우수하여 단어범주예측률이 20.22%를 차지하였다. 이것은 기존의 NETgram을 이용한 방식에 비하여 2.14% 향상된 것이다.</P>"
        },
        {
          "rank": 39,
          "score": 0.6853682994842529,
          "doc_id": "JAKO199811921284763",
          "title": "은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식",
          "abstract": "한국어 연속 음성에서 발생하는 조음결합문제를 해결하기 위하여 단어를 기본 인식 단위로 사용할 경우 각 단어의 효율적인 표현 방법, 연속된 단어로 이루어진 여러 문장의 표현 방법 그리고 입력된 연속음성을 연속된 여러 단어로의 정합 방법에 관한 연구가 선행되어야 한다. 본 논문에서는 은닉 마르코프 모델과 레벨빌딩 알고리즘을 이용한 한국어 연속 음성 인식 시스템을 제안한다. 각 단어는 은닉 마르코프 모델로 표현하고 문장을 표현하기 위하여 단어 모델을 연결한 형태인 인식 네트워크를 구성한다. 인식네트워크의 탐색 알고리즘으로는 레벨 빌딩 알고리즘을 사용한다. 제안한 방법은 항공기 예약 시스템에 적용한 실험에서 인식율과 인식속도면에서 실용적이었으며 또한 비교적 적은 저장공간으로 전체 문장을 표현하고 쉽게 확장할 수 있다는 장점을 가지고 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199811921284763&target=NART&cn=JAKO199811921284763",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식 은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식 은닉 마르코프 모델과 레벨 빌딩을 이용한 한국어 연속 음성 인식 한국어 연속 음성에서 발생하는 조음결합문제를 해결하기 위하여 단어를 기본 인식 단위로 사용할 경우 각 단어의 효율적인 표현 방법, 연속된 단어로 이루어진 여러 문장의 표현 방법 그리고 입력된 연속음성을 연속된 여러 단어로의 정합 방법에 관한 연구가 선행되어야 한다. 본 논문에서는 은닉 마르코프 모델과 레벨빌딩 알고리즘을 이용한 한국어 연속 음성 인식 시스템을 제안한다. 각 단어는 은닉 마르코프 모델로 표현하고 문장을 표현하기 위하여 단어 모델을 연결한 형태인 인식 네트워크를 구성한다. 인식네트워크의 탐색 알고리즘으로는 레벨 빌딩 알고리즘을 사용한다. 제안한 방법은 항공기 예약 시스템에 적용한 실험에서 인식율과 인식속도면에서 실용적이었으며 또한 비교적 적은 저장공간으로 전체 문장을 표현하고 쉽게 확장할 수 있다는 장점을 가지고 있다."
        },
        {
          "rank": 40,
          "score": 0.6851271986961365,
          "doc_id": "NART13642943",
          "title": "Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments",
          "abstract": "<P>Handling background noise or echo (reverberation) etc. is very important for having an automated robot etc. recognize remote speech in a real environment. As effective schemes for handling this problem, noise reducing schemes such as model adaptation schemes including HMM decomposition and composition or microphone array (beamformer) signal processing, spectral subtraction, etc. have been proposed. In particular, a model adaptation scheme is very effective for speech recognition in a noisy environment and its recognition performance increases in proportion to the signal-to-noise ratio (SNR). In this paper, improving the recognition performance in a low-SNR environment by receiving speech at a high SNR using a microphone array before HMM decomposition and composition is attempted. The results of speech recognition experiments conducted in a noisy environment in an acoustic laboratory show an improvement in the recognition rate of about 25% by the proposed method for the case in which the SNR in a single microphone is 0 dB, as compared with the cases of using microphone array signal processing, HMM decomposition and composition alone. In addition, the proposed method shows recognition performance comparable to the case of using cepstrum mean normalization and spectral subtraction performed with an optimal coefficient given to the speech after microphone array processing. &copy; 2002 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 85(9): 13&ndash;22, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ecjb.10068</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART13642943&target=NART&cn=NART13642943",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments <P>Handling background noise or echo (reverberation) etc. is very important for having an automated robot etc. recognize remote speech in a real environment. As effective schemes for handling this problem, noise reducing schemes such as model adaptation schemes including HMM decomposition and composition or microphone array (beamformer) signal processing, spectral subtraction, etc. have been proposed. In particular, a model adaptation scheme is very effective for speech recognition in a noisy environment and its recognition performance increases in proportion to the signal-to-noise ratio (SNR). In this paper, improving the recognition performance in a low-SNR environment by receiving speech at a high SNR using a microphone array before HMM decomposition and composition is attempted. The results of speech recognition experiments conducted in a noisy environment in an acoustic laboratory show an improvement in the recognition rate of about 25% by the proposed method for the case in which the SNR in a single microphone is 0 dB, as compared with the cases of using microphone array signal processing, HMM decomposition and composition alone. In addition, the proposed method shows recognition performance comparable to the case of using cepstrum mean normalization and spectral subtraction performed with an optimal coefficient given to the speech after microphone array processing. &copy; 2002 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 85(9): 13&ndash;22, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ecjb.10068</P>"
        },
        {
          "rank": 41,
          "score": 0.6825098991394043,
          "doc_id": "NART20482871",
          "title": "Piecewise-linear transformation-based HMM adaptation for noisy speech",
          "abstract": "<P><B>Abstract</B></P><P>This paper proposes a new method using piecewise-linear transformation for adapting phone HMMs to noisy speech. Various noises are clustered according to their spectral property, and a noisy speech HMM corresponding to each clustered noise and SNR condition is made. Based on the likelihood maximization criterion, an HMM that best matches an input noisy speech is selected and further adapted using linear transformation. The proposed method is evaluated by its ability to recognize noisy broadcast-news speech. It is confirmed that the proposed method is effective in recognizing numerically noise-added speech and actual noisy speech under various noise conditions. The proposed method minimizes mismatches between noisy input speech and the HMM&#x2019;s, sentence by sentence, without requiring online noise spectrum/model estimation. The proposed method is therefore easily applicable to real world conditions with frequently changing noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20482871&target=NART&cn=NART20482871",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Piecewise-linear transformation-based HMM adaptation for noisy speech Piecewise-linear transformation-based HMM adaptation for noisy speech Piecewise-linear transformation-based HMM adaptation for noisy speech <P><B>Abstract</B></P><P>This paper proposes a new method using piecewise-linear transformation for adapting phone HMMs to noisy speech. Various noises are clustered according to their spectral property, and a noisy speech HMM corresponding to each clustered noise and SNR condition is made. Based on the likelihood maximization criterion, an HMM that best matches an input noisy speech is selected and further adapted using linear transformation. The proposed method is evaluated by its ability to recognize noisy broadcast-news speech. It is confirmed that the proposed method is effective in recognizing numerically noise-added speech and actual noisy speech under various noise conditions. The proposed method minimizes mismatches between noisy input speech and the HMM&#x2019;s, sentence by sentence, without requiring online noise spectrum/model estimation. The proposed method is therefore easily applicable to real world conditions with frequently changing noise.</P>"
        },
        {
          "rank": 42,
          "score": 0.6799342632293701,
          "doc_id": "JAKO201707851605473",
          "title": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표",
          "abstract": "본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201707851605473&target=NART&cn=JAKO201707851605473",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다."
        },
        {
          "rank": 43,
          "score": 0.6776024699211121,
          "doc_id": "JAKO200819858103682",
          "title": "A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments",
          "abstract": "In this paper, a MSS-NOVO method that combines the HMM composition method with a noise reduction method is proposed for speech recognition in noisy environments. This combined method starts with noise reduction with modified spectral subtraction (MSS) to enhance the input noisy speech, then the noise and voice composition (NOVO) method is applied for making noise adapted models by using the noise in the non-utterance regions of the enhanced noisy speech. In order to evaluate the effectiveness of our proposed method, we compare MSS-NOVO method with other methods, i.e., SS-NOVO, MWF-NOVO. To set up the noisy speech for test, we add White noise to KLE 452 database with different SNRs range from 0dB to 15dB, at 5dB intervals. From the tests, MSS-NOVO method shows average improvement of 66.5% and 13.6% compared with the existing SS-NOVO method and MWF-NOVO method, respectively. Especially our proposed MSS-NOVO method shows a big improvement at low SNRs.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200819858103682&target=NART&cn=JAKO200819858103682",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments In this paper, a MSS-NOVO method that combines the HMM composition method with a noise reduction method is proposed for speech recognition in noisy environments. This combined method starts with noise reduction with modified spectral subtraction (MSS) to enhance the input noisy speech, then the noise and voice composition (NOVO) method is applied for making noise adapted models by using the noise in the non-utterance regions of the enhanced noisy speech. In order to evaluate the effectiveness of our proposed method, we compare MSS-NOVO method with other methods, i.e., SS-NOVO, MWF-NOVO. To set up the noisy speech for test, we add White noise to KLE 452 database with different SNRs range from 0dB to 15dB, at 5dB intervals. From the tests, MSS-NOVO method shows average improvement of 66.5% and 13.6% compared with the existing SS-NOVO method and MWF-NOVO method, respectively. Especially our proposed MSS-NOVO method shows a big improvement at low SNRs."
        },
        {
          "rank": 44,
          "score": 0.6747882962226868,
          "doc_id": "JAKO200727500236879",
          "title": "자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식",
          "abstract": "In this paper, we carried out recognition experiments for noisy speech having various levels of car noise and output of an audio system using the speech interface. The speech interface consists of three parts: pre-processing, acoustic echo canceller, post-processing. First, a high pass filter is employed as a pre-processing part to remove some engine noises. Then, an echo canceller implemented by using an FIR-type filter with an NLMS adaptive algorithm is used to remove the music or speech coming from the audio system in a car. As a last part, the MMSE-STSA based speech enhancement method is applied to the out of the echo canceller to remove the residual noise further. For recognition experiments, we generated test signals by adding music to the car noisy speech from Aurora 2 database. The HTK-based continuous HMM system is constructed for a recognition system. Experimental results show that the proposed speech interface is very promising for robust speech recognition in a noisy car environment.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200727500236879&target=NART&cn=JAKO200727500236879",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 In this paper, we carried out recognition experiments for noisy speech having various levels of car noise and output of an audio system using the speech interface. The speech interface consists of three parts: pre-processing, acoustic echo canceller, post-processing. First, a high pass filter is employed as a pre-processing part to remove some engine noises. Then, an echo canceller implemented by using an FIR-type filter with an NLMS adaptive algorithm is used to remove the music or speech coming from the audio system in a car. As a last part, the MMSE-STSA based speech enhancement method is applied to the out of the echo canceller to remove the residual noise further. For recognition experiments, we generated test signals by adding music to the car noisy speech from Aurora 2 database. The HTK-based continuous HMM system is constructed for a recognition system. Experimental results show that the proposed speech interface is very promising for robust speech recognition in a noisy car environment."
        },
        {
          "rank": 45,
          "score": 0.6737416982650757,
          "doc_id": "NART32653959",
          "title": "Integrating support vector machines and neural networks",
          "abstract": "<P><B>Abstract</B></P><P>Support vector machines (SVMs) are a powerful technique developed in the last decade to effectively tackle classification and regression problems. In this paper we describe how support vector machines and artificial neural networks can be integrated in order to classify objects correctly. This technique has been successfully applied to the problem of determining the quality of tiles. Using an optical reader system, some features are automatically extracted, then a subset of the features is determined and the tiles are classified based on this subset.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART32653959&target=NART&cn=NART32653959",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Integrating support vector machines and neural networks Integrating support vector machines and neural networks Integrating support vector machines and neural networks <P><B>Abstract</B></P><P>Support vector machines (SVMs) are a powerful technique developed in the last decade to effectively tackle classification and regression problems. In this paper we describe how support vector machines and artificial neural networks can be integrated in order to classify objects correctly. This technique has been successfully applied to the problem of determining the quality of tiles. Using an optical reader system, some features are automatically extracted, then a subset of the features is determined and the tiles are classified based on this subset.</P>"
        },
        {
          "rank": 46,
          "score": 0.6692674160003662,
          "doc_id": "JAKO201120661418238",
          "title": "음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거",
          "abstract": "본 논문에서는 먼저 신경회로망의 학습에 오차역전파 학습 알고리즘을 사용하여 각 프레임에서의 음성 및 잡음 구간의 검출에 의한 음성인식 알고리즘을 제안한다. 그리고 신경회로망에 의하여 음성 및 잡음 구간의 검출에 따라서 각 프레임에서 잡음을 제거하는 스펙트럼 차감법을 제안한다. 본 실험에서는 제안한 음성인식알고리즘의 성능을 원음성에 백색잡음 및 자동차 잡음을 부가하여 인식율을 평가한다. 또한 인식시스템에 의하여 검출된 음성 및 잡음 구간을 이용하여 각 프레임에서의 스펙트럼 차감법에 의한 잡음제거의 실험결과를 나타낸다. 잡음에 의하여 오염된 음성에 대하여 신호대잡음비를 사용하여 본 알고리즘이 유효하다는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201120661418238&target=NART&cn=JAKO201120661418238",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 본 논문에서는 먼저 신경회로망의 학습에 오차역전파 학습 알고리즘을 사용하여 각 프레임에서의 음성 및 잡음 구간의 검출에 의한 음성인식 알고리즘을 제안한다. 그리고 신경회로망에 의하여 음성 및 잡음 구간의 검출에 따라서 각 프레임에서 잡음을 제거하는 스펙트럼 차감법을 제안한다. 본 실험에서는 제안한 음성인식알고리즘의 성능을 원음성에 백색잡음 및 자동차 잡음을 부가하여 인식율을 평가한다. 또한 인식시스템에 의하여 검출된 음성 및 잡음 구간을 이용하여 각 프레임에서의 스펙트럼 차감법에 의한 잡음제거의 실험결과를 나타낸다. 잡음에 의하여 오염된 음성에 대하여 신호대잡음비를 사용하여 본 알고리즘이 유효하다는 것을 확인한다."
        },
        {
          "rank": 47,
          "score": 0.6689739227294922,
          "doc_id": "JAKO199615875841474",
          "title": "천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식",
          "abstract": "본 논문에서는 상태간의 천이가 특정한 시간 구간에서만 발생하도록 하는 천이 제한(transition constrained) HMM를 제안하고 잡음 환경에서의 성능을 평가하였다. 천이 제한 HMM는 상태 지속을 제한하고 음성 신호의 시간적 변화를 단순하고 효과적으로 표현할 수 있다. 제안된 천이 제한 HMM은 기존 HMM 보다 성능이 우수할 뿐만아니라 계산량도 매우 감소한다.  제안된 방법의 성능을 평가하기 위하여 반연속(semi-continuous) HMM을 이용하여 잡음이 SNR 20, 10, 0 dB로 첨가된 음성에 화자독립 단독음 인식실험을 수행하였다. 실험 결과에서 제안된 방법은 잡음에 강인한 특성을 나타내었다. 두 가지 종류의 잡음을 SNR 10dB로 첨가하여 사용한 경우, 천이제한 HMM의 인식률은 기존 HMM의 단어 인식률 81.08%와 75.36%에 비하여 각각 7.31%와 10.35% 향상되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199615875841474&target=NART&cn=JAKO199615875841474",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식 천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식 천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식 본 논문에서는 상태간의 천이가 특정한 시간 구간에서만 발생하도록 하는 천이 제한(transition constrained) HMM를 제안하고 잡음 환경에서의 성능을 평가하였다. 천이 제한 HMM는 상태 지속을 제한하고 음성 신호의 시간적 변화를 단순하고 효과적으로 표현할 수 있다. 제안된 천이 제한 HMM은 기존 HMM 보다 성능이 우수할 뿐만아니라 계산량도 매우 감소한다.  제안된 방법의 성능을 평가하기 위하여 반연속(semi-continuous) HMM을 이용하여 잡음이 SNR 20, 10, 0 dB로 첨가된 음성에 화자독립 단독음 인식실험을 수행하였다. 실험 결과에서 제안된 방법은 잡음에 강인한 특성을 나타내었다. 두 가지 종류의 잡음을 SNR 10dB로 첨가하여 사용한 경우, 천이제한 HMM의 인식률은 기존 HMM의 단어 인식률 81.08%와 75.36%에 비하여 각각 7.31%와 10.35% 향상되었다."
        },
        {
          "rank": 48,
          "score": 0.6678153276443481,
          "doc_id": "NART01349148",
          "title": "Automatic segmentation and labeling of speech based on Hidden Markov Models",
          "abstract": "An accurate database documentation at phonetic level is very important for speech research: however, manual segmentation and labeling is a time consuming and error prone task. This article describes an automatic procedure for the segmentation of speech: given either the linguistic or the phonetic content of a speech utterance, the system provides phone boundaries. The technique is based on the use of an acoustic-phonetic unit Hidden Markov Model (HMM) recognizer: both the recognizer and the segmentation system have been designed exploiting the DARPA-TIMIT acoustic-phonetic continuous speech database of American English. Segmentation and labeling experiments have been conducted in different conditions to check the reliability of the resulting system. Satisfactory results have been obtained, especially when the system is trained with some manually presegmented material. The size of this material is a crucial factor; system performance has been evaluated with respect to this parameter. It turns out that the system provides 88.3% correct boundary location, given a tolerance of 20 ms, when only 256 phonetically balanced sentences are used for its training.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART01349148&target=NART&cn=NART01349148",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Automatic segmentation and labeling of speech based on Hidden Markov Models Automatic segmentation and labeling of speech based on Hidden Markov Models Automatic segmentation and labeling of speech based on Hidden Markov Models An accurate database documentation at phonetic level is very important for speech research: however, manual segmentation and labeling is a time consuming and error prone task. This article describes an automatic procedure for the segmentation of speech: given either the linguistic or the phonetic content of a speech utterance, the system provides phone boundaries. The technique is based on the use of an acoustic-phonetic unit Hidden Markov Model (HMM) recognizer: both the recognizer and the segmentation system have been designed exploiting the DARPA-TIMIT acoustic-phonetic continuous speech database of American English. Segmentation and labeling experiments have been conducted in different conditions to check the reliability of the resulting system. Satisfactory results have been obtained, especially when the system is trained with some manually presegmented material. The size of this material is a crucial factor; system performance has been evaluated with respect to this parameter. It turns out that the system provides 88.3% correct boundary location, given a tolerance of 20 ms, when only 256 phonetically balanced sentences are used for its training."
        },
        {
          "rank": 49,
          "score": 0.6673687696456909,
          "doc_id": "JAKO201734964189755",
          "title": "주목 메커니즘 기반의 심층신경망을 이용한 음성 감정인식",
          "abstract": "본 논문에서는 주목 메커니즘 기반의 심층 신경망을 사용한 음성 감정인식 방법을 제안한다. 제안하는 방식은 CNN(Convolution Neural Networks), GRU(Gated Recurrent Unit), DNN(Deep Neural Networks)의 결합으로 이루어진 심층 신경망 구조와 주목 메커니즘으로 구성된다. 음성의 스펙트로그램에는 감정에 따른 특징적인 패턴이 포함되어 있으므로 제안하는 방식에서는 일반적인 CNN에서 컨벌루션 필터를 tuned Gabor 필터로 사용하는 GCNN(Gabor CNN)을 사용하여 패턴을 효과적으로 모델링한다. 또한 CNN과 FC(Fully-Connected)레이어 기반의 주목 메커니즘을 적용하여 추출된 특징의 맥락 정보를 고려한 주목 가중치를 구해 감정인식에 사용한다. 본 논문에서 제안하는 방식의 검증을 위해 6가지 감정에 대해 인식 실험을 진행하였다. 실험 결과, 제안한 방식이 음성 감정인식에서 기존의 방식보다 더 높은 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201734964189755&target=NART&cn=JAKO201734964189755",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "주목 메커니즘 기반의 심층신경망을 이용한 음성 감정인식 주목 메커니즘 기반의 심층신경망을 이용한 음성 감정인식 주목 메커니즘 기반의 심층신경망을 이용한 음성 감정인식 본 논문에서는 주목 메커니즘 기반의 심층 신경망을 사용한 음성 감정인식 방법을 제안한다. 제안하는 방식은 CNN(Convolution Neural Networks), GRU(Gated Recurrent Unit), DNN(Deep Neural Networks)의 결합으로 이루어진 심층 신경망 구조와 주목 메커니즘으로 구성된다. 음성의 스펙트로그램에는 감정에 따른 특징적인 패턴이 포함되어 있으므로 제안하는 방식에서는 일반적인 CNN에서 컨벌루션 필터를 tuned Gabor 필터로 사용하는 GCNN(Gabor CNN)을 사용하여 패턴을 효과적으로 모델링한다. 또한 CNN과 FC(Fully-Connected)레이어 기반의 주목 메커니즘을 적용하여 추출된 특징의 맥락 정보를 고려한 주목 가중치를 구해 감정인식에 사용한다. 본 논문에서 제안하는 방식의 검증을 위해 6가지 감정에 대해 인식 실험을 진행하였다. 실험 결과, 제안한 방식이 음성 감정인식에서 기존의 방식보다 더 높은 성능을 보였다."
        },
        {
          "rank": 50,
          "score": 0.6646897792816162,
          "doc_id": "JAKO202011263332681",
          "title": "심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용",
          "abstract": "가우스 혼합 모델-은닉 마코프 모델(Gaussian Mixture Model-Hidden Markov Model, GMM-HMM)을 이용하는 전통적인 음성인식 시스템에서는, 극점 필터링 기반의 켑스트럼 특징 정규화 방식이 잡음 환경에서 짧은 발화의 인식 성능을 향상시키는데 효과적이었다. 본 논문에서는 심층신경망(Deep Neural Network, DNN)을 이용하는 최신의 음성인식 시스템에서도 이 방식의 유용성이 있는지 검토한다. AURORA 2 DB에 대한 실험 결과, 특히 훈련 및 테스트 환경 사이의 불일치가 클 때에, 극점 필터링 기반의 켑스트럼 평균 분산 정규화 방식이 극점 필터링을 사용하지 않는 방식에 비해 매우 짧은 발화의 인식 성능을 개선시킴을 보여 준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202011263332681&target=NART&cn=JAKO202011263332681",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 가우스 혼합 모델-은닉 마코프 모델(Gaussian Mixture Model-Hidden Markov Model, GMM-HMM)을 이용하는 전통적인 음성인식 시스템에서는, 극점 필터링 기반의 켑스트럼 특징 정규화 방식이 잡음 환경에서 짧은 발화의 인식 성능을 향상시키는데 효과적이었다. 본 논문에서는 심층신경망(Deep Neural Network, DNN)을 이용하는 최신의 음성인식 시스템에서도 이 방식의 유용성이 있는지 검토한다. AURORA 2 DB에 대한 실험 결과, 특히 훈련 및 테스트 환경 사이의 불일치가 클 때에, 극점 필터링 기반의 켑스트럼 평균 분산 정규화 방식이 극점 필터링을 사용하지 않는 방식에 비해 매우 짧은 발화의 인식 성능을 개선시킴을 보여 준다."
        }
      ]
    },
    {
      "query": "How does the integration of HMMs and NNs enhance robustness in noisy environments for audiovisual speech recognition?",
      "query_meta": {
        "type": "single_hop",
        "index": 3
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8070221543312073,
          "doc_id": "NART16453920",
          "title": "Neural-network-based HMM adaptation for noisy speech recognition.",
          "abstract": "<P>This paper proposes a new method, using neural networks, of adapting phone HMMs to noisy speech. The neural networks are designed to map clean speech HMMs to noise-adapted HMMs, using noise HMMs and signal-to-noise ratios (SNRs) as inputs. The neural network is trained by minimizing the mean square error between the output HMMs and the target noise-adapted HMMs. In an evaluation, the proposed method was used to recognize noisy broadcast-news speech in speaker-dependent and speaker-independent modes. The trained networks were found to be effective in recognizing new speakers under new noise and various SNR conditions.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART16453920&target=NART&cn=NART16453920",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural-network-based HMM adaptation for noisy speech recognition. Neural-network-based HMM adaptation for noisy speech recognition. Neural-network-based HMM adaptation for noisy speech recognition. <P>This paper proposes a new method, using neural networks, of adapting phone HMMs to noisy speech. The neural networks are designed to map clean speech HMMs to noise-adapted HMMs, using noise HMMs and signal-to-noise ratios (SNRs) as inputs. The neural network is trained by minimizing the mean square error between the output HMMs and the target noise-adapted HMMs. In an evaluation, the proposed method was used to recognize noisy broadcast-news speech in speaker-dependent and speaker-independent modes. The trained networks were found to be effective in recognizing new speakers under new noise and various SNR conditions.</P>"
        },
        {
          "rank": 2,
          "score": 0.8006957769393921,
          "doc_id": "DIKO0007842188",
          "title": "신경망 예측 HMM을 이용한 음성인식에 관한 연구",
          "abstract": "음성은 인간의 가장 자연스러운 의사소통의 수단이며 이러한 음성에 의한 인간-기계 인터페이스는 속도가 빠르고 특별한 훈련이 없어도 이루어진다. 또한 컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식 기술은 중요한 연구 과제가 되고 있다. 이러한 음성인식에 관한 연구는 HMM과 신경망에 의한 방법들이 활발히 진행되고 있다. 그러나 HMM은 모델구조의 결정에 어려움이 있으며, 음성의 과도적 정보를 경시하는 경향이 있기 때문에 시간적 상관 표현력이 부족한 결점이 있다. 따라서 이러한 단점을 극복하기 위하여 음성의 동적특성을 표현할 수 있는 회귀계수와 지속시간 확률등을 파라메터로 추가하거나, 언어학적 제약을 가하여 최적의 단어별을 탐색하는 언어적 모델을 결합한 음성이해 시스템으로 발전하고 있다. 또한 신경망의 경우 그 구조나 가중치에 시간적인 것이 고려되지 않으므로 패턴이 정적인 경우 인식률이 높으나 음성과 같은 시계열 패턴의 비선형 신축을 취급하기 어렵다. 따라서 이러한 단점을 보완하기 위하여 회귀신경망, 예측형 신경망등이 제안되었다. 본 논문에서는 이러한 점들을 고려하여 HMM의 패턴에 대한 시간적 상관표현력을 개선시킬 수 있는 신경망 예측 HMM을 제안한다. 신경망 예측 HMM은 HMM과 신경망의 장점을 함께 사용할 수 있는 하이브리드형 네트워크로 예측형 신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하는 것이다. 따라서 예측형 신경망은 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가지므로 HMM에 동적 특성을 부여할 수 있다. 실험에서는 단독 숫자음과 100음절 데이터를 이용하여 HMM과 신경망 그리고 본 논문에서 제안한 신경망 예측 HMM의 인식성능을 비교·검토하였다. 신경망 예측 HMM은 MLP 예측 HMM, Elman망 예측 HMM, Jordan망 예측 HMM의 3가지이다. 실험결과 평가용 데이터를 기준으로 단독 숫자음에 대해서는 MLP 예측 HMM이 99.5%로 가장 우수한 결과를, 100음절 데이터에 대해서는 HMM이 87.7%로 가장 우수한 결과를 보였다. Elman망 예측 HMM과 Jordan망 예측 HMM의 경우 회귀구조임에도 불구하고 MLP 예측 HMM의 인식성능을 상회하지는 못하였다. 따라서 신경망 예측 HMM의 인식성능을 향상시키기 위해서는 데이터에 따른 효과의 검토와 음성패턴의 시간적 상관관계를 보다 더 잘 표현 할 수 있는 신경망의 구조에 대한 연구가 요구되며, 신경망과 HMM의 학습알고리즘에 대한 연구가 필요하다고 판단된다. 본 논문에서 제안한 신경망 예측 HMM은 HMM과 신경망의 결합이라고 하는 향후 가장 유효한 방법의 하나로 사료되며 연속음성 인식시스템으로 확장할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0007842188&target=NART&cn=DIKO0007842188",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망 예측 HMM을 이용한 음성인식에 관한 연구 신경망 예측 HMM을 이용한 음성인식에 관한 연구 신경망 예측 HMM을 이용한 음성인식에 관한 연구 음성은 인간의 가장 자연스러운 의사소통의 수단이며 이러한 음성에 의한 인간-기계 인터페이스는 속도가 빠르고 특별한 훈련이 없어도 이루어진다. 또한 컴퓨터 및 정보통신 기술의 급속한 발전으로 음성인식 기술은 중요한 연구 과제가 되고 있다. 이러한 음성인식에 관한 연구는 HMM과 신경망에 의한 방법들이 활발히 진행되고 있다. 그러나 HMM은 모델구조의 결정에 어려움이 있으며, 음성의 과도적 정보를 경시하는 경향이 있기 때문에 시간적 상관 표현력이 부족한 결점이 있다. 따라서 이러한 단점을 극복하기 위하여 음성의 동적특성을 표현할 수 있는 회귀계수와 지속시간 확률등을 파라메터로 추가하거나, 언어학적 제약을 가하여 최적의 단어별을 탐색하는 언어적 모델을 결합한 음성이해 시스템으로 발전하고 있다. 또한 신경망의 경우 그 구조나 가중치에 시간적인 것이 고려되지 않으므로 패턴이 정적인 경우 인식률이 높으나 음성과 같은 시계열 패턴의 비선형 신축을 취급하기 어렵다. 따라서 이러한 단점을 보완하기 위하여 회귀신경망, 예측형 신경망등이 제안되었다. 본 논문에서는 이러한 점들을 고려하여 HMM의 패턴에 대한 시간적 상관표현력을 개선시킬 수 있는 신경망 예측 HMM을 제안한다. 신경망 예측 HMM은 HMM과 신경망의 장점을 함께 사용할 수 있는 하이브리드형 네트워크로 예측형 신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하는 것이다. 따라서 예측형 신경망은 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가지므로 HMM에 동적 특성을 부여할 수 있다. 실험에서는 단독 숫자음과 100음절 데이터를 이용하여 HMM과 신경망 그리고 본 논문에서 제안한 신경망 예측 HMM의 인식성능을 비교·검토하였다. 신경망 예측 HMM은 MLP 예측 HMM, Elman망 예측 HMM, Jordan망 예측 HMM의 3가지이다. 실험결과 평가용 데이터를 기준으로 단독 숫자음에 대해서는 MLP 예측 HMM이 99.5%로 가장 우수한 결과를, 100음절 데이터에 대해서는 HMM이 87.7%로 가장 우수한 결과를 보였다. Elman망 예측 HMM과 Jordan망 예측 HMM의 경우 회귀구조임에도 불구하고 MLP 예측 HMM의 인식성능을 상회하지는 못하였다. 따라서 신경망 예측 HMM의 인식성능을 향상시키기 위해서는 데이터에 따른 효과의 검토와 음성패턴의 시간적 상관관계를 보다 더 잘 표현 할 수 있는 신경망의 구조에 대한 연구가 요구되며, 신경망과 HMM의 학습알고리즘에 대한 연구가 필요하다고 판단된다. 본 논문에서 제안한 신경망 예측 HMM은 HMM과 신경망의 결합이라고 하는 향후 가장 유효한 방법의 하나로 사료되며 연속음성 인식시스템으로 확장할 수 있을 것으로 기대된다."
        },
        {
          "rank": 3,
          "score": 0.7856262922286987,
          "doc_id": "NPAP12270893",
          "title": "Speech Recognition in Noisy Environments with Convolutional Neural Networks",
          "abstract": "<P>One of the biggest challenges in speech recognition today is its use on a daily basis, in which distortion and noise in the environment are present and hinder the recognition task. In the last thirty years, hundreds of methods for noise-robust recognition were proposed, each with its own advantages and disadvantages. In this paper, the use of convolutional neural networks (CNN) as acoustic models in automatic speech recognition systems (ASR) is proposed as an alternative to the classical recognition methods based on HMM without any noise-robust method applied. The experiment showed that the presented method reduces the equal error rate in word recognition tasks with additive noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12270893&target=NART&cn=NPAP12270893",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech Recognition in Noisy Environments with Convolutional Neural Networks Speech Recognition in Noisy Environments with Convolutional Neural Networks Speech Recognition in Noisy Environments with Convolutional Neural Networks <P>One of the biggest challenges in speech recognition today is its use on a daily basis, in which distortion and noise in the environment are present and hinder the recognition task. In the last thirty years, hundreds of methods for noise-robust recognition were proposed, each with its own advantages and disadvantages. In this paper, the use of convolutional neural networks (CNN) as acoustic models in automatic speech recognition systems (ASR) is proposed as an alternative to the classical recognition methods based on HMM without any noise-robust method applied. The experiment showed that the presented method reduces the equal error rate in word recognition tasks with additive noise.</P>"
        },
        {
          "rank": 4,
          "score": 0.7845563888549805,
          "doc_id": "JAKO200411922338894",
          "title": "신경망 기반 음성, 영상 및 문맥 통합 음성인식",
          "abstract": "최근 잡음환경에서 신뢰도 높은 음성인식을 위해 음성정보와 영상정보를 융합하는 방법이 활발히 연구되고 있다. 본 논문에서는 이절적인 정보의 융합에 적합한 신경망 모델을 기반으로 음성, 영상 및 문맥 정보 등 다양한 정보를 융합하여 잡음 환경에서 고려단어를 인식하는 음성인식 기법에 대하여 기술한다. 음성과 영상 특징을 이용한 이중 모드 신경망 BMNN(BiModal Neural Network)을 제안한다. BMM은 4개 층으로 이루어진 다층퍼셉트론의 구조를 가지며 각 층은 입력 특징의 추상화 기능을 수행한다. BMNN에서는 제 3층이 잡음에 의한 음성 정보의 손실을 보상하기 위하여 음성과 영상 특징을 통합하는 기능을 수행한다. 또한, 잡음환경에서 음성 인식률을 향상시키기 위해 사용자가 말한 단어들의 순차 패턴을 나타내는 문맥정보를 이용한 후처리 방법을 제안한다. 잡음환경에서 BMNN은 단순히 음성만을 사용한 것 보다 높은 성능을 보임으로써 그 타당성을 확인할 수 있을 뿐 아니라, 특히 문맥을 이용한 후처리를 하였을 경우 잡음 환경에서 90%이상의 인식률을 달성하였다 본 연구는 잡음환경에서 강인한 음성인식을 위해 다양한 추가 정보를 사용함으로써 성능을 향상시킬 수 있음을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200411922338894&target=NART&cn=JAKO200411922338894",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망 기반 음성, 영상 및 문맥 통합 음성인식 신경망 기반 음성, 영상 및 문맥 통합 음성인식 신경망 기반 음성, 영상 및 문맥 통합 음성인식 최근 잡음환경에서 신뢰도 높은 음성인식을 위해 음성정보와 영상정보를 융합하는 방법이 활발히 연구되고 있다. 본 논문에서는 이절적인 정보의 융합에 적합한 신경망 모델을 기반으로 음성, 영상 및 문맥 정보 등 다양한 정보를 융합하여 잡음 환경에서 고려단어를 인식하는 음성인식 기법에 대하여 기술한다. 음성과 영상 특징을 이용한 이중 모드 신경망 BMNN(BiModal Neural Network)을 제안한다. BMM은 4개 층으로 이루어진 다층퍼셉트론의 구조를 가지며 각 층은 입력 특징의 추상화 기능을 수행한다. BMNN에서는 제 3층이 잡음에 의한 음성 정보의 손실을 보상하기 위하여 음성과 영상 특징을 통합하는 기능을 수행한다. 또한, 잡음환경에서 음성 인식률을 향상시키기 위해 사용자가 말한 단어들의 순차 패턴을 나타내는 문맥정보를 이용한 후처리 방법을 제안한다. 잡음환경에서 BMNN은 단순히 음성만을 사용한 것 보다 높은 성능을 보임으로써 그 타당성을 확인할 수 있을 뿐 아니라, 특히 문맥을 이용한 후처리를 하였을 경우 잡음 환경에서 90%이상의 인식률을 달성하였다 본 연구는 잡음환경에서 강인한 음성인식을 위해 다양한 추가 정보를 사용함으로써 성능을 향상시킬 수 있음을 제시한다."
        },
        {
          "rank": 5,
          "score": 0.768815815448761,
          "doc_id": "DIKO0011019580",
          "title": "시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합",
          "abstract": "음성인식은 인간과 기계의 인터페이스 서비스를 위한 중요한 기술이다. 현재까지 개발된 많은 음성인식 시스템은 제어된 환경에서는 높은 인식율을 보이지만, 잡음 환경에서는 성능이 크게 저하되는 한계가 있다. 이러한 한계를 극복하기 위한 방법의 하나인 시청각 음성인식은 잡음이 존재하는 환경에서 강인한 인식을 위해 마이크로 녹음한 음성신호 (목소리)와 카메라로 기록한 영상신호(입술의 움직임)를 모두 이용하여 음성을 인식하는 기술이다. 영상신호를 이용한 인식은 잡음이 적은 상황에서는 기존의 음성인식에 비해 낮은 인식 성능을 보이지만 소리잡음에 영향을 받지 않기 때문에 잡음 환경에서 음성인식을 보완하는 유용한 방법이 된다. 본 논문에서는 시청각 음성인식 시스템을 구성하는 청각신호를 이용한 인식, 시각정보를 이용한 인식, 그리고 두 정보의 통합 등의 세 부분을 고려하여 각 부분의 성능을 향상시킴으로써 잡음환경에서의 강인함을 향상시키고자 한다. 첫째, 시각정보를 이용한 인식의 성능을 향상시키기 위해 인식기인 은닉 마르코프 모델(hidden Markov model)의 확률적인 최적화 알고리즘을 제안한다. 알고리즘의 성능과 수렴속도를 개선하기 위해 확률적인 최적화 알고리즘의 하나인 모의 담금질 기법(simulated annealing)과 지역 탐색 연산자를 결합한 하이브리드 모의 담금질 기법을 개발한다. 기존의 은닉 마르코프 모델의 학습 알고리즘인 기대-최대(expectation-maximization) 알고리즘은 가능도 (likelihood) 함수에 대해 지역 최적화만을 수행하는 반면, 제안하는 알고리즘은 전 영역에서 탐색을 수행하며 결과적으로 은닉 마르코프 모델의 인식 성능을 높인다. 제안하는 알고리즘이 전역최적해에 확률로써 수렴하는 것을 수학적으로 증명한다. 둘째, 청각정보를 이용한 인식을 강인하게 하기 위해 은닉 마르코프 모델에서 관측 프레임간의 상관관계를 모델링하는 기법을 개발한다. 음성의 동적 특성은 사람이 잡음환경에서 강인한 인식 성능을 보이는데 도움을 주는 것으로 알려져 있으나 기존의 은닉 마르코프 모델을 이용한 음성 모델링에서는 충분히 고려되지 않고 있다. 본 논문에서는 가우시안 혼합 모델(Gaussian mixture model)로 서로 다른 프레임의 결합 확률분포를 모델링하여 프레임간의 조건부 의존관계를 다루는 기법을 제안한다. 또한, 기대-최대 알고리즘에 기반하여 제안하는 모델의 학습 알고리즘을 개발한다. 제안하는 모델을 이용함으로써 청각 정보를 이용한 인식에서 잡음에 더욱 강인한 성능을 얻도록 한다. 셋째, 시각정보와 청각정보의 상호보완성을 효과적으로 이용하여 잡음에 강인한 최종 인식결과를 얻기 위해 정보 통합 단계에서 신경회로망을 이용하는 기법을 제안한다. 정보 통합 단계에서는 시각정보와 청각정보를 각각 따로 인식한 결과를 가중치 기법을 통해 최종 결과를 얻는데, 학습된 신경회로망은 잡음의 종류와 수준을 알 수 없는 주어진 시청각 데이터에 대해 적절한 가중치를 출력함으로써 최적의 인식결과를 얻도록 한다. 이를 통해 통합 인식 결과가 시각 또는 청각정보만을 이용한 인식결과보다 최소한 같거나 좋을 뿐 아니라 두 정보에 의한 시너지 효과를 최대화하도록 한다. 제안하는 시스템을 화자독립 고립단어 인식문제에 적용하여 그 성능을 보인다. 실험 결과 제안하는 시스템이 기존의 시스템에 비해 다양한 잡음 환경에서 더욱 강인한 성능을 보이는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011019580&target=NART&cn=DIKO0011019580",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 시청각 음성인식 : 은닉 마르코프 모델의 확률적 최적화, 프레임간 상관관계의 모델링 및 신경회로망을 이용한 통합 음성인식은 인간과 기계의 인터페이스 서비스를 위한 중요한 기술이다. 현재까지 개발된 많은 음성인식 시스템은 제어된 환경에서는 높은 인식율을 보이지만, 잡음 환경에서는 성능이 크게 저하되는 한계가 있다. 이러한 한계를 극복하기 위한 방법의 하나인 시청각 음성인식은 잡음이 존재하는 환경에서 강인한 인식을 위해 마이크로 녹음한 음성신호 (목소리)와 카메라로 기록한 영상신호(입술의 움직임)를 모두 이용하여 음성을 인식하는 기술이다. 영상신호를 이용한 인식은 잡음이 적은 상황에서는 기존의 음성인식에 비해 낮은 인식 성능을 보이지만 소리잡음에 영향을 받지 않기 때문에 잡음 환경에서 음성인식을 보완하는 유용한 방법이 된다. 본 논문에서는 시청각 음성인식 시스템을 구성하는 청각신호를 이용한 인식, 시각정보를 이용한 인식, 그리고 두 정보의 통합 등의 세 부분을 고려하여 각 부분의 성능을 향상시킴으로써 잡음환경에서의 강인함을 향상시키고자 한다. 첫째, 시각정보를 이용한 인식의 성능을 향상시키기 위해 인식기인 은닉 마르코프 모델(hidden Markov model)의 확률적인 최적화 알고리즘을 제안한다. 알고리즘의 성능과 수렴속도를 개선하기 위해 확률적인 최적화 알고리즘의 하나인 모의 담금질 기법(simulated annealing)과 지역 탐색 연산자를 결합한 하이브리드 모의 담금질 기법을 개발한다. 기존의 은닉 마르코프 모델의 학습 알고리즘인 기대-최대(expectation-maximization) 알고리즘은 가능도 (likelihood) 함수에 대해 지역 최적화만을 수행하는 반면, 제안하는 알고리즘은 전 영역에서 탐색을 수행하며 결과적으로 은닉 마르코프 모델의 인식 성능을 높인다. 제안하는 알고리즘이 전역최적해에 확률로써 수렴하는 것을 수학적으로 증명한다. 둘째, 청각정보를 이용한 인식을 강인하게 하기 위해 은닉 마르코프 모델에서 관측 프레임간의 상관관계를 모델링하는 기법을 개발한다. 음성의 동적 특성은 사람이 잡음환경에서 강인한 인식 성능을 보이는데 도움을 주는 것으로 알려져 있으나 기존의 은닉 마르코프 모델을 이용한 음성 모델링에서는 충분히 고려되지 않고 있다. 본 논문에서는 가우시안 혼합 모델(Gaussian mixture model)로 서로 다른 프레임의 결합 확률분포를 모델링하여 프레임간의 조건부 의존관계를 다루는 기법을 제안한다. 또한, 기대-최대 알고리즘에 기반하여 제안하는 모델의 학습 알고리즘을 개발한다. 제안하는 모델을 이용함으로써 청각 정보를 이용한 인식에서 잡음에 더욱 강인한 성능을 얻도록 한다. 셋째, 시각정보와 청각정보의 상호보완성을 효과적으로 이용하여 잡음에 강인한 최종 인식결과를 얻기 위해 정보 통합 단계에서 신경회로망을 이용하는 기법을 제안한다. 정보 통합 단계에서는 시각정보와 청각정보를 각각 따로 인식한 결과를 가중치 기법을 통해 최종 결과를 얻는데, 학습된 신경회로망은 잡음의 종류와 수준을 알 수 없는 주어진 시청각 데이터에 대해 적절한 가중치를 출력함으로써 최적의 인식결과를 얻도록 한다. 이를 통해 통합 인식 결과가 시각 또는 청각정보만을 이용한 인식결과보다 최소한 같거나 좋을 뿐 아니라 두 정보에 의한 시너지 효과를 최대화하도록 한다. 제안하는 시스템을 화자독립 고립단어 인식문제에 적용하여 그 성능을 보인다. 실험 결과 제안하는 시스템이 기존의 시스템에 비해 다양한 잡음 환경에서 더욱 강인한 성능을 보이는 것을 확인한다."
        },
        {
          "rank": 6,
          "score": 0.7667385339736938,
          "doc_id": "DIKO0011930560",
          "title": "시청각 음성인식을 위한 새로운 통합방법",
          "abstract": "The automatic speech recognition (ASR) is one of the most interesting problems applied to human computer interaction applications; for example, spoken digit recognition for mobile environments. One of the challenges of this problem is that the accuracy of speech recognition will be decrease much if the speaker talks under noisy place such as: restaurant, subway, street… The invention of lip-reading opens a potential chance to improve the performance of recognition. Indeed, human perception considers both auditory and visual nature of speech. The speech recognition system will be more intelligible if the lip motion of speaker is available together with acoustic signal. The combined audio visual speech recognition has been proved to be able enhance the overall performance of recognition, especially under noisy environment. In general, if the two streams are available for speech recognition, they can be integrated by two ways: early integration and late integration. The early integration approach combines the features of two streams into one concatenated feature vector, and uses single classifier for recognition. The late integration approach combines the results of two separate classifiers for recognition in which the reliability of modalities is applied to summation based fusion. The late integration method shows the better performance actually through many experiments. There are several factors are considered to measure reliability including word confusability, SNR level of acoustic signal, the noise type, and illumination change in visual stream rather than the only SNR level based confidence of conventional audio visual speech recognition. In this study, we propose an effective fusion scheme for audio visual speech recognition (AVSR) in which the appropriate combination weights are measured by using an integrated reliability. The significant idea of integrated reliability is the combination of not only acoustic noise but also model confusability for audio visual reliability measurement The experimental results using Samsung AVSR database shows the improved performance of our approach compared to conventional ones. This demonstrates the effectiveness and feasibility of this invention for real speech recognition applications.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011930560&target=NART&cn=DIKO0011930560",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시청각 음성인식을 위한 새로운 통합방법 시청각 음성인식을 위한 새로운 통합방법 시청각 음성인식을 위한 새로운 통합방법 The automatic speech recognition (ASR) is one of the most interesting problems applied to human computer interaction applications; for example, spoken digit recognition for mobile environments. One of the challenges of this problem is that the accuracy of speech recognition will be decrease much if the speaker talks under noisy place such as: restaurant, subway, street… The invention of lip-reading opens a potential chance to improve the performance of recognition. Indeed, human perception considers both auditory and visual nature of speech. The speech recognition system will be more intelligible if the lip motion of speaker is available together with acoustic signal. The combined audio visual speech recognition has been proved to be able enhance the overall performance of recognition, especially under noisy environment. In general, if the two streams are available for speech recognition, they can be integrated by two ways: early integration and late integration. The early integration approach combines the features of two streams into one concatenated feature vector, and uses single classifier for recognition. The late integration approach combines the results of two separate classifiers for recognition in which the reliability of modalities is applied to summation based fusion. The late integration method shows the better performance actually through many experiments. There are several factors are considered to measure reliability including word confusability, SNR level of acoustic signal, the noise type, and illumination change in visual stream rather than the only SNR level based confidence of conventional audio visual speech recognition. In this study, we propose an effective fusion scheme for audio visual speech recognition (AVSR) in which the appropriate combination weights are measured by using an integrated reliability. The significant idea of integrated reliability is the combination of not only acoustic noise but also model confusability for audio visual reliability measurement The experimental results using Samsung AVSR database shows the improved performance of our approach compared to conventional ones. This demonstrates the effectiveness and feasibility of this invention for real speech recognition applications."
        },
        {
          "rank": 7,
          "score": 0.7666785717010498,
          "doc_id": "NART37979687",
          "title": "Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network",
          "abstract": "<P>A new method for noisy speech recognition based on a hybrid model of hidden Markov models (HMM) and wavelet neural network (WNN) is presented. The HMM was employed to compute the Viterbi output score. Then the score was used as the input of WNN to acquire the classification information. The result of recognition was made by these two kinds of recognition information. Recognition experiment shows that this hybrid model has higher performance than hidden Markov model in noisy speech recognition.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART37979687&target=NART&cn=NART37979687",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network Noisy Speech Recognition Based on Hybrid Model of Hidden Markov Models and Wavelet Neural Network <P>A new method for noisy speech recognition based on a hybrid model of hidden Markov models (HMM) and wavelet neural network (WNN) is presented. The HMM was employed to compute the Viterbi output score. Then the score was used as the input of WNN to acquire the classification information. The result of recognition was made by these two kinds of recognition information. Recognition experiment shows that this hybrid model has higher performance than hidden Markov model in noisy speech recognition.</P>"
        },
        {
          "rank": 8,
          "score": 0.7652249336242676,
          "doc_id": "NART00705015",
          "title": "Speaker recognition using HMM composition in noisy environments",
          "abstract": "<P><B>Abstract</B></P><P>This paper investigates a speaker recognition method that is robust against background noise. In noisy environments, one important issue is how to create a model for each speaker so as to compensate for noise. The method described here is based on hidden Markov model (HMM) composition, which combines a speaker HMM and a noise-source HMM into a noise-added speaker HMM with a particular signal-to-noise ratio (SNR). Since it is difficult to measure the SNR of input speech with non-stationary noise exactly, this method creates several noise-added speaker HMMs with various SNRs. The HMM that has the highest likelihood value for the input speech is selected, and a speaker decision is made using this likelihood value. Experimental application of this method to text-independent speaker identification and verification in various kinds of noisy environments demonstrated considerable improvement in speaker recognition for speech utterances of male speakers.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART00705015&target=NART&cn=NART00705015",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speaker recognition using HMM composition in noisy environments Speaker recognition using HMM composition in noisy environments Speaker recognition using HMM composition in noisy environments <P><B>Abstract</B></P><P>This paper investigates a speaker recognition method that is robust against background noise. In noisy environments, one important issue is how to create a model for each speaker so as to compensate for noise. The method described here is based on hidden Markov model (HMM) composition, which combines a speaker HMM and a noise-source HMM into a noise-added speaker HMM with a particular signal-to-noise ratio (SNR). Since it is difficult to measure the SNR of input speech with non-stationary noise exactly, this method creates several noise-added speaker HMMs with various SNRs. The HMM that has the highest likelihood value for the input speech is selected, and a speaker decision is made using this likelihood value. Experimental application of this method to text-independent speaker identification and verification in various kinds of noisy environments demonstrated considerable improvement in speaker recognition for speech utterances of male speakers.</P>"
        },
        {
          "rank": 9,
          "score": 0.7638264298439026,
          "doc_id": "NART18014750",
          "title": "Neural nets and hidden Markov models: Review and generalizations",
          "abstract": "Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs).",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART18014750&target=NART&cn=NART18014750",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural nets and hidden Markov models: Review and generalizations Neural nets and hidden Markov models: Review and generalizations Neural nets and hidden Markov models: Review and generalizations Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs). The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence. While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms. For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models. After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models. Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs)."
        },
        {
          "rank": 10,
          "score": 0.7625345587730408,
          "doc_id": "JAKO201403359905324",
          "title": "가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원",
          "abstract": "This paper describes a robust speech recognition technique by reconstructing spectral components mismatched with a training environment. Although the cluster-based reconstruction method can compensate the unreliable components from reliable components in the same spectral vector by assuming an independent, identically distributed Gaussian-mixture process of training spectral vectors, the presented method exploits the temporal dependency of speech to reconstruct the components by introducing a hidden-Markov-model prior which incorporates an internal state transition plausible for an observed spectral vector sequence. The experimental results indicate that the described method can provide temporally consistent reconstruction and further improve recognition performance on average compared to the conventional method.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201403359905324&target=NART&cn=JAKO201403359905324",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 가산잡음환경에서 강인음성인식을 위한 은닉 마르코프 모델 기반 손실 특징 복원 This paper describes a robust speech recognition technique by reconstructing spectral components mismatched with a training environment. Although the cluster-based reconstruction method can compensate the unreliable components from reliable components in the same spectral vector by assuming an independent, identically distributed Gaussian-mixture process of training spectral vectors, the presented method exploits the temporal dependency of speech to reconstruct the components by introducing a hidden-Markov-model prior which incorporates an internal state transition plausible for an observed spectral vector sequence. The experimental results indicate that the described method can provide temporally consistent reconstruction and further improve recognition performance on average compared to the conventional method."
        },
        {
          "rank": 11,
          "score": 0.7604843378067017,
          "doc_id": "NART20682074",
          "title": "Robust combination of neural networks and hidden Markov models for speech recognition",
          "abstract": "Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the 'divergence problem'-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20682074&target=NART&cn=NART20682074",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Robust combination of neural networks and hidden Markov models for speech recognition Robust combination of neural networks and hidden Markov models for speech recognition Robust combination of neural networks and hidden Markov models for speech recognition Acoustic modeling in state-of-the-art speech recognition systems usually relies on hidden Markov models (HMMs) with Gaussian emission densities. HMMs suffer from intrinsic limitations, mainly due to their arbitrary parametric assumption. Artificial neural networks (ANNs) appear to be a promising alternative in this respect, but they historically failed as a general solution to the acoustic modeling problem. This paper introduces algorithms based on a gradient-ascent technique for global training of a hybrid ANN/HMM system, in which the ANN is trained for estimating the emission probabilities of the states of the HMM. The approach is related to the major hybrid systems proposed by Bourlard and Morgan and by Bengio, with the aim of combining their benefits within a unified framework and to overcome their limitations. Several viable solutions to the 'divergence problem'-that may arise when training is accomplished over the maximum-likelihood (ML) criterion-are proposed. Experimental results in speaker-independent, continuous speech recognition over Italian digit-strings validate the novel hybrid framework, allowing for improved recognition performance over HMMs with mixtures of Gaussian components, as well as over Bourlard and Morgan's paradigm. In particular, it is shown that the maximum a posteriori (MAP) version of the algorithm yields a 46.34% relative word error rate reduction with respect to standard HMMs."
        },
        {
          "rank": 12,
          "score": 0.7599502801895142,
          "doc_id": "JAKO201415642601987",
          "title": "SNR 매핑을 이용한 환경적응 기반 음성인식",
          "abstract": "다 모델 기반의 음성인식기는 음성인식에서 매우 성공적임이 알려져 있다. 그것은 다양한 신호-대-잡음비(SNR)와 잡음종류에 해당하는 다수의 HMM을 사용함으로서 선택된 음향모델이 인식잡음음성에 매우 근접한 일치성을 가질 수 있기 때문이다. 그러나 실제 사용시에 HMM의 개수가 제한됨에 따라서 음향모델의 불일치는 여전히 문제로 남아 있다. 본 논문에서는 인식잡음음성과 HMM 간의 SNR 불일치를 줄이고자 이들 간의 최적의 SNR 매핑 (mapping)을 실험적으로 결정하였다. 인식잡음음성으로 부터 추정된 SNR 값을 사용하는 대신 제안된 SNR 매핑을 사용함으로서 향상된 인식결과를 얻을 수 있었다. 다 모델 기반인식기에 제안된 방법을 적용하여 Aurora 2 데이터베이스에 대해서 인식 실험한 결과 기존의 MTR 이나 다 모델 기반 음성인식기에 비해서 6.3%와 9.4%의 상대적 단어 오인식율 감소를 이룰 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201415642601987&target=NART&cn=JAKO201415642601987",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SNR 매핑을 이용한 환경적응 기반 음성인식 SNR 매핑을 이용한 환경적응 기반 음성인식 SNR 매핑을 이용한 환경적응 기반 음성인식 다 모델 기반의 음성인식기는 음성인식에서 매우 성공적임이 알려져 있다. 그것은 다양한 신호-대-잡음비(SNR)와 잡음종류에 해당하는 다수의 HMM을 사용함으로서 선택된 음향모델이 인식잡음음성에 매우 근접한 일치성을 가질 수 있기 때문이다. 그러나 실제 사용시에 HMM의 개수가 제한됨에 따라서 음향모델의 불일치는 여전히 문제로 남아 있다. 본 논문에서는 인식잡음음성과 HMM 간의 SNR 불일치를 줄이고자 이들 간의 최적의 SNR 매핑 (mapping)을 실험적으로 결정하였다. 인식잡음음성으로 부터 추정된 SNR 값을 사용하는 대신 제안된 SNR 매핑을 사용함으로서 향상된 인식결과를 얻을 수 있었다. 다 모델 기반인식기에 제안된 방법을 적용하여 Aurora 2 데이터베이스에 대해서 인식 실험한 결과 기존의 MTR 이나 다 모델 기반 음성인식기에 비해서 6.3%와 9.4%의 상대적 단어 오인식율 감소를 이룰 수 있었다."
        },
        {
          "rank": 13,
          "score": 0.7592165470123291,
          "doc_id": "NART48832461",
          "title": "Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments",
          "abstract": "In this paper, we propose a model for the incorporation of voicing information into a speech recognition system in noisy environments. The employed voicing information is estimated by a novel method that can provide this information for each filter-bank channel and does not require information about the fundamental frequency. The voicing information is modelled by employing the Bernoulli distribution. The voicing model is obtained for each HMM state and mixture by a Viterbi-style training procedure. The proposed voicing incorporation is evaluated both within a standard model and two other models that had compensated for the noise effect, the missing-feature and the multi-conditional training model. Experiments are first performed on noisy speech data from the Aurora 2 database. Significant performance improvements are achieved when the voicing information is incorporated within the standard model as well as the noise-compensated models. The employment of voicing information is also demonstrated on a phoneme recognition task on the noise-corrupted TIMIT database and considerable improvements are observed.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART48832461&target=NART&cn=NART48832461",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments Incorporating the voicing information into HMM-based automatic speech recognition in noisy environments In this paper, we propose a model for the incorporation of voicing information into a speech recognition system in noisy environments. The employed voicing information is estimated by a novel method that can provide this information for each filter-bank channel and does not require information about the fundamental frequency. The voicing information is modelled by employing the Bernoulli distribution. The voicing model is obtained for each HMM state and mixture by a Viterbi-style training procedure. The proposed voicing incorporation is evaluated both within a standard model and two other models that had compensated for the noise effect, the missing-feature and the multi-conditional training model. Experiments are first performed on noisy speech data from the Aurora 2 database. Significant performance improvements are achieved when the voicing information is incorporated within the standard model as well as the noise-compensated models. The employment of voicing information is also demonstrated on a phoneme recognition task on the noise-corrupted TIMIT database and considerable improvements are observed."
        },
        {
          "rank": 14,
          "score": 0.755990743637085,
          "doc_id": "NART30128358",
          "title": "Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer",
          "abstract": "<P><B>Abstract</B></P><P>Spoken human&ndash;machine interaction in real-world environments requires acoustic models that are robust to changes in acoustic conditions, e.g. presence of noise. Unfortunately, the popular hidden Markov models (HMM) are not noise tolerant. One way to increase recognition performance is to acquire a small adaptation set of noisy utterances, which is used to estimate a normalization mapping between noisy and clean features to be fed into the acoustic model. This paper proposes an unsupervised maximum-likelihood gradient-ascent training algorithm (instead of the usual least squares regression) for a neural feature adaptation module, properly combined with a hybrid connectionist/HMM speech recognizer. The algorithm is inspired by the so-called &ldquo;inversion principle&rdquo;, that prescribes the optimization of the input features instead of the model parameters. Simulation results on a real-world speaker-independent continuous speech corpus of connected Italian digits, corrupted by noise, validate the approach. A small neural net (13 hidden neurons) trained over a single adaptation utterance for one iteration yields a 18.79% relative word error rate (WER) reduction over the bare hybrid, and a 65.10% relative WER reduction over the Gaussian-based HMM.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART30128358&target=NART&cn=NART30128358",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer Inversion-based nonlinear adaptation of noisy acoustic parameters for a neural/HMM speech recognizer <P><B>Abstract</B></P><P>Spoken human&ndash;machine interaction in real-world environments requires acoustic models that are robust to changes in acoustic conditions, e.g. presence of noise. Unfortunately, the popular hidden Markov models (HMM) are not noise tolerant. One way to increase recognition performance is to acquire a small adaptation set of noisy utterances, which is used to estimate a normalization mapping between noisy and clean features to be fed into the acoustic model. This paper proposes an unsupervised maximum-likelihood gradient-ascent training algorithm (instead of the usual least squares regression) for a neural feature adaptation module, properly combined with a hybrid connectionist/HMM speech recognizer. The algorithm is inspired by the so-called &ldquo;inversion principle&rdquo;, that prescribes the optimization of the input features instead of the model parameters. Simulation results on a real-world speaker-independent continuous speech corpus of connected Italian digits, corrupted by noise, validate the approach. A small neural net (13 hidden neurons) trained over a single adaptation utterance for one iteration yields a 18.79% relative word error rate (WER) reduction over the bare hybrid, and a 65.10% relative WER reduction over the Gaussian-based HMM.</P>"
        },
        {
          "rank": 15,
          "score": 0.7554944753646851,
          "doc_id": "NPAP00072266",
          "title": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models",
          "abstract": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP00072266&target=NART&cn=NPAP00072266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error."
        },
        {
          "rank": 16,
          "score": 0.7540889382362366,
          "doc_id": "NART70632792",
          "title": "Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis",
          "abstract": "<P>This paper investigates joint speaker-dependent audiovisual Hidden Semi-Markov Models (HSMM) where the visual models produce a sequence of 3D motion tracking data that is used to animate a talking head and the acoustic models are used for speech synthesis. Different acoustic, visual, and joint audiovisual models for four different Austrian German speakers were trained and we show that the joint models perform better compared to other approaches in terms of synchronization quality of the synthesized visual speech. In addition, a detailed analysis of the acoustic and visual alignment is provided for the different models. Importantly, the joint audiovisual modeling does not decrease the acoustic synthetic speech quality compared to acoustic-only modeling so that there is a clear advantage in the common duration model of the joint audiovisual modeling approach that is used for synchronizing acoustic and visual parameter sequences. Finally, it provides a model that integrates the visual and acoustic speech dynamics.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART70632792&target=NART&cn=NART70632792",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis Joint Audiovisual Hidden Semi-Markov Model-Based Speech Synthesis <P>This paper investigates joint speaker-dependent audiovisual Hidden Semi-Markov Models (HSMM) where the visual models produce a sequence of 3D motion tracking data that is used to animate a talking head and the acoustic models are used for speech synthesis. Different acoustic, visual, and joint audiovisual models for four different Austrian German speakers were trained and we show that the joint models perform better compared to other approaches in terms of synchronization quality of the synthesized visual speech. In addition, a detailed analysis of the acoustic and visual alignment is provided for the different models. Importantly, the joint audiovisual modeling does not decrease the acoustic synthetic speech quality compared to acoustic-only modeling so that there is a clear advantage in the common duration model of the joint audiovisual modeling approach that is used for synchronizing acoustic and visual parameter sequences. Finally, it provides a model that integrates the visual and acoustic speech dynamics.</P>"
        },
        {
          "rank": 17,
          "score": 0.753727912902832,
          "doc_id": "NART80772700",
          "title": "Noisy training for deep neural networks in speech recognition",
          "abstract": "<P><B>Abstract</B><P>Deep neural networks (DNNs) have gained remarkable success in speech recognition, partially attributed to the flexibility of DNN models in learning complex patterns of speech signals. This flexibility, however, may lead to serious over-fitting and hence miserable performance degradation in adverse acoustic conditions such as those with high ambient noises. We propose a noisy training approach to tackle this problem: by injecting moderate noises into the training data intentionally and randomly, more generalizable DNN models can be learned. This &lsquo;noise injection&rsquo; technique, although known to the neural computation community already, has not been studied with DNNs which involve a highly complex objective function. The experiments presented in this paper confirm that the noisy training approach works well for the DNN model and can provide substantial performance improvement for DNN-based speech recognition.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART80772700&target=NART&cn=NART80772700",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Noisy training for deep neural networks in speech recognition Noisy training for deep neural networks in speech recognition Noisy training for deep neural networks in speech recognition <P><B>Abstract</B><P>Deep neural networks (DNNs) have gained remarkable success in speech recognition, partially attributed to the flexibility of DNN models in learning complex patterns of speech signals. This flexibility, however, may lead to serious over-fitting and hence miserable performance degradation in adverse acoustic conditions such as those with high ambient noises. We propose a noisy training approach to tackle this problem: by injecting moderate noises into the training data intentionally and randomly, more generalizable DNN models can be learned. This &lsquo;noise injection&rsquo; technique, although known to the neural computation community already, has not been studied with DNNs which involve a highly complex objective function. The experiments presented in this paper confirm that the noisy training approach works well for the DNN model and can provide substantial performance improvement for DNN-based speech recognition.</P></P>"
        },
        {
          "rank": 18,
          "score": 0.7485768795013428,
          "doc_id": "NART13081886",
          "title": "Speech recognition under noisy environments using segmental unit input HMM",
          "abstract": "<P>In this paper, we apply a segmental unit input HMM to noisy speech recognition. In this modeling, several successive frames are combined and treated as an input vector. We expect that the segmental unit input HMM will be effective for noisy speech recognition because segmental statistics considering correlation between frames reduce noise effects when the correlation of noise between frames is assumed to be small. In recognition experiments, we compared the segmental unit input HMM with a conventional frame-based HMM and found the segmental unit input HMM to be superior. We also compared the segmental unit input HMM with dynamic cepstral coefficients, which have both static and dynamic features, and found that the segmental unit input HMM is more effective than the dynamic cepstrum. We also combined the segmental unit input HMM with a spectral subtraction method and confirmed the effectiveness of the method. Additionally, in experiments using acoustic models trained with noisy speech, the segmental unit input HMM outperformed the conventional HMM. From these results, we propose PMC for the segmental unit input HMM. Experimental results showed the PMC for segmental unit input HMM offered better recognition performance than the original PMC. &copy; 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(8): 111&ndash;120, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1151</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART13081886&target=NART&cn=NART13081886",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition under noisy environments using segmental unit input HMM Speech recognition under noisy environments using segmental unit input HMM Speech recognition under noisy environments using segmental unit input HMM <P>In this paper, we apply a segmental unit input HMM to noisy speech recognition. In this modeling, several successive frames are combined and treated as an input vector. We expect that the segmental unit input HMM will be effective for noisy speech recognition because segmental statistics considering correlation between frames reduce noise effects when the correlation of noise between frames is assumed to be small. In recognition experiments, we compared the segmental unit input HMM with a conventional frame-based HMM and found the segmental unit input HMM to be superior. We also compared the segmental unit input HMM with dynamic cepstral coefficients, which have both static and dynamic features, and found that the segmental unit input HMM is more effective than the dynamic cepstrum. We also combined the segmental unit input HMM with a spectral subtraction method and confirmed the effectiveness of the method. Additionally, in experiments using acoustic models trained with noisy speech, the segmental unit input HMM outperformed the conventional HMM. From these results, we propose PMC for the segmental unit input HMM. Experimental results showed the PMC for segmental unit input HMM offered better recognition performance than the original PMC. &copy; 2002 Wiley Periodicals, Inc. Syst Comp Jpn, 33(8): 111&ndash;120, 2002; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/scj.1151</P>"
        },
        {
          "rank": 19,
          "score": 0.7457976937294006,
          "doc_id": "NART95825020",
          "title": "End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition",
          "abstract": "<P><B>Abstract</B></P>  <P>In hidden Markov model (HMM) based automatic speech recognition (ASR) system, modeling the statistical relationship between the acoustic speech signal and the HMM states that represent linguistically motivated subword units such as phonemes is a crucial step. This is typically achieved by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then training a classifier such as artificial neural networks (ANN), Gaussian mixture model that estimates the emission probabilities of the HMM states. This paper investigates an end-to-end acoustic modeling approach using convolutional neural networks (CNNs), where the CNN takes as input raw speech signal and estimates the HMM states class conditional probabilities at the output. Alternately, as opposed to a divide and conquer strategy (i.e., separating feature extraction and statistical modeling steps), in the proposed acoustic modeling approach the relevant features and the classifier are jointly learned from the raw speech signal. Through ASR studies and analyses on multiple languages and multiple tasks, we show that: (a) the proposed approach yields consistently a better system with fewer parameters when compared to the conventional approach of cepstral feature extraction followed by ANN training, (b) unlike conventional method of speech processing, in the proposed approach the relevant feature representations are learned by first processing the input raw speech at the sub-segmental level ( &asymp; 2 ms). Specifically, through an analysis we show that the filters in the first convolution layer automatically learn &ldquo;in-parts&rdquo; formant-like information present in the sub-segmental speech, and (c) the intermediate feature representations obtained by subsequent filtering of the first convolution layer output are more discriminative compared to standard cepstral features and could be transferred across languages and domains.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Novel CNN-based end-to-end acoustic modeling approach is proposed. </LI> <LI>  Relevant features are automatically learned from the signal by discriminating phones. </LI> <LI>  Learned features are more discriminative than cepstral-based features. </LI> <LI>  Learned features are somewhat invariant to languages and domains. </LI> <LI>  Proposed approach leads to better ASR systems. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART95825020&target=NART&cn=NART95825020",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition End-to-end acoustic modeling using convolutional neural networks for HMM-based automatic speech recognition <P><B>Abstract</B></P>  <P>In hidden Markov model (HMM) based automatic speech recognition (ASR) system, modeling the statistical relationship between the acoustic speech signal and the HMM states that represent linguistically motivated subword units such as phonemes is a crucial step. This is typically achieved by first extracting acoustic features from the speech signal based on prior knowledge such as, speech perception or/and speech production knowledge, and, then training a classifier such as artificial neural networks (ANN), Gaussian mixture model that estimates the emission probabilities of the HMM states. This paper investigates an end-to-end acoustic modeling approach using convolutional neural networks (CNNs), where the CNN takes as input raw speech signal and estimates the HMM states class conditional probabilities at the output. Alternately, as opposed to a divide and conquer strategy (i.e., separating feature extraction and statistical modeling steps), in the proposed acoustic modeling approach the relevant features and the classifier are jointly learned from the raw speech signal. Through ASR studies and analyses on multiple languages and multiple tasks, we show that: (a) the proposed approach yields consistently a better system with fewer parameters when compared to the conventional approach of cepstral feature extraction followed by ANN training, (b) unlike conventional method of speech processing, in the proposed approach the relevant feature representations are learned by first processing the input raw speech at the sub-segmental level ( &asymp; 2 ms). Specifically, through an analysis we show that the filters in the first convolution layer automatically learn &ldquo;in-parts&rdquo; formant-like information present in the sub-segmental speech, and (c) the intermediate feature representations obtained by subsequent filtering of the first convolution layer output are more discriminative compared to standard cepstral features and could be transferred across languages and domains.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Novel CNN-based end-to-end acoustic modeling approach is proposed. </LI> <LI>  Relevant features are automatically learned from the signal by discriminating phones. </LI> <LI>  Learned features are more discriminative than cepstral-based features. </LI> <LI>  Learned features are somewhat invariant to languages and domains. </LI> <LI>  Proposed approach leads to better ASR systems. </LI> </UL> </P>"
        },
        {
          "rank": 20,
          "score": 0.7425810694694519,
          "doc_id": "NPAP07942137",
          "title": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP07942137&target=NART&cn=NPAP07942137",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 21,
          "score": 0.7415726780891418,
          "doc_id": "JAKO202029462558904",
          "title": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식",
          "abstract": "특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202029462558904&target=NART&cn=JAKO202029462558904",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다."
        },
        {
          "rank": 22,
          "score": 0.7413370609283447,
          "doc_id": "JAKO200428635215914",
          "title": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구",
          "abstract": "본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200428635215914&target=NART&cn=JAKO200428635215914",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다."
        },
        {
          "rank": 23,
          "score": 0.740944504737854,
          "doc_id": "NART13642943",
          "title": "Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments",
          "abstract": "<P>Handling background noise or echo (reverberation) etc. is very important for having an automated robot etc. recognize remote speech in a real environment. As effective schemes for handling this problem, noise reducing schemes such as model adaptation schemes including HMM decomposition and composition or microphone array (beamformer) signal processing, spectral subtraction, etc. have been proposed. In particular, a model adaptation scheme is very effective for speech recognition in a noisy environment and its recognition performance increases in proportion to the signal-to-noise ratio (SNR). In this paper, improving the recognition performance in a low-SNR environment by receiving speech at a high SNR using a microphone array before HMM decomposition and composition is attempted. The results of speech recognition experiments conducted in a noisy environment in an acoustic laboratory show an improvement in the recognition rate of about 25% by the proposed method for the case in which the SNR in a single microphone is 0 dB, as compared with the cases of using microphone array signal processing, HMM decomposition and composition alone. In addition, the proposed method shows recognition performance comparable to the case of using cepstrum mean normalization and spectral subtraction performed with an optimal coefficient given to the speech after microphone array processing. &copy; 2002 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 85(9): 13&ndash;22, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ecjb.10068</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART13642943&target=NART&cn=NART13642943",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments Speech recognition based on HMM decomposition and composition method with a microphone array in noisy reverberant environments <P>Handling background noise or echo (reverberation) etc. is very important for having an automated robot etc. recognize remote speech in a real environment. As effective schemes for handling this problem, noise reducing schemes such as model adaptation schemes including HMM decomposition and composition or microphone array (beamformer) signal processing, spectral subtraction, etc. have been proposed. In particular, a model adaptation scheme is very effective for speech recognition in a noisy environment and its recognition performance increases in proportion to the signal-to-noise ratio (SNR). In this paper, improving the recognition performance in a low-SNR environment by receiving speech at a high SNR using a microphone array before HMM decomposition and composition is attempted. The results of speech recognition experiments conducted in a noisy environment in an acoustic laboratory show an improvement in the recognition rate of about 25% by the proposed method for the case in which the SNR in a single microphone is 0 dB, as compared with the cases of using microphone array signal processing, HMM decomposition and composition alone. In addition, the proposed method shows recognition performance comparable to the case of using cepstrum mean normalization and spectral subtraction performed with an optimal coefficient given to the speech after microphone array processing. &copy; 2002 Wiley Periodicals, Inc. Electron Comm Jpn Pt 2, 85(9): 13&ndash;22, 2002; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/ecjb.10068</P>"
        },
        {
          "rank": 24,
          "score": 0.7407451868057251,
          "doc_id": "JAKO200819858103682",
          "title": "A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments",
          "abstract": "In this paper, a MSS-NOVO method that combines the HMM composition method with a noise reduction method is proposed for speech recognition in noisy environments. This combined method starts with noise reduction with modified spectral subtraction (MSS) to enhance the input noisy speech, then the noise and voice composition (NOVO) method is applied for making noise adapted models by using the noise in the non-utterance regions of the enhanced noisy speech. In order to evaluate the effectiveness of our proposed method, we compare MSS-NOVO method with other methods, i.e., SS-NOVO, MWF-NOVO. To set up the noisy speech for test, we add White noise to KLE 452 database with different SNRs range from 0dB to 15dB, at 5dB intervals. From the tests, MSS-NOVO method shows average improvement of 66.5% and 13.6% compared with the existing SS-NOVO method and MWF-NOVO method, respectively. Especially our proposed MSS-NOVO method shows a big improvement at low SNRs.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200819858103682&target=NART&cn=JAKO200819858103682",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments A Noise Reduction Method Combined with HMM Composition for Speech Recognition in Noisy Environments In this paper, a MSS-NOVO method that combines the HMM composition method with a noise reduction method is proposed for speech recognition in noisy environments. This combined method starts with noise reduction with modified spectral subtraction (MSS) to enhance the input noisy speech, then the noise and voice composition (NOVO) method is applied for making noise adapted models by using the noise in the non-utterance regions of the enhanced noisy speech. In order to evaluate the effectiveness of our proposed method, we compare MSS-NOVO method with other methods, i.e., SS-NOVO, MWF-NOVO. To set up the noisy speech for test, we add White noise to KLE 452 database with different SNRs range from 0dB to 15dB, at 5dB intervals. From the tests, MSS-NOVO method shows average improvement of 66.5% and 13.6% compared with the existing SS-NOVO method and MWF-NOVO method, respectively. Especially our proposed MSS-NOVO method shows a big improvement at low SNRs."
        },
        {
          "rank": 25,
          "score": 0.7372997999191284,
          "doc_id": "NART20482871",
          "title": "Piecewise-linear transformation-based HMM adaptation for noisy speech",
          "abstract": "<P><B>Abstract</B></P><P>This paper proposes a new method using piecewise-linear transformation for adapting phone HMMs to noisy speech. Various noises are clustered according to their spectral property, and a noisy speech HMM corresponding to each clustered noise and SNR condition is made. Based on the likelihood maximization criterion, an HMM that best matches an input noisy speech is selected and further adapted using linear transformation. The proposed method is evaluated by its ability to recognize noisy broadcast-news speech. It is confirmed that the proposed method is effective in recognizing numerically noise-added speech and actual noisy speech under various noise conditions. The proposed method minimizes mismatches between noisy input speech and the HMM&#x2019;s, sentence by sentence, without requiring online noise spectrum/model estimation. The proposed method is therefore easily applicable to real world conditions with frequently changing noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20482871&target=NART&cn=NART20482871",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Piecewise-linear transformation-based HMM adaptation for noisy speech Piecewise-linear transformation-based HMM adaptation for noisy speech Piecewise-linear transformation-based HMM adaptation for noisy speech <P><B>Abstract</B></P><P>This paper proposes a new method using piecewise-linear transformation for adapting phone HMMs to noisy speech. Various noises are clustered according to their spectral property, and a noisy speech HMM corresponding to each clustered noise and SNR condition is made. Based on the likelihood maximization criterion, an HMM that best matches an input noisy speech is selected and further adapted using linear transformation. The proposed method is evaluated by its ability to recognize noisy broadcast-news speech. It is confirmed that the proposed method is effective in recognizing numerically noise-added speech and actual noisy speech under various noise conditions. The proposed method minimizes mismatches between noisy input speech and the HMM&#x2019;s, sentence by sentence, without requiring online noise spectrum/model estimation. The proposed method is therefore easily applicable to real world conditions with frequently changing noise.</P>"
        },
        {
          "rank": 26,
          "score": 0.7352911233901978,
          "doc_id": "NART06155061",
          "title": "Speech enhancement based on neural predictive hidden Markov model",
          "abstract": "<P><B>Abstract</B></P><P>In this paper, we describe a new approach to speech enhancement by modeling directly the statistical characteristics of the speech waveform. To represent the nonlinear and nonstationary nature of speech, it is assumed that speech is the output of a neural predictive hidden Markov model (NPHMM). The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a Markov chain. Given some speech data, the parameter of NPHMM is estimated by a learning algorithm based on the combination of Baum&#x2013;Welch algorithm and a neural network learning algorithm using the well known back propagation technique. Given the parameters of NPHMM, a recursive estimation method using multiple Kalman filters, governed by a Markov state chain according to the transition probabilities is developed for enhancing speech signals degraded by statistically independent additive noise characteristics assumed to be white and Gaussian. Under various input signal-to-noise ratios (SNRs), the proposed recursive speech enhancement method achieves an improvement over the method based on hidden filter model (Lee and Shirai, 1996) of about 0.8&#x2013;1.2dB in terms of the measured output SNR.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART06155061&target=NART&cn=NART06155061",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech enhancement based on neural predictive hidden Markov model Speech enhancement based on neural predictive hidden Markov model Speech enhancement based on neural predictive hidden Markov model <P><B>Abstract</B></P><P>In this paper, we describe a new approach to speech enhancement by modeling directly the statistical characteristics of the speech waveform. To represent the nonlinear and nonstationary nature of speech, it is assumed that speech is the output of a neural predictive hidden Markov model (NPHMM). The NPHMM is a nonlinear autoregressive process whose time-varying parameters are controlled by a Markov chain. Given some speech data, the parameter of NPHMM is estimated by a learning algorithm based on the combination of Baum&#x2013;Welch algorithm and a neural network learning algorithm using the well known back propagation technique. Given the parameters of NPHMM, a recursive estimation method using multiple Kalman filters, governed by a Markov state chain according to the transition probabilities is developed for enhancing speech signals degraded by statistically independent additive noise characteristics assumed to be white and Gaussian. Under various input signal-to-noise ratios (SNRs), the proposed recursive speech enhancement method achieves an improvement over the method based on hidden filter model (Lee and Shirai, 1996) of about 0.8&#x2013;1.2dB in terms of the measured output SNR.</P>"
        },
        {
          "rank": 27,
          "score": 0.7338853478431702,
          "doc_id": "JAKO200311922043899",
          "title": "Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구",
          "abstract": "본 논문에서는 한국어 음성 데이터를 대상으로 HM-Net(Hidden Markov Network) 음성인식 시스템의 성능평가를 수행하였다. 음향모델 작성은 음성인식에서 널리 사용되고 있는 통계적인 모델링 방법인 HMM(Hidden Markov Model)을 개량한 HM-Net을 도입하였다. HM-Net은 기존의 SSS(Successive State Splitting) 알고리즘을 개량한 PDT(Phonetic Decision Tree)-SSS 알고리즘에 의해 문맥방향과 시간방향의 상태분할을 수행하여 생성되는데, 특히 문맥방향 상태분할의 경우 학습 음성데이터에 출현하지 않는 문맥정보를 효과적으로 표현하기 위해 음소결정트리를 채용하고 있으며, 시간방향 상태분할의 경우 학습 음성데이터에서 각 음소별 지속시간 정보를 효과적으로 표현하기 위한 상태분할을 수행하며, 마지막으로 파라미터의 공유를 통해 triphone 형태의 최적인 모델 네트워크를 작성하게 된다. 인식에 사용된 알고리즘은 음소 및 단어인식의 경우에는 One-Pass Viterbi 빔 탐색을 사용하며 트리 구조 형태의 사전과 phone/word-pair 문법을 채용하고 있다. 연속음성인식의 경우에는 단어 bigram과 단어 trigram 언어모델과 목구조 형태의 사전을 채용한 Multi-Pass 빔 탐색을 사용하고 있다. 전체적으로 본 논문에서는 다양한 조건에서 HM-Net 음성인식 시스템의 성능평가를 수행하였으며, 지금까지 소개된 음성인식 시스템과 비교하여 매우 우수한 인식성능을 보임을 실험을 통해 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200311922043899&target=NART&cn=JAKO200311922043899",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 Hidden Markov Network 음성인식 시스템의 성능평가에 관한 연구 본 논문에서는 한국어 음성 데이터를 대상으로 HM-Net(Hidden Markov Network) 음성인식 시스템의 성능평가를 수행하였다. 음향모델 작성은 음성인식에서 널리 사용되고 있는 통계적인 모델링 방법인 HMM(Hidden Markov Model)을 개량한 HM-Net을 도입하였다. HM-Net은 기존의 SSS(Successive State Splitting) 알고리즘을 개량한 PDT(Phonetic Decision Tree)-SSS 알고리즘에 의해 문맥방향과 시간방향의 상태분할을 수행하여 생성되는데, 특히 문맥방향 상태분할의 경우 학습 음성데이터에 출현하지 않는 문맥정보를 효과적으로 표현하기 위해 음소결정트리를 채용하고 있으며, 시간방향 상태분할의 경우 학습 음성데이터에서 각 음소별 지속시간 정보를 효과적으로 표현하기 위한 상태분할을 수행하며, 마지막으로 파라미터의 공유를 통해 triphone 형태의 최적인 모델 네트워크를 작성하게 된다. 인식에 사용된 알고리즘은 음소 및 단어인식의 경우에는 One-Pass Viterbi 빔 탐색을 사용하며 트리 구조 형태의 사전과 phone/word-pair 문법을 채용하고 있다. 연속음성인식의 경우에는 단어 bigram과 단어 trigram 언어모델과 목구조 형태의 사전을 채용한 Multi-Pass 빔 탐색을 사용하고 있다. 전체적으로 본 논문에서는 다양한 조건에서 HM-Net 음성인식 시스템의 성능평가를 수행하였으며, 지금까지 소개된 음성인식 시스템과 비교하여 매우 우수한 인식성능을 보임을 실험을 통해 확인할 수 있었다."
        },
        {
          "rank": 28,
          "score": 0.7319321632385254,
          "doc_id": "NART17510385",
          "title": "Hidden-articulator Markov models for speech recognition",
          "abstract": "<P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART17510385&target=NART&cn=NART17510385",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition <P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>"
        },
        {
          "rank": 29,
          "score": 0.7283927202224731,
          "doc_id": "JAKO201115537947340",
          "title": "멀티밴드 스펙트럼 차감법과 엔트로피 하모닉을 이용한 잡음환경에 강인한 분산음성인식",
          "abstract": "음성인식의 실용화에 가장 저해되는 요소는 배경잡음과 채널에 의한 왜곡이다. 일반적으로 잡음은 음성인식 시스템의 성능을 저하시키고 이로 인해 사용 장소의 제약을 많이 받고 있다. DSR(Distributed Speech Recognition) 기반의 음성인식 역시 이 같은 문제로 성능 향상에 어려움을 겪고 있다. 이 논문은 잡음환경에서 DSR기반의 음성인식률 향상을 위해 정확한 음성구간을 검출하고, 잡음을 제거하여 잡음에 강인한 특징추출을 하도록 설계하였다. 제안된 방법은 엔트로피와 음성의 하모닉을 이용해 음성구간을 검출하며 멀티밴드 스펙트럼 차감법을 이용하여 잡음을 제거한다. 음성의 스펙트럼 에너지에 대한 엔트로피를 사용하여 음성검출을 하게 되면 비교적 높은 SNR 환경 (SNR 15dB) 에서는 성능이 우수하나 잡음환경의 변화에 따라 음성과 비음성의 문턱 값이 변화하여 낮은 SNR환경(SNR 0dB)에시는 정확한 음성 검출이 어렵다. 이 논문은 낮은 SNR 환경(0dB)에서도 정확한 음성을 검출할 수 있도록 음성의 스펙트럴 엔트로피와 하모닉 성분을 이용하였으며 정확한 음성 구간 검출에 따라 잡음을 제거하여 잡음에 강인한 특정을 추출하도록 하였다. 실험결과 잡음환경에 따른 인식조건에서 개선된 인식성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201115537947340&target=NART&cn=JAKO201115537947340",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "멀티밴드 스펙트럼 차감법과 엔트로피 하모닉을 이용한 잡음환경에 강인한 분산음성인식 멀티밴드 스펙트럼 차감법과 엔트로피 하모닉을 이용한 잡음환경에 강인한 분산음성인식 멀티밴드 스펙트럼 차감법과 엔트로피 하모닉을 이용한 잡음환경에 강인한 분산음성인식 음성인식의 실용화에 가장 저해되는 요소는 배경잡음과 채널에 의한 왜곡이다. 일반적으로 잡음은 음성인식 시스템의 성능을 저하시키고 이로 인해 사용 장소의 제약을 많이 받고 있다. DSR(Distributed Speech Recognition) 기반의 음성인식 역시 이 같은 문제로 성능 향상에 어려움을 겪고 있다. 이 논문은 잡음환경에서 DSR기반의 음성인식률 향상을 위해 정확한 음성구간을 검출하고, 잡음을 제거하여 잡음에 강인한 특징추출을 하도록 설계하였다. 제안된 방법은 엔트로피와 음성의 하모닉을 이용해 음성구간을 검출하며 멀티밴드 스펙트럼 차감법을 이용하여 잡음을 제거한다. 음성의 스펙트럼 에너지에 대한 엔트로피를 사용하여 음성검출을 하게 되면 비교적 높은 SNR 환경 (SNR 15dB) 에서는 성능이 우수하나 잡음환경의 변화에 따라 음성과 비음성의 문턱 값이 변화하여 낮은 SNR환경(SNR 0dB)에시는 정확한 음성 검출이 어렵다. 이 논문은 낮은 SNR 환경(0dB)에서도 정확한 음성을 검출할 수 있도록 음성의 스펙트럴 엔트로피와 하모닉 성분을 이용하였으며 정확한 음성 구간 검출에 따라 잡음을 제거하여 잡음에 강인한 특정을 추출하도록 하였다. 실험결과 잡음환경에 따른 인식조건에서 개선된 인식성능을 보였다."
        },
        {
          "rank": 30,
          "score": 0.7225214838981628,
          "doc_id": "JAKO200211921444549",
          "title": "2층 구조의 입체 시각형 신경망 기반 음소인식",
          "abstract": "본 연구는 입체 시각을 위한 신경망에 대한 연구 결과로서 인간의 음성을 인식하는데 적용된다. 입체 시각신경망(SVNN)에 기반한 음성인식에서, 먼저 입력된 음성 신호를 표준 모델과 비교함으로써 유사성이 얻어진다. 이 값들은 다이나믹한 처리 과정으로 주어지고 이웃한 신경소자들 사이에서 경쟁적이고 협력적인 처리를 거치게 된다. 이러한 다이나믹한 처리과정을 통해 단 하나의 가장 우수한 신경세포(winner neuron)만이 최후에 검출된다. 비교연구에서 2층 구조의 SVNN은 HMM 인식기보다 인식정확도 측면에서 7.7% 더 높았다. 평가 결과. SVNN은 기손리 HMM 인식기 성능을 능가하는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200211921444549&target=NART&cn=JAKO200211921444549",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "2층 구조의 입체 시각형 신경망 기반 음소인식 2층 구조의 입체 시각형 신경망 기반 음소인식 2층 구조의 입체 시각형 신경망 기반 음소인식 본 연구는 입체 시각을 위한 신경망에 대한 연구 결과로서 인간의 음성을 인식하는데 적용된다. 입체 시각신경망(SVNN)에 기반한 음성인식에서, 먼저 입력된 음성 신호를 표준 모델과 비교함으로써 유사성이 얻어진다. 이 값들은 다이나믹한 처리 과정으로 주어지고 이웃한 신경소자들 사이에서 경쟁적이고 협력적인 처리를 거치게 된다. 이러한 다이나믹한 처리과정을 통해 단 하나의 가장 우수한 신경세포(winner neuron)만이 최후에 검출된다. 비교연구에서 2층 구조의 SVNN은 HMM 인식기보다 인식정확도 측면에서 7.7% 더 높았다. 평가 결과. SVNN은 기손리 HMM 인식기 성능을 능가하는 것으로 나타났다."
        },
        {
          "rank": 31,
          "score": 0.722236156463623,
          "doc_id": "JAKO200416642157049",
          "title": "Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식",
          "abstract": "In this paper, a new noise compensation method based on the eigenvoice framework in feature space is proposed to reduce the mismatch between training and testing environments. The difference between clean and noisy environments is represented by the linear combination of K eigenvectors that represent the variation among environments. In the proposed method, the performance improvement of speech recognition systems is largely affected by how to construct the noisy models and the bias vector set. In this paper, two methods, the one based on MAP adaptation method and the other using stereo DB, are proposed to construct the noisy models. In experiments using Aurora 2 DB, we obtained 44.86% relative improvement with eigen-environment method in comparison with baseline system. Especially, in clean condition training mode, our proposed method yielded 66.74% relative improvement, which is better performance than several methods previously proposed in Aurora project.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200416642157049&target=NART&cn=JAKO200416642157049",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식 Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식 Eigen - Environment 잡음 보상 방법을 이용한 강인한 음성인식 In this paper, a new noise compensation method based on the eigenvoice framework in feature space is proposed to reduce the mismatch between training and testing environments. The difference between clean and noisy environments is represented by the linear combination of K eigenvectors that represent the variation among environments. In the proposed method, the performance improvement of speech recognition systems is largely affected by how to construct the noisy models and the bias vector set. In this paper, two methods, the one based on MAP adaptation method and the other using stereo DB, are proposed to construct the noisy models. In experiments using Aurora 2 DB, we obtained 44.86% relative improvement with eigen-environment method in comparison with baseline system. Especially, in clean condition training mode, our proposed method yielded 66.74% relative improvement, which is better performance than several methods previously proposed in Aurora project."
        },
        {
          "rank": 32,
          "score": 0.7203251123428345,
          "doc_id": "JAKO200011920774657",
          "title": "은닉 마코프 모델 기반 병렬음성인식 시스템",
          "abstract": "본 논문의 병렬음성인식 모델은 연속 은닉 마코프 모델(HMM; hidden Markov model)에 기반한 병렬 음소인식모듈과 계층구조의 지식베이스에 기반한 병렬 문장인식모듈로 구성된다. 병렬 음소인식 모듈은 수천개의 HMM을 병렬 프로세서에 분산시킨 수, 할당된 HMM에 대한 출력확률 계산과 Viterbi 알고리즘을 담당한다. 지식베이스 기반 병렬 문장인식모듈은 음소모듈에서 공급되는 음소열과 지안하는 병렬 음성인식 알고리즘은 분산메모리 MIMD 구조의 다중 트랜스퓨터와 Parsytec CC 상에 구현되었다. 실험결과, 병렬 음소인식모듈을 통한 실행시간 향상과 병렬 문장인식모듈을 통한 인식률 향상을 얻을 수 있었으며 병렬 음성인식 시스템의 실시간 구현 가능성을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200011920774657&target=NART&cn=JAKO200011920774657",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마코프 모델 기반 병렬음성인식 시스템 은닉 마코프 모델 기반 병렬음성인식 시스템 은닉 마코프 모델 기반 병렬음성인식 시스템 본 논문의 병렬음성인식 모델은 연속 은닉 마코프 모델(HMM; hidden Markov model)에 기반한 병렬 음소인식모듈과 계층구조의 지식베이스에 기반한 병렬 문장인식모듈로 구성된다. 병렬 음소인식 모듈은 수천개의 HMM을 병렬 프로세서에 분산시킨 수, 할당된 HMM에 대한 출력확률 계산과 Viterbi 알고리즘을 담당한다. 지식베이스 기반 병렬 문장인식모듈은 음소모듈에서 공급되는 음소열과 지안하는 병렬 음성인식 알고리즘은 분산메모리 MIMD 구조의 다중 트랜스퓨터와 Parsytec CC 상에 구현되었다. 실험결과, 병렬 음소인식모듈을 통한 실행시간 향상과 병렬 문장인식모듈을 통한 인식률 향상을 얻을 수 있었으며 병렬 음성인식 시스템의 실시간 구현 가능성을 확인하였다."
        },
        {
          "rank": 33,
          "score": 0.7180594205856323,
          "doc_id": "NART20042187",
          "title": "Neural networks with hidden Markov process",
          "abstract": "Observed data contain global structural information and local statistical fluctuation.  To catch both global and local information, neural network models with hidden Markov process are presented. Hidden Markov model(HMM) is a rather general framework which can effectively deal with sequential structure based on stochastic process theory.  Neural networks have strong learning ability which makes it easier to free statistical fluctuation.  By introducing the dependence of weights of neural networks on the states of the systems, neural networks combined with HMM are realized.  The process underlying the transitions of the states is a Markov process. Dependent observations are generated by the neural networks corresponding to the states.  A modified maximum likelihood procedure are employed to estimate both transition probability of Markov process and weights of neural networks.  Applications of neural networks with HMM to econometrics, pattern matching, speech recognition and image process are discussed.(Author abstract)",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART20042187&target=NART&cn=NART20042187",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Neural networks with hidden Markov process Neural networks with hidden Markov process Neural networks with hidden Markov process Observed data contain global structural information and local statistical fluctuation.  To catch both global and local information, neural network models with hidden Markov process are presented. Hidden Markov model(HMM) is a rather general framework which can effectively deal with sequential structure based on stochastic process theory.  Neural networks have strong learning ability which makes it easier to free statistical fluctuation.  By introducing the dependence of weights of neural networks on the states of the systems, neural networks combined with HMM are realized.  The process underlying the transitions of the states is a Markov process. Dependent observations are generated by the neural networks corresponding to the states.  A modified maximum likelihood procedure are employed to estimate both transition probability of Markov process and weights of neural networks.  Applications of neural networks with HMM to econometrics, pattern matching, speech recognition and image process are discussed.(Author abstract)"
        },
        {
          "rank": 34,
          "score": 0.7152249813079834,
          "doc_id": "JAKO200727500236879",
          "title": "자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식",
          "abstract": "In this paper, we carried out recognition experiments for noisy speech having various levels of car noise and output of an audio system using the speech interface. The speech interface consists of three parts: pre-processing, acoustic echo canceller, post-processing. First, a high pass filter is employed as a pre-processing part to remove some engine noises. Then, an echo canceller implemented by using an FIR-type filter with an NLMS adaptive algorithm is used to remove the music or speech coming from the audio system in a car. As a last part, the MMSE-STSA based speech enhancement method is applied to the out of the echo canceller to remove the residual noise further. For recognition experiments, we generated test signals by adding music to the car noisy speech from Aurora 2 database. The HTK-based continuous HMM system is constructed for a recognition system. Experimental results show that the proposed speech interface is very promising for robust speech recognition in a noisy car environment.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200727500236879&target=NART&cn=JAKO200727500236879",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 자동차 잡음 및 오디오 출력신호가 존재하는 자동차 실내 환경에서의 강인한 음성인식 In this paper, we carried out recognition experiments for noisy speech having various levels of car noise and output of an audio system using the speech interface. The speech interface consists of three parts: pre-processing, acoustic echo canceller, post-processing. First, a high pass filter is employed as a pre-processing part to remove some engine noises. Then, an echo canceller implemented by using an FIR-type filter with an NLMS adaptive algorithm is used to remove the music or speech coming from the audio system in a car. As a last part, the MMSE-STSA based speech enhancement method is applied to the out of the echo canceller to remove the residual noise further. For recognition experiments, we generated test signals by adding music to the car noisy speech from Aurora 2 database. The HTK-based continuous HMM system is constructed for a recognition system. Experimental results show that the proposed speech interface is very promising for robust speech recognition in a noisy car environment."
        },
        {
          "rank": 35,
          "score": 0.7130817174911499,
          "doc_id": "JAKO200111921140843",
          "title": "회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM (Hidden Markov Model)의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경 망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용 데이터에 대하여 Elman망 예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 98.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200111921140843&target=NART&cn=JAKO200111921140843",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM (Hidden Markov Model)의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경 망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용 데이터에 대하여 Elman망 예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 98.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 36,
          "score": 0.7118148803710938,
          "doc_id": "JAKO199615875841474",
          "title": "천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식",
          "abstract": "본 논문에서는 상태간의 천이가 특정한 시간 구간에서만 발생하도록 하는 천이 제한(transition constrained) HMM를 제안하고 잡음 환경에서의 성능을 평가하였다. 천이 제한 HMM는 상태 지속을 제한하고 음성 신호의 시간적 변화를 단순하고 효과적으로 표현할 수 있다. 제안된 천이 제한 HMM은 기존 HMM 보다 성능이 우수할 뿐만아니라 계산량도 매우 감소한다.  제안된 방법의 성능을 평가하기 위하여 반연속(semi-continuous) HMM을 이용하여 잡음이 SNR 20, 10, 0 dB로 첨가된 음성에 화자독립 단독음 인식실험을 수행하였다. 실험 결과에서 제안된 방법은 잡음에 강인한 특성을 나타내었다. 두 가지 종류의 잡음을 SNR 10dB로 첨가하여 사용한 경우, 천이제한 HMM의 인식률은 기존 HMM의 단어 인식률 81.08%와 75.36%에 비하여 각각 7.31%와 10.35% 향상되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199615875841474&target=NART&cn=JAKO199615875841474",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식 천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식 천이 제한 HMM을 이용한 잡음 환경에서의 음성 인식 본 논문에서는 상태간의 천이가 특정한 시간 구간에서만 발생하도록 하는 천이 제한(transition constrained) HMM를 제안하고 잡음 환경에서의 성능을 평가하였다. 천이 제한 HMM는 상태 지속을 제한하고 음성 신호의 시간적 변화를 단순하고 효과적으로 표현할 수 있다. 제안된 천이 제한 HMM은 기존 HMM 보다 성능이 우수할 뿐만아니라 계산량도 매우 감소한다.  제안된 방법의 성능을 평가하기 위하여 반연속(semi-continuous) HMM을 이용하여 잡음이 SNR 20, 10, 0 dB로 첨가된 음성에 화자독립 단독음 인식실험을 수행하였다. 실험 결과에서 제안된 방법은 잡음에 강인한 특성을 나타내었다. 두 가지 종류의 잡음을 SNR 10dB로 첨가하여 사용한 경우, 천이제한 HMM의 인식률은 기존 HMM의 단어 인식률 81.08%와 75.36%에 비하여 각각 7.31%와 10.35% 향상되었다."
        },
        {
          "rank": 37,
          "score": 0.7116135358810425,
          "doc_id": "JAKO201630932328344",
          "title": "가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법",
          "abstract": "실세계 환경의 원거리에서 녹음된 음성은 가산 잡음이나 반향 성분으로 왜곡되기 때문에 음성인식 성능이 현저히 떨어진다. 따라서 음성 전처리 과정은 실세계 환경에서 강인한 음성인식을 위한 필수과정이다. 모델 기반 특징 향상 방법은 전처리 방법 중 하나로 특징 영역 데이터의 적절한 동적 범위(dynamic range)와 차원 수로 인하여 실시간 처리가 가능하고 깨끗한 음성의 선험적 정보를 모델링하기에 용이하다. 또, 인식을 위한 최종 특징 입력에 가까운 단계에서 데이터를 처리하므로 인식에 밀접한 영향을 준다는 장점이 있다. 그러나 대략적인 왜곡 요인 관련 파라미터 추정 때문에 음성인식 성능이 하락되는 단점이 있다. 최근에 기존 모델 기반 특징 향상의 단점을 개선하여 가산 잡음이나 반향 환경에 적합한 방법이 제안되었다. 이글에서는 특징 향상 방법을 소개하고 개선된 방법의 음성인식 강인성을 알아보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201630932328344&target=NART&cn=JAKO201630932328344",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법 가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법 가산 잡음 또는 반향 환경에 강인한 음성인식을 위한 은닉 마르코프 모델 기반 특징 향상 방법 실세계 환경의 원거리에서 녹음된 음성은 가산 잡음이나 반향 성분으로 왜곡되기 때문에 음성인식 성능이 현저히 떨어진다. 따라서 음성 전처리 과정은 실세계 환경에서 강인한 음성인식을 위한 필수과정이다. 모델 기반 특징 향상 방법은 전처리 방법 중 하나로 특징 영역 데이터의 적절한 동적 범위(dynamic range)와 차원 수로 인하여 실시간 처리가 가능하고 깨끗한 음성의 선험적 정보를 모델링하기에 용이하다. 또, 인식을 위한 최종 특징 입력에 가까운 단계에서 데이터를 처리하므로 인식에 밀접한 영향을 준다는 장점이 있다. 그러나 대략적인 왜곡 요인 관련 파라미터 추정 때문에 음성인식 성능이 하락되는 단점이 있다. 최근에 기존 모델 기반 특징 향상의 단점을 개선하여 가산 잡음이나 반향 환경에 적합한 방법이 제안되었다. 이글에서는 특징 향상 방법을 소개하고 개선된 방법의 음성인식 강인성을 알아보고자 한다."
        },
        {
          "rank": 38,
          "score": 0.7093851566314697,
          "doc_id": "NART18015173",
          "title": "Speech recognition using hidden Markov models: A CMU perspective",
          "abstract": "Hidden Markov Models (HMMs) have become the predominant approach for speech recognition systems. One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU. In this paper, we introduce Hidden Markov Modelling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used in SPHINX.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART18015173&target=NART&cn=NART18015173",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Speech recognition using hidden Markov models: A CMU perspective Speech recognition using hidden Markov models: A CMU perspective Speech recognition using hidden Markov models: A CMU perspective Hidden Markov Models (HMMs) have become the predominant approach for speech recognition systems. One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU. In this paper, we introduce Hidden Markov Modelling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used in SPHINX."
        },
        {
          "rank": 39,
          "score": 0.7092621922492981,
          "doc_id": "JAKO201809242561431",
          "title": "합성곱 신경망 기반 환경잡음에 강인한 교통 소음 분류 모델",
          "abstract": "도시 유동인구가 증가함에 따라 도시 환경 소음에 관한 연구의 중요성이 증가하고 있다. 본 연구에서는 교통상황에서 발생하는 이상 소음을 최근 환경 소음 분류 연구에서 높은 성능을 보이는 딥러닝 알고리즘을 이용하여 분류한다. 구체적으로는 타이어 제동 마찰음, 자동차 충돌음, 자동차 경적음, 정상 소음 네 개의 클래스에 대하여 합성곱 신경망을 이용하여 분류한다. 또한, 실제 교통 상황에서의 환경잡음에 강인한 분류 성능을 갖기 위해 빗소리, 바람 소리, 군중 소리의 세 가지 환경잡음을 설정하였고 이를 활용하여 분류 모델을 설계하였으며 3 dB SNR(Signal to Noise Ratio) 조건에서 88 % 이상의 분류 성능을 가진다. 제시한 교통 소음에 대하여 기존 선행연구 대비 높은 분류 성능을 보이고, 빗소리, 바람 소리, 군중 소리의 세 가지 환경잡음에 강인한 교통 소음 분류 모델을 제안한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201809242561431&target=NART&cn=JAKO201809242561431",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "합성곱 신경망 기반 환경잡음에 강인한 교통 소음 분류 모델 합성곱 신경망 기반 환경잡음에 강인한 교통 소음 분류 모델 합성곱 신경망 기반 환경잡음에 강인한 교통 소음 분류 모델 도시 유동인구가 증가함에 따라 도시 환경 소음에 관한 연구의 중요성이 증가하고 있다. 본 연구에서는 교통상황에서 발생하는 이상 소음을 최근 환경 소음 분류 연구에서 높은 성능을 보이는 딥러닝 알고리즘을 이용하여 분류한다. 구체적으로는 타이어 제동 마찰음, 자동차 충돌음, 자동차 경적음, 정상 소음 네 개의 클래스에 대하여 합성곱 신경망을 이용하여 분류한다. 또한, 실제 교통 상황에서의 환경잡음에 강인한 분류 성능을 갖기 위해 빗소리, 바람 소리, 군중 소리의 세 가지 환경잡음을 설정하였고 이를 활용하여 분류 모델을 설계하였으며 3 dB SNR(Signal to Noise Ratio) 조건에서 88 % 이상의 분류 성능을 가진다. 제시한 교통 소음에 대하여 기존 선행연구 대비 높은 분류 성능을 보이고, 빗소리, 바람 소리, 군중 소리의 세 가지 환경잡음에 강인한 교통 소음 분류 모델을 제안한다."
        },
        {
          "rank": 40,
          "score": 0.7078408598899841,
          "doc_id": "JAKO201016450104831",
          "title": "잡음환경에서 음성인식 성능향상을 위한 바이너리 마스크를 이용한 스펙트럼 향상 방법",
          "abstract": "음성인식의 실용화에 가장 저해되는 요소는 배경잡음과 채널잡음에 의한 왜곡이다. 일반적으로 배경잡음은 음성인식 시스템의 성능을 저하시키고 이로 인해 사용 장소의 제약을 받게 한다. DSR (Distributed Speech Recognition) 기반의 음성인식 역시 이와 같은 문제로 성능 향상에 어려움을 겪고 있다. 이러한 문제를 해결하기 위해 다양한 잡음제거 알고리듬이 사용되고 있으나 낮은 SNR환경에서 부정확한 잡음추정으로 발생하는 스펙트럼 손상과 잔존 잡음은 음성인식기의 인식환경과 학습 환경의 불일치를 만들게 되어 인식률을 저하시키는 원인이 된다. 본 논문에서는 이와 같은 문제를 해결하기 위해 잡음제거 알고리듬으로 MMSE-STSA 방법을 사용하였고 손상된 스펙트럼을 보상하기 위해 Ideal Binary Mask를 이용하였다. 잡음환경 (SNR 15 ~ 0 dB)에 따른 실험결과 제안된 방법을 사용했을 때 향상된 스펙트럼을 얻을 수 있었고 향상된 인식성능을 확인했다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201016450104831&target=NART&cn=JAKO201016450104831",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "잡음환경에서 음성인식 성능향상을 위한 바이너리 마스크를 이용한 스펙트럼 향상 방법 잡음환경에서 음성인식 성능향상을 위한 바이너리 마스크를 이용한 스펙트럼 향상 방법 잡음환경에서 음성인식 성능향상을 위한 바이너리 마스크를 이용한 스펙트럼 향상 방법 음성인식의 실용화에 가장 저해되는 요소는 배경잡음과 채널잡음에 의한 왜곡이다. 일반적으로 배경잡음은 음성인식 시스템의 성능을 저하시키고 이로 인해 사용 장소의 제약을 받게 한다. DSR (Distributed Speech Recognition) 기반의 음성인식 역시 이와 같은 문제로 성능 향상에 어려움을 겪고 있다. 이러한 문제를 해결하기 위해 다양한 잡음제거 알고리듬이 사용되고 있으나 낮은 SNR환경에서 부정확한 잡음추정으로 발생하는 스펙트럼 손상과 잔존 잡음은 음성인식기의 인식환경과 학습 환경의 불일치를 만들게 되어 인식률을 저하시키는 원인이 된다. 본 논문에서는 이와 같은 문제를 해결하기 위해 잡음제거 알고리듬으로 MMSE-STSA 방법을 사용하였고 손상된 스펙트럼을 보상하기 위해 Ideal Binary Mask를 이용하였다. 잡음환경 (SNR 15 ~ 0 dB)에 따른 실험결과 제안된 방법을 사용했을 때 향상된 스펙트럼을 얻을 수 있었고 향상된 인식성능을 확인했다."
        },
        {
          "rank": 41,
          "score": 0.7061977386474609,
          "doc_id": "JAKO201935164467523",
          "title": "심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구",
          "abstract": "본 논문에서는 구개인두부전증(VeloPharyngeal Insufficiency, VPI) 환자의 음성을 효과적으로 인식하기 위해 컨볼루션 신경망 (Convolutional Neural Network, CNN), 장단기 모델(Long Short Term Memory, LSTM) 구조 신경망을 은닉 마르코프 모델(Hidden Markov Model, HMM)과 결합한 하이브리드 구조의 음성 인식 시스템을 구축하고 모델 적응 기법을 적용하여, 기존 Gaussian Mixture Model(GMM-HMM), 완전 연결형 Deep Neural Network(DNN-HMM) 기반의 음성 인식 시스템과 성능을 비교한다. 정상인 화자가 PBW452단어를 발화한 데이터를 이용하여 초기 모델을 학습하고 정상인 화자의 VPI 모의 음성을 이용하여 화자 적응의 사전 모델을 생성한 후에 VPI 환자들의 음성으로 추가 적응 학습을 진행한다. VPI환자의 화자 적응 시에 CNN-HMM 기반 모델에서는 일부층만 적응 학습하고, LSTM-HMM 기반 모델의 경우에는 드롭 아웃 규제기법을 적용하여 성능을 관찰한 결과 기존 완전 연결형 DNN-HMM 인식기보다 3.68 % 향상된 음성 인식 성능을 나타낸다. 이러한 결과는 본 논문에서 제안하는 LSTM-HMM 기반의 하이브리드 음성 인식 기법이 많은 데이터를 확보하기 어려운 VPI 환자 음성에 대해 보다 향상된 인식률의 음성 인식 시스템을 구축하는데 효과적임을 입증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201935164467523&target=NART&cn=JAKO201935164467523",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 심층신경망 구조에 따른 구개인두부전증 환자 음성 인식 향상 연구 본 논문에서는 구개인두부전증(VeloPharyngeal Insufficiency, VPI) 환자의 음성을 효과적으로 인식하기 위해 컨볼루션 신경망 (Convolutional Neural Network, CNN), 장단기 모델(Long Short Term Memory, LSTM) 구조 신경망을 은닉 마르코프 모델(Hidden Markov Model, HMM)과 결합한 하이브리드 구조의 음성 인식 시스템을 구축하고 모델 적응 기법을 적용하여, 기존 Gaussian Mixture Model(GMM-HMM), 완전 연결형 Deep Neural Network(DNN-HMM) 기반의 음성 인식 시스템과 성능을 비교한다. 정상인 화자가 PBW452단어를 발화한 데이터를 이용하여 초기 모델을 학습하고 정상인 화자의 VPI 모의 음성을 이용하여 화자 적응의 사전 모델을 생성한 후에 VPI 환자들의 음성으로 추가 적응 학습을 진행한다. VPI환자의 화자 적응 시에 CNN-HMM 기반 모델에서는 일부층만 적응 학습하고, LSTM-HMM 기반 모델의 경우에는 드롭 아웃 규제기법을 적용하여 성능을 관찰한 결과 기존 완전 연결형 DNN-HMM 인식기보다 3.68 % 향상된 음성 인식 성능을 나타낸다. 이러한 결과는 본 논문에서 제안하는 LSTM-HMM 기반의 하이브리드 음성 인식 기법이 많은 데이터를 확보하기 어려운 VPI 환자 음성에 대해 보다 향상된 인식률의 음성 인식 시스템을 구축하는데 효과적임을 입증한다."
        },
        {
          "rank": 42,
          "score": 0.6985713243484497,
          "doc_id": "NART56157676",
          "title": "온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합",
          "abstract": "<P> 최근에 음성인식 분야에서 널리 사용되고 있는 은닉 마르코프 모델(HMM)을 이용하여 필기문자를 인식하고자 하는 연구가 활발히 진행되고 있다. 하지만, HMM은 시간에 따라서 변하는 입력특성을 잘 처리하는 장점이 있는 반면에, 각 모델을 독립적으로 학습시키는 경우에 각 패턴 사이의 분별력이 다소 떨어지는 문제가 있다. 본 논문에서는 HMM을 통해서 얻어진 각 모델의 내부 출력값을 이용하여 신경망 분류기로 추가적인 분류작업을 수행하는 방법을 제시한다. 또, 온라인 필기 데이타로 숫자와 영문자 대소문자를 인식하는 실험을 통해서 제시된 방법의 유용성을 입증한다.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART56157676&target=NART&cn=NART56157676",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 온라인 필기문자의 인식을 위한 은닉 마르코프 모델과 신경망 분류기의 결합 <P> 최근에 음성인식 분야에서 널리 사용되고 있는 은닉 마르코프 모델(HMM)을 이용하여 필기문자를 인식하고자 하는 연구가 활발히 진행되고 있다. 하지만, HMM은 시간에 따라서 변하는 입력특성을 잘 처리하는 장점이 있는 반면에, 각 모델을 독립적으로 학습시키는 경우에 각 패턴 사이의 분별력이 다소 떨어지는 문제가 있다. 본 논문에서는 HMM을 통해서 얻어진 각 모델의 내부 출력값을 이용하여 신경망 분류기로 추가적인 분류작업을 수행하는 방법을 제시한다. 또, 온라인 필기 데이타로 숫자와 영문자 대소문자를 인식하는 실험을 통해서 제시된 방법의 유용성을 입증한다.</P>"
        },
        {
          "rank": 43,
          "score": 0.6983432769775391,
          "doc_id": "JAKO201707851605473",
          "title": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표",
          "abstract": "본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201707851605473&target=NART&cn=JAKO201707851605473",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다."
        },
        {
          "rank": 44,
          "score": 0.6964537501335144,
          "doc_id": "JAKO201911338887557",
          "title": "잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법",
          "abstract": "본 논문에서는 잡음 환경에서 효과적인 음성 인식을 위하여 DNN(Deep Neural Network) 기반의 잡음 오염 함수 예측을 이용한 음향 모델 적응 기법을 제안한다. 깨끗한 음성과 잡음 정보를 입력으로 하고 오염된 음성에 대한 특징 벡터를 출력으로 하는 DNN을 학습하여 비선형 관계를 갖는 잡음 오염 함수를 예측한다. 예측된 잡음 오염 함수를 음향모델의 평균 벡터에 적용하여 잡음 환경에 적응된 음향 모델을 생성한다. Aurora 2.0 데이터를 이용한 음성 인식 성능 평가에서 본 논문에서 제안한 모델 적응 기법이 기존의 전처리, 모델 적응 기법에 비해 일치, 불일치 잡음 환경에서 모두 평균적으로 우수한 성능을 나타낸다. 특히 불일치 잡음 환경에서 평균 오류율이 15.87 %의 상대 향상률을 나타낸다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201911338887557&target=NART&cn=JAKO201911338887557",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 본 논문에서는 잡음 환경에서 효과적인 음성 인식을 위하여 DNN(Deep Neural Network) 기반의 잡음 오염 함수 예측을 이용한 음향 모델 적응 기법을 제안한다. 깨끗한 음성과 잡음 정보를 입력으로 하고 오염된 음성에 대한 특징 벡터를 출력으로 하는 DNN을 학습하여 비선형 관계를 갖는 잡음 오염 함수를 예측한다. 예측된 잡음 오염 함수를 음향모델의 평균 벡터에 적용하여 잡음 환경에 적응된 음향 모델을 생성한다. Aurora 2.0 데이터를 이용한 음성 인식 성능 평가에서 본 논문에서 제안한 모델 적응 기법이 기존의 전처리, 모델 적응 기법에 비해 일치, 불일치 잡음 환경에서 모두 평균적으로 우수한 성능을 나타낸다. 특히 불일치 잡음 환경에서 평균 오류율이 15.87 %의 상대 향상률을 나타낸다."
        },
        {
          "rank": 45,
          "score": 0.6958069801330566,
          "doc_id": "JAKO201120661418238",
          "title": "음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거",
          "abstract": "본 논문에서는 먼저 신경회로망의 학습에 오차역전파 학습 알고리즘을 사용하여 각 프레임에서의 음성 및 잡음 구간의 검출에 의한 음성인식 알고리즘을 제안한다. 그리고 신경회로망에 의하여 음성 및 잡음 구간의 검출에 따라서 각 프레임에서 잡음을 제거하는 스펙트럼 차감법을 제안한다. 본 실험에서는 제안한 음성인식알고리즘의 성능을 원음성에 백색잡음 및 자동차 잡음을 부가하여 인식율을 평가한다. 또한 인식시스템에 의하여 검출된 음성 및 잡음 구간을 이용하여 각 프레임에서의 스펙트럼 차감법에 의한 잡음제거의 실험결과를 나타낸다. 잡음에 의하여 오염된 음성에 대하여 신호대잡음비를 사용하여 본 알고리즘이 유효하다는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201120661418238&target=NART&cn=JAKO201120661418238",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 음성 및 잡음 인식 알고리즘을 이용한 환경 배경잡음의 제거 본 논문에서는 먼저 신경회로망의 학습에 오차역전파 학습 알고리즘을 사용하여 각 프레임에서의 음성 및 잡음 구간의 검출에 의한 음성인식 알고리즘을 제안한다. 그리고 신경회로망에 의하여 음성 및 잡음 구간의 검출에 따라서 각 프레임에서 잡음을 제거하는 스펙트럼 차감법을 제안한다. 본 실험에서는 제안한 음성인식알고리즘의 성능을 원음성에 백색잡음 및 자동차 잡음을 부가하여 인식율을 평가한다. 또한 인식시스템에 의하여 검출된 음성 및 잡음 구간을 이용하여 각 프레임에서의 스펙트럼 차감법에 의한 잡음제거의 실험결과를 나타낸다. 잡음에 의하여 오염된 음성에 대하여 신호대잡음비를 사용하여 본 알고리즘이 유효하다는 것을 확인한다."
        },
        {
          "rank": 46,
          "score": 0.6956661939620972,
          "doc_id": "JAKO202011263332681",
          "title": "심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용",
          "abstract": "가우스 혼합 모델-은닉 마코프 모델(Gaussian Mixture Model-Hidden Markov Model, GMM-HMM)을 이용하는 전통적인 음성인식 시스템에서는, 극점 필터링 기반의 켑스트럼 특징 정규화 방식이 잡음 환경에서 짧은 발화의 인식 성능을 향상시키는데 효과적이었다. 본 논문에서는 심층신경망(Deep Neural Network, DNN)을 이용하는 최신의 음성인식 시스템에서도 이 방식의 유용성이 있는지 검토한다. AURORA 2 DB에 대한 실험 결과, 특히 훈련 및 테스트 환경 사이의 불일치가 클 때에, 극점 필터링 기반의 켑스트럼 평균 분산 정규화 방식이 극점 필터링을 사용하지 않는 방식에 비해 매우 짧은 발화의 인식 성능을 개선시킴을 보여 준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202011263332681&target=NART&cn=JAKO202011263332681",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 가우스 혼합 모델-은닉 마코프 모델(Gaussian Mixture Model-Hidden Markov Model, GMM-HMM)을 이용하는 전통적인 음성인식 시스템에서는, 극점 필터링 기반의 켑스트럼 특징 정규화 방식이 잡음 환경에서 짧은 발화의 인식 성능을 향상시키는데 효과적이었다. 본 논문에서는 심층신경망(Deep Neural Network, DNN)을 이용하는 최신의 음성인식 시스템에서도 이 방식의 유용성이 있는지 검토한다. AURORA 2 DB에 대한 실험 결과, 특히 훈련 및 테스트 환경 사이의 불일치가 클 때에, 극점 필터링 기반의 켑스트럼 평균 분산 정규화 방식이 극점 필터링을 사용하지 않는 방식에 비해 매우 짧은 발화의 인식 성능을 개선시킴을 보여 준다."
        },
        {
          "rank": 47,
          "score": 0.6936048269271851,
          "doc_id": "NART17510378",
          "title": "A spatio-temporal speech enhancement scheme for robust speech recognition in noisy environments",
          "abstract": "<P><B>Abstract</B></P><P>A new speech enhancement scheme is presented integrating spatial and temporal signal processing methods for robust speech recognition in noisy environments. The scheme first separates spatially localized point sources from noisy speech signals recorded by two microphones. Blind source separation algorithms assuming no a priori knowledge about the sources involved are applied in this spatial processing stage. Then denoising of distributed background noise is achieved in a combined spatial/temporal processing approach. The desired speaker signal is first processed along with an artificially constructed noise signal in a supplementary blind source separation step. It is further denoised by exploiting differences in temporal speech and noise statistics in a wavelet filterbank. The scheme&#x2019;s performance is illustrated by speech recognition experiments on real recordings in a noisy car environment. In comparison to a common multi-microphone technique like beamforming with spectral subtraction, the scheme is shown to enable more accurate speech recognition in the presence of a highly interfering point source and strong background noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART17510378&target=NART&cn=NART17510378",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A spatio-temporal speech enhancement scheme for robust speech recognition in noisy environments A spatio-temporal speech enhancement scheme for robust speech recognition in noisy environments A spatio-temporal speech enhancement scheme for robust speech recognition in noisy environments <P><B>Abstract</B></P><P>A new speech enhancement scheme is presented integrating spatial and temporal signal processing methods for robust speech recognition in noisy environments. The scheme first separates spatially localized point sources from noisy speech signals recorded by two microphones. Blind source separation algorithms assuming no a priori knowledge about the sources involved are applied in this spatial processing stage. Then denoising of distributed background noise is achieved in a combined spatial/temporal processing approach. The desired speaker signal is first processed along with an artificially constructed noise signal in a supplementary blind source separation step. It is further denoised by exploiting differences in temporal speech and noise statistics in a wavelet filterbank. The scheme&#x2019;s performance is illustrated by speech recognition experiments on real recordings in a noisy car environment. In comparison to a common multi-microphone technique like beamforming with spectral subtraction, the scheme is shown to enable more accurate speech recognition in the presence of a highly interfering point source and strong background noise.</P>"
        },
        {
          "rank": 48,
          "score": 0.6925187706947327,
          "doc_id": "NART06964546",
          "title": "Low resolution, degraded document recognition using neural networks and hidden Markov models",
          "abstract": "<P><B>Abstract</B></P><P>We collected a large, real world database, containing degraded, old and faxed documents and present a comparison between two leading edge commercial software packages and human reading performance which shows quantitatively the huge performance gap between humans and machines, even on random character documents where no context can be used. This indicates room for possible improvements. We implemented an integrated segmentation and recognition algorithm using neural networks and hidden Markov models trained on the database and present results which show the superior performance of the algorithm.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART06964546&target=NART&cn=NART06964546",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Low resolution, degraded document recognition using neural networks and hidden Markov models Low resolution, degraded document recognition using neural networks and hidden Markov models Low resolution, degraded document recognition using neural networks and hidden Markov models <P><B>Abstract</B></P><P>We collected a large, real world database, containing degraded, old and faxed documents and present a comparison between two leading edge commercial software packages and human reading performance which shows quantitatively the huge performance gap between humans and machines, even on random character documents where no context can be used. This indicates room for possible improvements. We implemented an integrated segmentation and recognition algorithm using neural networks and hidden Markov models trained on the database and present results which show the superior performance of the algorithm.</P>"
        },
        {
          "rank": 49,
          "score": 0.689365565776825,
          "doc_id": "JAKO201719951669089",
          "title": "원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링",
          "abstract": "This paper proposes a new method to train Deep Neural Network (DNN)-based acoustic models for speech recognition of native and foreign speakers. The proposed method consists of determining multi-set state clusters with various acoustic properties, training a DNN-based acoustic model, and recognizing speech based on the model. In the proposed method, hidden nodes of DNN are shared, but output nodes are separated to accommodate different acoustic properties for native and foreign speech. In an English speech recognition task for speakers of Korean and English respectively, the proposed method is shown to slightly improve recognition accuracy compared to the conventional multi-condition training method.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201719951669089&target=NART&cn=JAKO201719951669089",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 원어민 및 외국인 화자의 음성인식을 위한 심층 신경망 기반 음향모델링 This paper proposes a new method to train Deep Neural Network (DNN)-based acoustic models for speech recognition of native and foreign speakers. The proposed method consists of determining multi-set state clusters with various acoustic properties, training a DNN-based acoustic model, and recognizing speech based on the model. In the proposed method, hidden nodes of DNN are shared, but output nodes are separated to accommodate different acoustic properties for native and foreign speech. In an English speech recognition task for speakers of Korean and English respectively, the proposed method is shown to slightly improve recognition accuracy compared to the conventional multi-condition training method."
        },
        {
          "rank": 50,
          "score": 0.6874414086341858,
          "doc_id": "JAKO200606140790765",
          "title": "Discrimination of Pathological Speech Using Hidden Markov Models",
          "abstract": "Diagnosis of pathological voice is one of the important issues in biomedical applications of speech technology. This study focuses on the discrimination of voice disorder using HMM (Hidden Markov Model) for automatic detection between normal voice and vocal fold disorder voice. This is a non-intrusive, non-expensive and fully automated method using only a speech sample of the subject. Speech data from normal people and patients were collected. Mel-frequency filter cepstral coefficients (MFCCs) were modeled by HMM classifier. Different states (3 states, 5 states and 7 states), 3 mixtures and left to right HMMs were formed. This method gives an accuracy of 93.8% for train data and 91.7% for test data in the discrimination of normal and vocal fold disorder voice for sustained /a/.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200606140790765&target=NART&cn=JAKO200606140790765",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Discrimination of Pathological Speech Using Hidden Markov Models Discrimination of Pathological Speech Using Hidden Markov Models Discrimination of Pathological Speech Using Hidden Markov Models Diagnosis of pathological voice is one of the important issues in biomedical applications of speech technology. This study focuses on the discrimination of voice disorder using HMM (Hidden Markov Model) for automatic detection between normal voice and vocal fold disorder voice. This is a non-intrusive, non-expensive and fully automated method using only a speech sample of the subject. Speech data from normal people and patients were collected. Mel-frequency filter cepstral coefficients (MFCCs) were modeled by HMM classifier. Different states (3 states, 5 states and 7 states), 3 mixtures and left to right HMMs were formed. This method gives an accuracy of 93.8% for train data and 91.7% for test data in the discrimination of normal and vocal fold disorder voice for sustained /a/."
        }
      ]
    }
  ],
  "meta": {
    "model": "gemini-2.5-flash",
    "temperature": 0.2
  }
}