{
  "id": "row_000026",
  "model_name": "Alibaba-NLP/gte-multilingual-base",
  "timestamp_kst": "2025-09-08T23:55:35.835860+09:00",
  "trial_id": "fadfb696",
  "queries": [
    {
      "query": "클라우드 컴퓨팅 환경에서 오픈소스 기반 딥 러닝 프레임워크들 간의 성능 비교 결과와 그 의의를 간결하게 정리해 주실 수 있나요?",
      "query_meta": {
        "type": "original"
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8642516136169434,
          "doc_id": "DIKO0014373092",
          "title": "Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서",
          "abstract": "통신 기술의 발달로 인한 사람과 사람, 장치와 장치, 사람과 장치 간의 연결성의 증가와 저장 매체 기술의 발달, 그리고 데이터 저장 비용의 감소로 인해 데이터의 양이 폭발적으로 증가했다. 이에 따라 다양한 형태의 대규모의 데이터를 빠른 시간 내에 효율적으로 처리할 수 있는 Cloud Computing 기술이 주목 받고 있고, Cloud Computing을 위한 오픈소스 기반의 솔루션 또한 많이 나타나게 되었다. &amp;#xD; 2012년부터 주목받기 시작한 Deep Learning은 전 세계적으로 가장 많은 관심을 받는 연구 분야 중 하나이며, 이중 CNN(Convolution Neural Network)은 가장 대표적 알고리즘이다. Deep Learning은 미래사회를 이끌어갈 분야로 평가받고 있으며, 이에 따라 많은 연구들이 진행되고 있고, Deep Learning을 쉽게 활용할 수 있도록 하는 많은 프레임워크가 개발되었다. &amp;#xD; 이에 따라 많은 사람들이 쉽게 Deep Learning을 접할 수 있게 되었지만 특정 환경에서 어떤 프레임워크가 더 우수한 성능을 보이는지에 대한 연구는 부족한 실정이다. 본 논문에서는 특정 환경에서의 성능 비교가 부족하다는 기존 연구의 한계점을 개선하고자 가장 대표적인 Cloud Computing용 오픈소스 소프트웨어 중 하나인 OpenStack을 이용하여 Cloud Computing 환경에서 어떤 Deep Learning 프레임워크가 더 우수한 성능을 보이는지 비교해 보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014373092&target=NART&cn=DIKO0014373092",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 통신 기술의 발달로 인한 사람과 사람, 장치와 장치, 사람과 장치 간의 연결성의 증가와 저장 매체 기술의 발달, 그리고 데이터 저장 비용의 감소로 인해 데이터의 양이 폭발적으로 증가했다. 이에 따라 다양한 형태의 대규모의 데이터를 빠른 시간 내에 효율적으로 처리할 수 있는 Cloud Computing 기술이 주목 받고 있고, Cloud Computing을 위한 오픈소스 기반의 솔루션 또한 많이 나타나게 되었다. &amp;#xD; 2012년부터 주목받기 시작한 Deep Learning은 전 세계적으로 가장 많은 관심을 받는 연구 분야 중 하나이며, 이중 CNN(Convolution Neural Network)은 가장 대표적 알고리즘이다. Deep Learning은 미래사회를 이끌어갈 분야로 평가받고 있으며, 이에 따라 많은 연구들이 진행되고 있고, Deep Learning을 쉽게 활용할 수 있도록 하는 많은 프레임워크가 개발되었다. &amp;#xD; 이에 따라 많은 사람들이 쉽게 Deep Learning을 접할 수 있게 되었지만 특정 환경에서 어떤 프레임워크가 더 우수한 성능을 보이는지에 대한 연구는 부족한 실정이다. 본 논문에서는 특정 환경에서의 성능 비교가 부족하다는 기존 연구의 한계점을 개선하고자 가장 대표적인 Cloud Computing용 오픈소스 소프트웨어 중 하나인 OpenStack을 이용하여 Cloud Computing 환경에서 어떤 Deep Learning 프레임워크가 더 우수한 성능을 보이는지 비교해 보고자 한다."
        },
        {
          "rank": 2,
          "score": 0.7945667505264282,
          "doc_id": "DIKO0015889140",
          "title": "딥 러닝 프레임워크 성능 비교 및 개선 방안",
          "abstract": "현 시대는 4차 산업혁명이 대두되는 시대로 요소 기술들 중 인공지능의 중 요성은 아무리 강조하더라도 지나치지 않으며, 기업들 경쟁력의 척도라고 불 릴만큼 모든 산업에서 활용되고있다. 2016년 경 DeepMind 의 AlphaGo 와 이 세돌 선수의 경기로 국내에서는 처음으로 인공지능의 위력과 Deep Learning 이라는 단어가 대중들에게 알려지게 되었다.&amp;#xD; 특정 IT 산업이 발전하게 되면 해당 분야의 개발자들의 생산성과 접근성을 높이기 위해 Framework 들이 등장, 발전하게 된다. 통신기술과 스마트폰의 출현으로 WEB 붐이 이르렀을 때, Server-side 에서는 Spring, django, Ruby on Rails 등이 출현하였고, Client-side 에서는 Angular, React, jQuery 와 같이 다양한 Framework 들이 등장 발전하였다. 컴퓨터 성능의 발전과 다양 한 컴퓨팅 기술의 발전으로 현 시대는 인공지능 3차 붐으로 Machine Learning 과 Deep Learning 의 시대로 불리고있다.&amp;#xD; 이와 같이 Deep Learning 분야에서도 다양한 Framework 들이 개발되었다. 이런 다양한 Framework 제품들의 목적은 개발자들의 생산성을 향상시키기 위 해 내부 알고리즘이나 메커니즘을 Black Box 형식으로 감추고 High Level API 를 제공하기 때문에, 내부적인 구현 방식은 Framework 별로 다르다. 본 논문에서는 현 시대에 가장 많이 사용하는 대표적인 Framework 들을 선정한 다. 그리고 선정된 Framework 들을 이용하여 Convolutional Neural Network 알고리즘을 구현, 동일한 Training Data 를 이용하여 학습 Model 을 만들어 낸다. 그리고 동일한 Cloud 환경에서 각 Framework 별 학습을 수행하여 성 능을 비교한다. 성능 비교 환경은 총 3가지로 CPU, GPU 1 Core, Multi GPU Core 환경에서 각 Framework 별 성능 지표를 추출한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015889140&target=NART&cn=DIKO0015889140",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 프레임워크 성능 비교 및 개선 방안 딥 러닝 프레임워크 성능 비교 및 개선 방안 딥 러닝 프레임워크 성능 비교 및 개선 방안 현 시대는 4차 산업혁명이 대두되는 시대로 요소 기술들 중 인공지능의 중 요성은 아무리 강조하더라도 지나치지 않으며, 기업들 경쟁력의 척도라고 불 릴만큼 모든 산업에서 활용되고있다. 2016년 경 DeepMind 의 AlphaGo 와 이 세돌 선수의 경기로 국내에서는 처음으로 인공지능의 위력과 Deep Learning 이라는 단어가 대중들에게 알려지게 되었다.&amp;#xD; 특정 IT 산업이 발전하게 되면 해당 분야의 개발자들의 생산성과 접근성을 높이기 위해 Framework 들이 등장, 발전하게 된다. 통신기술과 스마트폰의 출현으로 WEB 붐이 이르렀을 때, Server-side 에서는 Spring, django, Ruby on Rails 등이 출현하였고, Client-side 에서는 Angular, React, jQuery 와 같이 다양한 Framework 들이 등장 발전하였다. 컴퓨터 성능의 발전과 다양 한 컴퓨팅 기술의 발전으로 현 시대는 인공지능 3차 붐으로 Machine Learning 과 Deep Learning 의 시대로 불리고있다.&amp;#xD; 이와 같이 Deep Learning 분야에서도 다양한 Framework 들이 개발되었다. 이런 다양한 Framework 제품들의 목적은 개발자들의 생산성을 향상시키기 위 해 내부 알고리즘이나 메커니즘을 Black Box 형식으로 감추고 High Level API 를 제공하기 때문에, 내부적인 구현 방식은 Framework 별로 다르다. 본 논문에서는 현 시대에 가장 많이 사용하는 대표적인 Framework 들을 선정한 다. 그리고 선정된 Framework 들을 이용하여 Convolutional Neural Network 알고리즘을 구현, 동일한 Training Data 를 이용하여 학습 Model 을 만들어 낸다. 그리고 동일한 Cloud 환경에서 각 Framework 별 학습을 수행하여 성 능을 비교한다. 성능 비교 환경은 총 3가지로 CPU, GPU 1 Core, Multi GPU Core 환경에서 각 Framework 별 성능 지표를 추출한다."
        },
        {
          "rank": 3,
          "score": 0.7759628295898438,
          "doc_id": "JAKO201718054814596",
          "title": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석",
          "abstract": "딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201718054814596&target=NART&cn=JAKO201718054814596",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다."
        },
        {
          "rank": 4,
          "score": 0.7754297256469727,
          "doc_id": "DIKO0016819326",
          "title": "클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석",
          "abstract": "DNN(Deep Neural Network)은 이미지 인식, 자연어 처리를 포함한 다양한 분야에서 널리 사용되고 있으며 이러한 모델을 실제 환경에서 효율적으로 실행하는 것이 중요하다. 사용자의 요구를 충족하기 위해 딥러닝 추론을 최적화하는 것은 더욱 중요하지만 다양한 클라우드 환경에서 딥러닝 추론 최적화 구성을 찾는것은 어렵다.&amp;#xD; 본 논문에서는 다양한 하드웨어와 최적화 기법을 활용하여 딥러닝 추론을 실험하고 분석한다. 딥러닝 추론의 특성을 파악하고 클라우드에서 제공하는하드웨어들 중 서버리스 컴퓨팅 아키텍처를 사용하여 딥러닝 추론 작업을 배포하는데 용이한 프로토타입을 제안하고 결과를 분석한다. 하드웨어 유형, 모델 그래프 최적화, 하드웨어 최적화 및 컴파일, 서버리스 메모리 및 배치 크기 설정 등 서버리스 컴퓨팅 환경에서 제공할 때 고려해야할 많은 요소들이 있으며 사용자는 완전 관리형 웹 서비스를 통해 쉽게 다양한 구성에 대해서 시도해볼 수 있다. 제안한 시스템을 통해 최적의 서버리스 환경 구성을 찾을 수 있으며 공개된 소스를 통해 FaaS 구성에 비교적 쉽게 접근할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016819326&target=NART&cn=DIKO0016819326",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석 클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석 클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석 DNN(Deep Neural Network)은 이미지 인식, 자연어 처리를 포함한 다양한 분야에서 널리 사용되고 있으며 이러한 모델을 실제 환경에서 효율적으로 실행하는 것이 중요하다. 사용자의 요구를 충족하기 위해 딥러닝 추론을 최적화하는 것은 더욱 중요하지만 다양한 클라우드 환경에서 딥러닝 추론 최적화 구성을 찾는것은 어렵다.&amp;#xD; 본 논문에서는 다양한 하드웨어와 최적화 기법을 활용하여 딥러닝 추론을 실험하고 분석한다. 딥러닝 추론의 특성을 파악하고 클라우드에서 제공하는하드웨어들 중 서버리스 컴퓨팅 아키텍처를 사용하여 딥러닝 추론 작업을 배포하는데 용이한 프로토타입을 제안하고 결과를 분석한다. 하드웨어 유형, 모델 그래프 최적화, 하드웨어 최적화 및 컴파일, 서버리스 메모리 및 배치 크기 설정 등 서버리스 컴퓨팅 환경에서 제공할 때 고려해야할 많은 요소들이 있으며 사용자는 완전 관리형 웹 서비스를 통해 쉽게 다양한 구성에 대해서 시도해볼 수 있다. 제안한 시스템을 통해 최적의 서버리스 환경 구성을 찾을 수 있으며 공개된 소스를 통해 FaaS 구성에 비교적 쉽게 접근할 수 있다."
        },
        {
          "rank": 5,
          "score": 0.7624368667602539,
          "doc_id": "JAKO201713056893580",
          "title": "딥 러닝 프레임워크의 비교 및 분석",
          "abstract": "딥 러닝은 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 딥 러닝은 세상을 이해하고 감지하는 인공지능을 개발하는데 가장 촉망받는 기술이 되고 있으며, 구글, 바이두, 페이스북 등이 가장 앞서서 개발을 하고 있다. 본 논문에서는 딥 러닝을 구현하는 딥 러닝 프레임워크의 종류에 대해 논의하고, 딥 러닝 프레임워크의 영상과 음성 인식 분야의 효율성에 대해 비교, 분석하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201713056893580&target=NART&cn=JAKO201713056893580",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 프레임워크의 비교 및 분석 딥 러닝 프레임워크의 비교 및 분석 딥 러닝 프레임워크의 비교 및 분석 딥 러닝은 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 딥 러닝은 세상을 이해하고 감지하는 인공지능을 개발하는데 가장 촉망받는 기술이 되고 있으며, 구글, 바이두, 페이스북 등이 가장 앞서서 개발을 하고 있다. 본 논문에서는 딥 러닝을 구현하는 딥 러닝 프레임워크의 종류에 대해 논의하고, 딥 러닝 프레임워크의 영상과 음성 인식 분야의 효율성에 대해 비교, 분석하고자 한다."
        },
        {
          "rank": 6,
          "score": 0.7528901696205139,
          "doc_id": "JAKO202512254006340",
          "title": "컨테이너 환경에서 딥러닝 워크로드의 성능 분석",
          "abstract": "최근 딥러닝 워크로드가 컨테이너 환경에서 실행되는 사례가 늘고 있다. 컨테이너는 가상머신에 비해 낮은 오버 헤드와 높은 이식성을 제공하지만, 딥러닝 워크로드의 실행 시 시스템 자원의 비효율적 활용 문제가 발생할 수 있다. 본 논문에서는 컨테이너 환경에서 딥러닝 워크로드 실행으로 인한 오버헤드와 비효율성을 분석하기 위해 시스템콜 및 이벤트 추적 트레이스를 수집 및 분석하였다. 특히, 동일한 워크로드를 호스트 머신에서 직접 실행한 경우와 컨테이너 환경에서 실행한 경우를 비교하여 자원 소비 및 간섭과 관련된 컨테이너 환경의 오버헤드를 정량적으로 확인하였다. 분석 결과 딥러닝 워크로드의 컨테이너 실행 시 성능 병목을 초래하는 주요 원인으로 주기적인 스토리지 플러시 작업이 확인되었으며, 다중 테넌트 환경에서는 자원 경합으로 인해 이러한 문제가 더욱 심화됨을 확인하였다. 본 연구의 결과는 컨테이너 환경에서 딥러닝 워크로드를 효율적으로 실행하기 위한 클라우드 및 엣지 시스템 설계에 중요한 인사이트를 제공할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202512254006340&target=NART&cn=JAKO202512254006340",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "컨테이너 환경에서 딥러닝 워크로드의 성능 분석 컨테이너 환경에서 딥러닝 워크로드의 성능 분석 컨테이너 환경에서 딥러닝 워크로드의 성능 분석 최근 딥러닝 워크로드가 컨테이너 환경에서 실행되는 사례가 늘고 있다. 컨테이너는 가상머신에 비해 낮은 오버 헤드와 높은 이식성을 제공하지만, 딥러닝 워크로드의 실행 시 시스템 자원의 비효율적 활용 문제가 발생할 수 있다. 본 논문에서는 컨테이너 환경에서 딥러닝 워크로드 실행으로 인한 오버헤드와 비효율성을 분석하기 위해 시스템콜 및 이벤트 추적 트레이스를 수집 및 분석하였다. 특히, 동일한 워크로드를 호스트 머신에서 직접 실행한 경우와 컨테이너 환경에서 실행한 경우를 비교하여 자원 소비 및 간섭과 관련된 컨테이너 환경의 오버헤드를 정량적으로 확인하였다. 분석 결과 딥러닝 워크로드의 컨테이너 실행 시 성능 병목을 초래하는 주요 원인으로 주기적인 스토리지 플러시 작업이 확인되었으며, 다중 테넌트 환경에서는 자원 경합으로 인해 이러한 문제가 더욱 심화됨을 확인하였다. 본 연구의 결과는 컨테이너 환경에서 딥러닝 워크로드를 효율적으로 실행하기 위한 클라우드 및 엣지 시스템 설계에 중요한 인사이트를 제공할 수 있을 것으로 기대된다."
        },
        {
          "rank": 7,
          "score": 0.7459378242492676,
          "doc_id": "JAKO202009135419341",
          "title": "딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출",
          "abstract": "많은 정보통신기술 기업들은 자체적으로 개발한 인공지능 기술을 오픈소스로 공개하였다. 예를 들어, 구글의 TensorFlow, 페이스북의 PyTorch, 마이크로소프트의 CNTK 등 여러 기업들은 자신들의 인공지능 기술들을 공개하고 있다. 이처럼 대중에게 딥러닝 오픈소스 소프트웨어를 공개함으로써 개발자 커뮤니티와의 관계와 인공지능 생태계를 강화하고, 사용자들의 실험, 적용, 개선을 얻을 수 있다. 이에 따라 머신러닝 분야는 급속히 성장하고 있고, 개발자들 또한 여러가지 학습 알고리즘을 재생산하여 각 영역에 활용하고 있다. 하지만 오픈소스 소프트웨어에 대한 다양한 분석들이 이루어진 데 반해, 실제 산업현장에서 딥러닝 오픈소스 소프트웨어를 개발하거나 활용하는데 유용한 연구 결과는 미흡한 실정이다. 따라서 본 연구에서는 딥러닝 프레임워크 사례연구를 통해 해당 프레임워크의 도입 전략을 도출하고자 한다. 기술-조직-환경 프레임워크를 기반으로 기존의 오픈 소스 소프트웨어 도입과 관련된 연구들을 리뷰하고, 이를 바탕으로 두 기업의 성공 사례와 한 기업의 실패 사례를 포함한 총 3 가지 기업의 도입 사례 분석을 통해 딥러닝 프레임워크 도입을 위한 중요한 5가지 성공 요인을 도출하였다: 팀 내 개발자의 지식과 전문성, 하드웨어(GPU) 환경, 데이터 전사 협력 체계, 딥러닝 프레임워크 플랫폼, 딥러닝 프레임워크 도구 서비스. 그리고 도출한 성공 요인을 실현하기 위한 딥러닝 프레임워크의 단계적 도입 전략을 제안하였다: 프로젝트 문제 정의, 딥러닝 방법론이 적합한 기법인지 확인, 딥러닝 프레임워크가 적합한 도구인지 확인, 기업의 딥러닝 프레임워크 사용, 기업의 딥러닝 프레임워크 확산. 본 연구를 통해 각 산업과 사업의 니즈에 따라, 딥러닝 프레임워크를 개발하거나 활용하고자 하는 기업에게 전략적인 시사점을 제공할 수 있을 것이라 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202009135419341&target=NART&cn=JAKO202009135419341",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출 딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출 딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출 많은 정보통신기술 기업들은 자체적으로 개발한 인공지능 기술을 오픈소스로 공개하였다. 예를 들어, 구글의 TensorFlow, 페이스북의 PyTorch, 마이크로소프트의 CNTK 등 여러 기업들은 자신들의 인공지능 기술들을 공개하고 있다. 이처럼 대중에게 딥러닝 오픈소스 소프트웨어를 공개함으로써 개발자 커뮤니티와의 관계와 인공지능 생태계를 강화하고, 사용자들의 실험, 적용, 개선을 얻을 수 있다. 이에 따라 머신러닝 분야는 급속히 성장하고 있고, 개발자들 또한 여러가지 학습 알고리즘을 재생산하여 각 영역에 활용하고 있다. 하지만 오픈소스 소프트웨어에 대한 다양한 분석들이 이루어진 데 반해, 실제 산업현장에서 딥러닝 오픈소스 소프트웨어를 개발하거나 활용하는데 유용한 연구 결과는 미흡한 실정이다. 따라서 본 연구에서는 딥러닝 프레임워크 사례연구를 통해 해당 프레임워크의 도입 전략을 도출하고자 한다. 기술-조직-환경 프레임워크를 기반으로 기존의 오픈 소스 소프트웨어 도입과 관련된 연구들을 리뷰하고, 이를 바탕으로 두 기업의 성공 사례와 한 기업의 실패 사례를 포함한 총 3 가지 기업의 도입 사례 분석을 통해 딥러닝 프레임워크 도입을 위한 중요한 5가지 성공 요인을 도출하였다: 팀 내 개발자의 지식과 전문성, 하드웨어(GPU) 환경, 데이터 전사 협력 체계, 딥러닝 프레임워크 플랫폼, 딥러닝 프레임워크 도구 서비스. 그리고 도출한 성공 요인을 실현하기 위한 딥러닝 프레임워크의 단계적 도입 전략을 제안하였다: 프로젝트 문제 정의, 딥러닝 방법론이 적합한 기법인지 확인, 딥러닝 프레임워크가 적합한 도구인지 확인, 기업의 딥러닝 프레임워크 사용, 기업의 딥러닝 프레임워크 확산. 본 연구를 통해 각 산업과 사업의 니즈에 따라, 딥러닝 프레임워크를 개발하거나 활용하고자 하는 기업에게 전략적인 시사점을 제공할 수 있을 것이라 기대된다."
        },
        {
          "rank": 8,
          "score": 0.734073281288147,
          "doc_id": "JAKO201719950757340",
          "title": "딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",
          "abstract": "딥러닝 프레임워크의 대표적인 기능으로는 '자동미분'과 'GPU의 활용' 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각 프레임워크의 실행속도에 대한 평가는 '큰 차이는 없다'는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만 빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데, 위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로 구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201719950757340&target=NART&cn=JAKO201719950757340",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 대표적인 기능으로는 '자동미분'과 'GPU의 활용' 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각 프레임워크의 실행속도에 대한 평가는 '큰 차이는 없다'는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만 빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데, 위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로 구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다."
        },
        {
          "rank": 9,
          "score": 0.7213565111160278,
          "doc_id": "JAKO202223540366088",
          "title": "이미지 학습을 위한 딥러닝 프레임워크 비교분석",
          "abstract": "딥러닝 프레임워크는 현재에도 계속해서 발전되어 가고 있으며, 다양한 프레임워크들이 존재한다. 딥러닝의 대표적인 프레임워크는 TensorFlow, PyTorch, Keras 등이 있다. 딥러님 프레임워크는 이미지 학습을 통해 이미지 분류에서의 최적화 모델을 이용한다. 본 논문에서는 딥러닝 이미지 인식 분야에서 가장 많이 사용하고 있는 TensorFlow와 PyTorch 프레임워크를 활용하여 이미지 학습을 진행하였으며, 이 과정에서 도출한 결과를 비교 분석하여 최적화된 프레임워크을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202223540366088&target=NART&cn=JAKO202223540366088",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "이미지 학습을 위한 딥러닝 프레임워크 비교분석 이미지 학습을 위한 딥러닝 프레임워크 비교분석 이미지 학습을 위한 딥러닝 프레임워크 비교분석 딥러닝 프레임워크는 현재에도 계속해서 발전되어 가고 있으며, 다양한 프레임워크들이 존재한다. 딥러닝의 대표적인 프레임워크는 TensorFlow, PyTorch, Keras 등이 있다. 딥러님 프레임워크는 이미지 학습을 통해 이미지 분류에서의 최적화 모델을 이용한다. 본 논문에서는 딥러닝 이미지 인식 분야에서 가장 많이 사용하고 있는 TensorFlow와 PyTorch 프레임워크를 활용하여 이미지 학습을 진행하였으며, 이 과정에서 도출한 결과를 비교 분석하여 최적화된 프레임워크을 알 수 있었다."
        },
        {
          "rank": 10,
          "score": 0.7109584212303162,
          "doc_id": "JAKO201722163438668",
          "title": "통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가",
          "abstract": "최근, 딥러닝을 사용 가능한 임베디드 디바이스가 상용화됨에 따라 임베디드 시스템 영역에서도 딥러닝 활용에 대한 다양한 연구가 진행되고 있다. 그러나 임베디드 시스템을 고성능 PC 환경과 비교하면 상대적으로 저사양의 CPU/GPU 프로세서와 메모리를 탑재하고 있으므로 딥러닝 기술의 적용에 있어서 많은 제약이 있다. 본 논문에서는 다양한 최신 딥러닝 네트워크들을 임베디드 디바이스에 적용했을때의 성능을 시간과 전력이라는 관점에서 실험적으로 평가한다. 또한, 호스트 CPU와 GPU 디바이스간의 메모리를 공유하는 임베디드 시스템들의 아키텍처적인 특성을 이용하여 메모리 복사를 줄임으로써 실시간 성능과 저전력성을 높이는 방법을 제시한다. 제안된 방법은 대표적인 공개 딥러닝 프레임워크인 Caffe를 수정하여 구현되었으며, 임베디드 GPU를 탑재한 NVIDIA Jetson TK1에서 성능평가 되었다. 실험결과, 대부분의 딥러닝 네트워크에서 뚜렷한 성능향상을 관찰할 수 있었다. 특히, 메모리 사용량이 높은 AlexNet에서 약 33%의 이미지 인식 속도 단축과 50%의 소비 전력량 감소를 관찰할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201722163438668&target=NART&cn=JAKO201722163438668",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 최근, 딥러닝을 사용 가능한 임베디드 디바이스가 상용화됨에 따라 임베디드 시스템 영역에서도 딥러닝 활용에 대한 다양한 연구가 진행되고 있다. 그러나 임베디드 시스템을 고성능 PC 환경과 비교하면 상대적으로 저사양의 CPU/GPU 프로세서와 메모리를 탑재하고 있으므로 딥러닝 기술의 적용에 있어서 많은 제약이 있다. 본 논문에서는 다양한 최신 딥러닝 네트워크들을 임베디드 디바이스에 적용했을때의 성능을 시간과 전력이라는 관점에서 실험적으로 평가한다. 또한, 호스트 CPU와 GPU 디바이스간의 메모리를 공유하는 임베디드 시스템들의 아키텍처적인 특성을 이용하여 메모리 복사를 줄임으로써 실시간 성능과 저전력성을 높이는 방법을 제시한다. 제안된 방법은 대표적인 공개 딥러닝 프레임워크인 Caffe를 수정하여 구현되었으며, 임베디드 GPU를 탑재한 NVIDIA Jetson TK1에서 성능평가 되었다. 실험결과, 대부분의 딥러닝 네트워크에서 뚜렷한 성능향상을 관찰할 수 있었다. 특히, 메모리 사용량이 높은 AlexNet에서 약 33%의 이미지 인식 속도 단축과 50%의 소비 전력량 감소를 관찰할 수 있었다."
        },
        {
          "rank": 11,
          "score": 0.7090919017791748,
          "doc_id": "NPAP12898051",
          "title": "딥러닝 프레임워크 비교 및 분석",
          "abstract": "딥러닝(Deep Learning)을 효과적으로 연구하고 개발할 수 있도록 도와주는 다양한 딥러닝 프레임워크(Deep Learning Framework)가 있다. 딥러닝 프레임워크는 현재 100 가지도 넘는 종류가 있다. 그렇기 때문에 개발의 목적에 가장 적합한 딥러닝 프레임워크를 선택하는 것은 쉽지 않다. 본고에서는 5가지 대표적인 딥러닝 프레임워크에 대해서 각각의 특징을 분석하고 비교한다. 이를 통하여 딥러닝을 개발하기 전에 개발 목적에 적합한 프레임워크를 선택할 수 있는 간단한 안목을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12898051&target=NART&cn=NPAP12898051",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 프레임워크 비교 및 분석 딥러닝 프레임워크 비교 및 분석 딥러닝 프레임워크 비교 및 분석 딥러닝(Deep Learning)을 효과적으로 연구하고 개발할 수 있도록 도와주는 다양한 딥러닝 프레임워크(Deep Learning Framework)가 있다. 딥러닝 프레임워크는 현재 100 가지도 넘는 종류가 있다. 그렇기 때문에 개발의 목적에 가장 적합한 딥러닝 프레임워크를 선택하는 것은 쉽지 않다. 본고에서는 5가지 대표적인 딥러닝 프레임워크에 대해서 각각의 특징을 분석하고 비교한다. 이를 통하여 딥러닝을 개발하기 전에 개발 목적에 적합한 프레임워크를 선택할 수 있는 간단한 안목을 제시한다."
        },
        {
          "rank": 12,
          "score": 0.7030766010284424,
          "doc_id": "JAKO201305262618384",
          "title": "오픈소스 클라우드 컴퓨팅 기반 교육 실습 시스템 구축",
          "abstract": "근래 이슈가 되고 있는 클라우드 컴퓨팅은 분산컴퓨팅 환경에서 사용자가 요구하는 컴퓨팅 자원을 최적화하여 유연하고 확장성 있게 지원할 수 있어 각광받는 새로운 패러다임이다. 클라우드 컴퓨팅 환경은 가상화 환경을 구성함으로써 실제적인 구현 및 서비스가 가능해진다. 본 논문에서는 오픈소스 기반의 클라우드 컴퓨팅을 연구하고 대학교 내에서 컴퓨터를 이용한 실습을 수행할 시 요구되는 시스템 환경을 오픈소스 클라우드 컴퓨팅 기반의 환경을 통해 구현하고자 한다. 클라우드 컴퓨팅을 통한 가상화 기반의 실습 환경은 최적화된 자원을 제공할 수 있을 뿐만 아니라 실습 자원 관리의 편리성, 실습 결과에 대한 손쉬운 관리 등의 효율성을 가져올 수 있다. 이로 인해 실습환경 설정에 소요되는 시간을 줄일 수 있을 뿐만 아니라, 교수 입장에서는 실습결과물들을 쉽게 관리 할 수 있게 된다. 또한 다양한 실습환경의 요구사항들을 유연성 있게 적용함으로써 시스템에 대한 활용성 또한 높아지게 된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201305262618384&target=NART&cn=JAKO201305262618384",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈소스 클라우드 컴퓨팅 기반 교육 실습 시스템 구축 오픈소스 클라우드 컴퓨팅 기반 교육 실습 시스템 구축 오픈소스 클라우드 컴퓨팅 기반 교육 실습 시스템 구축 근래 이슈가 되고 있는 클라우드 컴퓨팅은 분산컴퓨팅 환경에서 사용자가 요구하는 컴퓨팅 자원을 최적화하여 유연하고 확장성 있게 지원할 수 있어 각광받는 새로운 패러다임이다. 클라우드 컴퓨팅 환경은 가상화 환경을 구성함으로써 실제적인 구현 및 서비스가 가능해진다. 본 논문에서는 오픈소스 기반의 클라우드 컴퓨팅을 연구하고 대학교 내에서 컴퓨터를 이용한 실습을 수행할 시 요구되는 시스템 환경을 오픈소스 클라우드 컴퓨팅 기반의 환경을 통해 구현하고자 한다. 클라우드 컴퓨팅을 통한 가상화 기반의 실습 환경은 최적화된 자원을 제공할 수 있을 뿐만 아니라 실습 자원 관리의 편리성, 실습 결과에 대한 손쉬운 관리 등의 효율성을 가져올 수 있다. 이로 인해 실습환경 설정에 소요되는 시간을 줄일 수 있을 뿐만 아니라, 교수 입장에서는 실습결과물들을 쉽게 관리 할 수 있게 된다. 또한 다양한 실습환경의 요구사항들을 유연성 있게 적용함으로써 시스템에 대한 활용성 또한 높아지게 된다."
        },
        {
          "rank": 13,
          "score": 0.7021796703338623,
          "doc_id": "NART107287464",
          "title": "Benchmarking open source deep learning frameworks",
          "abstract": "<P>Deep Learning (DL) is one of the hottest fields. To foster the growth of DL, several open source frameworks appeared providing implementations of the most common DL algorithms. These frameworks vary in the algorithms they support and in the quality of their implementations. The purpose of this work is to provide a qualitative and quantitative comparison among three such frameworks: TensorFlow, Theano and CNTK. To ensure that our study is as comprehensive as possible, we consider multiple benchmark datasets from different fields (image processing, NLP, etc.) and measure the performance of the frameworks' implementations of different DL algorithms. For most of our experiments, we find out that CNTK's implementations are superior to the other ones under consideration.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART107287464&target=NART&cn=NART107287464",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Benchmarking open source deep learning frameworks Benchmarking open source deep learning frameworks Benchmarking open source deep learning frameworks <P>Deep Learning (DL) is one of the hottest fields. To foster the growth of DL, several open source frameworks appeared providing implementations of the most common DL algorithms. These frameworks vary in the algorithms they support and in the quality of their implementations. The purpose of this work is to provide a qualitative and quantitative comparison among three such frameworks: TensorFlow, Theano and CNTK. To ensure that our study is as comprehensive as possible, we consider multiple benchmark datasets from different fields (image processing, NLP, etc.) and measure the performance of the frameworks' implementations of different DL algorithms. For most of our experiments, we find out that CNTK's implementations are superior to the other ones under consideration.</P>"
        },
        {
          "rank": 14,
          "score": 0.69206702709198,
          "doc_id": "ATN0037463572",
          "title": "서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가",
          "abstract": "Serverless computing is a new computing paradigm which can develop and execute the application program without server management overhead. Recently, as the enterprise application architectures are evolved to container and micro-services, serverless-based cloud services make it easy to expand and distribute the micro-services. In this regard, we compared the technologies and performed an experiment to measure average response time and response success rate of distributed functions for the three typical serverless open source frameworks: OpenFaaS, Kubeless and Fission.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037463572&target=NART&cn=ATN0037463572",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 Serverless computing is a new computing paradigm which can develop and execute the application program without server management overhead. Recently, as the enterprise application architectures are evolved to container and micro-services, serverless-based cloud services make it easy to expand and distribute the micro-services. In this regard, we compared the technologies and performed an experiment to measure average response time and response success rate of distributed functions for the three typical serverless open source frameworks: OpenFaaS, Kubeless and Fission."
        },
        {
          "rank": 15,
          "score": 0.683761715888977,
          "doc_id": "DIKO0008948393",
          "title": "소프트웨어 컴포넌트 프레임워크 성능 비교",
          "abstract": "프레임워크는 포함하는 컴포넌트의 영역적 특성에 따라 시스템의 하부 구조를 중심으로 정의된 시스템 프레임워크와 특정 응용 영역을 위한 응용 애플리케이션 프레임워크로 나뉠 수 있다. 본 논문에서는 각각의 프레임워크 사용 제품에 대해 비교 분석하였다. 시스템 프레임워크는 시스템의 기본 성능인 '객체모델', '객체 버스', '언어의 독립성', '위치 투명성'등을 기준으로 비교 하였다. CORBA는 컴포넌트 간 상호 운용성이 뛰어나며, 다양한 서비스와 응용 컴포넌트를 지원한다. 이에 비하여 DCOM은 윈도우 사용자를 기반으로 하고 있으며 MTS를 기반으로 하는 분산 트랜젝션 기능을 지원한다. EJB는 구현언어의 독립성을 갖는 CORBA와 DCOM과 달리 자바라는 단일 언어를 기반으로 한다. 이로 인해 언어 독립적이지는 않지만 컨테이너 차원에서 데이터베이스, 트랜잭션, 보안문제, 데이터베이스의 커넥션 플링, 쓰레딩과 같은 기능을 제공한다 특정 비즈니스 영역을 위한 Steelpia, SanFrancisco의 경우에는 각 프레임워크의 재사용 컴포넌트 레이어에 대한 비교를 하였다 Steelpia는 철강 업무를 지원하며, 핵심 비즈니스 컴포넌트를 크기에 맞게 특화하여 기존의 개발 방식보다 30-50% 정도의 기간비용 효과를 얻을 수 있다. SanFrancisco 는 비즈니스 컴포넌트를 지원하여 특화함으로 애플리케이션 개발시 전체 노력의 40%정도의 시간적, 비용적 이익을 가져다 준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0008948393&target=NART&cn=DIKO0008948393",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "소프트웨어 컴포넌트 프레임워크 성능 비교 소프트웨어 컴포넌트 프레임워크 성능 비교 소프트웨어 컴포넌트 프레임워크 성능 비교 프레임워크는 포함하는 컴포넌트의 영역적 특성에 따라 시스템의 하부 구조를 중심으로 정의된 시스템 프레임워크와 특정 응용 영역을 위한 응용 애플리케이션 프레임워크로 나뉠 수 있다. 본 논문에서는 각각의 프레임워크 사용 제품에 대해 비교 분석하였다. 시스템 프레임워크는 시스템의 기본 성능인 '객체모델', '객체 버스', '언어의 독립성', '위치 투명성'등을 기준으로 비교 하였다. CORBA는 컴포넌트 간 상호 운용성이 뛰어나며, 다양한 서비스와 응용 컴포넌트를 지원한다. 이에 비하여 DCOM은 윈도우 사용자를 기반으로 하고 있으며 MTS를 기반으로 하는 분산 트랜젝션 기능을 지원한다. EJB는 구현언어의 독립성을 갖는 CORBA와 DCOM과 달리 자바라는 단일 언어를 기반으로 한다. 이로 인해 언어 독립적이지는 않지만 컨테이너 차원에서 데이터베이스, 트랜잭션, 보안문제, 데이터베이스의 커넥션 플링, 쓰레딩과 같은 기능을 제공한다 특정 비즈니스 영역을 위한 Steelpia, SanFrancisco의 경우에는 각 프레임워크의 재사용 컴포넌트 레이어에 대한 비교를 하였다 Steelpia는 철강 업무를 지원하며, 핵심 비즈니스 컴포넌트를 크기에 맞게 특화하여 기존의 개발 방식보다 30-50% 정도의 기간비용 효과를 얻을 수 있다. SanFrancisco 는 비즈니스 컴포넌트를 지원하여 특화함으로 애플리케이션 개발시 전체 노력의 40%정도의 시간적, 비용적 이익을 가져다 준다."
        },
        {
          "rank": 16,
          "score": 0.6779617071151733,
          "doc_id": "DIKO0009828582",
          "title": "오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석",
          "abstract": "객체 지향 기술과 관계형 기술은 대부분의 기업에서 어플리케이션을 개발할 때 공통적으로 사용되는 기술이다. 객체 지향 기술은 데이터와 행위를 가진 객체를 통해 어플리케이션 구축을 지원하며, 관계형 기술은 데이터 저장과 프로시저나 SQL를 통한 데이터 조작을 지원한다. 하지만 명확하게 두 기술은 서로 다르다. 이처럼 객체 기술과 관계형 기술을 같이 사용했을 매 발생하는 두 기술간의 불일치를 'object-relational impedance mismatch'라고 한다. 이러한 문제를 해결하기 위해 등장한 기술중의 하나가 바로 ORM(Object/Relational Mapping)이다. 본 논문에서는 ORM 기술을 지원하는 ORM 툴로서의 오픈 소스 자바 프레임 워크를 성능이나 코드 복잡성, 관리 용이성 등 다양한 측면에서 비교 분석하였다. 현재 30여가지가 넘는 다양한 오픈 소스 자바 프레임워크가 개발되어 배포되고 있지만, 본 논문에서는 Hibernate, iBatis SqlMaps, Apache OJB 이렇게 새 개의 프래임워크를 현재 객체 지속성을 위해 가장 많이 사용되는 JDBC 기술을 기준으로 비교 분석하였다. 데이터에 대한 CRUD(저장,추출,수정,삭제)를 수행하는 시간을 통해 성능 분석을 실시하였으며, 사례 어플리케이션 구현을 통해 각 프레임워크 별로 CRUD를 수행하는 메소드 구현 시 코드량 분석을 통해 코드 복잡성을, 요구 사항 변경 시 어떻게 각 프레임워크가 이를 반영하는지를 통해 관리 용이성을 분석하였다. 이러한 분석을 통해 각 프레임워크가 어떠한 서비스를 제공하는지, 각 프레임워크의 성능은 어떠한지 쉽게 알 수 있다. 따라서 기업은 좀더 명확한 근거를 통해 어플리케이션 개발에 적절한 퍼시스턴스 프레임워크를 선택할 수 있을 것이다. 또한 상용과 오픈 소스 기반의 프레임워크 중 어떠한 것을 도입해야 할지 결정해야 할 경우 중요한 참고 자료로 활용할 수 있을 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0009828582&target=NART&cn=DIKO0009828582",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 객체 지향 기술과 관계형 기술은 대부분의 기업에서 어플리케이션을 개발할 때 공통적으로 사용되는 기술이다. 객체 지향 기술은 데이터와 행위를 가진 객체를 통해 어플리케이션 구축을 지원하며, 관계형 기술은 데이터 저장과 프로시저나 SQL를 통한 데이터 조작을 지원한다. 하지만 명확하게 두 기술은 서로 다르다. 이처럼 객체 기술과 관계형 기술을 같이 사용했을 매 발생하는 두 기술간의 불일치를 'object-relational impedance mismatch'라고 한다. 이러한 문제를 해결하기 위해 등장한 기술중의 하나가 바로 ORM(Object/Relational Mapping)이다. 본 논문에서는 ORM 기술을 지원하는 ORM 툴로서의 오픈 소스 자바 프레임 워크를 성능이나 코드 복잡성, 관리 용이성 등 다양한 측면에서 비교 분석하였다. 현재 30여가지가 넘는 다양한 오픈 소스 자바 프레임워크가 개발되어 배포되고 있지만, 본 논문에서는 Hibernate, iBatis SqlMaps, Apache OJB 이렇게 새 개의 프래임워크를 현재 객체 지속성을 위해 가장 많이 사용되는 JDBC 기술을 기준으로 비교 분석하였다. 데이터에 대한 CRUD(저장,추출,수정,삭제)를 수행하는 시간을 통해 성능 분석을 실시하였으며, 사례 어플리케이션 구현을 통해 각 프레임워크 별로 CRUD를 수행하는 메소드 구현 시 코드량 분석을 통해 코드 복잡성을, 요구 사항 변경 시 어떻게 각 프레임워크가 이를 반영하는지를 통해 관리 용이성을 분석하였다. 이러한 분석을 통해 각 프레임워크가 어떠한 서비스를 제공하는지, 각 프레임워크의 성능은 어떠한지 쉽게 알 수 있다. 따라서 기업은 좀더 명확한 근거를 통해 어플리케이션 개발에 적절한 퍼시스턴스 프레임워크를 선택할 수 있을 것이다. 또한 상용과 오픈 소스 기반의 프레임워크 중 어떠한 것을 도입해야 할지 결정해야 할 경우 중요한 참고 자료로 활용할 수 있을 것이다."
        },
        {
          "rank": 17,
          "score": 0.6779579520225525,
          "doc_id": "NART66450565",
          "title": "Evaluating open-source cloud computing solutions for geosciences",
          "abstract": "Many organizations start to adopt cloud computing for better utilizing computing resources by taking advantage of its scalability, cost reduction, and easy to access characteristics. Many private or community cloud computing platforms are being built using open-source cloud solutions. However, little has been done to systematically compare and evaluate the features and performance of open-source solutions in supporting Geosciences. This paper provides a comprehensive study of three open-source cloud solutions, including OpenNebula, Eucalyptus, and CloudStack. We compared a variety of features, capabilities, technologies and performances including: (1) general features and supported services for cloud resource creation and management, (2) advanced capabilities for networking and security, and (3) the performance of the cloud solutions in provisioning and operating the cloud resources as well as the performance of virtual machines initiated and managed by the cloud solutions in supporting selected geoscience applications. Our study found that: (1) no significant performance differences in central processing unit (CPU), memory and I/O of virtual machines created and managed by different solutions, (2) OpenNebula has the fastest internal network while both Eucalyptus and CloudStack have better virtual machine isolation and security strategies, (3) Cloudstack has the fastest operations in handling virtual machines, images, snapshots, volumes and networking, followed by OpenNebula, and (4) the selected cloud computing solutions are capable for supporting concurrent intensive web applications, computing intensive applications, and small-scale model simulations without intensive data communication.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART66450565&target=NART&cn=NART66450565",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Evaluating open-source cloud computing solutions for geosciences Evaluating open-source cloud computing solutions for geosciences Evaluating open-source cloud computing solutions for geosciences Many organizations start to adopt cloud computing for better utilizing computing resources by taking advantage of its scalability, cost reduction, and easy to access characteristics. Many private or community cloud computing platforms are being built using open-source cloud solutions. However, little has been done to systematically compare and evaluate the features and performance of open-source solutions in supporting Geosciences. This paper provides a comprehensive study of three open-source cloud solutions, including OpenNebula, Eucalyptus, and CloudStack. We compared a variety of features, capabilities, technologies and performances including: (1) general features and supported services for cloud resource creation and management, (2) advanced capabilities for networking and security, and (3) the performance of the cloud solutions in provisioning and operating the cloud resources as well as the performance of virtual machines initiated and managed by the cloud solutions in supporting selected geoscience applications. Our study found that: (1) no significant performance differences in central processing unit (CPU), memory and I/O of virtual machines created and managed by different solutions, (2) OpenNebula has the fastest internal network while both Eucalyptus and CloudStack have better virtual machine isolation and security strategies, (3) Cloudstack has the fastest operations in handling virtual machines, images, snapshots, volumes and networking, followed by OpenNebula, and (4) the selected cloud computing solutions are capable for supporting concurrent intensive web applications, computing intensive applications, and small-scale model simulations without intensive data communication."
        },
        {
          "rank": 18,
          "score": 0.6766793727874756,
          "doc_id": "JAKO202322957802897",
          "title": "R과 텐서플로우 딥러닝 성능 비교",
          "abstract": "본 연구에서는 무료 딥러닝 도구인 R과 텐서플로우에 대한 성능 비교를 수행하였다. 실험에서는 각 도구를 사용하여 6종류의 심층 신경망을 구축하고 10년간의 한국 온도 데이터셋을 사용하여 신경망을 학습시켰다. 구축된 신경망의 입력층 노드 갯수는 10개, 출력층은 5개로 설정 하였으며, 은닉층은 5, 10, 20개로 설정하여 실험을 진행 하였다. 학습 데이터는 2013년 3월 1일부터 2023년 3월 29일까지 서울시 강남구에서 수집된 온도 데이터 3681건을 사용하였다. 성능 비교를 위해, 학습된 신경망을 사용하여, 5일간의 온도를 예측하고 예측된 값과 실제값을 사용하여 평균 제곱근 오차(root mean square error, RMSE)값을 측정하였다. 실험결과, 은닉층이 1개인 경우, R의 학습 오차는 0.04731176이었으며, 텐서플로우는 0.06677193으로 측정되었으며, 은닉층이 2개인 경우에는 R이 0.04782134, 텐서플로 우는 0.05799060로 측정되었다. 전체적으로 R이 더 우수한 성능을 보였다. 우리는 기계학습을 처음 접하는 사용자들에게 두 도구에 대한 정량적 성능 정보를 제공함으로써, 도구 선택에서 발생하는 어려움을 해소하고자 하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202322957802897&target=NART&cn=JAKO202322957802897",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "R과 텐서플로우 딥러닝 성능 비교 R과 텐서플로우 딥러닝 성능 비교 R과 텐서플로우 딥러닝 성능 비교 본 연구에서는 무료 딥러닝 도구인 R과 텐서플로우에 대한 성능 비교를 수행하였다. 실험에서는 각 도구를 사용하여 6종류의 심층 신경망을 구축하고 10년간의 한국 온도 데이터셋을 사용하여 신경망을 학습시켰다. 구축된 신경망의 입력층 노드 갯수는 10개, 출력층은 5개로 설정 하였으며, 은닉층은 5, 10, 20개로 설정하여 실험을 진행 하였다. 학습 데이터는 2013년 3월 1일부터 2023년 3월 29일까지 서울시 강남구에서 수집된 온도 데이터 3681건을 사용하였다. 성능 비교를 위해, 학습된 신경망을 사용하여, 5일간의 온도를 예측하고 예측된 값과 실제값을 사용하여 평균 제곱근 오차(root mean square error, RMSE)값을 측정하였다. 실험결과, 은닉층이 1개인 경우, R의 학습 오차는 0.04731176이었으며, 텐서플로우는 0.06677193으로 측정되었으며, 은닉층이 2개인 경우에는 R이 0.04782134, 텐서플로 우는 0.05799060로 측정되었다. 전체적으로 R이 더 우수한 성능을 보였다. 우리는 기계학습을 처음 접하는 사용자들에게 두 도구에 대한 정량적 성능 정보를 제공함으로써, 도구 선택에서 발생하는 어려움을 해소하고자 하였다."
        },
        {
          "rank": 19,
          "score": 0.6728557348251343,
          "doc_id": "NART104972735",
          "title": "PHDFS: Optimizing I/O performance of HDFS in deep learning cloud computing platform",
          "abstract": "<P><B>Abstract</B></P>  <P>For deep learning cloud computing platforms, file system is a fundamental and critical component. Hadoop distributed file system (HDFS) is widely used in large scale clusters due to its high performance and high availability. However, in deep learning datasets, the number of files is huge but the file size is small, making HDFS suffer a severe performance penalty. Although there have been many optimizing methods for addressing the <I>small file problem</I>, none of them take the file correlation in deep learning datasets into consideration. To address such problem, this paper proposes a <I>Pile</I>-HDFS (PHDFS) based on a new file aggregation approach. <I>Pile</I> is designed as the I/O unit merging a group of small files according to their correlation. In order to effectively access small files, we design a two-layer manager and add the inner organization information to data blocks. Experimental results demonstrate that, compared with the original HDFS, PHDFS can dramatically decrease the latency when accessing small files and improve the FPS (Frames Per Second) of typical deep learning models by 40%.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART104972735&target=NART&cn=NART104972735",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PHDFS: Optimizing I/O performance of HDFS in deep learning cloud computing platform PHDFS: Optimizing I/O performance of HDFS in deep learning cloud computing platform PHDFS: Optimizing I/O performance of HDFS in deep learning cloud computing platform <P><B>Abstract</B></P>  <P>For deep learning cloud computing platforms, file system is a fundamental and critical component. Hadoop distributed file system (HDFS) is widely used in large scale clusters due to its high performance and high availability. However, in deep learning datasets, the number of files is huge but the file size is small, making HDFS suffer a severe performance penalty. Although there have been many optimizing methods for addressing the <I>small file problem</I>, none of them take the file correlation in deep learning datasets into consideration. To address such problem, this paper proposes a <I>Pile</I>-HDFS (PHDFS) based on a new file aggregation approach. <I>Pile</I> is designed as the I/O unit merging a group of small files according to their correlation. In order to effectively access small files, we design a two-layer manager and add the inner organization information to data blocks. Experimental results demonstrate that, compared with the original HDFS, PHDFS can dramatically decrease the latency when accessing small files and improve the FPS (Frames Per Second) of typical deep learning models by 40%.</P>"
        },
        {
          "rank": 20,
          "score": 0.6656914949417114,
          "doc_id": "JAKO201306366998119",
          "title": "클라우드 컴퓨팅 정보보호 프레임워크에 관한 연구",
          "abstract": "탄력성(elasticity), 빠른 적용과 릴리즈, 광대역 네트워크 접속, 다중 접속(multi-tenancy), 활용에 제한이 없는(ubiquity) 유연성 등 클라우드 컴퓨팅의 고유한 속성들은 클라우드를 선택한 기업과 기관에게 획기적인 효율성을 제공하지만 원천적으로 내재된 보안 위협을 제거해야 하는 대책수립이 필요하다. 이를 위해 본 논문에서는 전략적 연계 모델을 참조하여 클라우드 컴퓨팅 정보보호 프레임워크를 제시하였다. 클라우드 컴퓨팅 정보보호 프레임워크는 클라우드 위협, 보안통제 활동, 클라우드 이해관계자를 중심 축으로 합목적성, 책임성, 투명한 책임소재의 벽면으로 구성된다. 중심 축은 클라우드 환경에서 정보보호 활동을 수행하는 주요 목적인 위협 최소화목표와 이해관계자를 지정하고 그들이 해야 할 정보보호 활동을 정의하고 있다. 또한, 3개 벽면은 클라우드 환경에서 정보보호 활동을 수행하기 위한 원칙이며 중심 축 간의 접점에서 7개 서비스 패키지 도출을 위한 방향을 제공한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201306366998119&target=NART&cn=JAKO201306366998119",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 컴퓨팅 정보보호 프레임워크에 관한 연구 클라우드 컴퓨팅 정보보호 프레임워크에 관한 연구 클라우드 컴퓨팅 정보보호 프레임워크에 관한 연구 탄력성(elasticity), 빠른 적용과 릴리즈, 광대역 네트워크 접속, 다중 접속(multi-tenancy), 활용에 제한이 없는(ubiquity) 유연성 등 클라우드 컴퓨팅의 고유한 속성들은 클라우드를 선택한 기업과 기관에게 획기적인 효율성을 제공하지만 원천적으로 내재된 보안 위협을 제거해야 하는 대책수립이 필요하다. 이를 위해 본 논문에서는 전략적 연계 모델을 참조하여 클라우드 컴퓨팅 정보보호 프레임워크를 제시하였다. 클라우드 컴퓨팅 정보보호 프레임워크는 클라우드 위협, 보안통제 활동, 클라우드 이해관계자를 중심 축으로 합목적성, 책임성, 투명한 책임소재의 벽면으로 구성된다. 중심 축은 클라우드 환경에서 정보보호 활동을 수행하는 주요 목적인 위협 최소화목표와 이해관계자를 지정하고 그들이 해야 할 정보보호 활동을 정의하고 있다. 또한, 3개 벽면은 클라우드 환경에서 정보보호 활동을 수행하기 위한 원칙이며 중심 축 간의 접점에서 7개 서비스 패키지 도출을 위한 방향을 제공한다."
        },
        {
          "rank": 21,
          "score": 0.6641640067100525,
          "doc_id": "JAKO201607457888322",
          "title": "멀티사이트 기반 클라우드 환경의 구성 자동화를 위한 SmartX 프로비저닝 프레임워크",
          "abstract": "다양한 ICT 인프라 기술들을 종합적으로 활용하는 클라우드의 대표적인 오픈소스 프로젝트인 오픈스택을 활용하여 멀티사이트 기반의 클라우드 인프라 구축 시 설치 복잡성 및 지리적인 제약으로 인한 인적, 시간적인 비효율성을 내포한다. 이러한 비효율성을 해소하기 위해 멀티사이트 환경의 오픈스택 실증 테스트베드인 OF@KOREN SmartX 놀이터 (Playground)를 대상으로 리눅스 및 오픈스택을 설치/설정을 자동화하는 도구를 데브옵스 (DevOps) 개발 방법론에 따라 점진적으로 개발해왔다. 하지만 이전 개발도구들이 고정된 형태의 프로비저닝만을 제공한다는 한계를 해결하고자 본 논문에서는 소프트웨어 정의 인프라의 자원관리 구도에 따라 자동 설치/설정도구를 엮어 Playground 수준의 프로비저닝을 수행하는 SmartX 프로비저닝 프레임워크의 프로토타입을 설계, 개발한다. 그리고 멀티사이트 오픈스택 클라우드를 자동으로 구축하는 과정을 제시함으로써 프레임워크의 효율적인 놀이터 프로비저닝 기능에 대해 검증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201607457888322&target=NART&cn=JAKO201607457888322",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "멀티사이트 기반 클라우드 환경의 구성 자동화를 위한 SmartX 프로비저닝 프레임워크 멀티사이트 기반 클라우드 환경의 구성 자동화를 위한 SmartX 프로비저닝 프레임워크 멀티사이트 기반 클라우드 환경의 구성 자동화를 위한 SmartX 프로비저닝 프레임워크 다양한 ICT 인프라 기술들을 종합적으로 활용하는 클라우드의 대표적인 오픈소스 프로젝트인 오픈스택을 활용하여 멀티사이트 기반의 클라우드 인프라 구축 시 설치 복잡성 및 지리적인 제약으로 인한 인적, 시간적인 비효율성을 내포한다. 이러한 비효율성을 해소하기 위해 멀티사이트 환경의 오픈스택 실증 테스트베드인 OF@KOREN SmartX 놀이터 (Playground)를 대상으로 리눅스 및 오픈스택을 설치/설정을 자동화하는 도구를 데브옵스 (DevOps) 개발 방법론에 따라 점진적으로 개발해왔다. 하지만 이전 개발도구들이 고정된 형태의 프로비저닝만을 제공한다는 한계를 해결하고자 본 논문에서는 소프트웨어 정의 인프라의 자원관리 구도에 따라 자동 설치/설정도구를 엮어 Playground 수준의 프로비저닝을 수행하는 SmartX 프로비저닝 프레임워크의 프로토타입을 설계, 개발한다. 그리고 멀티사이트 오픈스택 클라우드를 자동으로 구축하는 과정을 제시함으로써 프레임워크의 효율적인 놀이터 프로비저닝 기능에 대해 검증한다."
        },
        {
          "rank": 22,
          "score": 0.6627283692359924,
          "doc_id": "NART110448816",
          "title": "PERFORMANCE ANALYSIS OF OPEN SOURCE STORAGE CLOUDS IN CLOUD COMPUTING",
          "abstract": "<P>Cloud computing is one of the latest research area that helps in storing the information permanently on the servers and manages the different resources for the requested users to provide on-demand services. In order to create the more usable and economic value based cloud computing, the principles, goals and structure of the cloud engineering is of vital importance. The objective of this study is to analyze the CPU and memory performance of different open source clouds. We will use different open source cloud to measure the different performance metrics like CPU time for downloading and uploading of file, memory usage while downloading and uploading the file, standard deviation of CPU usage and standard deviation of memory usage.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART110448816&target=NART&cn=NART110448816",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PERFORMANCE ANALYSIS OF OPEN SOURCE STORAGE CLOUDS IN CLOUD COMPUTING PERFORMANCE ANALYSIS OF OPEN SOURCE STORAGE CLOUDS IN CLOUD COMPUTING PERFORMANCE ANALYSIS OF OPEN SOURCE STORAGE CLOUDS IN CLOUD COMPUTING <P>Cloud computing is one of the latest research area that helps in storing the information permanently on the servers and manages the different resources for the requested users to provide on-demand services. In order to create the more usable and economic value based cloud computing, the principles, goals and structure of the cloud engineering is of vital importance. The objective of this study is to analyze the CPU and memory performance of different open source clouds. We will use different open source cloud to measure the different performance metrics like CPU time for downloading and uploading of file, memory usage while downloading and uploading the file, standard deviation of CPU usage and standard deviation of memory usage.</P>"
        },
        {
          "rank": 23,
          "score": 0.6604024767875671,
          "doc_id": "JAKO202433861648179",
          "title": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교",
          "abstract": "본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202433861648179&target=NART&cn=JAKO202433861648179",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다."
        },
        {
          "rank": 24,
          "score": 0.6600748896598816,
          "doc_id": "JAKO201310635656332",
          "title": "클라우드 환경을 고려한 디지털 포렌식 프레임워크",
          "abstract": "최근 세계적인 경제위기 속에서 국내외의 기업들이 IT 투자를 보류하거나 예산을 대폭 삭감하고 있다. 이에 기업들은 IT 부문에 있어서의 비용 절감을 통한 위기 극복 방안을 모색하고 있으며, 이러한 상황에서 클라우드 컴퓨팅(Cloud Computing)은 위기 극복을 위한 최적의 솔루션으로 빠르게 부상하고 있다. 또한 디지털 포렌식 조사과정에서 조사 대상 시스템의 사용자가 클라우드 서비스를 사용했는지 여부는 추가적은 조사 대상의 선정에 매우 중요한 요소이다. Daum Cloud, Google Docs와 같은 클라우드 서비스를 사용하였을 경우, 로그인 정보를 획득하여 원격지의 클라우드 서비스에 접속이 가능한 경우가 있다. 이러한 경우에는 원격지의 증거 데이터를 수집할 수 있는다. 따라서 다양한 클라우드 서비스에서 데이터를 수집하고 분석하는 방안에 대하여 연구가 필요하다. 이에 본 연구에서는 서비스별 데이터 수집 및 분석 기법에 대해 연구하여 클라우드 환경을 고려한 디지털 포렌식 프레임워크를 제안한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201310635656332&target=NART&cn=JAKO201310635656332",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 환경을 고려한 디지털 포렌식 프레임워크 클라우드 환경을 고려한 디지털 포렌식 프레임워크 클라우드 환경을 고려한 디지털 포렌식 프레임워크 최근 세계적인 경제위기 속에서 국내외의 기업들이 IT 투자를 보류하거나 예산을 대폭 삭감하고 있다. 이에 기업들은 IT 부문에 있어서의 비용 절감을 통한 위기 극복 방안을 모색하고 있으며, 이러한 상황에서 클라우드 컴퓨팅(Cloud Computing)은 위기 극복을 위한 최적의 솔루션으로 빠르게 부상하고 있다. 또한 디지털 포렌식 조사과정에서 조사 대상 시스템의 사용자가 클라우드 서비스를 사용했는지 여부는 추가적은 조사 대상의 선정에 매우 중요한 요소이다. Daum Cloud, Google Docs와 같은 클라우드 서비스를 사용하였을 경우, 로그인 정보를 획득하여 원격지의 클라우드 서비스에 접속이 가능한 경우가 있다. 이러한 경우에는 원격지의 증거 데이터를 수집할 수 있는다. 따라서 다양한 클라우드 서비스에서 데이터를 수집하고 분석하는 방안에 대하여 연구가 필요하다. 이에 본 연구에서는 서비스별 데이터 수집 및 분석 기법에 대해 연구하여 클라우드 환경을 고려한 디지털 포렌식 프레임워크를 제안한다."
        },
        {
          "rank": 25,
          "score": 0.6600184440612793,
          "doc_id": "JAKO202125240402705",
          "title": "흉부 X-ray 기반 딥 러닝 손실함수 성능 비교&#x00B7;분석",
          "abstract": "4차 산업의 발전과 고성능의 컴퓨팅 환경 구축으로 다양한 산업분야에서 인공지능이 적용되고 있다. 의료분야에서는 X-Ray, MRI, PET 등의 의료 영상 및 임상 자료를 이용하여 암, COVID-19, 골 연령 측정 등의 딥 러닝 학습이 진행되었다. 또한 스마트 의료기기, IoT 디바이스와 딥 러닝 알고리즘을 적용하여 ICT 의료 융합 기술 등이 연구되고 있다. 이러한 기술 중 의료 영상 기반 딥 러닝 학습은 의료 영상의 바이오마커를 정확히 찾아내고, 최소한의 손실률과 높은 정확도가 필요하다. 따라서 본 논문은 흉부 X-Ray 이미지 기반 딥 러닝 학습 과정에서 손실률을 도출하는 손실 함수 중 영상분류 알고리즘에서 사용되는 Cross-Entropy 함수들의 성능을 비교&#x00B7;분석하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202125240402705&target=NART&cn=JAKO202125240402705",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "흉부 X-ray 기반 딥 러닝 손실함수 성능 비교&#x00B7;분석 흉부 X-ray 기반 딥 러닝 손실함수 성능 비교&#x00B7;분석 흉부 X-ray 기반 딥 러닝 손실함수 성능 비교&#x00B7;분석 4차 산업의 발전과 고성능의 컴퓨팅 환경 구축으로 다양한 산업분야에서 인공지능이 적용되고 있다. 의료분야에서는 X-Ray, MRI, PET 등의 의료 영상 및 임상 자료를 이용하여 암, COVID-19, 골 연령 측정 등의 딥 러닝 학습이 진행되었다. 또한 스마트 의료기기, IoT 디바이스와 딥 러닝 알고리즘을 적용하여 ICT 의료 융합 기술 등이 연구되고 있다. 이러한 기술 중 의료 영상 기반 딥 러닝 학습은 의료 영상의 바이오마커를 정확히 찾아내고, 최소한의 손실률과 높은 정확도가 필요하다. 따라서 본 논문은 흉부 X-Ray 이미지 기반 딥 러닝 학습 과정에서 손실률을 도출하는 손실 함수 중 영상분류 알고리즘에서 사용되는 Cross-Entropy 함수들의 성능을 비교&#x00B7;분석하고자 한다."
        },
        {
          "rank": 26,
          "score": 0.6520923376083374,
          "doc_id": "DIKO0012654277",
          "title": "클라우드 컴퓨팅을 이용한 DFSaaS 프레임워크 연구",
          "abstract": "디지털 포렌식이란 과학적이거나 기술적인 기법을 사용하여 범죄수사 또는 증거를 수집하는 행위이이다. 이를 위한 도구로 디지털 포렌식 도구가 존재한다. 이 도구는 법과 기술 간의 매개체가 될 수 있는 핵심 요소라 할 수 있다[1]. 디지털 포렌식은 증거 이미징, 분석, 검색, 보고서 작성 등의 일련의 절차를 요구한다. 기존의 디지털 포렌식 도구는 이러한 절차적 기능을 제공하는 것을 목적으로 개발되었다. 현존하는 대부분의 포렌식 도구들은 단일 플랫폼 상의 윈도우 운영체제에서 운용되는 통합 도구로 제공된다. 이동성을 위해 휴대형 하드디스크 드라이브에 저장되며 해당 매체 내에서 실행된다. 목적에 따라 전용 기능을 제공하는 하드웨어 형태의 도구로도 제작 된다. 추가적으로 압수한 데스크탑을 포렌식 연구실로 이동하기 위해서 특수한 장치가 필요할 수 있다. 현재 단일 플랫폼 형태의 포렌식 도구에서 2TB의 데이터를 이미징 하는데 7시간이 걸리며 비트와이즈 검색을 20MB/s 정도의 속도로 처리해 1TB 이미지를 검색 했을 때에는 14시간이 소요된다[2]. 이는 양적으로 증가하는 디지털 증거의 추세에 미루어 볼 때 향후 도구의 증거 처리 속도 문제를 야기 시킬 것이다. 또, 증거물을 포렌식 연구실로 이송을 하거나 고속 처리를 하기위해서는 기타 하드웨어 장치를 이용해야 하고, 이는 도구 사용에 있어 불필요한 번거로움을 초래한다. 따라서 기존의 도구를 아우를 수 있는 새로운 도구 개발이 시급하다. 프로세싱 속도 향상을 위해 단일 플랫폼의 한계를 극복해야하고, 이용의 번거로움을 피하기 위해 모든 장비들을 하나의 도구로 합쳐야 한다. 기존의 도구를 기능을 아우를 수 있고, 단점을 제거하기위해 클라우드 컴퓨팅 개념을 적용해 해결 방안을 모색하였다. 본 논문에서는 클라우드 컴퓨팅[3]을 이용하여 디지털 포렌식 절차에 따라 어디에서든 포렌식을 수행 할 수 있는 DFSaaS(Digital Forensic Software as a Service) 구조와 이에 대한 시나리오를 제시하고, 이들에 대한 프레임워크를 연구한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0012654277&target=NART&cn=DIKO0012654277",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 컴퓨팅을 이용한 DFSaaS 프레임워크 연구 클라우드 컴퓨팅을 이용한 DFSaaS 프레임워크 연구 클라우드 컴퓨팅을 이용한 DFSaaS 프레임워크 연구 디지털 포렌식이란 과학적이거나 기술적인 기법을 사용하여 범죄수사 또는 증거를 수집하는 행위이이다. 이를 위한 도구로 디지털 포렌식 도구가 존재한다. 이 도구는 법과 기술 간의 매개체가 될 수 있는 핵심 요소라 할 수 있다[1]. 디지털 포렌식은 증거 이미징, 분석, 검색, 보고서 작성 등의 일련의 절차를 요구한다. 기존의 디지털 포렌식 도구는 이러한 절차적 기능을 제공하는 것을 목적으로 개발되었다. 현존하는 대부분의 포렌식 도구들은 단일 플랫폼 상의 윈도우 운영체제에서 운용되는 통합 도구로 제공된다. 이동성을 위해 휴대형 하드디스크 드라이브에 저장되며 해당 매체 내에서 실행된다. 목적에 따라 전용 기능을 제공하는 하드웨어 형태의 도구로도 제작 된다. 추가적으로 압수한 데스크탑을 포렌식 연구실로 이동하기 위해서 특수한 장치가 필요할 수 있다. 현재 단일 플랫폼 형태의 포렌식 도구에서 2TB의 데이터를 이미징 하는데 7시간이 걸리며 비트와이즈 검색을 20MB/s 정도의 속도로 처리해 1TB 이미지를 검색 했을 때에는 14시간이 소요된다[2]. 이는 양적으로 증가하는 디지털 증거의 추세에 미루어 볼 때 향후 도구의 증거 처리 속도 문제를 야기 시킬 것이다. 또, 증거물을 포렌식 연구실로 이송을 하거나 고속 처리를 하기위해서는 기타 하드웨어 장치를 이용해야 하고, 이는 도구 사용에 있어 불필요한 번거로움을 초래한다. 따라서 기존의 도구를 아우를 수 있는 새로운 도구 개발이 시급하다. 프로세싱 속도 향상을 위해 단일 플랫폼의 한계를 극복해야하고, 이용의 번거로움을 피하기 위해 모든 장비들을 하나의 도구로 합쳐야 한다. 기존의 도구를 기능을 아우를 수 있고, 단점을 제거하기위해 클라우드 컴퓨팅 개념을 적용해 해결 방안을 모색하였다. 본 논문에서는 클라우드 컴퓨팅[3]을 이용하여 디지털 포렌식 절차에 따라 어디에서든 포렌식을 수행 할 수 있는 DFSaaS(Digital Forensic Software as a Service) 구조와 이에 대한 시나리오를 제시하고, 이들에 대한 프레임워크를 연구한다."
        },
        {
          "rank": 27,
          "score": 0.6509741544723511,
          "doc_id": "NPAP10408992",
          "title": "Open Source Cloud Computing Platforms",
          "abstract": "<P>With the popularization of cloud computing, several enterprises and open-source communities have developed their own cloud solutions. A number of factors weigh on user selection, as each one has peculiar characteristics and may target different usage scenarios. Considering such challenge, this paper focuses on giving the reader an understanding of some major existing open cloud computing solutions - XCP, Eucalyptus and Open Nebula. Hopefully, a deep comparison of such solutions can leverage the cloud computing research area providing a good starting point to research groups and interested readers.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP10408992&target=NART&cn=NPAP10408992",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Open Source Cloud Computing Platforms Open Source Cloud Computing Platforms Open Source Cloud Computing Platforms <P>With the popularization of cloud computing, several enterprises and open-source communities have developed their own cloud solutions. A number of factors weigh on user selection, as each one has peculiar characteristics and may target different usage scenarios. Considering such challenge, this paper focuses on giving the reader an understanding of some major existing open cloud computing solutions - XCP, Eucalyptus and Open Nebula. Hopefully, a deep comparison of such solutions can leverage the cloud computing research area providing a good starting point to research groups and interested readers.</P>"
        },
        {
          "rank": 28,
          "score": 0.6471286416053772,
          "doc_id": "DIKO0014041997",
          "title": "클라우드 서비스 품질 측정 시스템 프레임워크 개발",
          "abstract": "클라우드 서비스는 IT 자원 운영관리 및 에너지 소비 효율성 극대화를 위한 그린 IT의 차세대 핵심 서비스이다. 클라우드 서비스는 이용자 중심의 편리성 제공과, IT 인프라의 운영관리 효율화, 산업 간의 융합 등을 통해 높은 파급효과가 기대되는 분야로 지속적으로 시장규모가 성장할 것으로 전망되고 있다. 그럼에도 불구하고, ICT 이용자들은 클라우드 서비스 도입 및 이용 시, 보안과 비용 이외에도 통제권 상실, 신뢰성, 성능 등에 대한 불확실성 등을 고려사항으로 꼽고 있다. 본 연구에서는 이용자에게 클라우드 서비스에 대한 객관적이고 정량적인 평가 결과를 제공하여 요구에 맞는 클라우드 서비스를 신뢰하고 선택할 수 있는 기준을 제공하기 위한 클라우드 서비스 성능 평가를 위한 모델링과, 모델링을 기반으로 한 성능 평가 시스템을 제안한다. 특히, 본 연구에서는 다양한 클라우드 서비스 분야 중 인프라 서비스(IaaS)를 대상으로 가상머신(VM)의 성능 평가를 위한 범위를 정의하고, 요구사항을 분석하여 성능 평가 항목을 도출한다. 제안한 시스템에서는 성능 평가 대상을 시스템 성능, 네트워크 성능, 서비스 성능, 가용성, 보안 성능 5개 항목으로 모듈화하여 구성한다. 또한 이를 바탕으로 가상머신(VM)의 성능 평가를 위한 시스템을 개발하여 클라우드 인프라 서비스의 성능 관리에 활용 할 수 있도록 도움을 주고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014041997&target=NART&cn=DIKO0014041997",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 서비스 품질 측정 시스템 프레임워크 개발 클라우드 서비스 품질 측정 시스템 프레임워크 개발 클라우드 서비스 품질 측정 시스템 프레임워크 개발 클라우드 서비스는 IT 자원 운영관리 및 에너지 소비 효율성 극대화를 위한 그린 IT의 차세대 핵심 서비스이다. 클라우드 서비스는 이용자 중심의 편리성 제공과, IT 인프라의 운영관리 효율화, 산업 간의 융합 등을 통해 높은 파급효과가 기대되는 분야로 지속적으로 시장규모가 성장할 것으로 전망되고 있다. 그럼에도 불구하고, ICT 이용자들은 클라우드 서비스 도입 및 이용 시, 보안과 비용 이외에도 통제권 상실, 신뢰성, 성능 등에 대한 불확실성 등을 고려사항으로 꼽고 있다. 본 연구에서는 이용자에게 클라우드 서비스에 대한 객관적이고 정량적인 평가 결과를 제공하여 요구에 맞는 클라우드 서비스를 신뢰하고 선택할 수 있는 기준을 제공하기 위한 클라우드 서비스 성능 평가를 위한 모델링과, 모델링을 기반으로 한 성능 평가 시스템을 제안한다. 특히, 본 연구에서는 다양한 클라우드 서비스 분야 중 인프라 서비스(IaaS)를 대상으로 가상머신(VM)의 성능 평가를 위한 범위를 정의하고, 요구사항을 분석하여 성능 평가 항목을 도출한다. 제안한 시스템에서는 성능 평가 대상을 시스템 성능, 네트워크 성능, 서비스 성능, 가용성, 보안 성능 5개 항목으로 모듈화하여 구성한다. 또한 이를 바탕으로 가상머신(VM)의 성능 평가를 위한 시스템을 개발하여 클라우드 인프라 서비스의 성능 관리에 활용 할 수 있도록 도움을 주고자 한다."
        },
        {
          "rank": 29,
          "score": 0.6444386839866638,
          "doc_id": "JAKO201810760745496",
          "title": "PaaS 클라우드 컴퓨팅 환경에서 전자정부 표준프레임워크 성능평가: 공간영상 정보처리 사례",
          "abstract": "클라우드 컴퓨팅 서비스 모델 중 하나인 PaaS(Platform as a Service)와 행정안전부 전자정부 표준프레임워크는 모두 웹 서비스 개발자가 개발 목적을 만족하는 핵심 기능 개발에 집중할 수 있도록 기본 공통자원을 제공하기 위한 컴퓨팅 환경이다. 웹 기반 공간정보 서비스를 구축하는 경우에도 이러한 기술들을 사용하면 미들웨어 소프트웨어 또는 플랫폼 공통 기능들을 바로 활용할 수 있다. 그러나 공간정보 서비스 개발 분야에서 이러한 기반 기술들의 적용 가능성을 검토하거나 적용 시스템의 성능을 평가한 연구는 아직 발표된 사례가 없다. 따라서 이번 연구에서는 공간정보 서비스에 대한 적용 가능성을 살펴보고자 성능평가 실험을 수행하였다. 실험 대상 시스템은 OGC WPS 2.0 표준을 적용한 공간영상 정보처리 서비스를 대상으로 하였다. 실험 시스템은 Cloud Foundry 오픈소스를 이용한 PaaS 환경을 구축한 뒤 전자정부 표준프레임워크를 적용한 웹 서비스로 설계, 구축하였다. 실제 성능 평가실험은 영상처리 기능을 PaaS 클라우드 환경에서 구동하는 경우와 PaaS에 전자정부 표준프레임워크를 같이 적용한 경우를 비교하는 방식으로 수행하였다. 실험 조건은 300명과 500명에 해당하는 동시 사용자가 3,600초 동안 이 시스템에 접속하여 정보처리를 요청하고 실험 환경으로 구축한 웹 서비스가 이에 대하여 응답하는 시간을 기록하는 방식으로 하였다. 성능 측정 결과 PaaS 클라우드 환경에서 전자정부 표준프레임워크를 기반으로 시스템 구축하였을 때 성능이 우수한 것을 확인할 수 있었다. 앞으로 공공 부분의 웹 기반 공간정보 응용 서비스 구축에서 PaaS 클라우드 컴퓨팅과 전자정부 표준프레임워크가 중요한 요소가 될 것으로 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201810760745496&target=NART&cn=JAKO201810760745496",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PaaS 클라우드 컴퓨팅 환경에서 전자정부 표준프레임워크 성능평가: 공간영상 정보처리 사례 PaaS 클라우드 컴퓨팅 환경에서 전자정부 표준프레임워크 성능평가: 공간영상 정보처리 사례 PaaS 클라우드 컴퓨팅 환경에서 전자정부 표준프레임워크 성능평가: 공간영상 정보처리 사례 클라우드 컴퓨팅 서비스 모델 중 하나인 PaaS(Platform as a Service)와 행정안전부 전자정부 표준프레임워크는 모두 웹 서비스 개발자가 개발 목적을 만족하는 핵심 기능 개발에 집중할 수 있도록 기본 공통자원을 제공하기 위한 컴퓨팅 환경이다. 웹 기반 공간정보 서비스를 구축하는 경우에도 이러한 기술들을 사용하면 미들웨어 소프트웨어 또는 플랫폼 공통 기능들을 바로 활용할 수 있다. 그러나 공간정보 서비스 개발 분야에서 이러한 기반 기술들의 적용 가능성을 검토하거나 적용 시스템의 성능을 평가한 연구는 아직 발표된 사례가 없다. 따라서 이번 연구에서는 공간정보 서비스에 대한 적용 가능성을 살펴보고자 성능평가 실험을 수행하였다. 실험 대상 시스템은 OGC WPS 2.0 표준을 적용한 공간영상 정보처리 서비스를 대상으로 하였다. 실험 시스템은 Cloud Foundry 오픈소스를 이용한 PaaS 환경을 구축한 뒤 전자정부 표준프레임워크를 적용한 웹 서비스로 설계, 구축하였다. 실제 성능 평가실험은 영상처리 기능을 PaaS 클라우드 환경에서 구동하는 경우와 PaaS에 전자정부 표준프레임워크를 같이 적용한 경우를 비교하는 방식으로 수행하였다. 실험 조건은 300명과 500명에 해당하는 동시 사용자가 3,600초 동안 이 시스템에 접속하여 정보처리를 요청하고 실험 환경으로 구축한 웹 서비스가 이에 대하여 응답하는 시간을 기록하는 방식으로 하였다. 성능 측정 결과 PaaS 클라우드 환경에서 전자정부 표준프레임워크를 기반으로 시스템 구축하였을 때 성능이 우수한 것을 확인할 수 있었다. 앞으로 공공 부분의 웹 기반 공간정보 응용 서비스 구축에서 PaaS 클라우드 컴퓨팅과 전자정부 표준프레임워크가 중요한 요소가 될 것으로 기대한다."
        },
        {
          "rank": 30,
          "score": 0.6435556411743164,
          "doc_id": "JAKO202013261023095",
          "title": "딥러닝을 위한 경사하강법 비교",
          "abstract": "본 논문에서는 신경망을 학습하는 데 가장 많이 사용되고 있는 경사하강법에 대해 분석하였다. 학습이란 손실함수가 최소값이 되도록 매개변수를 갱신하는 것이다. 손실함수는 실제값과 예측값의 차이를 수치화 해주는 함수이다. 경사하강법은 오차가 최소화되도록 매개변수를 갱신하는데 손실함수의 기울기를 사용하는 것으로 현재 최고의 딥러닝 학습알고리즘을 제공하는 라이브러리에서 사용되고 있다. 그러나 이 알고리즘들은 블랙박스형태로 제공되고 있어서 다양한 경사하강법들의 장단점을 파악하는 것이 쉽지 않다. 경사하강법에서 현재 대표적으로 사용되고 있는 확률적 경사하강법(Stochastic Gradient Descent method), 모멘텀법(Momentum method), AdaGrad법 그리고 Adadelta법의 특성에 대하여 분석하였다. 실험 데이터는 신경망을 검증하는 데 널리 사용되는 MNIST 데이터 셋을 사용하였다. 은닉층은 2개의 층으로 첫 번째 층은 500개 그리고 두 번째 층은 300개의 뉴런으로 구성하였다. 출력 층의 활성화함수는 소프트 맥스함수이고 나머지 입력 층과 은닉 층의 활성화함수는 ReLu함수를 사용하였다. 그리고 손실함수는 교차 엔트로피 오차를 사용하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202013261023095&target=NART&cn=JAKO202013261023095",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 위한 경사하강법 비교 딥러닝을 위한 경사하강법 비교 딥러닝을 위한 경사하강법 비교 본 논문에서는 신경망을 학습하는 데 가장 많이 사용되고 있는 경사하강법에 대해 분석하였다. 학습이란 손실함수가 최소값이 되도록 매개변수를 갱신하는 것이다. 손실함수는 실제값과 예측값의 차이를 수치화 해주는 함수이다. 경사하강법은 오차가 최소화되도록 매개변수를 갱신하는데 손실함수의 기울기를 사용하는 것으로 현재 최고의 딥러닝 학습알고리즘을 제공하는 라이브러리에서 사용되고 있다. 그러나 이 알고리즘들은 블랙박스형태로 제공되고 있어서 다양한 경사하강법들의 장단점을 파악하는 것이 쉽지 않다. 경사하강법에서 현재 대표적으로 사용되고 있는 확률적 경사하강법(Stochastic Gradient Descent method), 모멘텀법(Momentum method), AdaGrad법 그리고 Adadelta법의 특성에 대하여 분석하였다. 실험 데이터는 신경망을 검증하는 데 널리 사용되는 MNIST 데이터 셋을 사용하였다. 은닉층은 2개의 층으로 첫 번째 층은 500개 그리고 두 번째 층은 300개의 뉴런으로 구성하였다. 출력 층의 활성화함수는 소프트 맥스함수이고 나머지 입력 층과 은닉 층의 활성화함수는 ReLu함수를 사용하였다. 그리고 손실함수는 교차 엔트로피 오차를 사용하였다."
        },
        {
          "rank": 31,
          "score": 0.642951250076294,
          "doc_id": "JAKO202404861562091",
          "title": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교",
          "abstract": "대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202404861562091&target=NART&cn=JAKO202404861562091",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다."
        },
        {
          "rank": 32,
          "score": 0.6420772075653076,
          "doc_id": "JAKO201905653788881",
          "title": "실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안",
          "abstract": "오늘날의 기술 환경에서 대다수의 빅 데이터 기반 애플리케이션 및 솔루션은 스트리밍 데이터의 실시간 처리를 기반으로 한다. 빅 데이터 스트림의 실시간 처리 및 분석은 빅 데이터 기반 애플리케이션 및 솔루션 개발에서 중요한 역할을 한다. 특히 해사 분야 데이터 처리 환경에서도 데이터의 폭발적 증대에 따른 대용량 실시간 데이터를 빠르게 처리 및 분석할 수 있는 기술 개발의 필요성이 가속화되고 있다. 따라서 본 논문에서는 다양한 빅 데이터 처리를 위한 오픈소스 기술 중에 적합한 오픈소스로 NiFi, Kafka, Druid의 특징을 분석하여 한국형 e-Navigation 서비스에서 해사 분야 서비스 분석에 필요한 외부 연계 필요 정보들을 상시 최신 정보로 제공할 수 있도록 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 기술 적용의 기초를 마련하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201905653788881&target=NART&cn=JAKO201905653788881",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안 오늘날의 기술 환경에서 대다수의 빅 데이터 기반 애플리케이션 및 솔루션은 스트리밍 데이터의 실시간 처리를 기반으로 한다. 빅 데이터 스트림의 실시간 처리 및 분석은 빅 데이터 기반 애플리케이션 및 솔루션 개발에서 중요한 역할을 한다. 특히 해사 분야 데이터 처리 환경에서도 데이터의 폭발적 증대에 따른 대용량 실시간 데이터를 빠르게 처리 및 분석할 수 있는 기술 개발의 필요성이 가속화되고 있다. 따라서 본 논문에서는 다양한 빅 데이터 처리를 위한 오픈소스 기술 중에 적합한 오픈소스로 NiFi, Kafka, Druid의 특징을 분석하여 한국형 e-Navigation 서비스에서 해사 분야 서비스 분석에 필요한 외부 연계 필요 정보들을 상시 최신 정보로 제공할 수 있도록 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 기술 적용의 기초를 마련하고자 한다."
        },
        {
          "rank": 33,
          "score": 0.6420661211013794,
          "doc_id": "ATN0038661375",
          "title": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험",
          "abstract": "Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0038661375&target=NART&cn=ATN0038661375",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model."
        },
        {
          "rank": 34,
          "score": 0.6414116024971008,
          "doc_id": "JAKO201129362563090",
          "title": "스타 스키마 조인 처리에 대한 세로-지향 데이터베이스 시스템과 가로-지향 데이터베이스 시스템의 성능 비교",
          "abstract": "세로-지향 데이터베이스 시스템은 기존의 가로-지향 데이터베이스 시스템과 달리 데이터를 가로(row) 위주가 아닌 세로(column) 위주로 저장한다. 최근에는 데이터 웨어하우스나 의사 결정 시스템 같은 대용량 데이터를 갖는 읽기 위주의 응용들에서 세로-지향데이터베이스의 우수성이 관찰되었다. 본 논문에서는 세로-지향데이터베이스에서의 조인 전략을 구체적으로 분석하고 데이터 웨어하우스 시스템에서 세로-지향 데이터베이스의 우수성을 검증하고자 한다. 두 시스템간의 객관적인 비교를 위해 데이터 웨어하우스 분석 모델인 스타 스키마 벤치마크를 통해 스타스키마조인 질의에 대한 성능분석을 실시하고자 한다. 또한 세로-지향 데이터베이스의 조인 전략으로 조기 실체화(early materialization)와 지연 실체화(late materialization)를 고려하였다. 성능 분석을 통해 스타 스키마 조인 질의처리에 있어 가로-지향 시스템보다는 세로-지향 시스템에서 디스크 I/O 비용이 더 효율적인 결과를 확인할 수 있었다. 세로-지향 데이터베이스 시스템 측면에서는 조기 실체화보다는 지연 실체화 조인전략이 훨씬 우수한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201129362563090&target=NART&cn=JAKO201129362563090",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스타 스키마 조인 처리에 대한 세로-지향 데이터베이스 시스템과 가로-지향 데이터베이스 시스템의 성능 비교 스타 스키마 조인 처리에 대한 세로-지향 데이터베이스 시스템과 가로-지향 데이터베이스 시스템의 성능 비교 스타 스키마 조인 처리에 대한 세로-지향 데이터베이스 시스템과 가로-지향 데이터베이스 시스템의 성능 비교 세로-지향 데이터베이스 시스템은 기존의 가로-지향 데이터베이스 시스템과 달리 데이터를 가로(row) 위주가 아닌 세로(column) 위주로 저장한다. 최근에는 데이터 웨어하우스나 의사 결정 시스템 같은 대용량 데이터를 갖는 읽기 위주의 응용들에서 세로-지향데이터베이스의 우수성이 관찰되었다. 본 논문에서는 세로-지향데이터베이스에서의 조인 전략을 구체적으로 분석하고 데이터 웨어하우스 시스템에서 세로-지향 데이터베이스의 우수성을 검증하고자 한다. 두 시스템간의 객관적인 비교를 위해 데이터 웨어하우스 분석 모델인 스타 스키마 벤치마크를 통해 스타스키마조인 질의에 대한 성능분석을 실시하고자 한다. 또한 세로-지향 데이터베이스의 조인 전략으로 조기 실체화(early materialization)와 지연 실체화(late materialization)를 고려하였다. 성능 분석을 통해 스타 스키마 조인 질의처리에 있어 가로-지향 시스템보다는 세로-지향 시스템에서 디스크 I/O 비용이 더 효율적인 결과를 확인할 수 있었다. 세로-지향 데이터베이스 시스템 측면에서는 조기 실체화보다는 지연 실체화 조인전략이 훨씬 우수한 성능을 보였다."
        },
        {
          "rank": 35,
          "score": 0.6388970613479614,
          "doc_id": "NART133017760",
          "title": "Leveraging Machine and Deep Learning Models for Load Balancing Strategies in Cloud Computing",
          "abstract": "<P>Objectives: To evaluate the efficiency of task prediction and resource allocation for load balancing (LB) in the cloud environment using the combined approach like random Forest(RF) for task prediction and Particle Swarm optimization for optimization and Convolutional Neural Networks (PSO-CNN) for resource prediction and allocation. Methods: The ensemble approach in the present study uses Random Forest (RF), a machine learning (ML) model for task prediction and Particle Swarm Optimization (PSO+CNN), a bio-inspired algorithm and Deep Learning (DL) model for optimization and resource allocation. The study employs PSO techniques to optimize CNN in order to address the investigation of algorithmic optimization in DL. The results show that the suggested model outperforms the other models like CNN-LSTM(Long Short-term memory), CNN-GRU(Gated Recurrent Unit), and PSO -SVM(Support Vector Machine) to increase the performance and efficacy of the cloud systems. The experiment is implemented using Python and assessed using Google Cluster dataset that is accessible to the public. Findings: The use of ML and DL techniques are found to be more efficient in cloud infrastructure than the conventional methods. The study examines the performance of the RF, PSO and CNN and the hybrid RF-PSO-CNN models. The accuracy, precision, and F1. Score metrics were used to assess the performance of the classification models. The recommended model RF-PSO-CNN outperforms them with an accuracy of 90% than the contrasted methods like CNN-LSTM, CNN- GRU and PSO-SVM. As a result, both the classification assessment metrics and resource consumption show that the proposed model performs effectively. Novelty: The novel ensemble approach suggests the combined RF-PSO-CNN for LB in cloud Computing. The task predicted by RF is assigned to the resource chosen by PSO and CNN, thereby improving the efficiency of task prediction and resource allocation. Most of the research uses any two ML or DL methods for either predicting the tasks to be scheduled or which resource to allocate. The study uses a combination of the ML (RF) method, bio-inspired algorithm (PSO) and a DL (CNN) model for both task and resource prediction concurrently and it examines the effectiveness of LB in the cloud context. Keywords: Load Balancing (LB), Task scheduling, Resource allocation, Random Forest (RF), Convolutional Neural Networks (CNN), Particle Swarm Optimization (PSO)</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART133017760&target=NART&cn=NART133017760",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Leveraging Machine and Deep Learning Models for Load Balancing Strategies in Cloud Computing Leveraging Machine and Deep Learning Models for Load Balancing Strategies in Cloud Computing Leveraging Machine and Deep Learning Models for Load Balancing Strategies in Cloud Computing <P>Objectives: To evaluate the efficiency of task prediction and resource allocation for load balancing (LB) in the cloud environment using the combined approach like random Forest(RF) for task prediction and Particle Swarm optimization for optimization and Convolutional Neural Networks (PSO-CNN) for resource prediction and allocation. Methods: The ensemble approach in the present study uses Random Forest (RF), a machine learning (ML) model for task prediction and Particle Swarm Optimization (PSO+CNN), a bio-inspired algorithm and Deep Learning (DL) model for optimization and resource allocation. The study employs PSO techniques to optimize CNN in order to address the investigation of algorithmic optimization in DL. The results show that the suggested model outperforms the other models like CNN-LSTM(Long Short-term memory), CNN-GRU(Gated Recurrent Unit), and PSO -SVM(Support Vector Machine) to increase the performance and efficacy of the cloud systems. The experiment is implemented using Python and assessed using Google Cluster dataset that is accessible to the public. Findings: The use of ML and DL techniques are found to be more efficient in cloud infrastructure than the conventional methods. The study examines the performance of the RF, PSO and CNN and the hybrid RF-PSO-CNN models. The accuracy, precision, and F1. Score metrics were used to assess the performance of the classification models. The recommended model RF-PSO-CNN outperforms them with an accuracy of 90% than the contrasted methods like CNN-LSTM, CNN- GRU and PSO-SVM. As a result, both the classification assessment metrics and resource consumption show that the proposed model performs effectively. Novelty: The novel ensemble approach suggests the combined RF-PSO-CNN for LB in cloud Computing. The task predicted by RF is assigned to the resource chosen by PSO and CNN, thereby improving the efficiency of task prediction and resource allocation. Most of the research uses any two ML or DL methods for either predicting the tasks to be scheduled or which resource to allocate. The study uses a combination of the ML (RF) method, bio-inspired algorithm (PSO) and a DL (CNN) model for both task and resource prediction concurrently and it examines the effectiveness of LB in the cloud context. Keywords: Load Balancing (LB), Task scheduling, Resource allocation, Random Forest (RF), Convolutional Neural Networks (CNN), Particle Swarm Optimization (PSO)</P>"
        },
        {
          "rank": 36,
          "score": 0.6379081010818481,
          "doc_id": "JAKO201610235349520",
          "title": "오픈소스 하드웨어에서 효율적인 임베디드 소프트웨어 개발을 위한 프레임워크",
          "abstract": "무선인터넷이 보급되고 IoT 기술이 발달함에 따라 여러 종류의 센서 디바이스가 발전하였다. 그리고 IoT 환경에서 사용자들의 요구를 충족하는 다양한 서비스 개발을 위해 오픈소스 하드웨어가 도입되었다. 하지만 오픈소스 하드웨어는 개발 인력의 부족으로 인해 충분히 활용되지 못하고 있다. 따라서 본 논문에서는 오픈소스 하드웨어에서 효율적으로 임베디드 소프트웨어 개발을 교육하기 위한 소프트웨어 프레임워크를 제안한다. 제안하는 프레임워크는 비주얼 프로그래밍 언어와 빠른 결과 확인을 통해 다양한 오픈소스 하드웨어에서 빠르고 직관적으로 임베디드 소프트웨어를 개발할 수 있게 한다. 또한 제안한 프레임워크를 실제 오픈소스 하드웨어 개발 환경에 구현하여 장단점을 분석하고 개선방안을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201610235349520&target=NART&cn=JAKO201610235349520",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈소스 하드웨어에서 효율적인 임베디드 소프트웨어 개발을 위한 프레임워크 오픈소스 하드웨어에서 효율적인 임베디드 소프트웨어 개발을 위한 프레임워크 오픈소스 하드웨어에서 효율적인 임베디드 소프트웨어 개발을 위한 프레임워크 무선인터넷이 보급되고 IoT 기술이 발달함에 따라 여러 종류의 센서 디바이스가 발전하였다. 그리고 IoT 환경에서 사용자들의 요구를 충족하는 다양한 서비스 개발을 위해 오픈소스 하드웨어가 도입되었다. 하지만 오픈소스 하드웨어는 개발 인력의 부족으로 인해 충분히 활용되지 못하고 있다. 따라서 본 논문에서는 오픈소스 하드웨어에서 효율적으로 임베디드 소프트웨어 개발을 교육하기 위한 소프트웨어 프레임워크를 제안한다. 제안하는 프레임워크는 비주얼 프로그래밍 언어와 빠른 결과 확인을 통해 다양한 오픈소스 하드웨어에서 빠르고 직관적으로 임베디드 소프트웨어를 개발할 수 있게 한다. 또한 제안한 프레임워크를 실제 오픈소스 하드웨어 개발 환경에 구현하여 장단점을 분석하고 개선방안을 확인하였다."
        },
        {
          "rank": 37,
          "score": 0.6374354362487793,
          "doc_id": "JAKO201823952432020",
          "title": "모바일 클라우드 컴퓨팅을 위한 예측 기반 동적 컴포넌트 오프로딩 프레임워크",
          "abstract": "모바일 디바이스의 보편적인 보급으로 인하여 모바일 컴퓨팅은 사용자들의 일상 생활에 편리를 가져다 주는 컴퓨팅 패러다임으로 되었다. 다양한 타입의 모바일 애플리케이션의 출현으로 인하여 사용자들은 언제 어디서나 자신의 스케줄 관리 등 다양한 업무 수행이 가능해졌지만 모바일 디바이스의 리소스 제한적인 문제로 인하여 일정 수준의 컴퓨팅 작업만 수행 가능하고 비교적 큰 컴퓨팅 작업을 수행하기에는 불편한 점이 존재한다. 클라우드 컴퓨팅 연구에서는 제한된 모바일 디바이스의 자원을 해결하기 위하여 기능 컴포넌트를 다른 서버 노드로 오프로딩(Offloading) 시킴으로써, 모바일 노드의 자원 문제를 해결하는 솔루션을 제공하였다. 그러나, 현재 진행되고 있는 동적 오프로딩 기법에 관한 연구는 개념적인 수준의 기법만 제시되고 있다. 본 논문에서는 모바일 노드의 성능을 보장하기 위한 예측 기반 동적 오프로딩 기법 및 프레임워크 설계 모델을 제안한다. 그리고 제안한 예측 기반 오프로딩 기법의 유효성 검증을 위한 실험 및 평가를 수행한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201823952432020&target=NART&cn=JAKO201823952432020",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "모바일 클라우드 컴퓨팅을 위한 예측 기반 동적 컴포넌트 오프로딩 프레임워크 모바일 클라우드 컴퓨팅을 위한 예측 기반 동적 컴포넌트 오프로딩 프레임워크 모바일 클라우드 컴퓨팅을 위한 예측 기반 동적 컴포넌트 오프로딩 프레임워크 모바일 디바이스의 보편적인 보급으로 인하여 모바일 컴퓨팅은 사용자들의 일상 생활에 편리를 가져다 주는 컴퓨팅 패러다임으로 되었다. 다양한 타입의 모바일 애플리케이션의 출현으로 인하여 사용자들은 언제 어디서나 자신의 스케줄 관리 등 다양한 업무 수행이 가능해졌지만 모바일 디바이스의 리소스 제한적인 문제로 인하여 일정 수준의 컴퓨팅 작업만 수행 가능하고 비교적 큰 컴퓨팅 작업을 수행하기에는 불편한 점이 존재한다. 클라우드 컴퓨팅 연구에서는 제한된 모바일 디바이스의 자원을 해결하기 위하여 기능 컴포넌트를 다른 서버 노드로 오프로딩(Offloading) 시킴으로써, 모바일 노드의 자원 문제를 해결하는 솔루션을 제공하였다. 그러나, 현재 진행되고 있는 동적 오프로딩 기법에 관한 연구는 개념적인 수준의 기법만 제시되고 있다. 본 논문에서는 모바일 노드의 성능을 보장하기 위한 예측 기반 동적 오프로딩 기법 및 프레임워크 설계 모델을 제안한다. 그리고 제안한 예측 기반 오프로딩 기법의 유효성 검증을 위한 실험 및 평가를 수행한다."
        },
        {
          "rank": 38,
          "score": 0.6363178491592407,
          "doc_id": "JAKO201723840540692",
          "title": "빅데이터 통합모형 비교분석",
          "abstract": "빅데이터가 4차 산업혁명의 핵심으로 자리하면서 빅데이터 기반 처리 및 분석 능력이 기업의 미래 경쟁력을 좌우할 전망이다. 빅데이터 처리 및 분석을 위한 RHadoop과 RHIPE 모형은 R과 Hadoop의 통합모형으로 지금까지 각각의 모형에 대해서는 연구가 많이 진행되어 왔으나 두 모형간 비교 연구는 거의 이루어 지지 않았다. 본 논문에서는 대용량의 실제 데이터와 모의실험 데이터에서 다중 회귀 (multiple regression)와 로지스틱 회귀 (logistic regression) 추정을 위한 머신러닝 (machine learning) 알고리즘을 MapReduce 프로그램 구현을 통해 RHadoop과 RHIPE 간의 비교 분석하고자 한다. 구축된 분산 클러스터 (distributed cluster) 하에서 두 모형간 성능 실험 결과, RHIPE은 RHadoop에 비해 대체로 빠른 처리속도를 보인 반면에 설치, 사용면에서 어려움을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201723840540692&target=NART&cn=JAKO201723840540692",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "빅데이터 통합모형 비교분석 빅데이터 통합모형 비교분석 빅데이터 통합모형 비교분석 빅데이터가 4차 산업혁명의 핵심으로 자리하면서 빅데이터 기반 처리 및 분석 능력이 기업의 미래 경쟁력을 좌우할 전망이다. 빅데이터 처리 및 분석을 위한 RHadoop과 RHIPE 모형은 R과 Hadoop의 통합모형으로 지금까지 각각의 모형에 대해서는 연구가 많이 진행되어 왔으나 두 모형간 비교 연구는 거의 이루어 지지 않았다. 본 논문에서는 대용량의 실제 데이터와 모의실험 데이터에서 다중 회귀 (multiple regression)와 로지스틱 회귀 (logistic regression) 추정을 위한 머신러닝 (machine learning) 알고리즘을 MapReduce 프로그램 구현을 통해 RHadoop과 RHIPE 간의 비교 분석하고자 한다. 구축된 분산 클러스터 (distributed cluster) 하에서 두 모형간 성능 실험 결과, RHIPE은 RHadoop에 비해 대체로 빠른 처리속도를 보인 반면에 설치, 사용면에서 어려움을 보였다."
        },
        {
          "rank": 39,
          "score": 0.6352596282958984,
          "doc_id": "NART95625496",
          "title": "Open source column : deep learning with Keras",
          "abstract": "<P>Following the last column on MatConvNet, let us continue to look at open source frameworks for deep learning. In this column we are going to check Keras, a Python API that allows to use several different backends like Tensorflow and CNTK. Actually, it also supports Theano, although the development of this framework has been halted by the original developers in 2017.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART95625496&target=NART&cn=NART95625496",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Open source column : deep learning with Keras Open source column : deep learning with Keras Open source column : deep learning with Keras <P>Following the last column on MatConvNet, let us continue to look at open source frameworks for deep learning. In this column we are going to check Keras, a Python API that allows to use several different backends like Tensorflow and CNTK. Actually, it also supports Theano, although the development of this framework has been halted by the original developers in 2017.</P>"
        },
        {
          "rank": 40,
          "score": 0.6349667310714722,
          "doc_id": "NART76077564",
          "title": "Performance analysis and framework optimization of open source cloud storage system",
          "abstract": "<P>More and more embedded devices, such as mobile phones, tablet PCs and laptops, are used in every field, so huge files need to be stored or backed up into cloud storage. Optimizing the performance of cloud storage is very important for Internet development. This paper presents the performance evaluation of the open source distributed storage system, a highly available, distributed, eventually consistent object/blob store from OpenStack cloud computing components. This paper mainly focuses on the mechanism of cloud storage as well as the optimization methods to process different sized files. This work provides two major contributions through comprehensive performance evaluations. First, it provides different configurations for OpenStack Swift system and an analysis of how every component affects the performance. Second, it presents the detailed optimization methods to improve the performance in processing different sized files. The experimental results show that our method improves the performance and the structure. We give the methods to optimize the object-based cloud storage system to deploy the readily available storage system.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART76077564&target=NART&cn=NART76077564",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Performance analysis and framework optimization of open source cloud storage system Performance analysis and framework optimization of open source cloud storage system Performance analysis and framework optimization of open source cloud storage system <P>More and more embedded devices, such as mobile phones, tablet PCs and laptops, are used in every field, so huge files need to be stored or backed up into cloud storage. Optimizing the performance of cloud storage is very important for Internet development. This paper presents the performance evaluation of the open source distributed storage system, a highly available, distributed, eventually consistent object/blob store from OpenStack cloud computing components. This paper mainly focuses on the mechanism of cloud storage as well as the optimization methods to process different sized files. This work provides two major contributions through comprehensive performance evaluations. First, it provides different configurations for OpenStack Swift system and an analysis of how every component affects the performance. Second, it presents the detailed optimization methods to improve the performance in processing different sized files. The experimental results show that our method improves the performance and the structure. We give the methods to optimize the object-based cloud storage system to deploy the readily available storage system.</P>"
        },
        {
          "rank": 41,
          "score": 0.6344510316848755,
          "doc_id": "ART003222390",
          "title": "Comparison of CNN-based deep learning architectures for unsteady CFD acceleration on small datasets",
          "abstract": "CFD acceleration for virtual nuclear reactors or digital twin technology is a primary goal in the nuclear industry.This study compares advanced convolutional neural network (CNN) architectures for accelerating unsteady computational fluid dynamics (CFD) simulations using small datasets based on a challenging natural convection flow dataset. The advanced architectures such as autoencoders, UNet, and ConvLSTM-UNet, were evaluated under identical conditions to determine their predictive accuracy and robustness in autoregressive time-series predictions. ConvLSTM-UNet consistently outperformed other models, particularly in difference value calculation, achieving lower maximum errors and stable residuals. However, error accumulation remains a challenge, limiting reliable predictions to approximately 10 timesteps. This highlights the need for enhanced strategies to improve long-term prediction stability. The novelty of this work lies in its fair comparison of state-of-the-art CNN models within the RePIT framework, demonstrating their potential for accelerating CFD simulations while identifying limitations under small data conditions. Future research will focus on exploring alternative models, such as graph neural networks and implicit neural representations. These efforts aim to develop a robust hybrid approach for long-term unsteady CFD acceleration, contributing to practical applications in virtual nuclear reactors.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART003222390&target=NART&cn=ART003222390",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Comparison of CNN-based deep learning architectures for unsteady CFD acceleration on small datasets Comparison of CNN-based deep learning architectures for unsteady CFD acceleration on small datasets Comparison of CNN-based deep learning architectures for unsteady CFD acceleration on small datasets CFD acceleration for virtual nuclear reactors or digital twin technology is a primary goal in the nuclear industry.This study compares advanced convolutional neural network (CNN) architectures for accelerating unsteady computational fluid dynamics (CFD) simulations using small datasets based on a challenging natural convection flow dataset. The advanced architectures such as autoencoders, UNet, and ConvLSTM-UNet, were evaluated under identical conditions to determine their predictive accuracy and robustness in autoregressive time-series predictions. ConvLSTM-UNet consistently outperformed other models, particularly in difference value calculation, achieving lower maximum errors and stable residuals. However, error accumulation remains a challenge, limiting reliable predictions to approximately 10 timesteps. This highlights the need for enhanced strategies to improve long-term prediction stability. The novelty of this work lies in its fair comparison of state-of-the-art CNN models within the RePIT framework, demonstrating their potential for accelerating CFD simulations while identifying limitations under small data conditions. Future research will focus on exploring alternative models, such as graph neural networks and implicit neural representations. These efforts aim to develop a robust hybrid approach for long-term unsteady CFD acceleration, contributing to practical applications in virtual nuclear reactors."
        },
        {
          "rank": 42,
          "score": 0.6344249248504639,
          "doc_id": "DIKO0014861002",
          "title": "딥 러닝기반 고객평점 예측모델",
          "abstract": "인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014861002&target=NART&cn=DIKO0014861002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다."
        },
        {
          "rank": 43,
          "score": 0.6329221725463867,
          "doc_id": "JAKO202201253148351",
          "title": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰",
          "abstract": "위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202201253148351&target=NART&cn=JAKO202201253148351",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다."
        },
        {
          "rank": 44,
          "score": 0.6327913999557495,
          "doc_id": "NART96288640",
          "title": "Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms",
          "abstract": "<P><B>Abstract</B></P>  <P>One of the primary goals of the artificial intelligence field is to produce fully autonomous agents that interact with theirenvironments to learn optimal behaviors, improving over time through trial and error. A mathematical principled framework for experience-driven autonomous learning is reinforcement learning, but they are inherently limited to low-dimensional problems,but the deep learning boom has provided new tools to overcome these problems. For deep reinforcement learning teaching, we do not have an appropriate platform for making optimal labs. In the article, after studying the theoretical foundations and the requirements of the main platforms, we selected two open source platforms, according to their characteristics: robotic simulators platforms for teaching and benchmarking deep reinforcement learning algorithms. The first platform was <I>Gym and V-REP</I> and the second one, <I>KNIME Deeplearning4J Integration supports and Teaching-Box.</I> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART96288640&target=NART&cn=NART96288640",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms <P><B>Abstract</B></P>  <P>One of the primary goals of the artificial intelligence field is to produce fully autonomous agents that interact with theirenvironments to learn optimal behaviors, improving over time through trial and error. A mathematical principled framework for experience-driven autonomous learning is reinforcement learning, but they are inherently limited to low-dimensional problems,but the deep learning boom has provided new tools to overcome these problems. For deep reinforcement learning teaching, we do not have an appropriate platform for making optimal labs. In the article, after studying the theoretical foundations and the requirements of the main platforms, we selected two open source platforms, according to their characteristics: robotic simulators platforms for teaching and benchmarking deep reinforcement learning algorithms. The first platform was <I>Gym and V-REP</I> and the second one, <I>KNIME Deeplearning4J Integration supports and Teaching-Box.</I> </P>"
        },
        {
          "rank": 45,
          "score": 0.6326336860656738,
          "doc_id": "NART84800827",
          "title": "Smart in-car camera system using mobile cloud computing framework for deep learning",
          "abstract": "<P><B>Abstract</B></P>  <P>Deep learning is becoming a popular technology in various applications, such as image recognition, gaming, information retrieval, for intelligent data processing. However, huge amount of data and complex computations prevent deep learning from being practical on mobile devices. In this paper, we designed a smart in-car camera system that utilizes mobile cloud computing framework for deep learning. The smart in-car camera can detect objects in recorded videos during driving, and can decide which part of videos needs to be stored in cloud platforms to save local storage space. The system puts the training process and model repository in cloud platforms, and the recognition process and data gathering in mobile devices. The mobile side is implemented in NVIDIA Jetson TK1, and the communication is carried out via Git protocol to ensure the success of data transmission in unstable network environments. Experimental results show that detection rate can achieve up to four frame-per-second with Faster R-CNN, and the system can work well even when the network connection is unstable. We also compared the performance of system with and without GPU, and found that GPU still plays a critical role in the recognition side for deep learning.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART84800827&target=NART&cn=NART84800827",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Smart in-car camera system using mobile cloud computing framework for deep learning Smart in-car camera system using mobile cloud computing framework for deep learning Smart in-car camera system using mobile cloud computing framework for deep learning <P><B>Abstract</B></P>  <P>Deep learning is becoming a popular technology in various applications, such as image recognition, gaming, information retrieval, for intelligent data processing. However, huge amount of data and complex computations prevent deep learning from being practical on mobile devices. In this paper, we designed a smart in-car camera system that utilizes mobile cloud computing framework for deep learning. The smart in-car camera can detect objects in recorded videos during driving, and can decide which part of videos needs to be stored in cloud platforms to save local storage space. The system puts the training process and model repository in cloud platforms, and the recognition process and data gathering in mobile devices. The mobile side is implemented in NVIDIA Jetson TK1, and the communication is carried out via Git protocol to ensure the success of data transmission in unstable network environments. Experimental results show that detection rate can achieve up to four frame-per-second with Faster R-CNN, and the system can work well even when the network connection is unstable. We also compared the performance of system with and without GPU, and found that GPU still plays a critical role in the recognition side for deep learning.</P>"
        },
        {
          "rank": 46,
          "score": 0.6324717998504639,
          "doc_id": "JAKO202116047225054",
          "title": "신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어",
          "abstract": "최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형 교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은 고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를 분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에 적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은 내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기 위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와 더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의 정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116047225054&target=NART&cn=JAKO202116047225054",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어 신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어 신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어 최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형 교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은 고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를 분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에 적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은 내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기 위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와 더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의 정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다."
        },
        {
          "rank": 47,
          "score": 0.6319684982299805,
          "doc_id": "ATN0037504188",
          "title": "오픈소스 기반 서버리스 프레임워크에서의 서비스 메시 구조 및 서비스 성능 평가",
          "abstract": "Service mesh is a technology that enables service calls between internal services in a micro service architecture, and is widely used in a serverless framework that implements the functions of a micro service architecture in a cloud environment. However, the increase in service calls between internal services has a problem of delaying the overall service response speed, so frequent service calls within the framework should be avoided. Therefore, in this paper, a method of using service mesh without deteriorating the overall performance of the serverless framework was proposed, and for this purpose, the performance and function of the implemented service mesh was verified using OpenFx, an open source based serverless framework applying gRPC. This will be helpful to service developers for service distribution and management targeting serverless frameworks.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037504188&target=NART&cn=ATN0037504188",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈소스 기반 서버리스 프레임워크에서의 서비스 메시 구조 및 서비스 성능 평가 오픈소스 기반 서버리스 프레임워크에서의 서비스 메시 구조 및 서비스 성능 평가 오픈소스 기반 서버리스 프레임워크에서의 서비스 메시 구조 및 서비스 성능 평가 Service mesh is a technology that enables service calls between internal services in a micro service architecture, and is widely used in a serverless framework that implements the functions of a micro service architecture in a cloud environment. However, the increase in service calls between internal services has a problem of delaying the overall service response speed, so frequent service calls within the framework should be avoided. Therefore, in this paper, a method of using service mesh without deteriorating the overall performance of the serverless framework was proposed, and for this purpose, the performance and function of the implemented service mesh was verified using OpenFx, an open source based serverless framework applying gRPC. This will be helpful to service developers for service distribution and management targeting serverless frameworks."
        },
        {
          "rank": 48,
          "score": 0.6313789486885071,
          "doc_id": "DIKO0016938177",
          "title": "분산학습 환경을 적용한 연속학습",
          "abstract": "본 논문은 의료 분야에 특화된 연속 학습과 분할 학습 기법을 통합한 혁신적인 프레임워크를 제안한다. 이 프레임워크는 의료 인공지능 모델이 학습할 때 실제 환경에서 발생하는 데이터 프라이버시, 보안, 그리고 변화하는 임상 조건과 같은 주요 과제를 해결한다. 제안된 프레임워크를 통해 의료 클라이언트는 민감한 데이터를 공유하지 않으면서 분산 컴퓨팅 리소스에서 딥러닝 모델을 공동으로 학습할 수 있으며, 연산 비용(computational cost)을 줄일 수 있다. 또한, 제안된 프레임워크는 변이와 새로운 질병과 같은 의료 환경의 증분 특성(incremental characteristic)을 고려함으로써 실제 임상 시나리오에서 의료 인공지능 모델의 질병 진단 능력을 향상할 수 있다. 본 논문은 여러 가지의 의료 이미지 데이터셋과 일반 이미지 데이터셋을 사용하여 프레임워크의 성능을 검증했다. 그 결과 제안된 프레임워크는 분할 학습 상황에서 다른 연속 학습 방법보다 평균 정확도(average accuracy)와 평균 망각(average forgetting) 성능이 우수함을 보였으며, 핵심 요소 분석을 통해 성능을 여러 방면에서 분석했다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016938177&target=NART&cn=DIKO0016938177",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "분산학습 환경을 적용한 연속학습 분산학습 환경을 적용한 연속학습 분산학습 환경을 적용한 연속학습 본 논문은 의료 분야에 특화된 연속 학습과 분할 학습 기법을 통합한 혁신적인 프레임워크를 제안한다. 이 프레임워크는 의료 인공지능 모델이 학습할 때 실제 환경에서 발생하는 데이터 프라이버시, 보안, 그리고 변화하는 임상 조건과 같은 주요 과제를 해결한다. 제안된 프레임워크를 통해 의료 클라이언트는 민감한 데이터를 공유하지 않으면서 분산 컴퓨팅 리소스에서 딥러닝 모델을 공동으로 학습할 수 있으며, 연산 비용(computational cost)을 줄일 수 있다. 또한, 제안된 프레임워크는 변이와 새로운 질병과 같은 의료 환경의 증분 특성(incremental characteristic)을 고려함으로써 실제 임상 시나리오에서 의료 인공지능 모델의 질병 진단 능력을 향상할 수 있다. 본 논문은 여러 가지의 의료 이미지 데이터셋과 일반 이미지 데이터셋을 사용하여 프레임워크의 성능을 검증했다. 그 결과 제안된 프레임워크는 분할 학습 상황에서 다른 연속 학습 방법보다 평균 정확도(average accuracy)와 평균 망각(average forgetting) 성능이 우수함을 보였으며, 핵심 요소 분석을 통해 성능을 여러 방면에서 분석했다."
        },
        {
          "rank": 49,
          "score": 0.6304968595504761,
          "doc_id": "DIKO0017011976",
          "title": "대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형",
          "abstract": "본 연구는 온라인 리뷰의 유용성을 예측하기 위한 모델을 제안하며, 이를 위해 대형 언어 모델과 다양한 딥러닝 기법을 통합적으로 활용하였다. 연구의 시작에서는 온라인 리뷰 및 리뷰 유용성에 대한 이론적 배경을 탐구하였으며, 여러 기존 연구들을 통해 리뷰 유용성에 영향을 미치는 요인들을 정리하였다. 특히, 통계기법, 머신러닝, 딥러닝, 그리고 대형 언어 모델을 중심으로 한 기존의 리뷰 유용성 예측 모형들을 비교 및 분석하였다. 이후, KoBERT와 KoGPT2와 같은 한국어 대형 언어 모델을 기반으로 한 리뷰 유용성 예측모형을 구축하였으며, K-NN 알고리즘으로 통합하여 모델의 성능을 향상시켰다. 실증분석 결과, 본 연구에서 제안한 모델은 기존의 모델들에 비해 높은 예측 성능을 보여주었고, 특히 대형 언어 모델의 통합은 리뷰 유용성 예측의 정확도를 크게 향상시켰다. 이러한 결과는 온라인 리뷰의 품질 및 유용성 평가에 큰 도움을 제공할 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017011976&target=NART&cn=DIKO0017011976",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형 대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형 대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형 본 연구는 온라인 리뷰의 유용성을 예측하기 위한 모델을 제안하며, 이를 위해 대형 언어 모델과 다양한 딥러닝 기법을 통합적으로 활용하였다. 연구의 시작에서는 온라인 리뷰 및 리뷰 유용성에 대한 이론적 배경을 탐구하였으며, 여러 기존 연구들을 통해 리뷰 유용성에 영향을 미치는 요인들을 정리하였다. 특히, 통계기법, 머신러닝, 딥러닝, 그리고 대형 언어 모델을 중심으로 한 기존의 리뷰 유용성 예측 모형들을 비교 및 분석하였다. 이후, KoBERT와 KoGPT2와 같은 한국어 대형 언어 모델을 기반으로 한 리뷰 유용성 예측모형을 구축하였으며, K-NN 알고리즘으로 통합하여 모델의 성능을 향상시켰다. 실증분석 결과, 본 연구에서 제안한 모델은 기존의 모델들에 비해 높은 예측 성능을 보여주었고, 특히 대형 언어 모델의 통합은 리뷰 유용성 예측의 정확도를 크게 향상시켰다. 이러한 결과는 온라인 리뷰의 품질 및 유용성 평가에 큰 도움을 제공할 것으로 기대된다."
        },
        {
          "rank": 50,
          "score": 0.6294692754745483,
          "doc_id": "JAKO202313933270962",
          "title": "딥 러닝 기반 이미지 압축 기법의 성능 비교 분석",
          "abstract": "Image compression is a fundamental technique in the field of digital image processing, which will help to decrease the storage space and to transmit the files efficiently. Recently many deep learning techniques have been proposed to promise results on image compression field. Since many image compression techniques have artifact problems, this paper has compared two deep learning approaches to verify their performance experimentally to solve the problems. One of the approaches is a deep autoencoder technique, and another is a deep convolutional neural network (CNN). For those results in the performance of peak signal-to-noise and root mean square error, this paper shows that deep autoencoder method has more advantages than deep CNN approach.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202313933270962&target=NART&cn=JAKO202313933270962",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 기반 이미지 압축 기법의 성능 비교 분석 딥 러닝 기반 이미지 압축 기법의 성능 비교 분석 딥 러닝 기반 이미지 압축 기법의 성능 비교 분석 Image compression is a fundamental technique in the field of digital image processing, which will help to decrease the storage space and to transmit the files efficiently. Recently many deep learning techniques have been proposed to promise results on image compression field. Since many image compression techniques have artifact problems, this paper has compared two deep learning approaches to verify their performance experimentally to solve the problems. One of the approaches is a deep autoencoder technique, and another is a deep convolutional neural network (CNN). For those results in the performance of peak signal-to-noise and root mean square error, this paper shows that deep autoencoder method has more advantages than deep CNN approach."
        }
      ]
    },
    {
      "query": "클라우드 컴퓨팅 환경에서 주로 사용되는 오픈소스 딥 러닝 프레임워크는 무엇인가요?",
      "query_meta": {
        "type": "single_hop",
        "index": 0
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.7961536645889282,
          "doc_id": "DIKO0014373092",
          "title": "Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서",
          "abstract": "통신 기술의 발달로 인한 사람과 사람, 장치와 장치, 사람과 장치 간의 연결성의 증가와 저장 매체 기술의 발달, 그리고 데이터 저장 비용의 감소로 인해 데이터의 양이 폭발적으로 증가했다. 이에 따라 다양한 형태의 대규모의 데이터를 빠른 시간 내에 효율적으로 처리할 수 있는 Cloud Computing 기술이 주목 받고 있고, Cloud Computing을 위한 오픈소스 기반의 솔루션 또한 많이 나타나게 되었다. &amp;#xD; 2012년부터 주목받기 시작한 Deep Learning은 전 세계적으로 가장 많은 관심을 받는 연구 분야 중 하나이며, 이중 CNN(Convolution Neural Network)은 가장 대표적 알고리즘이다. Deep Learning은 미래사회를 이끌어갈 분야로 평가받고 있으며, 이에 따라 많은 연구들이 진행되고 있고, Deep Learning을 쉽게 활용할 수 있도록 하는 많은 프레임워크가 개발되었다. &amp;#xD; 이에 따라 많은 사람들이 쉽게 Deep Learning을 접할 수 있게 되었지만 특정 환경에서 어떤 프레임워크가 더 우수한 성능을 보이는지에 대한 연구는 부족한 실정이다. 본 논문에서는 특정 환경에서의 성능 비교가 부족하다는 기존 연구의 한계점을 개선하고자 가장 대표적인 Cloud Computing용 오픈소스 소프트웨어 중 하나인 OpenStack을 이용하여 Cloud Computing 환경에서 어떤 Deep Learning 프레임워크가 더 우수한 성능을 보이는지 비교해 보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014373092&target=NART&cn=DIKO0014373092",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 통신 기술의 발달로 인한 사람과 사람, 장치와 장치, 사람과 장치 간의 연결성의 증가와 저장 매체 기술의 발달, 그리고 데이터 저장 비용의 감소로 인해 데이터의 양이 폭발적으로 증가했다. 이에 따라 다양한 형태의 대규모의 데이터를 빠른 시간 내에 효율적으로 처리할 수 있는 Cloud Computing 기술이 주목 받고 있고, Cloud Computing을 위한 오픈소스 기반의 솔루션 또한 많이 나타나게 되었다. &amp;#xD; 2012년부터 주목받기 시작한 Deep Learning은 전 세계적으로 가장 많은 관심을 받는 연구 분야 중 하나이며, 이중 CNN(Convolution Neural Network)은 가장 대표적 알고리즘이다. Deep Learning은 미래사회를 이끌어갈 분야로 평가받고 있으며, 이에 따라 많은 연구들이 진행되고 있고, Deep Learning을 쉽게 활용할 수 있도록 하는 많은 프레임워크가 개발되었다. &amp;#xD; 이에 따라 많은 사람들이 쉽게 Deep Learning을 접할 수 있게 되었지만 특정 환경에서 어떤 프레임워크가 더 우수한 성능을 보이는지에 대한 연구는 부족한 실정이다. 본 논문에서는 특정 환경에서의 성능 비교가 부족하다는 기존 연구의 한계점을 개선하고자 가장 대표적인 Cloud Computing용 오픈소스 소프트웨어 중 하나인 OpenStack을 이용하여 Cloud Computing 환경에서 어떤 Deep Learning 프레임워크가 더 우수한 성능을 보이는지 비교해 보고자 한다."
        },
        {
          "rank": 2,
          "score": 0.7886651754379272,
          "doc_id": "JAKO202009135419341",
          "title": "딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출",
          "abstract": "많은 정보통신기술 기업들은 자체적으로 개발한 인공지능 기술을 오픈소스로 공개하였다. 예를 들어, 구글의 TensorFlow, 페이스북의 PyTorch, 마이크로소프트의 CNTK 등 여러 기업들은 자신들의 인공지능 기술들을 공개하고 있다. 이처럼 대중에게 딥러닝 오픈소스 소프트웨어를 공개함으로써 개발자 커뮤니티와의 관계와 인공지능 생태계를 강화하고, 사용자들의 실험, 적용, 개선을 얻을 수 있다. 이에 따라 머신러닝 분야는 급속히 성장하고 있고, 개발자들 또한 여러가지 학습 알고리즘을 재생산하여 각 영역에 활용하고 있다. 하지만 오픈소스 소프트웨어에 대한 다양한 분석들이 이루어진 데 반해, 실제 산업현장에서 딥러닝 오픈소스 소프트웨어를 개발하거나 활용하는데 유용한 연구 결과는 미흡한 실정이다. 따라서 본 연구에서는 딥러닝 프레임워크 사례연구를 통해 해당 프레임워크의 도입 전략을 도출하고자 한다. 기술-조직-환경 프레임워크를 기반으로 기존의 오픈 소스 소프트웨어 도입과 관련된 연구들을 리뷰하고, 이를 바탕으로 두 기업의 성공 사례와 한 기업의 실패 사례를 포함한 총 3 가지 기업의 도입 사례 분석을 통해 딥러닝 프레임워크 도입을 위한 중요한 5가지 성공 요인을 도출하였다: 팀 내 개발자의 지식과 전문성, 하드웨어(GPU) 환경, 데이터 전사 협력 체계, 딥러닝 프레임워크 플랫폼, 딥러닝 프레임워크 도구 서비스. 그리고 도출한 성공 요인을 실현하기 위한 딥러닝 프레임워크의 단계적 도입 전략을 제안하였다: 프로젝트 문제 정의, 딥러닝 방법론이 적합한 기법인지 확인, 딥러닝 프레임워크가 적합한 도구인지 확인, 기업의 딥러닝 프레임워크 사용, 기업의 딥러닝 프레임워크 확산. 본 연구를 통해 각 산업과 사업의 니즈에 따라, 딥러닝 프레임워크를 개발하거나 활용하고자 하는 기업에게 전략적인 시사점을 제공할 수 있을 것이라 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202009135419341&target=NART&cn=JAKO202009135419341",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출 딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출 딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출 많은 정보통신기술 기업들은 자체적으로 개발한 인공지능 기술을 오픈소스로 공개하였다. 예를 들어, 구글의 TensorFlow, 페이스북의 PyTorch, 마이크로소프트의 CNTK 등 여러 기업들은 자신들의 인공지능 기술들을 공개하고 있다. 이처럼 대중에게 딥러닝 오픈소스 소프트웨어를 공개함으로써 개발자 커뮤니티와의 관계와 인공지능 생태계를 강화하고, 사용자들의 실험, 적용, 개선을 얻을 수 있다. 이에 따라 머신러닝 분야는 급속히 성장하고 있고, 개발자들 또한 여러가지 학습 알고리즘을 재생산하여 각 영역에 활용하고 있다. 하지만 오픈소스 소프트웨어에 대한 다양한 분석들이 이루어진 데 반해, 실제 산업현장에서 딥러닝 오픈소스 소프트웨어를 개발하거나 활용하는데 유용한 연구 결과는 미흡한 실정이다. 따라서 본 연구에서는 딥러닝 프레임워크 사례연구를 통해 해당 프레임워크의 도입 전략을 도출하고자 한다. 기술-조직-환경 프레임워크를 기반으로 기존의 오픈 소스 소프트웨어 도입과 관련된 연구들을 리뷰하고, 이를 바탕으로 두 기업의 성공 사례와 한 기업의 실패 사례를 포함한 총 3 가지 기업의 도입 사례 분석을 통해 딥러닝 프레임워크 도입을 위한 중요한 5가지 성공 요인을 도출하였다: 팀 내 개발자의 지식과 전문성, 하드웨어(GPU) 환경, 데이터 전사 협력 체계, 딥러닝 프레임워크 플랫폼, 딥러닝 프레임워크 도구 서비스. 그리고 도출한 성공 요인을 실현하기 위한 딥러닝 프레임워크의 단계적 도입 전략을 제안하였다: 프로젝트 문제 정의, 딥러닝 방법론이 적합한 기법인지 확인, 딥러닝 프레임워크가 적합한 도구인지 확인, 기업의 딥러닝 프레임워크 사용, 기업의 딥러닝 프레임워크 확산. 본 연구를 통해 각 산업과 사업의 니즈에 따라, 딥러닝 프레임워크를 개발하거나 활용하고자 하는 기업에게 전략적인 시사점을 제공할 수 있을 것이라 기대된다."
        },
        {
          "rank": 3,
          "score": 0.7770777940750122,
          "doc_id": "DIKO0015889140",
          "title": "딥 러닝 프레임워크 성능 비교 및 개선 방안",
          "abstract": "현 시대는 4차 산업혁명이 대두되는 시대로 요소 기술들 중 인공지능의 중 요성은 아무리 강조하더라도 지나치지 않으며, 기업들 경쟁력의 척도라고 불 릴만큼 모든 산업에서 활용되고있다. 2016년 경 DeepMind 의 AlphaGo 와 이 세돌 선수의 경기로 국내에서는 처음으로 인공지능의 위력과 Deep Learning 이라는 단어가 대중들에게 알려지게 되었다.&amp;#xD; 특정 IT 산업이 발전하게 되면 해당 분야의 개발자들의 생산성과 접근성을 높이기 위해 Framework 들이 등장, 발전하게 된다. 통신기술과 스마트폰의 출현으로 WEB 붐이 이르렀을 때, Server-side 에서는 Spring, django, Ruby on Rails 등이 출현하였고, Client-side 에서는 Angular, React, jQuery 와 같이 다양한 Framework 들이 등장 발전하였다. 컴퓨터 성능의 발전과 다양 한 컴퓨팅 기술의 발전으로 현 시대는 인공지능 3차 붐으로 Machine Learning 과 Deep Learning 의 시대로 불리고있다.&amp;#xD; 이와 같이 Deep Learning 분야에서도 다양한 Framework 들이 개발되었다. 이런 다양한 Framework 제품들의 목적은 개발자들의 생산성을 향상시키기 위 해 내부 알고리즘이나 메커니즘을 Black Box 형식으로 감추고 High Level API 를 제공하기 때문에, 내부적인 구현 방식은 Framework 별로 다르다. 본 논문에서는 현 시대에 가장 많이 사용하는 대표적인 Framework 들을 선정한 다. 그리고 선정된 Framework 들을 이용하여 Convolutional Neural Network 알고리즘을 구현, 동일한 Training Data 를 이용하여 학습 Model 을 만들어 낸다. 그리고 동일한 Cloud 환경에서 각 Framework 별 학습을 수행하여 성 능을 비교한다. 성능 비교 환경은 총 3가지로 CPU, GPU 1 Core, Multi GPU Core 환경에서 각 Framework 별 성능 지표를 추출한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015889140&target=NART&cn=DIKO0015889140",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 프레임워크 성능 비교 및 개선 방안 딥 러닝 프레임워크 성능 비교 및 개선 방안 딥 러닝 프레임워크 성능 비교 및 개선 방안 현 시대는 4차 산업혁명이 대두되는 시대로 요소 기술들 중 인공지능의 중 요성은 아무리 강조하더라도 지나치지 않으며, 기업들 경쟁력의 척도라고 불 릴만큼 모든 산업에서 활용되고있다. 2016년 경 DeepMind 의 AlphaGo 와 이 세돌 선수의 경기로 국내에서는 처음으로 인공지능의 위력과 Deep Learning 이라는 단어가 대중들에게 알려지게 되었다.&amp;#xD; 특정 IT 산업이 발전하게 되면 해당 분야의 개발자들의 생산성과 접근성을 높이기 위해 Framework 들이 등장, 발전하게 된다. 통신기술과 스마트폰의 출현으로 WEB 붐이 이르렀을 때, Server-side 에서는 Spring, django, Ruby on Rails 등이 출현하였고, Client-side 에서는 Angular, React, jQuery 와 같이 다양한 Framework 들이 등장 발전하였다. 컴퓨터 성능의 발전과 다양 한 컴퓨팅 기술의 발전으로 현 시대는 인공지능 3차 붐으로 Machine Learning 과 Deep Learning 의 시대로 불리고있다.&amp;#xD; 이와 같이 Deep Learning 분야에서도 다양한 Framework 들이 개발되었다. 이런 다양한 Framework 제품들의 목적은 개발자들의 생산성을 향상시키기 위 해 내부 알고리즘이나 메커니즘을 Black Box 형식으로 감추고 High Level API 를 제공하기 때문에, 내부적인 구현 방식은 Framework 별로 다르다. 본 논문에서는 현 시대에 가장 많이 사용하는 대표적인 Framework 들을 선정한 다. 그리고 선정된 Framework 들을 이용하여 Convolutional Neural Network 알고리즘을 구현, 동일한 Training Data 를 이용하여 학습 Model 을 만들어 낸다. 그리고 동일한 Cloud 환경에서 각 Framework 별 학습을 수행하여 성 능을 비교한다. 성능 비교 환경은 총 3가지로 CPU, GPU 1 Core, Multi GPU Core 환경에서 각 Framework 별 성능 지표를 추출한다."
        },
        {
          "rank": 4,
          "score": 0.7582200765609741,
          "doc_id": "DIKO0016819326",
          "title": "클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석",
          "abstract": "DNN(Deep Neural Network)은 이미지 인식, 자연어 처리를 포함한 다양한 분야에서 널리 사용되고 있으며 이러한 모델을 실제 환경에서 효율적으로 실행하는 것이 중요하다. 사용자의 요구를 충족하기 위해 딥러닝 추론을 최적화하는 것은 더욱 중요하지만 다양한 클라우드 환경에서 딥러닝 추론 최적화 구성을 찾는것은 어렵다.&amp;#xD; 본 논문에서는 다양한 하드웨어와 최적화 기법을 활용하여 딥러닝 추론을 실험하고 분석한다. 딥러닝 추론의 특성을 파악하고 클라우드에서 제공하는하드웨어들 중 서버리스 컴퓨팅 아키텍처를 사용하여 딥러닝 추론 작업을 배포하는데 용이한 프로토타입을 제안하고 결과를 분석한다. 하드웨어 유형, 모델 그래프 최적화, 하드웨어 최적화 및 컴파일, 서버리스 메모리 및 배치 크기 설정 등 서버리스 컴퓨팅 환경에서 제공할 때 고려해야할 많은 요소들이 있으며 사용자는 완전 관리형 웹 서비스를 통해 쉽게 다양한 구성에 대해서 시도해볼 수 있다. 제안한 시스템을 통해 최적의 서버리스 환경 구성을 찾을 수 있으며 공개된 소스를 통해 FaaS 구성에 비교적 쉽게 접근할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016819326&target=NART&cn=DIKO0016819326",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석 클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석 클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석 DNN(Deep Neural Network)은 이미지 인식, 자연어 처리를 포함한 다양한 분야에서 널리 사용되고 있으며 이러한 모델을 실제 환경에서 효율적으로 실행하는 것이 중요하다. 사용자의 요구를 충족하기 위해 딥러닝 추론을 최적화하는 것은 더욱 중요하지만 다양한 클라우드 환경에서 딥러닝 추론 최적화 구성을 찾는것은 어렵다.&amp;#xD; 본 논문에서는 다양한 하드웨어와 최적화 기법을 활용하여 딥러닝 추론을 실험하고 분석한다. 딥러닝 추론의 특성을 파악하고 클라우드에서 제공하는하드웨어들 중 서버리스 컴퓨팅 아키텍처를 사용하여 딥러닝 추론 작업을 배포하는데 용이한 프로토타입을 제안하고 결과를 분석한다. 하드웨어 유형, 모델 그래프 최적화, 하드웨어 최적화 및 컴파일, 서버리스 메모리 및 배치 크기 설정 등 서버리스 컴퓨팅 환경에서 제공할 때 고려해야할 많은 요소들이 있으며 사용자는 완전 관리형 웹 서비스를 통해 쉽게 다양한 구성에 대해서 시도해볼 수 있다. 제안한 시스템을 통해 최적의 서버리스 환경 구성을 찾을 수 있으며 공개된 소스를 통해 FaaS 구성에 비교적 쉽게 접근할 수 있다."
        },
        {
          "rank": 5,
          "score": 0.7332687377929688,
          "doc_id": "JAKO201713056893580",
          "title": "딥 러닝 프레임워크의 비교 및 분석",
          "abstract": "딥 러닝은 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 딥 러닝은 세상을 이해하고 감지하는 인공지능을 개발하는데 가장 촉망받는 기술이 되고 있으며, 구글, 바이두, 페이스북 등이 가장 앞서서 개발을 하고 있다. 본 논문에서는 딥 러닝을 구현하는 딥 러닝 프레임워크의 종류에 대해 논의하고, 딥 러닝 프레임워크의 영상과 음성 인식 분야의 효율성에 대해 비교, 분석하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201713056893580&target=NART&cn=JAKO201713056893580",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 프레임워크의 비교 및 분석 딥 러닝 프레임워크의 비교 및 분석 딥 러닝 프레임워크의 비교 및 분석 딥 러닝은 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 딥 러닝은 세상을 이해하고 감지하는 인공지능을 개발하는데 가장 촉망받는 기술이 되고 있으며, 구글, 바이두, 페이스북 등이 가장 앞서서 개발을 하고 있다. 본 논문에서는 딥 러닝을 구현하는 딥 러닝 프레임워크의 종류에 대해 논의하고, 딥 러닝 프레임워크의 영상과 음성 인식 분야의 효율성에 대해 비교, 분석하고자 한다."
        },
        {
          "rank": 6,
          "score": 0.7240474224090576,
          "doc_id": "JAKO202223540366088",
          "title": "이미지 학습을 위한 딥러닝 프레임워크 비교분석",
          "abstract": "딥러닝 프레임워크는 현재에도 계속해서 발전되어 가고 있으며, 다양한 프레임워크들이 존재한다. 딥러닝의 대표적인 프레임워크는 TensorFlow, PyTorch, Keras 등이 있다. 딥러님 프레임워크는 이미지 학습을 통해 이미지 분류에서의 최적화 모델을 이용한다. 본 논문에서는 딥러닝 이미지 인식 분야에서 가장 많이 사용하고 있는 TensorFlow와 PyTorch 프레임워크를 활용하여 이미지 학습을 진행하였으며, 이 과정에서 도출한 결과를 비교 분석하여 최적화된 프레임워크을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202223540366088&target=NART&cn=JAKO202223540366088",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "이미지 학습을 위한 딥러닝 프레임워크 비교분석 이미지 학습을 위한 딥러닝 프레임워크 비교분석 이미지 학습을 위한 딥러닝 프레임워크 비교분석 딥러닝 프레임워크는 현재에도 계속해서 발전되어 가고 있으며, 다양한 프레임워크들이 존재한다. 딥러닝의 대표적인 프레임워크는 TensorFlow, PyTorch, Keras 등이 있다. 딥러님 프레임워크는 이미지 학습을 통해 이미지 분류에서의 최적화 모델을 이용한다. 본 논문에서는 딥러닝 이미지 인식 분야에서 가장 많이 사용하고 있는 TensorFlow와 PyTorch 프레임워크를 활용하여 이미지 학습을 진행하였으며, 이 과정에서 도출한 결과를 비교 분석하여 최적화된 프레임워크을 알 수 있었다."
        },
        {
          "rank": 7,
          "score": 0.7146428823471069,
          "doc_id": "NPAP12898051",
          "title": "딥러닝 프레임워크 비교 및 분석",
          "abstract": "딥러닝(Deep Learning)을 효과적으로 연구하고 개발할 수 있도록 도와주는 다양한 딥러닝 프레임워크(Deep Learning Framework)가 있다. 딥러닝 프레임워크는 현재 100 가지도 넘는 종류가 있다. 그렇기 때문에 개발의 목적에 가장 적합한 딥러닝 프레임워크를 선택하는 것은 쉽지 않다. 본고에서는 5가지 대표적인 딥러닝 프레임워크에 대해서 각각의 특징을 분석하고 비교한다. 이를 통하여 딥러닝을 개발하기 전에 개발 목적에 적합한 프레임워크를 선택할 수 있는 간단한 안목을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12898051&target=NART&cn=NPAP12898051",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 프레임워크 비교 및 분석 딥러닝 프레임워크 비교 및 분석 딥러닝 프레임워크 비교 및 분석 딥러닝(Deep Learning)을 효과적으로 연구하고 개발할 수 있도록 도와주는 다양한 딥러닝 프레임워크(Deep Learning Framework)가 있다. 딥러닝 프레임워크는 현재 100 가지도 넘는 종류가 있다. 그렇기 때문에 개발의 목적에 가장 적합한 딥러닝 프레임워크를 선택하는 것은 쉽지 않다. 본고에서는 5가지 대표적인 딥러닝 프레임워크에 대해서 각각의 특징을 분석하고 비교한다. 이를 통하여 딥러닝을 개발하기 전에 개발 목적에 적합한 프레임워크를 선택할 수 있는 간단한 안목을 제시한다."
        },
        {
          "rank": 8,
          "score": 0.7071444988250732,
          "doc_id": "JAKO201305262618384",
          "title": "오픈소스 클라우드 컴퓨팅 기반 교육 실습 시스템 구축",
          "abstract": "근래 이슈가 되고 있는 클라우드 컴퓨팅은 분산컴퓨팅 환경에서 사용자가 요구하는 컴퓨팅 자원을 최적화하여 유연하고 확장성 있게 지원할 수 있어 각광받는 새로운 패러다임이다. 클라우드 컴퓨팅 환경은 가상화 환경을 구성함으로써 실제적인 구현 및 서비스가 가능해진다. 본 논문에서는 오픈소스 기반의 클라우드 컴퓨팅을 연구하고 대학교 내에서 컴퓨터를 이용한 실습을 수행할 시 요구되는 시스템 환경을 오픈소스 클라우드 컴퓨팅 기반의 환경을 통해 구현하고자 한다. 클라우드 컴퓨팅을 통한 가상화 기반의 실습 환경은 최적화된 자원을 제공할 수 있을 뿐만 아니라 실습 자원 관리의 편리성, 실습 결과에 대한 손쉬운 관리 등의 효율성을 가져올 수 있다. 이로 인해 실습환경 설정에 소요되는 시간을 줄일 수 있을 뿐만 아니라, 교수 입장에서는 실습결과물들을 쉽게 관리 할 수 있게 된다. 또한 다양한 실습환경의 요구사항들을 유연성 있게 적용함으로써 시스템에 대한 활용성 또한 높아지게 된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201305262618384&target=NART&cn=JAKO201305262618384",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈소스 클라우드 컴퓨팅 기반 교육 실습 시스템 구축 오픈소스 클라우드 컴퓨팅 기반 교육 실습 시스템 구축 오픈소스 클라우드 컴퓨팅 기반 교육 실습 시스템 구축 근래 이슈가 되고 있는 클라우드 컴퓨팅은 분산컴퓨팅 환경에서 사용자가 요구하는 컴퓨팅 자원을 최적화하여 유연하고 확장성 있게 지원할 수 있어 각광받는 새로운 패러다임이다. 클라우드 컴퓨팅 환경은 가상화 환경을 구성함으로써 실제적인 구현 및 서비스가 가능해진다. 본 논문에서는 오픈소스 기반의 클라우드 컴퓨팅을 연구하고 대학교 내에서 컴퓨터를 이용한 실습을 수행할 시 요구되는 시스템 환경을 오픈소스 클라우드 컴퓨팅 기반의 환경을 통해 구현하고자 한다. 클라우드 컴퓨팅을 통한 가상화 기반의 실습 환경은 최적화된 자원을 제공할 수 있을 뿐만 아니라 실습 자원 관리의 편리성, 실습 결과에 대한 손쉬운 관리 등의 효율성을 가져올 수 있다. 이로 인해 실습환경 설정에 소요되는 시간을 줄일 수 있을 뿐만 아니라, 교수 입장에서는 실습결과물들을 쉽게 관리 할 수 있게 된다. 또한 다양한 실습환경의 요구사항들을 유연성 있게 적용함으로써 시스템에 대한 활용성 또한 높아지게 된다."
        },
        {
          "rank": 9,
          "score": 0.7069487571716309,
          "doc_id": "JAKO201718054814596",
          "title": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석",
          "abstract": "딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201718054814596&target=NART&cn=JAKO201718054814596",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다."
        },
        {
          "rank": 10,
          "score": 0.7023547887802124,
          "doc_id": "JAKO202512254006340",
          "title": "컨테이너 환경에서 딥러닝 워크로드의 성능 분석",
          "abstract": "최근 딥러닝 워크로드가 컨테이너 환경에서 실행되는 사례가 늘고 있다. 컨테이너는 가상머신에 비해 낮은 오버 헤드와 높은 이식성을 제공하지만, 딥러닝 워크로드의 실행 시 시스템 자원의 비효율적 활용 문제가 발생할 수 있다. 본 논문에서는 컨테이너 환경에서 딥러닝 워크로드 실행으로 인한 오버헤드와 비효율성을 분석하기 위해 시스템콜 및 이벤트 추적 트레이스를 수집 및 분석하였다. 특히, 동일한 워크로드를 호스트 머신에서 직접 실행한 경우와 컨테이너 환경에서 실행한 경우를 비교하여 자원 소비 및 간섭과 관련된 컨테이너 환경의 오버헤드를 정량적으로 확인하였다. 분석 결과 딥러닝 워크로드의 컨테이너 실행 시 성능 병목을 초래하는 주요 원인으로 주기적인 스토리지 플러시 작업이 확인되었으며, 다중 테넌트 환경에서는 자원 경합으로 인해 이러한 문제가 더욱 심화됨을 확인하였다. 본 연구의 결과는 컨테이너 환경에서 딥러닝 워크로드를 효율적으로 실행하기 위한 클라우드 및 엣지 시스템 설계에 중요한 인사이트를 제공할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202512254006340&target=NART&cn=JAKO202512254006340",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "컨테이너 환경에서 딥러닝 워크로드의 성능 분석 컨테이너 환경에서 딥러닝 워크로드의 성능 분석 컨테이너 환경에서 딥러닝 워크로드의 성능 분석 최근 딥러닝 워크로드가 컨테이너 환경에서 실행되는 사례가 늘고 있다. 컨테이너는 가상머신에 비해 낮은 오버 헤드와 높은 이식성을 제공하지만, 딥러닝 워크로드의 실행 시 시스템 자원의 비효율적 활용 문제가 발생할 수 있다. 본 논문에서는 컨테이너 환경에서 딥러닝 워크로드 실행으로 인한 오버헤드와 비효율성을 분석하기 위해 시스템콜 및 이벤트 추적 트레이스를 수집 및 분석하였다. 특히, 동일한 워크로드를 호스트 머신에서 직접 실행한 경우와 컨테이너 환경에서 실행한 경우를 비교하여 자원 소비 및 간섭과 관련된 컨테이너 환경의 오버헤드를 정량적으로 확인하였다. 분석 결과 딥러닝 워크로드의 컨테이너 실행 시 성능 병목을 초래하는 주요 원인으로 주기적인 스토리지 플러시 작업이 확인되었으며, 다중 테넌트 환경에서는 자원 경합으로 인해 이러한 문제가 더욱 심화됨을 확인하였다. 본 연구의 결과는 컨테이너 환경에서 딥러닝 워크로드를 효율적으로 실행하기 위한 클라우드 및 엣지 시스템 설계에 중요한 인사이트를 제공할 수 있을 것으로 기대된다."
        },
        {
          "rank": 11,
          "score": 0.7015455961227417,
          "doc_id": "DIKO0012654277",
          "title": "클라우드 컴퓨팅을 이용한 DFSaaS 프레임워크 연구",
          "abstract": "디지털 포렌식이란 과학적이거나 기술적인 기법을 사용하여 범죄수사 또는 증거를 수집하는 행위이이다. 이를 위한 도구로 디지털 포렌식 도구가 존재한다. 이 도구는 법과 기술 간의 매개체가 될 수 있는 핵심 요소라 할 수 있다[1]. 디지털 포렌식은 증거 이미징, 분석, 검색, 보고서 작성 등의 일련의 절차를 요구한다. 기존의 디지털 포렌식 도구는 이러한 절차적 기능을 제공하는 것을 목적으로 개발되었다. 현존하는 대부분의 포렌식 도구들은 단일 플랫폼 상의 윈도우 운영체제에서 운용되는 통합 도구로 제공된다. 이동성을 위해 휴대형 하드디스크 드라이브에 저장되며 해당 매체 내에서 실행된다. 목적에 따라 전용 기능을 제공하는 하드웨어 형태의 도구로도 제작 된다. 추가적으로 압수한 데스크탑을 포렌식 연구실로 이동하기 위해서 특수한 장치가 필요할 수 있다. 현재 단일 플랫폼 형태의 포렌식 도구에서 2TB의 데이터를 이미징 하는데 7시간이 걸리며 비트와이즈 검색을 20MB/s 정도의 속도로 처리해 1TB 이미지를 검색 했을 때에는 14시간이 소요된다[2]. 이는 양적으로 증가하는 디지털 증거의 추세에 미루어 볼 때 향후 도구의 증거 처리 속도 문제를 야기 시킬 것이다. 또, 증거물을 포렌식 연구실로 이송을 하거나 고속 처리를 하기위해서는 기타 하드웨어 장치를 이용해야 하고, 이는 도구 사용에 있어 불필요한 번거로움을 초래한다. 따라서 기존의 도구를 아우를 수 있는 새로운 도구 개발이 시급하다. 프로세싱 속도 향상을 위해 단일 플랫폼의 한계를 극복해야하고, 이용의 번거로움을 피하기 위해 모든 장비들을 하나의 도구로 합쳐야 한다. 기존의 도구를 기능을 아우를 수 있고, 단점을 제거하기위해 클라우드 컴퓨팅 개념을 적용해 해결 방안을 모색하였다. 본 논문에서는 클라우드 컴퓨팅[3]을 이용하여 디지털 포렌식 절차에 따라 어디에서든 포렌식을 수행 할 수 있는 DFSaaS(Digital Forensic Software as a Service) 구조와 이에 대한 시나리오를 제시하고, 이들에 대한 프레임워크를 연구한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0012654277&target=NART&cn=DIKO0012654277",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 컴퓨팅을 이용한 DFSaaS 프레임워크 연구 클라우드 컴퓨팅을 이용한 DFSaaS 프레임워크 연구 클라우드 컴퓨팅을 이용한 DFSaaS 프레임워크 연구 디지털 포렌식이란 과학적이거나 기술적인 기법을 사용하여 범죄수사 또는 증거를 수집하는 행위이이다. 이를 위한 도구로 디지털 포렌식 도구가 존재한다. 이 도구는 법과 기술 간의 매개체가 될 수 있는 핵심 요소라 할 수 있다[1]. 디지털 포렌식은 증거 이미징, 분석, 검색, 보고서 작성 등의 일련의 절차를 요구한다. 기존의 디지털 포렌식 도구는 이러한 절차적 기능을 제공하는 것을 목적으로 개발되었다. 현존하는 대부분의 포렌식 도구들은 단일 플랫폼 상의 윈도우 운영체제에서 운용되는 통합 도구로 제공된다. 이동성을 위해 휴대형 하드디스크 드라이브에 저장되며 해당 매체 내에서 실행된다. 목적에 따라 전용 기능을 제공하는 하드웨어 형태의 도구로도 제작 된다. 추가적으로 압수한 데스크탑을 포렌식 연구실로 이동하기 위해서 특수한 장치가 필요할 수 있다. 현재 단일 플랫폼 형태의 포렌식 도구에서 2TB의 데이터를 이미징 하는데 7시간이 걸리며 비트와이즈 검색을 20MB/s 정도의 속도로 처리해 1TB 이미지를 검색 했을 때에는 14시간이 소요된다[2]. 이는 양적으로 증가하는 디지털 증거의 추세에 미루어 볼 때 향후 도구의 증거 처리 속도 문제를 야기 시킬 것이다. 또, 증거물을 포렌식 연구실로 이송을 하거나 고속 처리를 하기위해서는 기타 하드웨어 장치를 이용해야 하고, 이는 도구 사용에 있어 불필요한 번거로움을 초래한다. 따라서 기존의 도구를 아우를 수 있는 새로운 도구 개발이 시급하다. 프로세싱 속도 향상을 위해 단일 플랫폼의 한계를 극복해야하고, 이용의 번거로움을 피하기 위해 모든 장비들을 하나의 도구로 합쳐야 한다. 기존의 도구를 기능을 아우를 수 있고, 단점을 제거하기위해 클라우드 컴퓨팅 개념을 적용해 해결 방안을 모색하였다. 본 논문에서는 클라우드 컴퓨팅[3]을 이용하여 디지털 포렌식 절차에 따라 어디에서든 포렌식을 수행 할 수 있는 DFSaaS(Digital Forensic Software as a Service) 구조와 이에 대한 시나리오를 제시하고, 이들에 대한 프레임워크를 연구한다."
        },
        {
          "rank": 12,
          "score": 0.7007641196250916,
          "doc_id": "JAKO201310635656332",
          "title": "클라우드 환경을 고려한 디지털 포렌식 프레임워크",
          "abstract": "최근 세계적인 경제위기 속에서 국내외의 기업들이 IT 투자를 보류하거나 예산을 대폭 삭감하고 있다. 이에 기업들은 IT 부문에 있어서의 비용 절감을 통한 위기 극복 방안을 모색하고 있으며, 이러한 상황에서 클라우드 컴퓨팅(Cloud Computing)은 위기 극복을 위한 최적의 솔루션으로 빠르게 부상하고 있다. 또한 디지털 포렌식 조사과정에서 조사 대상 시스템의 사용자가 클라우드 서비스를 사용했는지 여부는 추가적은 조사 대상의 선정에 매우 중요한 요소이다. Daum Cloud, Google Docs와 같은 클라우드 서비스를 사용하였을 경우, 로그인 정보를 획득하여 원격지의 클라우드 서비스에 접속이 가능한 경우가 있다. 이러한 경우에는 원격지의 증거 데이터를 수집할 수 있는다. 따라서 다양한 클라우드 서비스에서 데이터를 수집하고 분석하는 방안에 대하여 연구가 필요하다. 이에 본 연구에서는 서비스별 데이터 수집 및 분석 기법에 대해 연구하여 클라우드 환경을 고려한 디지털 포렌식 프레임워크를 제안한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201310635656332&target=NART&cn=JAKO201310635656332",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 환경을 고려한 디지털 포렌식 프레임워크 클라우드 환경을 고려한 디지털 포렌식 프레임워크 클라우드 환경을 고려한 디지털 포렌식 프레임워크 최근 세계적인 경제위기 속에서 국내외의 기업들이 IT 투자를 보류하거나 예산을 대폭 삭감하고 있다. 이에 기업들은 IT 부문에 있어서의 비용 절감을 통한 위기 극복 방안을 모색하고 있으며, 이러한 상황에서 클라우드 컴퓨팅(Cloud Computing)은 위기 극복을 위한 최적의 솔루션으로 빠르게 부상하고 있다. 또한 디지털 포렌식 조사과정에서 조사 대상 시스템의 사용자가 클라우드 서비스를 사용했는지 여부는 추가적은 조사 대상의 선정에 매우 중요한 요소이다. Daum Cloud, Google Docs와 같은 클라우드 서비스를 사용하였을 경우, 로그인 정보를 획득하여 원격지의 클라우드 서비스에 접속이 가능한 경우가 있다. 이러한 경우에는 원격지의 증거 데이터를 수집할 수 있는다. 따라서 다양한 클라우드 서비스에서 데이터를 수집하고 분석하는 방안에 대하여 연구가 필요하다. 이에 본 연구에서는 서비스별 데이터 수집 및 분석 기법에 대해 연구하여 클라우드 환경을 고려한 디지털 포렌식 프레임워크를 제안한다."
        },
        {
          "rank": 13,
          "score": 0.6946893930435181,
          "doc_id": "ATN0037463572",
          "title": "서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가",
          "abstract": "Serverless computing is a new computing paradigm which can develop and execute the application program without server management overhead. Recently, as the enterprise application architectures are evolved to container and micro-services, serverless-based cloud services make it easy to expand and distribute the micro-services. In this regard, we compared the technologies and performed an experiment to measure average response time and response success rate of distributed functions for the three typical serverless open source frameworks: OpenFaaS, Kubeless and Fission.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037463572&target=NART&cn=ATN0037463572",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 Serverless computing is a new computing paradigm which can develop and execute the application program without server management overhead. Recently, as the enterprise application architectures are evolved to container and micro-services, serverless-based cloud services make it easy to expand and distribute the micro-services. In this regard, we compared the technologies and performed an experiment to measure average response time and response success rate of distributed functions for the three typical serverless open source frameworks: OpenFaaS, Kubeless and Fission."
        },
        {
          "rank": 14,
          "score": 0.6866655349731445,
          "doc_id": "JAKO201719950757340",
          "title": "딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",
          "abstract": "딥러닝 프레임워크의 대표적인 기능으로는 '자동미분'과 'GPU의 활용' 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각 프레임워크의 실행속도에 대한 평가는 '큰 차이는 없다'는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만 빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데, 위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로 구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201719950757340&target=NART&cn=JAKO201719950757340",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 대표적인 기능으로는 '자동미분'과 'GPU의 활용' 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각 프레임워크의 실행속도에 대한 평가는 '큰 차이는 없다'는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만 빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데, 위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로 구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다."
        },
        {
          "rank": 15,
          "score": 0.6761388778686523,
          "doc_id": "NART104972735",
          "title": "PHDFS: Optimizing I/O performance of HDFS in deep learning cloud computing platform",
          "abstract": "<P><B>Abstract</B></P>  <P>For deep learning cloud computing platforms, file system is a fundamental and critical component. Hadoop distributed file system (HDFS) is widely used in large scale clusters due to its high performance and high availability. However, in deep learning datasets, the number of files is huge but the file size is small, making HDFS suffer a severe performance penalty. Although there have been many optimizing methods for addressing the <I>small file problem</I>, none of them take the file correlation in deep learning datasets into consideration. To address such problem, this paper proposes a <I>Pile</I>-HDFS (PHDFS) based on a new file aggregation approach. <I>Pile</I> is designed as the I/O unit merging a group of small files according to their correlation. In order to effectively access small files, we design a two-layer manager and add the inner organization information to data blocks. Experimental results demonstrate that, compared with the original HDFS, PHDFS can dramatically decrease the latency when accessing small files and improve the FPS (Frames Per Second) of typical deep learning models by 40%.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART104972735&target=NART&cn=NART104972735",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PHDFS: Optimizing I/O performance of HDFS in deep learning cloud computing platform PHDFS: Optimizing I/O performance of HDFS in deep learning cloud computing platform PHDFS: Optimizing I/O performance of HDFS in deep learning cloud computing platform <P><B>Abstract</B></P>  <P>For deep learning cloud computing platforms, file system is a fundamental and critical component. Hadoop distributed file system (HDFS) is widely used in large scale clusters due to its high performance and high availability. However, in deep learning datasets, the number of files is huge but the file size is small, making HDFS suffer a severe performance penalty. Although there have been many optimizing methods for addressing the <I>small file problem</I>, none of them take the file correlation in deep learning datasets into consideration. To address such problem, this paper proposes a <I>Pile</I>-HDFS (PHDFS) based on a new file aggregation approach. <I>Pile</I> is designed as the I/O unit merging a group of small files according to their correlation. In order to effectively access small files, we design a two-layer manager and add the inner organization information to data blocks. Experimental results demonstrate that, compared with the original HDFS, PHDFS can dramatically decrease the latency when accessing small files and improve the FPS (Frames Per Second) of typical deep learning models by 40%.</P>"
        },
        {
          "rank": 16,
          "score": 0.67598956823349,
          "doc_id": "JAKO201306366998119",
          "title": "클라우드 컴퓨팅 정보보호 프레임워크에 관한 연구",
          "abstract": "탄력성(elasticity), 빠른 적용과 릴리즈, 광대역 네트워크 접속, 다중 접속(multi-tenancy), 활용에 제한이 없는(ubiquity) 유연성 등 클라우드 컴퓨팅의 고유한 속성들은 클라우드를 선택한 기업과 기관에게 획기적인 효율성을 제공하지만 원천적으로 내재된 보안 위협을 제거해야 하는 대책수립이 필요하다. 이를 위해 본 논문에서는 전략적 연계 모델을 참조하여 클라우드 컴퓨팅 정보보호 프레임워크를 제시하였다. 클라우드 컴퓨팅 정보보호 프레임워크는 클라우드 위협, 보안통제 활동, 클라우드 이해관계자를 중심 축으로 합목적성, 책임성, 투명한 책임소재의 벽면으로 구성된다. 중심 축은 클라우드 환경에서 정보보호 활동을 수행하는 주요 목적인 위협 최소화목표와 이해관계자를 지정하고 그들이 해야 할 정보보호 활동을 정의하고 있다. 또한, 3개 벽면은 클라우드 환경에서 정보보호 활동을 수행하기 위한 원칙이며 중심 축 간의 접점에서 7개 서비스 패키지 도출을 위한 방향을 제공한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201306366998119&target=NART&cn=JAKO201306366998119",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 컴퓨팅 정보보호 프레임워크에 관한 연구 클라우드 컴퓨팅 정보보호 프레임워크에 관한 연구 클라우드 컴퓨팅 정보보호 프레임워크에 관한 연구 탄력성(elasticity), 빠른 적용과 릴리즈, 광대역 네트워크 접속, 다중 접속(multi-tenancy), 활용에 제한이 없는(ubiquity) 유연성 등 클라우드 컴퓨팅의 고유한 속성들은 클라우드를 선택한 기업과 기관에게 획기적인 효율성을 제공하지만 원천적으로 내재된 보안 위협을 제거해야 하는 대책수립이 필요하다. 이를 위해 본 논문에서는 전략적 연계 모델을 참조하여 클라우드 컴퓨팅 정보보호 프레임워크를 제시하였다. 클라우드 컴퓨팅 정보보호 프레임워크는 클라우드 위협, 보안통제 활동, 클라우드 이해관계자를 중심 축으로 합목적성, 책임성, 투명한 책임소재의 벽면으로 구성된다. 중심 축은 클라우드 환경에서 정보보호 활동을 수행하는 주요 목적인 위협 최소화목표와 이해관계자를 지정하고 그들이 해야 할 정보보호 활동을 정의하고 있다. 또한, 3개 벽면은 클라우드 환경에서 정보보호 활동을 수행하기 위한 원칙이며 중심 축 간의 접점에서 7개 서비스 패키지 도출을 위한 방향을 제공한다."
        },
        {
          "rank": 17,
          "score": 0.6740238070487976,
          "doc_id": "JAKO201620853199880",
          "title": "딥러닝의 모형과 응용사례",
          "abstract": "딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수 있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201620853199880&target=NART&cn=JAKO201620853199880",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝의 모형과 응용사례 딥러닝의 모형과 응용사례 딥러닝의 모형과 응용사례 딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수 있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다."
        },
        {
          "rank": 18,
          "score": 0.6707677245140076,
          "doc_id": "NART95625496",
          "title": "Open source column : deep learning with Keras",
          "abstract": "<P>Following the last column on MatConvNet, let us continue to look at open source frameworks for deep learning. In this column we are going to check Keras, a Python API that allows to use several different backends like Tensorflow and CNTK. Actually, it also supports Theano, although the development of this framework has been halted by the original developers in 2017.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART95625496&target=NART&cn=NART95625496",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Open source column : deep learning with Keras Open source column : deep learning with Keras Open source column : deep learning with Keras <P>Following the last column on MatConvNet, let us continue to look at open source frameworks for deep learning. In this column we are going to check Keras, a Python API that allows to use several different backends like Tensorflow and CNTK. Actually, it also supports Theano, although the development of this framework has been halted by the original developers in 2017.</P>"
        },
        {
          "rank": 19,
          "score": 0.6692333221435547,
          "doc_id": "DIKO0015069923",
          "title": "딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템",
          "abstract": "데이터 예측 시스템들은 데이터를 예측하기 위해 특정 분야의 데이터를 컴퓨터가 분석하여 규칙을 찾아내고 데이터를 예측하였다. 이러한 방법은 과거 데이터를 분석한 결과로 사람이 규칙을 도출할 수 있어야 데이터를 예측하는 것이 가능하였다. 이에 반해 규칙을 도출할 수 없는 데이터들의 데이터를 예측하는 것은 사람의 능력으로는 한계가 있어 정확도가 낮아지는 문제점이 발생할 수 있다.&amp;#xD; 이를 해결하기 위해 컴퓨터를 활용하여 방대한 데이터를 데이터 예측 프로그램에 학습 데이터로 입력하고 결과로 데이터를 예측하였다. 이러한 방법론을 활용하기 위해서 고성능 컴퓨터로 딥 러닝(Deep Learning) 기술을 적용하여 데이터를 예측하고 있다. 해당 방법론이 활용되고 있는 분야로는 기상 데이터를 분석하여 날씨를 예측하는 날씨 분석과 스포츠 경기의 데이터를 예측하는 것이 대표적이다. &amp;#xD; 딥 러닝 기술은 프로그램이 데이터를 기반으로 학습을 진행하고 진행된 학습을 기반으로 데이터를 처리하는 것이다. 이는 과거에 사람이 직접 데이터를 분석하는 것보다 대규모 데이터를 분석하기에 적합하고 이로 인해 정확도가 올라가는 이점이 있다. 또한 목적에 따라 적합한 딥 러닝 모델을 적용하여 데이터를 예측할 경우 정확도의 기댓값이 높아지는 이점이 있다.&amp;#xD; 현재 딥 러닝 모델 중에서 데이터를 예측하기 위해 사용되는 모델은 신경망 구조를 기반으로 하는 DNN(Deep Neural Network) 모델과 RNN(Recurrent Neural Network) 모델이다. DNN 모델은 학습 데이터 내에서 규칙을 찾아내지 못하더라도 반복 학습을 통해 데이터 예측에 대한 정확도를 올릴 수 있고, RNN은 학습 과정 중에서 은닉층에서 적용될 가중치가 학습을 진행할 수록 변화하여 데이터를 예측하고 이로 인해 정확도를 올릴 수 있다. 이에 반해 DNN은 반복 학습의 횟수가 많아야 정확도가 높아지고 RNN은 가중치 변화의 횟수가 많아져야 정확도가 높아지기 때문에 결국 두 모델들은 학습의 반복이 많아져야 하는 문제점이 있다.&amp;#xD; 본 논문에서는 데이터 예측을 위해 딥 러닝 모델 기반 순차 데이터 예측 시스템을 제안한다. 제안하는 시스템에서 비정형 데이터를 순차 데이터로 정제하기 위해 전처리기를 구현하였다. 전처리기는 딥 러닝 모델에 학습 데이터를 입력하기 전에 데이터들을 정제하는 기능을 수행한다. 데이터는 ‘데이터 : 인덱스’ 구조로 이루어진 데이터 쌍이 되고 이러한 데이터 쌍들의 집합을 딥 러닝 모델에 입력하여 학습을 진행한다.&amp;#xD; 딥 러닝 모델은 DNN 모델, 기본 LSTM 모델, 상태유지 LSTM 모델을 활용하여 시스템을 각각 구축한다. 그리고 각 모델들의 설정 값을 변경하면서 정확도의 변화량을 분석한다. 또한 시퀀스의 길이를 변경해가며 실험을 진행하여 가장 정확도가 높은 데이터 셋과 시퀀스 길이의 비율을 제시한다.&amp;#xD; 딥 러닝 모듈 기반 시스템의 실험을 바탕으로 순차 데이터 예측에 가장 정확도가 높고 효율적인 딥 러닝 모듈을 선정하고 기존 시스템들과 비교 분석을 진행하여 제안하는 시스템의 우수성을 검증한다.&amp;#xD; 제안하는 시스템을 활용할 경우 학습 데이터가 적어도 높은 정확도를 요구하는 분야에서 기존 시스템들에 비해 효율성이 높을 것으로 사료된다.&amp;#xD;",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015069923&target=NART&cn=DIKO0015069923",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 데이터 예측 시스템들은 데이터를 예측하기 위해 특정 분야의 데이터를 컴퓨터가 분석하여 규칙을 찾아내고 데이터를 예측하였다. 이러한 방법은 과거 데이터를 분석한 결과로 사람이 규칙을 도출할 수 있어야 데이터를 예측하는 것이 가능하였다. 이에 반해 규칙을 도출할 수 없는 데이터들의 데이터를 예측하는 것은 사람의 능력으로는 한계가 있어 정확도가 낮아지는 문제점이 발생할 수 있다.&amp;#xD; 이를 해결하기 위해 컴퓨터를 활용하여 방대한 데이터를 데이터 예측 프로그램에 학습 데이터로 입력하고 결과로 데이터를 예측하였다. 이러한 방법론을 활용하기 위해서 고성능 컴퓨터로 딥 러닝(Deep Learning) 기술을 적용하여 데이터를 예측하고 있다. 해당 방법론이 활용되고 있는 분야로는 기상 데이터를 분석하여 날씨를 예측하는 날씨 분석과 스포츠 경기의 데이터를 예측하는 것이 대표적이다. &amp;#xD; 딥 러닝 기술은 프로그램이 데이터를 기반으로 학습을 진행하고 진행된 학습을 기반으로 데이터를 처리하는 것이다. 이는 과거에 사람이 직접 데이터를 분석하는 것보다 대규모 데이터를 분석하기에 적합하고 이로 인해 정확도가 올라가는 이점이 있다. 또한 목적에 따라 적합한 딥 러닝 모델을 적용하여 데이터를 예측할 경우 정확도의 기댓값이 높아지는 이점이 있다.&amp;#xD; 현재 딥 러닝 모델 중에서 데이터를 예측하기 위해 사용되는 모델은 신경망 구조를 기반으로 하는 DNN(Deep Neural Network) 모델과 RNN(Recurrent Neural Network) 모델이다. DNN 모델은 학습 데이터 내에서 규칙을 찾아내지 못하더라도 반복 학습을 통해 데이터 예측에 대한 정확도를 올릴 수 있고, RNN은 학습 과정 중에서 은닉층에서 적용될 가중치가 학습을 진행할 수록 변화하여 데이터를 예측하고 이로 인해 정확도를 올릴 수 있다. 이에 반해 DNN은 반복 학습의 횟수가 많아야 정확도가 높아지고 RNN은 가중치 변화의 횟수가 많아져야 정확도가 높아지기 때문에 결국 두 모델들은 학습의 반복이 많아져야 하는 문제점이 있다.&amp;#xD; 본 논문에서는 데이터 예측을 위해 딥 러닝 모델 기반 순차 데이터 예측 시스템을 제안한다. 제안하는 시스템에서 비정형 데이터를 순차 데이터로 정제하기 위해 전처리기를 구현하였다. 전처리기는 딥 러닝 모델에 학습 데이터를 입력하기 전에 데이터들을 정제하는 기능을 수행한다. 데이터는 ‘데이터 : 인덱스’ 구조로 이루어진 데이터 쌍이 되고 이러한 데이터 쌍들의 집합을 딥 러닝 모델에 입력하여 학습을 진행한다.&amp;#xD; 딥 러닝 모델은 DNN 모델, 기본 LSTM 모델, 상태유지 LSTM 모델을 활용하여 시스템을 각각 구축한다. 그리고 각 모델들의 설정 값을 변경하면서 정확도의 변화량을 분석한다. 또한 시퀀스의 길이를 변경해가며 실험을 진행하여 가장 정확도가 높은 데이터 셋과 시퀀스 길이의 비율을 제시한다.&amp;#xD; 딥 러닝 모듈 기반 시스템의 실험을 바탕으로 순차 데이터 예측에 가장 정확도가 높고 효율적인 딥 러닝 모듈을 선정하고 기존 시스템들과 비교 분석을 진행하여 제안하는 시스템의 우수성을 검증한다.&amp;#xD; 제안하는 시스템을 활용할 경우 학습 데이터가 적어도 높은 정확도를 요구하는 분야에서 기존 시스템들에 비해 효율성이 높을 것으로 사료된다.&amp;#xD;"
        },
        {
          "rank": 20,
          "score": 0.6671067476272583,
          "doc_id": "JAKO201905653788881",
          "title": "실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안",
          "abstract": "오늘날의 기술 환경에서 대다수의 빅 데이터 기반 애플리케이션 및 솔루션은 스트리밍 데이터의 실시간 처리를 기반으로 한다. 빅 데이터 스트림의 실시간 처리 및 분석은 빅 데이터 기반 애플리케이션 및 솔루션 개발에서 중요한 역할을 한다. 특히 해사 분야 데이터 처리 환경에서도 데이터의 폭발적 증대에 따른 대용량 실시간 데이터를 빠르게 처리 및 분석할 수 있는 기술 개발의 필요성이 가속화되고 있다. 따라서 본 논문에서는 다양한 빅 데이터 처리를 위한 오픈소스 기술 중에 적합한 오픈소스로 NiFi, Kafka, Druid의 특징을 분석하여 한국형 e-Navigation 서비스에서 해사 분야 서비스 분석에 필요한 외부 연계 필요 정보들을 상시 최신 정보로 제공할 수 있도록 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 기술 적용의 기초를 마련하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201905653788881&target=NART&cn=JAKO201905653788881",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안 오늘날의 기술 환경에서 대다수의 빅 데이터 기반 애플리케이션 및 솔루션은 스트리밍 데이터의 실시간 처리를 기반으로 한다. 빅 데이터 스트림의 실시간 처리 및 분석은 빅 데이터 기반 애플리케이션 및 솔루션 개발에서 중요한 역할을 한다. 특히 해사 분야 데이터 처리 환경에서도 데이터의 폭발적 증대에 따른 대용량 실시간 데이터를 빠르게 처리 및 분석할 수 있는 기술 개발의 필요성이 가속화되고 있다. 따라서 본 논문에서는 다양한 빅 데이터 처리를 위한 오픈소스 기술 중에 적합한 오픈소스로 NiFi, Kafka, Druid의 특징을 분석하여 한국형 e-Navigation 서비스에서 해사 분야 서비스 분석에 필요한 외부 연계 필요 정보들을 상시 최신 정보로 제공할 수 있도록 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 기술 적용의 기초를 마련하고자 한다."
        },
        {
          "rank": 21,
          "score": 0.6649354100227356,
          "doc_id": "NART107287464",
          "title": "Benchmarking open source deep learning frameworks",
          "abstract": "<P>Deep Learning (DL) is one of the hottest fields. To foster the growth of DL, several open source frameworks appeared providing implementations of the most common DL algorithms. These frameworks vary in the algorithms they support and in the quality of their implementations. The purpose of this work is to provide a qualitative and quantitative comparison among three such frameworks: TensorFlow, Theano and CNTK. To ensure that our study is as comprehensive as possible, we consider multiple benchmark datasets from different fields (image processing, NLP, etc.) and measure the performance of the frameworks' implementations of different DL algorithms. For most of our experiments, we find out that CNTK's implementations are superior to the other ones under consideration.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART107287464&target=NART&cn=NART107287464",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Benchmarking open source deep learning frameworks Benchmarking open source deep learning frameworks Benchmarking open source deep learning frameworks <P>Deep Learning (DL) is one of the hottest fields. To foster the growth of DL, several open source frameworks appeared providing implementations of the most common DL algorithms. These frameworks vary in the algorithms they support and in the quality of their implementations. The purpose of this work is to provide a qualitative and quantitative comparison among three such frameworks: TensorFlow, Theano and CNTK. To ensure that our study is as comprehensive as possible, we consider multiple benchmark datasets from different fields (image processing, NLP, etc.) and measure the performance of the frameworks' implementations of different DL algorithms. For most of our experiments, we find out that CNTK's implementations are superior to the other ones under consideration.</P>"
        },
        {
          "rank": 22,
          "score": 0.6633464097976685,
          "doc_id": "JAKO201607457888322",
          "title": "멀티사이트 기반 클라우드 환경의 구성 자동화를 위한 SmartX 프로비저닝 프레임워크",
          "abstract": "다양한 ICT 인프라 기술들을 종합적으로 활용하는 클라우드의 대표적인 오픈소스 프로젝트인 오픈스택을 활용하여 멀티사이트 기반의 클라우드 인프라 구축 시 설치 복잡성 및 지리적인 제약으로 인한 인적, 시간적인 비효율성을 내포한다. 이러한 비효율성을 해소하기 위해 멀티사이트 환경의 오픈스택 실증 테스트베드인 OF@KOREN SmartX 놀이터 (Playground)를 대상으로 리눅스 및 오픈스택을 설치/설정을 자동화하는 도구를 데브옵스 (DevOps) 개발 방법론에 따라 점진적으로 개발해왔다. 하지만 이전 개발도구들이 고정된 형태의 프로비저닝만을 제공한다는 한계를 해결하고자 본 논문에서는 소프트웨어 정의 인프라의 자원관리 구도에 따라 자동 설치/설정도구를 엮어 Playground 수준의 프로비저닝을 수행하는 SmartX 프로비저닝 프레임워크의 프로토타입을 설계, 개발한다. 그리고 멀티사이트 오픈스택 클라우드를 자동으로 구축하는 과정을 제시함으로써 프레임워크의 효율적인 놀이터 프로비저닝 기능에 대해 검증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201607457888322&target=NART&cn=JAKO201607457888322",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "멀티사이트 기반 클라우드 환경의 구성 자동화를 위한 SmartX 프로비저닝 프레임워크 멀티사이트 기반 클라우드 환경의 구성 자동화를 위한 SmartX 프로비저닝 프레임워크 멀티사이트 기반 클라우드 환경의 구성 자동화를 위한 SmartX 프로비저닝 프레임워크 다양한 ICT 인프라 기술들을 종합적으로 활용하는 클라우드의 대표적인 오픈소스 프로젝트인 오픈스택을 활용하여 멀티사이트 기반의 클라우드 인프라 구축 시 설치 복잡성 및 지리적인 제약으로 인한 인적, 시간적인 비효율성을 내포한다. 이러한 비효율성을 해소하기 위해 멀티사이트 환경의 오픈스택 실증 테스트베드인 OF@KOREN SmartX 놀이터 (Playground)를 대상으로 리눅스 및 오픈스택을 설치/설정을 자동화하는 도구를 데브옵스 (DevOps) 개발 방법론에 따라 점진적으로 개발해왔다. 하지만 이전 개발도구들이 고정된 형태의 프로비저닝만을 제공한다는 한계를 해결하고자 본 논문에서는 소프트웨어 정의 인프라의 자원관리 구도에 따라 자동 설치/설정도구를 엮어 Playground 수준의 프로비저닝을 수행하는 SmartX 프로비저닝 프레임워크의 프로토타입을 설계, 개발한다. 그리고 멀티사이트 오픈스택 클라우드를 자동으로 구축하는 과정을 제시함으로써 프레임워크의 효율적인 놀이터 프로비저닝 기능에 대해 검증한다."
        },
        {
          "rank": 23,
          "score": 0.6626549959182739,
          "doc_id": "JAKO201722163438668",
          "title": "통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가",
          "abstract": "최근, 딥러닝을 사용 가능한 임베디드 디바이스가 상용화됨에 따라 임베디드 시스템 영역에서도 딥러닝 활용에 대한 다양한 연구가 진행되고 있다. 그러나 임베디드 시스템을 고성능 PC 환경과 비교하면 상대적으로 저사양의 CPU/GPU 프로세서와 메모리를 탑재하고 있으므로 딥러닝 기술의 적용에 있어서 많은 제약이 있다. 본 논문에서는 다양한 최신 딥러닝 네트워크들을 임베디드 디바이스에 적용했을때의 성능을 시간과 전력이라는 관점에서 실험적으로 평가한다. 또한, 호스트 CPU와 GPU 디바이스간의 메모리를 공유하는 임베디드 시스템들의 아키텍처적인 특성을 이용하여 메모리 복사를 줄임으로써 실시간 성능과 저전력성을 높이는 방법을 제시한다. 제안된 방법은 대표적인 공개 딥러닝 프레임워크인 Caffe를 수정하여 구현되었으며, 임베디드 GPU를 탑재한 NVIDIA Jetson TK1에서 성능평가 되었다. 실험결과, 대부분의 딥러닝 네트워크에서 뚜렷한 성능향상을 관찰할 수 있었다. 특히, 메모리 사용량이 높은 AlexNet에서 약 33%의 이미지 인식 속도 단축과 50%의 소비 전력량 감소를 관찰할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201722163438668&target=NART&cn=JAKO201722163438668",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 최근, 딥러닝을 사용 가능한 임베디드 디바이스가 상용화됨에 따라 임베디드 시스템 영역에서도 딥러닝 활용에 대한 다양한 연구가 진행되고 있다. 그러나 임베디드 시스템을 고성능 PC 환경과 비교하면 상대적으로 저사양의 CPU/GPU 프로세서와 메모리를 탑재하고 있으므로 딥러닝 기술의 적용에 있어서 많은 제약이 있다. 본 논문에서는 다양한 최신 딥러닝 네트워크들을 임베디드 디바이스에 적용했을때의 성능을 시간과 전력이라는 관점에서 실험적으로 평가한다. 또한, 호스트 CPU와 GPU 디바이스간의 메모리를 공유하는 임베디드 시스템들의 아키텍처적인 특성을 이용하여 메모리 복사를 줄임으로써 실시간 성능과 저전력성을 높이는 방법을 제시한다. 제안된 방법은 대표적인 공개 딥러닝 프레임워크인 Caffe를 수정하여 구현되었으며, 임베디드 GPU를 탑재한 NVIDIA Jetson TK1에서 성능평가 되었다. 실험결과, 대부분의 딥러닝 네트워크에서 뚜렷한 성능향상을 관찰할 수 있었다. 특히, 메모리 사용량이 높은 AlexNet에서 약 33%의 이미지 인식 속도 단축과 50%의 소비 전력량 감소를 관찰할 수 있었다."
        },
        {
          "rank": 24,
          "score": 0.6576288938522339,
          "doc_id": "NART96288640",
          "title": "Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms",
          "abstract": "<P><B>Abstract</B></P>  <P>One of the primary goals of the artificial intelligence field is to produce fully autonomous agents that interact with theirenvironments to learn optimal behaviors, improving over time through trial and error. A mathematical principled framework for experience-driven autonomous learning is reinforcement learning, but they are inherently limited to low-dimensional problems,but the deep learning boom has provided new tools to overcome these problems. For deep reinforcement learning teaching, we do not have an appropriate platform for making optimal labs. In the article, after studying the theoretical foundations and the requirements of the main platforms, we selected two open source platforms, according to their characteristics: robotic simulators platforms for teaching and benchmarking deep reinforcement learning algorithms. The first platform was <I>Gym and V-REP</I> and the second one, <I>KNIME Deeplearning4J Integration supports and Teaching-Box.</I> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART96288640&target=NART&cn=NART96288640",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms <P><B>Abstract</B></P>  <P>One of the primary goals of the artificial intelligence field is to produce fully autonomous agents that interact with theirenvironments to learn optimal behaviors, improving over time through trial and error. A mathematical principled framework for experience-driven autonomous learning is reinforcement learning, but they are inherently limited to low-dimensional problems,but the deep learning boom has provided new tools to overcome these problems. For deep reinforcement learning teaching, we do not have an appropriate platform for making optimal labs. In the article, after studying the theoretical foundations and the requirements of the main platforms, we selected two open source platforms, according to their characteristics: robotic simulators platforms for teaching and benchmarking deep reinforcement learning algorithms. The first platform was <I>Gym and V-REP</I> and the second one, <I>KNIME Deeplearning4J Integration supports and Teaching-Box.</I> </P>"
        },
        {
          "rank": 25,
          "score": 0.6548177003860474,
          "doc_id": "NART108328574",
          "title": "A fully open-source framework for deep learning protein real-valued distances",
          "abstract": "<P>As deep learning algorithms drive the progress in protein structure prediction, a lot remains to be studied at this merging superhighway of deep learning and protein structure prediction. Recent findings show that inter-residue distance prediction, a more granular version of the well-known contact prediction problem, is a key to predicting accurate models. However, deep learning methods that predict these distances are still in the early stages of their development. To advance these methods and develop other novel methods, a need exists for a small and representative dataset packaged for faster development and testing. In this work, we introduce protein distance net (PDNET), a framework that consists of one such representative dataset along with the scripts for training and testing deep learning methods. The framework also includes all the scripts that were used to curate the dataset, and generate the input features and distance maps. Deep learning models can also be trained and tested in a web browser using free platforms such as Google Colab. We discuss how PDNET can be used to predict contacts, distance intervals, and real-valued distances.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART108328574&target=NART&cn=NART108328574",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A fully open-source framework for deep learning protein real-valued distances A fully open-source framework for deep learning protein real-valued distances A fully open-source framework for deep learning protein real-valued distances <P>As deep learning algorithms drive the progress in protein structure prediction, a lot remains to be studied at this merging superhighway of deep learning and protein structure prediction. Recent findings show that inter-residue distance prediction, a more granular version of the well-known contact prediction problem, is a key to predicting accurate models. However, deep learning methods that predict these distances are still in the early stages of their development. To advance these methods and develop other novel methods, a need exists for a small and representative dataset packaged for faster development and testing. In this work, we introduce protein distance net (PDNET), a framework that consists of one such representative dataset along with the scripts for training and testing deep learning methods. The framework also includes all the scripts that were used to curate the dataset, and generate the input features and distance maps. Deep learning models can also be trained and tested in a web browser using free platforms such as Google Colab. We discuss how PDNET can be used to predict contacts, distance intervals, and real-valued distances.</P>"
        },
        {
          "rank": 26,
          "score": 0.6522198915481567,
          "doc_id": "JAKO201835146898566",
          "title": "클라우드 컴퓨팅 환경에서 가상화 관리 융합접근제어 모델",
          "abstract": "접근제어 목적은 컴퓨팅 자원을 불법적인 사용자로부터 유출, 수정, 파괴와 같은 비합법적인 행위로부터 원천적으로 차단하고 보호하는데 있다. 클라우드 컴퓨팅 환경이 가상화 기술을 활용한 자원공유 서비스로 확장됨에 따라 동적이고 안전한 클라우드 기반 서비스를 제공하기 위해서는 새로운 보안 모델과 접근제어 기법이 요구되어진다. 본 가상화 관리 융합접근제어 모델은 역할기반 접근제어 기법에 동적 권한 배정 기능을 적용하여 유연한 사용자 권한 부여 기능을 제공하였다. 또한 보안등급과 규칙에 의거한 접근제어 기법을 적용함으로써 공유개념의 가상머신 시스템에서 권한충돌 문제 해결과 물리적 자원의 안전성을 보장토록 하였다. 본 모델은 안전하고 효율적인 클라우드 기반의 가상화 관리 시스템을 구축하는데 도움이 될 것이며 향후 다단계 특성을 반영한 메카니즘으로 확장될 필요성이 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201835146898566&target=NART&cn=JAKO201835146898566",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 컴퓨팅 환경에서 가상화 관리 융합접근제어 모델 클라우드 컴퓨팅 환경에서 가상화 관리 융합접근제어 모델 클라우드 컴퓨팅 환경에서 가상화 관리 융합접근제어 모델 접근제어 목적은 컴퓨팅 자원을 불법적인 사용자로부터 유출, 수정, 파괴와 같은 비합법적인 행위로부터 원천적으로 차단하고 보호하는데 있다. 클라우드 컴퓨팅 환경이 가상화 기술을 활용한 자원공유 서비스로 확장됨에 따라 동적이고 안전한 클라우드 기반 서비스를 제공하기 위해서는 새로운 보안 모델과 접근제어 기법이 요구되어진다. 본 가상화 관리 융합접근제어 모델은 역할기반 접근제어 기법에 동적 권한 배정 기능을 적용하여 유연한 사용자 권한 부여 기능을 제공하였다. 또한 보안등급과 규칙에 의거한 접근제어 기법을 적용함으로써 공유개념의 가상머신 시스템에서 권한충돌 문제 해결과 물리적 자원의 안전성을 보장토록 하였다. 본 모델은 안전하고 효율적인 클라우드 기반의 가상화 관리 시스템을 구축하는데 도움이 될 것이며 향후 다단계 특성을 반영한 메카니즘으로 확장될 필요성이 있다."
        },
        {
          "rank": 27,
          "score": 0.6492005586624146,
          "doc_id": "DIKO0016938177",
          "title": "분산학습 환경을 적용한 연속학습",
          "abstract": "본 논문은 의료 분야에 특화된 연속 학습과 분할 학습 기법을 통합한 혁신적인 프레임워크를 제안한다. 이 프레임워크는 의료 인공지능 모델이 학습할 때 실제 환경에서 발생하는 데이터 프라이버시, 보안, 그리고 변화하는 임상 조건과 같은 주요 과제를 해결한다. 제안된 프레임워크를 통해 의료 클라이언트는 민감한 데이터를 공유하지 않으면서 분산 컴퓨팅 리소스에서 딥러닝 모델을 공동으로 학습할 수 있으며, 연산 비용(computational cost)을 줄일 수 있다. 또한, 제안된 프레임워크는 변이와 새로운 질병과 같은 의료 환경의 증분 특성(incremental characteristic)을 고려함으로써 실제 임상 시나리오에서 의료 인공지능 모델의 질병 진단 능력을 향상할 수 있다. 본 논문은 여러 가지의 의료 이미지 데이터셋과 일반 이미지 데이터셋을 사용하여 프레임워크의 성능을 검증했다. 그 결과 제안된 프레임워크는 분할 학습 상황에서 다른 연속 학습 방법보다 평균 정확도(average accuracy)와 평균 망각(average forgetting) 성능이 우수함을 보였으며, 핵심 요소 분석을 통해 성능을 여러 방면에서 분석했다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016938177&target=NART&cn=DIKO0016938177",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "분산학습 환경을 적용한 연속학습 분산학습 환경을 적용한 연속학습 분산학습 환경을 적용한 연속학습 본 논문은 의료 분야에 특화된 연속 학습과 분할 학습 기법을 통합한 혁신적인 프레임워크를 제안한다. 이 프레임워크는 의료 인공지능 모델이 학습할 때 실제 환경에서 발생하는 데이터 프라이버시, 보안, 그리고 변화하는 임상 조건과 같은 주요 과제를 해결한다. 제안된 프레임워크를 통해 의료 클라이언트는 민감한 데이터를 공유하지 않으면서 분산 컴퓨팅 리소스에서 딥러닝 모델을 공동으로 학습할 수 있으며, 연산 비용(computational cost)을 줄일 수 있다. 또한, 제안된 프레임워크는 변이와 새로운 질병과 같은 의료 환경의 증분 특성(incremental characteristic)을 고려함으로써 실제 임상 시나리오에서 의료 인공지능 모델의 질병 진단 능력을 향상할 수 있다. 본 논문은 여러 가지의 의료 이미지 데이터셋과 일반 이미지 데이터셋을 사용하여 프레임워크의 성능을 검증했다. 그 결과 제안된 프레임워크는 분할 학습 상황에서 다른 연속 학습 방법보다 평균 정확도(average accuracy)와 평균 망각(average forgetting) 성능이 우수함을 보였으며, 핵심 요소 분석을 통해 성능을 여러 방면에서 분석했다."
        },
        {
          "rank": 28,
          "score": 0.6479662656784058,
          "doc_id": "NART132433770",
          "title": "A Deep Learning-Based Car Accident Detection Framework Using Edge and Cloud Computing",
          "abstract": "<P> The evolving technological landscape has seen a pivotal shift with the advent of edge computing, transforming various sectors, particularly accident detection. Edge computing enhances road safety by enabling realtime data processing from onboard sensors, cameras, and connected devices, addressing limitations in traditional cloudbased systems. This paper introduces a deep learning-based accident detection framework within an edge-cloud setup. Utilizing a CNN model, accidents are detected at the edge node near the data source, ensuring low latency, reduced network usage, and faster execution times. The model achieves a remarkable 95.91% accuracy.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART132433770&target=NART&cn=NART132433770",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A Deep Learning-Based Car Accident Detection Framework Using Edge and Cloud Computing A Deep Learning-Based Car Accident Detection Framework Using Edge and Cloud Computing A Deep Learning-Based Car Accident Detection Framework Using Edge and Cloud Computing <P> The evolving technological landscape has seen a pivotal shift with the advent of edge computing, transforming various sectors, particularly accident detection. Edge computing enhances road safety by enabling realtime data processing from onboard sensors, cameras, and connected devices, addressing limitations in traditional cloudbased systems. This paper introduces a deep learning-based accident detection framework within an edge-cloud setup. Utilizing a CNN model, accidents are detected at the edge node near the data source, ensuring low latency, reduced network usage, and faster execution times. The model achieves a remarkable 95.91% accuracy.</P>"
        },
        {
          "rank": 29,
          "score": 0.6464154720306396,
          "doc_id": "DIKO0014861002",
          "title": "딥 러닝기반 고객평점 예측모델",
          "abstract": "인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014861002&target=NART&cn=DIKO0014861002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다."
        },
        {
          "rank": 30,
          "score": 0.6457102298736572,
          "doc_id": "ATN0037504188",
          "title": "오픈소스 기반 서버리스 프레임워크에서의 서비스 메시 구조 및 서비스 성능 평가",
          "abstract": "Service mesh is a technology that enables service calls between internal services in a micro service architecture, and is widely used in a serverless framework that implements the functions of a micro service architecture in a cloud environment. However, the increase in service calls between internal services has a problem of delaying the overall service response speed, so frequent service calls within the framework should be avoided. Therefore, in this paper, a method of using service mesh without deteriorating the overall performance of the serverless framework was proposed, and for this purpose, the performance and function of the implemented service mesh was verified using OpenFx, an open source based serverless framework applying gRPC. This will be helpful to service developers for service distribution and management targeting serverless frameworks.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037504188&target=NART&cn=ATN0037504188",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈소스 기반 서버리스 프레임워크에서의 서비스 메시 구조 및 서비스 성능 평가 오픈소스 기반 서버리스 프레임워크에서의 서비스 메시 구조 및 서비스 성능 평가 오픈소스 기반 서버리스 프레임워크에서의 서비스 메시 구조 및 서비스 성능 평가 Service mesh is a technology that enables service calls between internal services in a micro service architecture, and is widely used in a serverless framework that implements the functions of a micro service architecture in a cloud environment. However, the increase in service calls between internal services has a problem of delaying the overall service response speed, so frequent service calls within the framework should be avoided. Therefore, in this paper, a method of using service mesh without deteriorating the overall performance of the serverless framework was proposed, and for this purpose, the performance and function of the implemented service mesh was verified using OpenFx, an open source based serverless framework applying gRPC. This will be helpful to service developers for service distribution and management targeting serverless frameworks."
        },
        {
          "rank": 31,
          "score": 0.6450362205505371,
          "doc_id": "NPAP10408992",
          "title": "Open Source Cloud Computing Platforms",
          "abstract": "<P>With the popularization of cloud computing, several enterprises and open-source communities have developed their own cloud solutions. A number of factors weigh on user selection, as each one has peculiar characteristics and may target different usage scenarios. Considering such challenge, this paper focuses on giving the reader an understanding of some major existing open cloud computing solutions - XCP, Eucalyptus and Open Nebula. Hopefully, a deep comparison of such solutions can leverage the cloud computing research area providing a good starting point to research groups and interested readers.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP10408992&target=NART&cn=NPAP10408992",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Open Source Cloud Computing Platforms Open Source Cloud Computing Platforms Open Source Cloud Computing Platforms <P>With the popularization of cloud computing, several enterprises and open-source communities have developed their own cloud solutions. A number of factors weigh on user selection, as each one has peculiar characteristics and may target different usage scenarios. Considering such challenge, this paper focuses on giving the reader an understanding of some major existing open cloud computing solutions - XCP, Eucalyptus and Open Nebula. Hopefully, a deep comparison of such solutions can leverage the cloud computing research area providing a good starting point to research groups and interested readers.</P>"
        },
        {
          "rank": 32,
          "score": 0.6440966129302979,
          "doc_id": "DIKO0013400117",
          "title": "오픈소스를 이용한 포렌식 클라우드 시스템 프레임워크 설계 및 구현",
          "abstract": "With the growing use of digital devices such as computer, smart phone or portable data storages, digital information has become a greater priority as evidence for crime investigation. As a result, digital forensics is under the spotlight as the proper alternative to address these issues because it is a branch of forensic science encompassing the recovery and investigation of material which was found in digital devices, often in relation to computer crime. The process of digital forensics are generally divided into the two major tasks: data acquisition and analysis. This sounds so easy but it is not because there are many requirements closely related to the law. Specially, traditional forensic tools and techniques have various limitations considering the rapid changes of IT environment like Big data. Therefore, this thesis proposes forensics-cloud framework for the forensics tool based on open sources. In this thesis, first, limitations of existing forensics tools is analyzed and considerations for forensics-cloud system is examined. And then, the framework and architecture of the forensics-cloud system is proposed. Also, the usage flow diagram of the proposed forensics cloud tool is described. Finally, this thesis suggests the implementation result of SaaS model for the proposed system. The proposed framework has scalability and can improve functional aspects and efficiency of the existing forensics tools.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0013400117&target=NART&cn=DIKO0013400117",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈소스를 이용한 포렌식 클라우드 시스템 프레임워크 설계 및 구현 오픈소스를 이용한 포렌식 클라우드 시스템 프레임워크 설계 및 구현 오픈소스를 이용한 포렌식 클라우드 시스템 프레임워크 설계 및 구현 With the growing use of digital devices such as computer, smart phone or portable data storages, digital information has become a greater priority as evidence for crime investigation. As a result, digital forensics is under the spotlight as the proper alternative to address these issues because it is a branch of forensic science encompassing the recovery and investigation of material which was found in digital devices, often in relation to computer crime. The process of digital forensics are generally divided into the two major tasks: data acquisition and analysis. This sounds so easy but it is not because there are many requirements closely related to the law. Specially, traditional forensic tools and techniques have various limitations considering the rapid changes of IT environment like Big data. Therefore, this thesis proposes forensics-cloud framework for the forensics tool based on open sources. In this thesis, first, limitations of existing forensics tools is analyzed and considerations for forensics-cloud system is examined. And then, the framework and architecture of the forensics-cloud system is proposed. Also, the usage flow diagram of the proposed forensics cloud tool is described. Finally, this thesis suggests the implementation result of SaaS model for the proposed system. The proposed framework has scalability and can improve functional aspects and efficiency of the existing forensics tools."
        },
        {
          "rank": 33,
          "score": 0.6414532661437988,
          "doc_id": "ATN0037493744",
          "title": "digo: 생산성 향상을 위한 딥러닝 실험 관리 시스템",
          "abstract": "Recently, advanced service using artificial intelligence has become a necessity, not an option. As a result, research on artificial intelligence has been accelerated, drawing attention to methods for efficient artificial intelligence research. A typical method is to use tools to effectively manage experiments in the course of the study. Existing deep learning studies have been inefficient due to collaboration based on fragmentary work methods and repetitive tasks for optimizing learning results. To improve these problems, this work designs and implements Digo (a combination of words that represent repetitive deep learning research as a compound word of dig and go), a collaborative-based deep learning experiment management tool that can provide a convenient and productive research environment, focusing on deep learning among artificial intelligence. Experiments and surveys were conducted on machine learning researchers to validate the performance of deep learning experimental management tools, and to confirm the convenience of hyperparameter automatic optimization and learning result visualization features.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037493744&target=NART&cn=ATN0037493744",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "digo: 생산성 향상을 위한 딥러닝 실험 관리 시스템 digo: 생산성 향상을 위한 딥러닝 실험 관리 시스템 digo: 생산성 향상을 위한 딥러닝 실험 관리 시스템 Recently, advanced service using artificial intelligence has become a necessity, not an option. As a result, research on artificial intelligence has been accelerated, drawing attention to methods for efficient artificial intelligence research. A typical method is to use tools to effectively manage experiments in the course of the study. Existing deep learning studies have been inefficient due to collaboration based on fragmentary work methods and repetitive tasks for optimizing learning results. To improve these problems, this work designs and implements Digo (a combination of words that represent repetitive deep learning research as a compound word of dig and go), a collaborative-based deep learning experiment management tool that can provide a convenient and productive research environment, focusing on deep learning among artificial intelligence. Experiments and surveys were conducted on machine learning researchers to validate the performance of deep learning experimental management tools, and to confirm the convenience of hyperparameter automatic optimization and learning result visualization features."
        },
        {
          "rank": 34,
          "score": 0.6397545337677002,
          "doc_id": "DIKO0013710110",
          "title": "딥 러닝을 이용한 DC 모터 제어",
          "abstract": "딥 러닝(deep learning)은 최근에 많이 알려지게 된 심층 인공신경망 알고리즘이다. 일반적인 인공신경망보다 은닉층의 개수와 뉴런의 개수를 확장시키고, 학습이 효율적으로 될 수 있게 알고리즘을 개선한 것이 가장 큰 특징이다. 이러한 특징을 활용하여 기존의 인공신경망으로 풀지 못했던 크고 복잡한 문제들을 해결할 수 있게 되었다. 음성인식, 손 글씨 인식, 얼굴 인식 등 복잡한 패턴인식과 분류에 관련된 다양한 분야에 대한 적용 연구가 활발히 진행되고 있다. 하지만 이러한 장점에도 불구하고, 아직까지 딥 러닝이 제어문제를 해결하기 위해 적용된 사례는 찾아보기 어렵다. 본 논문에서는 간단한 사례를 통해 딥 러닝의 제어문제에 대한 적용 가능성을 확인해 본다. 딥 러닝 알고리즘 중에서 가장 잘 알려진, 깊은 믿음 네트워크(deep belief network) 알고리즘을 사용하여 산업현장에서 가장 많이 사용되고 있는 PID 제어기를 모방하는 딥 러닝 제어기를 설계한다. DC 모터를 제어하는 시스템에서 PID 제어기에 들어오는 입력과 PID 제어기에서 나오는 출력값을 학습 데이터로 사용하여 딥 러닝으로 학습하는 방법을 사용한다. 시뮬레이션을 통해 제안한 딥 러닝 제어기와 PID 제어기를 비교하여 딥 러닝 알고리즘의 성능을 검증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0013710110&target=NART&cn=DIKO0013710110",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝을 이용한 DC 모터 제어 딥 러닝을 이용한 DC 모터 제어 딥 러닝을 이용한 DC 모터 제어 딥 러닝(deep learning)은 최근에 많이 알려지게 된 심층 인공신경망 알고리즘이다. 일반적인 인공신경망보다 은닉층의 개수와 뉴런의 개수를 확장시키고, 학습이 효율적으로 될 수 있게 알고리즘을 개선한 것이 가장 큰 특징이다. 이러한 특징을 활용하여 기존의 인공신경망으로 풀지 못했던 크고 복잡한 문제들을 해결할 수 있게 되었다. 음성인식, 손 글씨 인식, 얼굴 인식 등 복잡한 패턴인식과 분류에 관련된 다양한 분야에 대한 적용 연구가 활발히 진행되고 있다. 하지만 이러한 장점에도 불구하고, 아직까지 딥 러닝이 제어문제를 해결하기 위해 적용된 사례는 찾아보기 어렵다. 본 논문에서는 간단한 사례를 통해 딥 러닝의 제어문제에 대한 적용 가능성을 확인해 본다. 딥 러닝 알고리즘 중에서 가장 잘 알려진, 깊은 믿음 네트워크(deep belief network) 알고리즘을 사용하여 산업현장에서 가장 많이 사용되고 있는 PID 제어기를 모방하는 딥 러닝 제어기를 설계한다. DC 모터를 제어하는 시스템에서 PID 제어기에 들어오는 입력과 PID 제어기에서 나오는 출력값을 학습 데이터로 사용하여 딥 러닝으로 학습하는 방법을 사용한다. 시뮬레이션을 통해 제안한 딥 러닝 제어기와 PID 제어기를 비교하여 딥 러닝 알고리즘의 성능을 검증한다."
        },
        {
          "rank": 35,
          "score": 0.6392089128494263,
          "doc_id": "JAKO201809258120261",
          "title": "클라우드서비스 활성화를 위한 서비스수준협약(SLA) 프레임워크",
          "abstract": "클라우드 서비스가 확대되고 있지만, 많은 이용자들은 클라우드 서비스의 도입에 어려움을 겪고 있다. 이는 이용자 입장에서 어떤 클라우드 서비스를 신뢰할 수 있는지에 대한 정보가 없기 때문이다. 클라우드 서비스수준협약(클라우드 SLA)은 클라우드 서비스를 제공하는 공급자와 이용자간에 서비스의 품질과 성능 등을 포함한 정성적인 지표와 정량적 지표를 이용하여 협의하는 것이다. 이 연구에서는 클라우드 서비스 사업자의 서비스 수준을 향상하고 이용자를 보호하기 위해 국내 클라우드 산업에 적용할 수 있는 클라우드 SLA를 위한 프레임워크 제안하고 이를 이용하여 국내 클라우드 산업에 적용할 수 있는 클라우드 SLA의 세부 구성항목을 도출하고자 한다. 이를 통해 정부의 '클라우드컴퓨팅 발전 및 이용자 보호에 관한 법률' 체계하의 클라우드 서비스 제공자와 이용자간의 신뢰도 향상을 위한 정책적 활용은 물론 궁극적으로 국내 클라우드 서비스의 품질 성능 수준 향상 및 이용자 신뢰기반 조성을 통해 클라우드 서비스의 활성화를 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201809258120261&target=NART&cn=JAKO201809258120261",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드서비스 활성화를 위한 서비스수준협약(SLA) 프레임워크 클라우드서비스 활성화를 위한 서비스수준협약(SLA) 프레임워크 클라우드서비스 활성화를 위한 서비스수준협약(SLA) 프레임워크 클라우드 서비스가 확대되고 있지만, 많은 이용자들은 클라우드 서비스의 도입에 어려움을 겪고 있다. 이는 이용자 입장에서 어떤 클라우드 서비스를 신뢰할 수 있는지에 대한 정보가 없기 때문이다. 클라우드 서비스수준협약(클라우드 SLA)은 클라우드 서비스를 제공하는 공급자와 이용자간에 서비스의 품질과 성능 등을 포함한 정성적인 지표와 정량적 지표를 이용하여 협의하는 것이다. 이 연구에서는 클라우드 서비스 사업자의 서비스 수준을 향상하고 이용자를 보호하기 위해 국내 클라우드 산업에 적용할 수 있는 클라우드 SLA를 위한 프레임워크 제안하고 이를 이용하여 국내 클라우드 산업에 적용할 수 있는 클라우드 SLA의 세부 구성항목을 도출하고자 한다. 이를 통해 정부의 '클라우드컴퓨팅 발전 및 이용자 보호에 관한 법률' 체계하의 클라우드 서비스 제공자와 이용자간의 신뢰도 향상을 위한 정책적 활용은 물론 궁극적으로 국내 클라우드 서비스의 품질 성능 수준 향상 및 이용자 신뢰기반 조성을 통해 클라우드 서비스의 활성화를 기대한다."
        },
        {
          "rank": 36,
          "score": 0.6386933326721191,
          "doc_id": "DIKO0009828582",
          "title": "오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석",
          "abstract": "객체 지향 기술과 관계형 기술은 대부분의 기업에서 어플리케이션을 개발할 때 공통적으로 사용되는 기술이다. 객체 지향 기술은 데이터와 행위를 가진 객체를 통해 어플리케이션 구축을 지원하며, 관계형 기술은 데이터 저장과 프로시저나 SQL를 통한 데이터 조작을 지원한다. 하지만 명확하게 두 기술은 서로 다르다. 이처럼 객체 기술과 관계형 기술을 같이 사용했을 매 발생하는 두 기술간의 불일치를 'object-relational impedance mismatch'라고 한다. 이러한 문제를 해결하기 위해 등장한 기술중의 하나가 바로 ORM(Object/Relational Mapping)이다. 본 논문에서는 ORM 기술을 지원하는 ORM 툴로서의 오픈 소스 자바 프레임 워크를 성능이나 코드 복잡성, 관리 용이성 등 다양한 측면에서 비교 분석하였다. 현재 30여가지가 넘는 다양한 오픈 소스 자바 프레임워크가 개발되어 배포되고 있지만, 본 논문에서는 Hibernate, iBatis SqlMaps, Apache OJB 이렇게 새 개의 프래임워크를 현재 객체 지속성을 위해 가장 많이 사용되는 JDBC 기술을 기준으로 비교 분석하였다. 데이터에 대한 CRUD(저장,추출,수정,삭제)를 수행하는 시간을 통해 성능 분석을 실시하였으며, 사례 어플리케이션 구현을 통해 각 프레임워크 별로 CRUD를 수행하는 메소드 구현 시 코드량 분석을 통해 코드 복잡성을, 요구 사항 변경 시 어떻게 각 프레임워크가 이를 반영하는지를 통해 관리 용이성을 분석하였다. 이러한 분석을 통해 각 프레임워크가 어떠한 서비스를 제공하는지, 각 프레임워크의 성능은 어떠한지 쉽게 알 수 있다. 따라서 기업은 좀더 명확한 근거를 통해 어플리케이션 개발에 적절한 퍼시스턴스 프레임워크를 선택할 수 있을 것이다. 또한 상용과 오픈 소스 기반의 프레임워크 중 어떠한 것을 도입해야 할지 결정해야 할 경우 중요한 참고 자료로 활용할 수 있을 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0009828582&target=NART&cn=DIKO0009828582",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 객체 지향 기술과 관계형 기술은 대부분의 기업에서 어플리케이션을 개발할 때 공통적으로 사용되는 기술이다. 객체 지향 기술은 데이터와 행위를 가진 객체를 통해 어플리케이션 구축을 지원하며, 관계형 기술은 데이터 저장과 프로시저나 SQL를 통한 데이터 조작을 지원한다. 하지만 명확하게 두 기술은 서로 다르다. 이처럼 객체 기술과 관계형 기술을 같이 사용했을 매 발생하는 두 기술간의 불일치를 'object-relational impedance mismatch'라고 한다. 이러한 문제를 해결하기 위해 등장한 기술중의 하나가 바로 ORM(Object/Relational Mapping)이다. 본 논문에서는 ORM 기술을 지원하는 ORM 툴로서의 오픈 소스 자바 프레임 워크를 성능이나 코드 복잡성, 관리 용이성 등 다양한 측면에서 비교 분석하였다. 현재 30여가지가 넘는 다양한 오픈 소스 자바 프레임워크가 개발되어 배포되고 있지만, 본 논문에서는 Hibernate, iBatis SqlMaps, Apache OJB 이렇게 새 개의 프래임워크를 현재 객체 지속성을 위해 가장 많이 사용되는 JDBC 기술을 기준으로 비교 분석하였다. 데이터에 대한 CRUD(저장,추출,수정,삭제)를 수행하는 시간을 통해 성능 분석을 실시하였으며, 사례 어플리케이션 구현을 통해 각 프레임워크 별로 CRUD를 수행하는 메소드 구현 시 코드량 분석을 통해 코드 복잡성을, 요구 사항 변경 시 어떻게 각 프레임워크가 이를 반영하는지를 통해 관리 용이성을 분석하였다. 이러한 분석을 통해 각 프레임워크가 어떠한 서비스를 제공하는지, 각 프레임워크의 성능은 어떠한지 쉽게 알 수 있다. 따라서 기업은 좀더 명확한 근거를 통해 어플리케이션 개발에 적절한 퍼시스턴스 프레임워크를 선택할 수 있을 것이다. 또한 상용과 오픈 소스 기반의 프레임워크 중 어떠한 것을 도입해야 할지 결정해야 할 경우 중요한 참고 자료로 활용할 수 있을 것이다."
        },
        {
          "rank": 37,
          "score": 0.6381663680076599,
          "doc_id": "JAKO201810063224485",
          "title": "딥러닝을 통한 의미&#183;주제 연관성 기반의 소셜 토픽 추출 시스템 개발",
          "abstract": "Users are sharing many of contents such as text, image, video, and so on in SNS. There are various information as like as personal interesting, opinion, and relationship in social media contents. Therefore, many of recommendation systems or search systems are being developed through analysis of social media contents. In order to extract subject-related topics of social context being collected from social media channels in developing those system, it is necessary to develop ontologies for semantic analysis. However, it is difficult to develop formal ontology because social media contents have the characteristics of non-formal data. Therefore, we develop a social topic system based on semantic and subject correlation. First of all, an extracting system of social topic based on semantic relationship analyzes semantic correlation and then extracts topics expressing semantic information of corresponding social context. Because the possibility of developing formal ontology expressing fully semantic information of various areas is limited, we develop a self-extensible architecture of ontology for semantic correlation. And then, a classifier of social contents and feed back classifies equivalent subject's social contents and feedbacks for extracting social topics according semantic correlation. The result of analyzing social contents and feedbacks extracts subject keyword, and index by measuring the degree of association based on social topic's semantic correlation. Deep Learning is applied into the process of indexing for improving accuracy and performance of mapping analysis of subject's extracting and semantic correlation. We expect that proposed system provides customized contents for users as well as optimized searching results because of analyzing semantic and subject correlation.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201810063224485&target=NART&cn=JAKO201810063224485",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 통한 의미&#183;주제 연관성 기반의 소셜 토픽 추출 시스템 개발 딥러닝을 통한 의미&#183;주제 연관성 기반의 소셜 토픽 추출 시스템 개발 딥러닝을 통한 의미&#183;주제 연관성 기반의 소셜 토픽 추출 시스템 개발 Users are sharing many of contents such as text, image, video, and so on in SNS. There are various information as like as personal interesting, opinion, and relationship in social media contents. Therefore, many of recommendation systems or search systems are being developed through analysis of social media contents. In order to extract subject-related topics of social context being collected from social media channels in developing those system, it is necessary to develop ontologies for semantic analysis. However, it is difficult to develop formal ontology because social media contents have the characteristics of non-formal data. Therefore, we develop a social topic system based on semantic and subject correlation. First of all, an extracting system of social topic based on semantic relationship analyzes semantic correlation and then extracts topics expressing semantic information of corresponding social context. Because the possibility of developing formal ontology expressing fully semantic information of various areas is limited, we develop a self-extensible architecture of ontology for semantic correlation. And then, a classifier of social contents and feed back classifies equivalent subject's social contents and feedbacks for extracting social topics according semantic correlation. The result of analyzing social contents and feedbacks extracts subject keyword, and index by measuring the degree of association based on social topic's semantic correlation. Deep Learning is applied into the process of indexing for improving accuracy and performance of mapping analysis of subject's extracting and semantic correlation. We expect that proposed system provides customized contents for users as well as optimized searching results because of analyzing semantic and subject correlation."
        },
        {
          "rank": 38,
          "score": 0.6360558867454529,
          "doc_id": "JAKO202116047225054",
          "title": "신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어",
          "abstract": "최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형 교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은 고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를 분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에 적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은 내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기 위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와 더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의 정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116047225054&target=NART&cn=JAKO202116047225054",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어 신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어 신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어 최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형 교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은 고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를 분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에 적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은 내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기 위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와 더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의 정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다."
        },
        {
          "rank": 39,
          "score": 0.6358500719070435,
          "doc_id": "JAKO201610235349520",
          "title": "오픈소스 하드웨어에서 효율적인 임베디드 소프트웨어 개발을 위한 프레임워크",
          "abstract": "무선인터넷이 보급되고 IoT 기술이 발달함에 따라 여러 종류의 센서 디바이스가 발전하였다. 그리고 IoT 환경에서 사용자들의 요구를 충족하는 다양한 서비스 개발을 위해 오픈소스 하드웨어가 도입되었다. 하지만 오픈소스 하드웨어는 개발 인력의 부족으로 인해 충분히 활용되지 못하고 있다. 따라서 본 논문에서는 오픈소스 하드웨어에서 효율적으로 임베디드 소프트웨어 개발을 교육하기 위한 소프트웨어 프레임워크를 제안한다. 제안하는 프레임워크는 비주얼 프로그래밍 언어와 빠른 결과 확인을 통해 다양한 오픈소스 하드웨어에서 빠르고 직관적으로 임베디드 소프트웨어를 개발할 수 있게 한다. 또한 제안한 프레임워크를 실제 오픈소스 하드웨어 개발 환경에 구현하여 장단점을 분석하고 개선방안을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201610235349520&target=NART&cn=JAKO201610235349520",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈소스 하드웨어에서 효율적인 임베디드 소프트웨어 개발을 위한 프레임워크 오픈소스 하드웨어에서 효율적인 임베디드 소프트웨어 개발을 위한 프레임워크 오픈소스 하드웨어에서 효율적인 임베디드 소프트웨어 개발을 위한 프레임워크 무선인터넷이 보급되고 IoT 기술이 발달함에 따라 여러 종류의 센서 디바이스가 발전하였다. 그리고 IoT 환경에서 사용자들의 요구를 충족하는 다양한 서비스 개발을 위해 오픈소스 하드웨어가 도입되었다. 하지만 오픈소스 하드웨어는 개발 인력의 부족으로 인해 충분히 활용되지 못하고 있다. 따라서 본 논문에서는 오픈소스 하드웨어에서 효율적으로 임베디드 소프트웨어 개발을 교육하기 위한 소프트웨어 프레임워크를 제안한다. 제안하는 프레임워크는 비주얼 프로그래밍 언어와 빠른 결과 확인을 통해 다양한 오픈소스 하드웨어에서 빠르고 직관적으로 임베디드 소프트웨어를 개발할 수 있게 한다. 또한 제안한 프레임워크를 실제 오픈소스 하드웨어 개발 환경에 구현하여 장단점을 분석하고 개선방안을 확인하였다."
        },
        {
          "rank": 40,
          "score": 0.6332909464836121,
          "doc_id": "JAKO202300957609703",
          "title": "딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합",
          "abstract": "위성 시계열 데이터가 증가함에 따라 원격탐사 자료의 활용도가 높아지고 있다. 시계열 자료를 통한 분석에 있어 영상 간의 상대적인 위치 정확도는 결과에 큰 영향을 미치기 때문에 이를 보정하기 위한 영상 정합 과정은 필수적으로 선행되어야 한다. 최근에는 기존 알고리즘의 성능을 상회하는 딥러닝 기반 영상 정합 연구의 사례가 증가하고 있다. 딥러닝 기반 정합 모델을 학습하기 위해서는 수 많은 영상 쌍이 필요하다. 또한, 기존 딥러닝 모델의 데이터 간의 상관도 map을 제작하고, 이에 추가적인 연산을 적용하여 정합점을 추출는데 이는 비효율적이다. 이러한 문제를 해결하기 위해 본 연구에서는 영상 정합 모델 학습을 위한 데이터 증강 기법을 구축하여 데이터셋을 제작하였고, 이를 오프셋(offset) 양 자체를 예측하는 정합 모델인 OffsetNet에 적용하여 KOMSAT-2, -3, -3A 영상 정합을 수행하였다. 모델 학습 결과, OffsetNet은 평가 데이터에 대해 높은 정확도로 오프셋 양을 예측하였고, 이를 통해 주영상과 부영상을 효과적으로 정합하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202300957609703&target=NART&cn=JAKO202300957609703",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 위성 시계열 데이터가 증가함에 따라 원격탐사 자료의 활용도가 높아지고 있다. 시계열 자료를 통한 분석에 있어 영상 간의 상대적인 위치 정확도는 결과에 큰 영향을 미치기 때문에 이를 보정하기 위한 영상 정합 과정은 필수적으로 선행되어야 한다. 최근에는 기존 알고리즘의 성능을 상회하는 딥러닝 기반 영상 정합 연구의 사례가 증가하고 있다. 딥러닝 기반 정합 모델을 학습하기 위해서는 수 많은 영상 쌍이 필요하다. 또한, 기존 딥러닝 모델의 데이터 간의 상관도 map을 제작하고, 이에 추가적인 연산을 적용하여 정합점을 추출는데 이는 비효율적이다. 이러한 문제를 해결하기 위해 본 연구에서는 영상 정합 모델 학습을 위한 데이터 증강 기법을 구축하여 데이터셋을 제작하였고, 이를 오프셋(offset) 양 자체를 예측하는 정합 모델인 OffsetNet에 적용하여 KOMSAT-2, -3, -3A 영상 정합을 수행하였다. 모델 학습 결과, OffsetNet은 평가 데이터에 대해 높은 정확도로 오프셋 양을 예측하였고, 이를 통해 주영상과 부영상을 효과적으로 정합하였다."
        },
        {
          "rank": 41,
          "score": 0.631712794303894,
          "doc_id": "NART84800827",
          "title": "Smart in-car camera system using mobile cloud computing framework for deep learning",
          "abstract": "<P><B>Abstract</B></P>  <P>Deep learning is becoming a popular technology in various applications, such as image recognition, gaming, information retrieval, for intelligent data processing. However, huge amount of data and complex computations prevent deep learning from being practical on mobile devices. In this paper, we designed a smart in-car camera system that utilizes mobile cloud computing framework for deep learning. The smart in-car camera can detect objects in recorded videos during driving, and can decide which part of videos needs to be stored in cloud platforms to save local storage space. The system puts the training process and model repository in cloud platforms, and the recognition process and data gathering in mobile devices. The mobile side is implemented in NVIDIA Jetson TK1, and the communication is carried out via Git protocol to ensure the success of data transmission in unstable network environments. Experimental results show that detection rate can achieve up to four frame-per-second with Faster R-CNN, and the system can work well even when the network connection is unstable. We also compared the performance of system with and without GPU, and found that GPU still plays a critical role in the recognition side for deep learning.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART84800827&target=NART&cn=NART84800827",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Smart in-car camera system using mobile cloud computing framework for deep learning Smart in-car camera system using mobile cloud computing framework for deep learning Smart in-car camera system using mobile cloud computing framework for deep learning <P><B>Abstract</B></P>  <P>Deep learning is becoming a popular technology in various applications, such as image recognition, gaming, information retrieval, for intelligent data processing. However, huge amount of data and complex computations prevent deep learning from being practical on mobile devices. In this paper, we designed a smart in-car camera system that utilizes mobile cloud computing framework for deep learning. The smart in-car camera can detect objects in recorded videos during driving, and can decide which part of videos needs to be stored in cloud platforms to save local storage space. The system puts the training process and model repository in cloud platforms, and the recognition process and data gathering in mobile devices. The mobile side is implemented in NVIDIA Jetson TK1, and the communication is carried out via Git protocol to ensure the success of data transmission in unstable network environments. Experimental results show that detection rate can achieve up to four frame-per-second with Faster R-CNN, and the system can work well even when the network connection is unstable. We also compared the performance of system with and without GPU, and found that GPU still plays a critical role in the recognition side for deep learning.</P>"
        },
        {
          "rank": 42,
          "score": 0.6315322518348694,
          "doc_id": "JAKO201734158606474",
          "title": "제조업의 심층신경망 기계학습(딥러닝)",
          "abstract": "인공지능 특히 심층신경망기계학습기법(딥러닝)의 제조업분야에서의 이용이 효율적이며 실용적일 수 있다는 인식이 넓게 수용되고 있다 이 보고서는 최근의 신경망기계학습 개발환경을 개관하고 제조업분야에서 활용되고 있는 딥 러닝기술을 개관한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201734158606474&target=NART&cn=JAKO201734158606474",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "제조업의 심층신경망 기계학습(딥러닝) 제조업의 심층신경망 기계학습(딥러닝) 제조업의 심층신경망 기계학습(딥러닝) 인공지능 특히 심층신경망기계학습기법(딥러닝)의 제조업분야에서의 이용이 효율적이며 실용적일 수 있다는 인식이 넓게 수용되고 있다 이 보고서는 최근의 신경망기계학습 개발환경을 개관하고 제조업분야에서 활용되고 있는 딥 러닝기술을 개관한다."
        },
        {
          "rank": 43,
          "score": 0.6312627196311951,
          "doc_id": "JAKO202210351407855",
          "title": "심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적",
          "abstract": "Nowadays, the complexity of object tracking models among hardware applications has become a more in-demand duty to complete in various indeterminable environment tracking situations with multifunctional algorithm skills. In this paper, we propose a virtual city environment using AirSim (Aerial Informatics and Robotics Simulation - AirSim, CityEnvironment) and use the DQN (Deep Q-Learning) model of deep reinforcement learning model in the virtual environment. The proposed object tracking DQN network observes the environment using a deep reinforcement learning model that receives continuous images taken by a virtual environment simulation system as input to control the operation of a virtual drone. The deep reinforcement learning model is pre-trained using various existing continuous image sets. Since the existing various continuous image sets are image data of real environments and objects, it is implemented in 3D to track virtual environments and moving objects in them.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202210351407855&target=NART&cn=JAKO202210351407855",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 Nowadays, the complexity of object tracking models among hardware applications has become a more in-demand duty to complete in various indeterminable environment tracking situations with multifunctional algorithm skills. In this paper, we propose a virtual city environment using AirSim (Aerial Informatics and Robotics Simulation - AirSim, CityEnvironment) and use the DQN (Deep Q-Learning) model of deep reinforcement learning model in the virtual environment. The proposed object tracking DQN network observes the environment using a deep reinforcement learning model that receives continuous images taken by a virtual environment simulation system as input to control the operation of a virtual drone. The deep reinforcement learning model is pre-trained using various existing continuous image sets. Since the existing various continuous image sets are image data of real environments and objects, it is implemented in 3D to track virtual environments and moving objects in them."
        },
        {
          "rank": 44,
          "score": 0.6310864090919495,
          "doc_id": "JAKO201974757494930",
          "title": "심층강화학습 라이브러리 기술동향",
          "abstract": "Reinforcement learning is a type of machine learning paradigm that forces agents to repeat the observation-action-reward process to assess and predict the values of possible future action sequences. This allows the agents to incrementally reinforce the desired behavior for a given observation. Thanks to the recent advancements of deep learning, reinforcement learning has evolved into deep reinforcement learning that introduces promising results in various control and optimization domains, such as games, robotics, autonomous vehicles, computing, industrial control, and so on. In addition to this trend, a number of programming libraries have been developed for importing deep reinforcement learning into a variety of applications. In this article, we briefly review and summarize 10 representative deep reinforcement learning libraries and compare them from a development project perspective.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201974757494930&target=NART&cn=JAKO201974757494930",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층강화학습 라이브러리 기술동향 심층강화학습 라이브러리 기술동향 심층강화학습 라이브러리 기술동향 Reinforcement learning is a type of machine learning paradigm that forces agents to repeat the observation-action-reward process to assess and predict the values of possible future action sequences. This allows the agents to incrementally reinforce the desired behavior for a given observation. Thanks to the recent advancements of deep learning, reinforcement learning has evolved into deep reinforcement learning that introduces promising results in various control and optimization domains, such as games, robotics, autonomous vehicles, computing, industrial control, and so on. In addition to this trend, a number of programming libraries have been developed for importing deep reinforcement learning into a variety of applications. In this article, we briefly review and summarize 10 representative deep reinforcement learning libraries and compare them from a development project perspective."
        },
        {
          "rank": 45,
          "score": 0.6309609413146973,
          "doc_id": "JAKO202334662554660",
          "title": "증강형 딥러닝 기반 미세먼지 예측 시스템",
          "abstract": "딥러닝은 심층신경망(Deep Neural Network)을 구축하고 대량의 훈련 데이터를 수집한 후, 구축된 신경망을 오랫동안 학습 시켜야 한다. 만약, 학습이 제대로 진행되지 않거나 과적합이 발생하면, 학습은 실패하게 된다. 현재까지 개발되고 있는 딥러닝 도구들을 사용할 경우, 훈련데이터 수집과 학습에 많은 시간이 소요된다. 하지만, 모바일 환경의 급격한 도래와 센서 데이터의 증가로 인해, 신경망 학습에 걸리는 시간을 획기적으로 줄일 수 있는 실시간 증강형 딥러닝 기술에 대한 요구가 급격하게 증가하고 있다. 본 연구에서는 미세먼지 센서를 장착한 아두이노 시스템을 사용하여 실시간 증강형 딥러닝 시스템을 구현 하였다. 구현된 시스템에서는 미세먼지 데이터를 5초마다 측정하고 최대 120개가 축적이 되면, 기존에 축적된 데이터와 새로이 축적된 데이터를 데이터셋으로 사용하여 학습을 수행하도록 하였다. 학습 수행을 위한 신경망은 입력층 1개, 은닉층 1개, 출력등 1개로 구성하였다. 구현된 시스템에 대한 성능을 평가하기 위해 학습 시간과 평균 제곱근 오차(root mean square error, RMSE)를 측정 하였다. 실험 결과, 평균 학습 오차는 0.04053796이었으며, 학습주기당(1 에포크) 평균 학습 시간은 3,447 초 정도의 시간이 걸렸다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202334662554660&target=NART&cn=JAKO202334662554660",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "증강형 딥러닝 기반 미세먼지 예측 시스템 증강형 딥러닝 기반 미세먼지 예측 시스템 증강형 딥러닝 기반 미세먼지 예측 시스템 딥러닝은 심층신경망(Deep Neural Network)을 구축하고 대량의 훈련 데이터를 수집한 후, 구축된 신경망을 오랫동안 학습 시켜야 한다. 만약, 학습이 제대로 진행되지 않거나 과적합이 발생하면, 학습은 실패하게 된다. 현재까지 개발되고 있는 딥러닝 도구들을 사용할 경우, 훈련데이터 수집과 학습에 많은 시간이 소요된다. 하지만, 모바일 환경의 급격한 도래와 센서 데이터의 증가로 인해, 신경망 학습에 걸리는 시간을 획기적으로 줄일 수 있는 실시간 증강형 딥러닝 기술에 대한 요구가 급격하게 증가하고 있다. 본 연구에서는 미세먼지 센서를 장착한 아두이노 시스템을 사용하여 실시간 증강형 딥러닝 시스템을 구현 하였다. 구현된 시스템에서는 미세먼지 데이터를 5초마다 측정하고 최대 120개가 축적이 되면, 기존에 축적된 데이터와 새로이 축적된 데이터를 데이터셋으로 사용하여 학습을 수행하도록 하였다. 학습 수행을 위한 신경망은 입력층 1개, 은닉층 1개, 출력등 1개로 구성하였다. 구현된 시스템에 대한 성능을 평가하기 위해 학습 시간과 평균 제곱근 오차(root mean square error, RMSE)를 측정 하였다. 실험 결과, 평균 학습 오차는 0.04053796이었으며, 학습주기당(1 에포크) 평균 학습 시간은 3,447 초 정도의 시간이 걸렸다."
        },
        {
          "rank": 46,
          "score": 0.6308253407478333,
          "doc_id": "JAKO202201253148351",
          "title": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰",
          "abstract": "위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202201253148351&target=NART&cn=JAKO202201253148351",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다."
        },
        {
          "rank": 47,
          "score": 0.6306110620498657,
          "doc_id": "DIKO0008948393",
          "title": "소프트웨어 컴포넌트 프레임워크 성능 비교",
          "abstract": "프레임워크는 포함하는 컴포넌트의 영역적 특성에 따라 시스템의 하부 구조를 중심으로 정의된 시스템 프레임워크와 특정 응용 영역을 위한 응용 애플리케이션 프레임워크로 나뉠 수 있다. 본 논문에서는 각각의 프레임워크 사용 제품에 대해 비교 분석하였다. 시스템 프레임워크는 시스템의 기본 성능인 '객체모델', '객체 버스', '언어의 독립성', '위치 투명성'등을 기준으로 비교 하였다. CORBA는 컴포넌트 간 상호 운용성이 뛰어나며, 다양한 서비스와 응용 컴포넌트를 지원한다. 이에 비하여 DCOM은 윈도우 사용자를 기반으로 하고 있으며 MTS를 기반으로 하는 분산 트랜젝션 기능을 지원한다. EJB는 구현언어의 독립성을 갖는 CORBA와 DCOM과 달리 자바라는 단일 언어를 기반으로 한다. 이로 인해 언어 독립적이지는 않지만 컨테이너 차원에서 데이터베이스, 트랜잭션, 보안문제, 데이터베이스의 커넥션 플링, 쓰레딩과 같은 기능을 제공한다 특정 비즈니스 영역을 위한 Steelpia, SanFrancisco의 경우에는 각 프레임워크의 재사용 컴포넌트 레이어에 대한 비교를 하였다 Steelpia는 철강 업무를 지원하며, 핵심 비즈니스 컴포넌트를 크기에 맞게 특화하여 기존의 개발 방식보다 30-50% 정도의 기간비용 효과를 얻을 수 있다. SanFrancisco 는 비즈니스 컴포넌트를 지원하여 특화함으로 애플리케이션 개발시 전체 노력의 40%정도의 시간적, 비용적 이익을 가져다 준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0008948393&target=NART&cn=DIKO0008948393",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "소프트웨어 컴포넌트 프레임워크 성능 비교 소프트웨어 컴포넌트 프레임워크 성능 비교 소프트웨어 컴포넌트 프레임워크 성능 비교 프레임워크는 포함하는 컴포넌트의 영역적 특성에 따라 시스템의 하부 구조를 중심으로 정의된 시스템 프레임워크와 특정 응용 영역을 위한 응용 애플리케이션 프레임워크로 나뉠 수 있다. 본 논문에서는 각각의 프레임워크 사용 제품에 대해 비교 분석하였다. 시스템 프레임워크는 시스템의 기본 성능인 '객체모델', '객체 버스', '언어의 독립성', '위치 투명성'등을 기준으로 비교 하였다. CORBA는 컴포넌트 간 상호 운용성이 뛰어나며, 다양한 서비스와 응용 컴포넌트를 지원한다. 이에 비하여 DCOM은 윈도우 사용자를 기반으로 하고 있으며 MTS를 기반으로 하는 분산 트랜젝션 기능을 지원한다. EJB는 구현언어의 독립성을 갖는 CORBA와 DCOM과 달리 자바라는 단일 언어를 기반으로 한다. 이로 인해 언어 독립적이지는 않지만 컨테이너 차원에서 데이터베이스, 트랜잭션, 보안문제, 데이터베이스의 커넥션 플링, 쓰레딩과 같은 기능을 제공한다 특정 비즈니스 영역을 위한 Steelpia, SanFrancisco의 경우에는 각 프레임워크의 재사용 컴포넌트 레이어에 대한 비교를 하였다 Steelpia는 철강 업무를 지원하며, 핵심 비즈니스 컴포넌트를 크기에 맞게 특화하여 기존의 개발 방식보다 30-50% 정도의 기간비용 효과를 얻을 수 있다. SanFrancisco 는 비즈니스 컴포넌트를 지원하여 특화함으로 애플리케이션 개발시 전체 노력의 40%정도의 시간적, 비용적 이익을 가져다 준다."
        },
        {
          "rank": 48,
          "score": 0.630592405796051,
          "doc_id": "ART002977356",
          "title": "Deep reinforcement learning based edge computing for video processing",
          "abstract": "In many of 5G applications, end devices with lack of computing power often need to carry out heavy computations involving multimedia data. Edge computing has emerged as a promising solution to circumvent scarce resources at end devices, with moderate delays compared to cloud computing. In this work, we study the problem of offloading video processing tasks to edge servers. To this end, we develop a deep reinforcement learning based method for selecting either local or edge server to process video frames. We demonstrate the performance of our method through experiments with video frame transform tasks.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002977356&target=NART&cn=ART002977356",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep reinforcement learning based edge computing for video processing Deep reinforcement learning based edge computing for video processing Deep reinforcement learning based edge computing for video processing In many of 5G applications, end devices with lack of computing power often need to carry out heavy computations involving multimedia data. Edge computing has emerged as a promising solution to circumvent scarce resources at end devices, with moderate delays compared to cloud computing. In this work, we study the problem of offloading video processing tasks to edge servers. To this end, we develop a deep reinforcement learning based method for selecting either local or edge server to process video frames. We demonstrate the performance of our method through experiments with video frame transform tasks."
        },
        {
          "rank": 49,
          "score": 0.6304064393043518,
          "doc_id": "JAKO202126048601456",
          "title": "유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구",
          "abstract": "딥 러닝을 활용한 컴퓨터 비전 연구는 여전히 대규모의 학습 데이터와 컴퓨팅 파워가 필수적이며, 최적의 네트워크 구조를 도출하기 위해 많은 시행착오가 수반된다. 본 연구에서는 네트워크 최적화나 데이터를 보강하는 것과 무관하게 데이터 자체의 특성만을 고려한 CR(Confusion Rate)기반의 유사 이미지 분류 성능 향상 기법을 제안한다. 제안 방법은 유사한 이미지 데이터를 정확히 분류하기 위해 CR을 산출하고 이를 손실 함수의 가중치에 반영함으로서 딥 러닝 모델의 성능을 향상시키는 기법을 제안한다. 제안 방법은 네트워크 최적화 결과와 독립적으로 이미지 분류 성능의 향상을 가져올 수 있으며, 클래스 간의 유사성을 고려해 유사도가 높은 이미지 식별에 적합하다. 제안 방법의 평가결과 HanDB에서는 0.22%, Animal-10N에서는 3.38%의 성능향상을 보였다. 제안한 방법은 다양한 Noisy Labeled 데이터를 활용한 인공지능 연구에 기반이 될 것을 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202126048601456&target=NART&cn=JAKO202126048601456",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 딥 러닝을 활용한 컴퓨터 비전 연구는 여전히 대규모의 학습 데이터와 컴퓨팅 파워가 필수적이며, 최적의 네트워크 구조를 도출하기 위해 많은 시행착오가 수반된다. 본 연구에서는 네트워크 최적화나 데이터를 보강하는 것과 무관하게 데이터 자체의 특성만을 고려한 CR(Confusion Rate)기반의 유사 이미지 분류 성능 향상 기법을 제안한다. 제안 방법은 유사한 이미지 데이터를 정확히 분류하기 위해 CR을 산출하고 이를 손실 함수의 가중치에 반영함으로서 딥 러닝 모델의 성능을 향상시키는 기법을 제안한다. 제안 방법은 네트워크 최적화 결과와 독립적으로 이미지 분류 성능의 향상을 가져올 수 있으며, 클래스 간의 유사성을 고려해 유사도가 높은 이미지 식별에 적합하다. 제안 방법의 평가결과 HanDB에서는 0.22%, Animal-10N에서는 3.38%의 성능향상을 보였다. 제안한 방법은 다양한 Noisy Labeled 데이터를 활용한 인공지능 연구에 기반이 될 것을 기대한다."
        },
        {
          "rank": 50,
          "score": 0.6303899884223938,
          "doc_id": "NPAP13263504",
          "title": "The Development of Deep Learning in China",
          "abstract": "This paper is to summarize the academic status of deep learning in Chinese scientific institutions and universities based on the literatures from CNKI. We analyzed the various development of deep learning in China based on the application of computer vision, voice recognition and natural language processing.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP13263504&target=NART&cn=NPAP13263504",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "The Development of Deep Learning in China The Development of Deep Learning in China The Development of Deep Learning in China This paper is to summarize the academic status of deep learning in Chinese scientific institutions and universities based on the literatures from CNKI. We analyzed the various development of deep learning in China based on the application of computer vision, voice recognition and natural language processing."
        }
      ]
    },
    {
      "query": "이러한 오픈소스 딥 러닝 프레임워크들 간의 성능 비교 결과는 어떻게 되나요?",
      "query_meta": {
        "type": "single_hop",
        "index": 1
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8627322912216187,
          "doc_id": "DIKO0015889140",
          "title": "딥 러닝 프레임워크 성능 비교 및 개선 방안",
          "abstract": "현 시대는 4차 산업혁명이 대두되는 시대로 요소 기술들 중 인공지능의 중 요성은 아무리 강조하더라도 지나치지 않으며, 기업들 경쟁력의 척도라고 불 릴만큼 모든 산업에서 활용되고있다. 2016년 경 DeepMind 의 AlphaGo 와 이 세돌 선수의 경기로 국내에서는 처음으로 인공지능의 위력과 Deep Learning 이라는 단어가 대중들에게 알려지게 되었다.&amp;#xD; 특정 IT 산업이 발전하게 되면 해당 분야의 개발자들의 생산성과 접근성을 높이기 위해 Framework 들이 등장, 발전하게 된다. 통신기술과 스마트폰의 출현으로 WEB 붐이 이르렀을 때, Server-side 에서는 Spring, django, Ruby on Rails 등이 출현하였고, Client-side 에서는 Angular, React, jQuery 와 같이 다양한 Framework 들이 등장 발전하였다. 컴퓨터 성능의 발전과 다양 한 컴퓨팅 기술의 발전으로 현 시대는 인공지능 3차 붐으로 Machine Learning 과 Deep Learning 의 시대로 불리고있다.&amp;#xD; 이와 같이 Deep Learning 분야에서도 다양한 Framework 들이 개발되었다. 이런 다양한 Framework 제품들의 목적은 개발자들의 생산성을 향상시키기 위 해 내부 알고리즘이나 메커니즘을 Black Box 형식으로 감추고 High Level API 를 제공하기 때문에, 내부적인 구현 방식은 Framework 별로 다르다. 본 논문에서는 현 시대에 가장 많이 사용하는 대표적인 Framework 들을 선정한 다. 그리고 선정된 Framework 들을 이용하여 Convolutional Neural Network 알고리즘을 구현, 동일한 Training Data 를 이용하여 학습 Model 을 만들어 낸다. 그리고 동일한 Cloud 환경에서 각 Framework 별 학습을 수행하여 성 능을 비교한다. 성능 비교 환경은 총 3가지로 CPU, GPU 1 Core, Multi GPU Core 환경에서 각 Framework 별 성능 지표를 추출한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015889140&target=NART&cn=DIKO0015889140",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 프레임워크 성능 비교 및 개선 방안 딥 러닝 프레임워크 성능 비교 및 개선 방안 딥 러닝 프레임워크 성능 비교 및 개선 방안 현 시대는 4차 산업혁명이 대두되는 시대로 요소 기술들 중 인공지능의 중 요성은 아무리 강조하더라도 지나치지 않으며, 기업들 경쟁력의 척도라고 불 릴만큼 모든 산업에서 활용되고있다. 2016년 경 DeepMind 의 AlphaGo 와 이 세돌 선수의 경기로 국내에서는 처음으로 인공지능의 위력과 Deep Learning 이라는 단어가 대중들에게 알려지게 되었다.&amp;#xD; 특정 IT 산업이 발전하게 되면 해당 분야의 개발자들의 생산성과 접근성을 높이기 위해 Framework 들이 등장, 발전하게 된다. 통신기술과 스마트폰의 출현으로 WEB 붐이 이르렀을 때, Server-side 에서는 Spring, django, Ruby on Rails 등이 출현하였고, Client-side 에서는 Angular, React, jQuery 와 같이 다양한 Framework 들이 등장 발전하였다. 컴퓨터 성능의 발전과 다양 한 컴퓨팅 기술의 발전으로 현 시대는 인공지능 3차 붐으로 Machine Learning 과 Deep Learning 의 시대로 불리고있다.&amp;#xD; 이와 같이 Deep Learning 분야에서도 다양한 Framework 들이 개발되었다. 이런 다양한 Framework 제품들의 목적은 개발자들의 생산성을 향상시키기 위 해 내부 알고리즘이나 메커니즘을 Black Box 형식으로 감추고 High Level API 를 제공하기 때문에, 내부적인 구현 방식은 Framework 별로 다르다. 본 논문에서는 현 시대에 가장 많이 사용하는 대표적인 Framework 들을 선정한 다. 그리고 선정된 Framework 들을 이용하여 Convolutional Neural Network 알고리즘을 구현, 동일한 Training Data 를 이용하여 학습 Model 을 만들어 낸다. 그리고 동일한 Cloud 환경에서 각 Framework 별 학습을 수행하여 성 능을 비교한다. 성능 비교 환경은 총 3가지로 CPU, GPU 1 Core, Multi GPU Core 환경에서 각 Framework 별 성능 지표를 추출한다."
        },
        {
          "rank": 2,
          "score": 0.8375346660614014,
          "doc_id": "JAKO201713056893580",
          "title": "딥 러닝 프레임워크의 비교 및 분석",
          "abstract": "딥 러닝은 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 딥 러닝은 세상을 이해하고 감지하는 인공지능을 개발하는데 가장 촉망받는 기술이 되고 있으며, 구글, 바이두, 페이스북 등이 가장 앞서서 개발을 하고 있다. 본 논문에서는 딥 러닝을 구현하는 딥 러닝 프레임워크의 종류에 대해 논의하고, 딥 러닝 프레임워크의 영상과 음성 인식 분야의 효율성에 대해 비교, 분석하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201713056893580&target=NART&cn=JAKO201713056893580",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 프레임워크의 비교 및 분석 딥 러닝 프레임워크의 비교 및 분석 딥 러닝 프레임워크의 비교 및 분석 딥 러닝은 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 딥 러닝은 세상을 이해하고 감지하는 인공지능을 개발하는데 가장 촉망받는 기술이 되고 있으며, 구글, 바이두, 페이스북 등이 가장 앞서서 개발을 하고 있다. 본 논문에서는 딥 러닝을 구현하는 딥 러닝 프레임워크의 종류에 대해 논의하고, 딥 러닝 프레임워크의 영상과 음성 인식 분야의 효율성에 대해 비교, 분석하고자 한다."
        },
        {
          "rank": 3,
          "score": 0.8372765779495239,
          "doc_id": "DIKO0014373092",
          "title": "Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서",
          "abstract": "통신 기술의 발달로 인한 사람과 사람, 장치와 장치, 사람과 장치 간의 연결성의 증가와 저장 매체 기술의 발달, 그리고 데이터 저장 비용의 감소로 인해 데이터의 양이 폭발적으로 증가했다. 이에 따라 다양한 형태의 대규모의 데이터를 빠른 시간 내에 효율적으로 처리할 수 있는 Cloud Computing 기술이 주목 받고 있고, Cloud Computing을 위한 오픈소스 기반의 솔루션 또한 많이 나타나게 되었다. &amp;#xD; 2012년부터 주목받기 시작한 Deep Learning은 전 세계적으로 가장 많은 관심을 받는 연구 분야 중 하나이며, 이중 CNN(Convolution Neural Network)은 가장 대표적 알고리즘이다. Deep Learning은 미래사회를 이끌어갈 분야로 평가받고 있으며, 이에 따라 많은 연구들이 진행되고 있고, Deep Learning을 쉽게 활용할 수 있도록 하는 많은 프레임워크가 개발되었다. &amp;#xD; 이에 따라 많은 사람들이 쉽게 Deep Learning을 접할 수 있게 되었지만 특정 환경에서 어떤 프레임워크가 더 우수한 성능을 보이는지에 대한 연구는 부족한 실정이다. 본 논문에서는 특정 환경에서의 성능 비교가 부족하다는 기존 연구의 한계점을 개선하고자 가장 대표적인 Cloud Computing용 오픈소스 소프트웨어 중 하나인 OpenStack을 이용하여 Cloud Computing 환경에서 어떤 Deep Learning 프레임워크가 더 우수한 성능을 보이는지 비교해 보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014373092&target=NART&cn=DIKO0014373092",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 통신 기술의 발달로 인한 사람과 사람, 장치와 장치, 사람과 장치 간의 연결성의 증가와 저장 매체 기술의 발달, 그리고 데이터 저장 비용의 감소로 인해 데이터의 양이 폭발적으로 증가했다. 이에 따라 다양한 형태의 대규모의 데이터를 빠른 시간 내에 효율적으로 처리할 수 있는 Cloud Computing 기술이 주목 받고 있고, Cloud Computing을 위한 오픈소스 기반의 솔루션 또한 많이 나타나게 되었다. &amp;#xD; 2012년부터 주목받기 시작한 Deep Learning은 전 세계적으로 가장 많은 관심을 받는 연구 분야 중 하나이며, 이중 CNN(Convolution Neural Network)은 가장 대표적 알고리즘이다. Deep Learning은 미래사회를 이끌어갈 분야로 평가받고 있으며, 이에 따라 많은 연구들이 진행되고 있고, Deep Learning을 쉽게 활용할 수 있도록 하는 많은 프레임워크가 개발되었다. &amp;#xD; 이에 따라 많은 사람들이 쉽게 Deep Learning을 접할 수 있게 되었지만 특정 환경에서 어떤 프레임워크가 더 우수한 성능을 보이는지에 대한 연구는 부족한 실정이다. 본 논문에서는 특정 환경에서의 성능 비교가 부족하다는 기존 연구의 한계점을 개선하고자 가장 대표적인 Cloud Computing용 오픈소스 소프트웨어 중 하나인 OpenStack을 이용하여 Cloud Computing 환경에서 어떤 Deep Learning 프레임워크가 더 우수한 성능을 보이는지 비교해 보고자 한다."
        },
        {
          "rank": 4,
          "score": 0.8234424591064453,
          "doc_id": "NPAP12898051",
          "title": "딥러닝 프레임워크 비교 및 분석",
          "abstract": "딥러닝(Deep Learning)을 효과적으로 연구하고 개발할 수 있도록 도와주는 다양한 딥러닝 프레임워크(Deep Learning Framework)가 있다. 딥러닝 프레임워크는 현재 100 가지도 넘는 종류가 있다. 그렇기 때문에 개발의 목적에 가장 적합한 딥러닝 프레임워크를 선택하는 것은 쉽지 않다. 본고에서는 5가지 대표적인 딥러닝 프레임워크에 대해서 각각의 특징을 분석하고 비교한다. 이를 통하여 딥러닝을 개발하기 전에 개발 목적에 적합한 프레임워크를 선택할 수 있는 간단한 안목을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12898051&target=NART&cn=NPAP12898051",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 프레임워크 비교 및 분석 딥러닝 프레임워크 비교 및 분석 딥러닝 프레임워크 비교 및 분석 딥러닝(Deep Learning)을 효과적으로 연구하고 개발할 수 있도록 도와주는 다양한 딥러닝 프레임워크(Deep Learning Framework)가 있다. 딥러닝 프레임워크는 현재 100 가지도 넘는 종류가 있다. 그렇기 때문에 개발의 목적에 가장 적합한 딥러닝 프레임워크를 선택하는 것은 쉽지 않다. 본고에서는 5가지 대표적인 딥러닝 프레임워크에 대해서 각각의 특징을 분석하고 비교한다. 이를 통하여 딥러닝을 개발하기 전에 개발 목적에 적합한 프레임워크를 선택할 수 있는 간단한 안목을 제시한다."
        },
        {
          "rank": 5,
          "score": 0.8218142986297607,
          "doc_id": "JAKO202223540366088",
          "title": "이미지 학습을 위한 딥러닝 프레임워크 비교분석",
          "abstract": "딥러닝 프레임워크는 현재에도 계속해서 발전되어 가고 있으며, 다양한 프레임워크들이 존재한다. 딥러닝의 대표적인 프레임워크는 TensorFlow, PyTorch, Keras 등이 있다. 딥러님 프레임워크는 이미지 학습을 통해 이미지 분류에서의 최적화 모델을 이용한다. 본 논문에서는 딥러닝 이미지 인식 분야에서 가장 많이 사용하고 있는 TensorFlow와 PyTorch 프레임워크를 활용하여 이미지 학습을 진행하였으며, 이 과정에서 도출한 결과를 비교 분석하여 최적화된 프레임워크을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202223540366088&target=NART&cn=JAKO202223540366088",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "이미지 학습을 위한 딥러닝 프레임워크 비교분석 이미지 학습을 위한 딥러닝 프레임워크 비교분석 이미지 학습을 위한 딥러닝 프레임워크 비교분석 딥러닝 프레임워크는 현재에도 계속해서 발전되어 가고 있으며, 다양한 프레임워크들이 존재한다. 딥러닝의 대표적인 프레임워크는 TensorFlow, PyTorch, Keras 등이 있다. 딥러님 프레임워크는 이미지 학습을 통해 이미지 분류에서의 최적화 모델을 이용한다. 본 논문에서는 딥러닝 이미지 인식 분야에서 가장 많이 사용하고 있는 TensorFlow와 PyTorch 프레임워크를 활용하여 이미지 학습을 진행하였으며, 이 과정에서 도출한 결과를 비교 분석하여 최적화된 프레임워크을 알 수 있었다."
        },
        {
          "rank": 6,
          "score": 0.8215435743331909,
          "doc_id": "JAKO201719950757340",
          "title": "딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",
          "abstract": "딥러닝 프레임워크의 대표적인 기능으로는 '자동미분'과 'GPU의 활용' 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각 프레임워크의 실행속도에 대한 평가는 '큰 차이는 없다'는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만 빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데, 위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로 구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201719950757340&target=NART&cn=JAKO201719950757340",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 대표적인 기능으로는 '자동미분'과 'GPU의 활용' 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각 프레임워크의 실행속도에 대한 평가는 '큰 차이는 없다'는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만 빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데, 위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로 구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다."
        },
        {
          "rank": 7,
          "score": 0.8184381127357483,
          "doc_id": "JAKO201718054814596",
          "title": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석",
          "abstract": "딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201718054814596&target=NART&cn=JAKO201718054814596",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다."
        },
        {
          "rank": 8,
          "score": 0.7903778553009033,
          "doc_id": "JAKO202009135419341",
          "title": "딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출",
          "abstract": "많은 정보통신기술 기업들은 자체적으로 개발한 인공지능 기술을 오픈소스로 공개하였다. 예를 들어, 구글의 TensorFlow, 페이스북의 PyTorch, 마이크로소프트의 CNTK 등 여러 기업들은 자신들의 인공지능 기술들을 공개하고 있다. 이처럼 대중에게 딥러닝 오픈소스 소프트웨어를 공개함으로써 개발자 커뮤니티와의 관계와 인공지능 생태계를 강화하고, 사용자들의 실험, 적용, 개선을 얻을 수 있다. 이에 따라 머신러닝 분야는 급속히 성장하고 있고, 개발자들 또한 여러가지 학습 알고리즘을 재생산하여 각 영역에 활용하고 있다. 하지만 오픈소스 소프트웨어에 대한 다양한 분석들이 이루어진 데 반해, 실제 산업현장에서 딥러닝 오픈소스 소프트웨어를 개발하거나 활용하는데 유용한 연구 결과는 미흡한 실정이다. 따라서 본 연구에서는 딥러닝 프레임워크 사례연구를 통해 해당 프레임워크의 도입 전략을 도출하고자 한다. 기술-조직-환경 프레임워크를 기반으로 기존의 오픈 소스 소프트웨어 도입과 관련된 연구들을 리뷰하고, 이를 바탕으로 두 기업의 성공 사례와 한 기업의 실패 사례를 포함한 총 3 가지 기업의 도입 사례 분석을 통해 딥러닝 프레임워크 도입을 위한 중요한 5가지 성공 요인을 도출하였다: 팀 내 개발자의 지식과 전문성, 하드웨어(GPU) 환경, 데이터 전사 협력 체계, 딥러닝 프레임워크 플랫폼, 딥러닝 프레임워크 도구 서비스. 그리고 도출한 성공 요인을 실현하기 위한 딥러닝 프레임워크의 단계적 도입 전략을 제안하였다: 프로젝트 문제 정의, 딥러닝 방법론이 적합한 기법인지 확인, 딥러닝 프레임워크가 적합한 도구인지 확인, 기업의 딥러닝 프레임워크 사용, 기업의 딥러닝 프레임워크 확산. 본 연구를 통해 각 산업과 사업의 니즈에 따라, 딥러닝 프레임워크를 개발하거나 활용하고자 하는 기업에게 전략적인 시사점을 제공할 수 있을 것이라 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202009135419341&target=NART&cn=JAKO202009135419341",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출 딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출 딥러닝 오픈소스 프레임워크의 사례연구를 통한 도입 전략 도출 많은 정보통신기술 기업들은 자체적으로 개발한 인공지능 기술을 오픈소스로 공개하였다. 예를 들어, 구글의 TensorFlow, 페이스북의 PyTorch, 마이크로소프트의 CNTK 등 여러 기업들은 자신들의 인공지능 기술들을 공개하고 있다. 이처럼 대중에게 딥러닝 오픈소스 소프트웨어를 공개함으로써 개발자 커뮤니티와의 관계와 인공지능 생태계를 강화하고, 사용자들의 실험, 적용, 개선을 얻을 수 있다. 이에 따라 머신러닝 분야는 급속히 성장하고 있고, 개발자들 또한 여러가지 학습 알고리즘을 재생산하여 각 영역에 활용하고 있다. 하지만 오픈소스 소프트웨어에 대한 다양한 분석들이 이루어진 데 반해, 실제 산업현장에서 딥러닝 오픈소스 소프트웨어를 개발하거나 활용하는데 유용한 연구 결과는 미흡한 실정이다. 따라서 본 연구에서는 딥러닝 프레임워크 사례연구를 통해 해당 프레임워크의 도입 전략을 도출하고자 한다. 기술-조직-환경 프레임워크를 기반으로 기존의 오픈 소스 소프트웨어 도입과 관련된 연구들을 리뷰하고, 이를 바탕으로 두 기업의 성공 사례와 한 기업의 실패 사례를 포함한 총 3 가지 기업의 도입 사례 분석을 통해 딥러닝 프레임워크 도입을 위한 중요한 5가지 성공 요인을 도출하였다: 팀 내 개발자의 지식과 전문성, 하드웨어(GPU) 환경, 데이터 전사 협력 체계, 딥러닝 프레임워크 플랫폼, 딥러닝 프레임워크 도구 서비스. 그리고 도출한 성공 요인을 실현하기 위한 딥러닝 프레임워크의 단계적 도입 전략을 제안하였다: 프로젝트 문제 정의, 딥러닝 방법론이 적합한 기법인지 확인, 딥러닝 프레임워크가 적합한 도구인지 확인, 기업의 딥러닝 프레임워크 사용, 기업의 딥러닝 프레임워크 확산. 본 연구를 통해 각 산업과 사업의 니즈에 따라, 딥러닝 프레임워크를 개발하거나 활용하고자 하는 기업에게 전략적인 시사점을 제공할 수 있을 것이라 기대된다."
        },
        {
          "rank": 9,
          "score": 0.7804138660430908,
          "doc_id": "NART107287464",
          "title": "Benchmarking open source deep learning frameworks",
          "abstract": "<P>Deep Learning (DL) is one of the hottest fields. To foster the growth of DL, several open source frameworks appeared providing implementations of the most common DL algorithms. These frameworks vary in the algorithms they support and in the quality of their implementations. The purpose of this work is to provide a qualitative and quantitative comparison among three such frameworks: TensorFlow, Theano and CNTK. To ensure that our study is as comprehensive as possible, we consider multiple benchmark datasets from different fields (image processing, NLP, etc.) and measure the performance of the frameworks' implementations of different DL algorithms. For most of our experiments, we find out that CNTK's implementations are superior to the other ones under consideration.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART107287464&target=NART&cn=NART107287464",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Benchmarking open source deep learning frameworks Benchmarking open source deep learning frameworks Benchmarking open source deep learning frameworks <P>Deep Learning (DL) is one of the hottest fields. To foster the growth of DL, several open source frameworks appeared providing implementations of the most common DL algorithms. These frameworks vary in the algorithms they support and in the quality of their implementations. The purpose of this work is to provide a qualitative and quantitative comparison among three such frameworks: TensorFlow, Theano and CNTK. To ensure that our study is as comprehensive as possible, we consider multiple benchmark datasets from different fields (image processing, NLP, etc.) and measure the performance of the frameworks' implementations of different DL algorithms. For most of our experiments, we find out that CNTK's implementations are superior to the other ones under consideration.</P>"
        },
        {
          "rank": 10,
          "score": 0.7383836507797241,
          "doc_id": "JAKO202322957802897",
          "title": "R과 텐서플로우 딥러닝 성능 비교",
          "abstract": "본 연구에서는 무료 딥러닝 도구인 R과 텐서플로우에 대한 성능 비교를 수행하였다. 실험에서는 각 도구를 사용하여 6종류의 심층 신경망을 구축하고 10년간의 한국 온도 데이터셋을 사용하여 신경망을 학습시켰다. 구축된 신경망의 입력층 노드 갯수는 10개, 출력층은 5개로 설정 하였으며, 은닉층은 5, 10, 20개로 설정하여 실험을 진행 하였다. 학습 데이터는 2013년 3월 1일부터 2023년 3월 29일까지 서울시 강남구에서 수집된 온도 데이터 3681건을 사용하였다. 성능 비교를 위해, 학습된 신경망을 사용하여, 5일간의 온도를 예측하고 예측된 값과 실제값을 사용하여 평균 제곱근 오차(root mean square error, RMSE)값을 측정하였다. 실험결과, 은닉층이 1개인 경우, R의 학습 오차는 0.04731176이었으며, 텐서플로우는 0.06677193으로 측정되었으며, 은닉층이 2개인 경우에는 R이 0.04782134, 텐서플로 우는 0.05799060로 측정되었다. 전체적으로 R이 더 우수한 성능을 보였다. 우리는 기계학습을 처음 접하는 사용자들에게 두 도구에 대한 정량적 성능 정보를 제공함으로써, 도구 선택에서 발생하는 어려움을 해소하고자 하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202322957802897&target=NART&cn=JAKO202322957802897",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "R과 텐서플로우 딥러닝 성능 비교 R과 텐서플로우 딥러닝 성능 비교 R과 텐서플로우 딥러닝 성능 비교 본 연구에서는 무료 딥러닝 도구인 R과 텐서플로우에 대한 성능 비교를 수행하였다. 실험에서는 각 도구를 사용하여 6종류의 심층 신경망을 구축하고 10년간의 한국 온도 데이터셋을 사용하여 신경망을 학습시켰다. 구축된 신경망의 입력층 노드 갯수는 10개, 출력층은 5개로 설정 하였으며, 은닉층은 5, 10, 20개로 설정하여 실험을 진행 하였다. 학습 데이터는 2013년 3월 1일부터 2023년 3월 29일까지 서울시 강남구에서 수집된 온도 데이터 3681건을 사용하였다. 성능 비교를 위해, 학습된 신경망을 사용하여, 5일간의 온도를 예측하고 예측된 값과 실제값을 사용하여 평균 제곱근 오차(root mean square error, RMSE)값을 측정하였다. 실험결과, 은닉층이 1개인 경우, R의 학습 오차는 0.04731176이었으며, 텐서플로우는 0.06677193으로 측정되었으며, 은닉층이 2개인 경우에는 R이 0.04782134, 텐서플로 우는 0.05799060로 측정되었다. 전체적으로 R이 더 우수한 성능을 보였다. 우리는 기계학습을 처음 접하는 사용자들에게 두 도구에 대한 정량적 성능 정보를 제공함으로써, 도구 선택에서 발생하는 어려움을 해소하고자 하였다."
        },
        {
          "rank": 11,
          "score": 0.7326418161392212,
          "doc_id": "JAKO201722163438668",
          "title": "통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가",
          "abstract": "최근, 딥러닝을 사용 가능한 임베디드 디바이스가 상용화됨에 따라 임베디드 시스템 영역에서도 딥러닝 활용에 대한 다양한 연구가 진행되고 있다. 그러나 임베디드 시스템을 고성능 PC 환경과 비교하면 상대적으로 저사양의 CPU/GPU 프로세서와 메모리를 탑재하고 있으므로 딥러닝 기술의 적용에 있어서 많은 제약이 있다. 본 논문에서는 다양한 최신 딥러닝 네트워크들을 임베디드 디바이스에 적용했을때의 성능을 시간과 전력이라는 관점에서 실험적으로 평가한다. 또한, 호스트 CPU와 GPU 디바이스간의 메모리를 공유하는 임베디드 시스템들의 아키텍처적인 특성을 이용하여 메모리 복사를 줄임으로써 실시간 성능과 저전력성을 높이는 방법을 제시한다. 제안된 방법은 대표적인 공개 딥러닝 프레임워크인 Caffe를 수정하여 구현되었으며, 임베디드 GPU를 탑재한 NVIDIA Jetson TK1에서 성능평가 되었다. 실험결과, 대부분의 딥러닝 네트워크에서 뚜렷한 성능향상을 관찰할 수 있었다. 특히, 메모리 사용량이 높은 AlexNet에서 약 33%의 이미지 인식 속도 단축과 50%의 소비 전력량 감소를 관찰할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201722163438668&target=NART&cn=JAKO201722163438668",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 최근, 딥러닝을 사용 가능한 임베디드 디바이스가 상용화됨에 따라 임베디드 시스템 영역에서도 딥러닝 활용에 대한 다양한 연구가 진행되고 있다. 그러나 임베디드 시스템을 고성능 PC 환경과 비교하면 상대적으로 저사양의 CPU/GPU 프로세서와 메모리를 탑재하고 있으므로 딥러닝 기술의 적용에 있어서 많은 제약이 있다. 본 논문에서는 다양한 최신 딥러닝 네트워크들을 임베디드 디바이스에 적용했을때의 성능을 시간과 전력이라는 관점에서 실험적으로 평가한다. 또한, 호스트 CPU와 GPU 디바이스간의 메모리를 공유하는 임베디드 시스템들의 아키텍처적인 특성을 이용하여 메모리 복사를 줄임으로써 실시간 성능과 저전력성을 높이는 방법을 제시한다. 제안된 방법은 대표적인 공개 딥러닝 프레임워크인 Caffe를 수정하여 구현되었으며, 임베디드 GPU를 탑재한 NVIDIA Jetson TK1에서 성능평가 되었다. 실험결과, 대부분의 딥러닝 네트워크에서 뚜렷한 성능향상을 관찰할 수 있었다. 특히, 메모리 사용량이 높은 AlexNet에서 약 33%의 이미지 인식 속도 단축과 50%의 소비 전력량 감소를 관찰할 수 있었다."
        },
        {
          "rank": 12,
          "score": 0.7288034558296204,
          "doc_id": "JAKO202512254006340",
          "title": "컨테이너 환경에서 딥러닝 워크로드의 성능 분석",
          "abstract": "최근 딥러닝 워크로드가 컨테이너 환경에서 실행되는 사례가 늘고 있다. 컨테이너는 가상머신에 비해 낮은 오버 헤드와 높은 이식성을 제공하지만, 딥러닝 워크로드의 실행 시 시스템 자원의 비효율적 활용 문제가 발생할 수 있다. 본 논문에서는 컨테이너 환경에서 딥러닝 워크로드 실행으로 인한 오버헤드와 비효율성을 분석하기 위해 시스템콜 및 이벤트 추적 트레이스를 수집 및 분석하였다. 특히, 동일한 워크로드를 호스트 머신에서 직접 실행한 경우와 컨테이너 환경에서 실행한 경우를 비교하여 자원 소비 및 간섭과 관련된 컨테이너 환경의 오버헤드를 정량적으로 확인하였다. 분석 결과 딥러닝 워크로드의 컨테이너 실행 시 성능 병목을 초래하는 주요 원인으로 주기적인 스토리지 플러시 작업이 확인되었으며, 다중 테넌트 환경에서는 자원 경합으로 인해 이러한 문제가 더욱 심화됨을 확인하였다. 본 연구의 결과는 컨테이너 환경에서 딥러닝 워크로드를 효율적으로 실행하기 위한 클라우드 및 엣지 시스템 설계에 중요한 인사이트를 제공할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202512254006340&target=NART&cn=JAKO202512254006340",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "컨테이너 환경에서 딥러닝 워크로드의 성능 분석 컨테이너 환경에서 딥러닝 워크로드의 성능 분석 컨테이너 환경에서 딥러닝 워크로드의 성능 분석 최근 딥러닝 워크로드가 컨테이너 환경에서 실행되는 사례가 늘고 있다. 컨테이너는 가상머신에 비해 낮은 오버 헤드와 높은 이식성을 제공하지만, 딥러닝 워크로드의 실행 시 시스템 자원의 비효율적 활용 문제가 발생할 수 있다. 본 논문에서는 컨테이너 환경에서 딥러닝 워크로드 실행으로 인한 오버헤드와 비효율성을 분석하기 위해 시스템콜 및 이벤트 추적 트레이스를 수집 및 분석하였다. 특히, 동일한 워크로드를 호스트 머신에서 직접 실행한 경우와 컨테이너 환경에서 실행한 경우를 비교하여 자원 소비 및 간섭과 관련된 컨테이너 환경의 오버헤드를 정량적으로 확인하였다. 분석 결과 딥러닝 워크로드의 컨테이너 실행 시 성능 병목을 초래하는 주요 원인으로 주기적인 스토리지 플러시 작업이 확인되었으며, 다중 테넌트 환경에서는 자원 경합으로 인해 이러한 문제가 더욱 심화됨을 확인하였다. 본 연구의 결과는 컨테이너 환경에서 딥러닝 워크로드를 효율적으로 실행하기 위한 클라우드 및 엣지 시스템 설계에 중요한 인사이트를 제공할 수 있을 것으로 기대된다."
        },
        {
          "rank": 13,
          "score": 0.7273454070091248,
          "doc_id": "DIKO0016819326",
          "title": "클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석",
          "abstract": "DNN(Deep Neural Network)은 이미지 인식, 자연어 처리를 포함한 다양한 분야에서 널리 사용되고 있으며 이러한 모델을 실제 환경에서 효율적으로 실행하는 것이 중요하다. 사용자의 요구를 충족하기 위해 딥러닝 추론을 최적화하는 것은 더욱 중요하지만 다양한 클라우드 환경에서 딥러닝 추론 최적화 구성을 찾는것은 어렵다.&amp;#xD; 본 논문에서는 다양한 하드웨어와 최적화 기법을 활용하여 딥러닝 추론을 실험하고 분석한다. 딥러닝 추론의 특성을 파악하고 클라우드에서 제공하는하드웨어들 중 서버리스 컴퓨팅 아키텍처를 사용하여 딥러닝 추론 작업을 배포하는데 용이한 프로토타입을 제안하고 결과를 분석한다. 하드웨어 유형, 모델 그래프 최적화, 하드웨어 최적화 및 컴파일, 서버리스 메모리 및 배치 크기 설정 등 서버리스 컴퓨팅 환경에서 제공할 때 고려해야할 많은 요소들이 있으며 사용자는 완전 관리형 웹 서비스를 통해 쉽게 다양한 구성에 대해서 시도해볼 수 있다. 제안한 시스템을 통해 최적의 서버리스 환경 구성을 찾을 수 있으며 공개된 소스를 통해 FaaS 구성에 비교적 쉽게 접근할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016819326&target=NART&cn=DIKO0016819326",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석 클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석 클라우드 환경에서 딥러닝 추론의 성능 최적화 및 분석 DNN(Deep Neural Network)은 이미지 인식, 자연어 처리를 포함한 다양한 분야에서 널리 사용되고 있으며 이러한 모델을 실제 환경에서 효율적으로 실행하는 것이 중요하다. 사용자의 요구를 충족하기 위해 딥러닝 추론을 최적화하는 것은 더욱 중요하지만 다양한 클라우드 환경에서 딥러닝 추론 최적화 구성을 찾는것은 어렵다.&amp;#xD; 본 논문에서는 다양한 하드웨어와 최적화 기법을 활용하여 딥러닝 추론을 실험하고 분석한다. 딥러닝 추론의 특성을 파악하고 클라우드에서 제공하는하드웨어들 중 서버리스 컴퓨팅 아키텍처를 사용하여 딥러닝 추론 작업을 배포하는데 용이한 프로토타입을 제안하고 결과를 분석한다. 하드웨어 유형, 모델 그래프 최적화, 하드웨어 최적화 및 컴파일, 서버리스 메모리 및 배치 크기 설정 등 서버리스 컴퓨팅 환경에서 제공할 때 고려해야할 많은 요소들이 있으며 사용자는 완전 관리형 웹 서비스를 통해 쉽게 다양한 구성에 대해서 시도해볼 수 있다. 제안한 시스템을 통해 최적의 서버리스 환경 구성을 찾을 수 있으며 공개된 소스를 통해 FaaS 구성에 비교적 쉽게 접근할 수 있다."
        },
        {
          "rank": 14,
          "score": 0.720721960067749,
          "doc_id": "NART96288640",
          "title": "Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms",
          "abstract": "<P><B>Abstract</B></P>  <P>One of the primary goals of the artificial intelligence field is to produce fully autonomous agents that interact with theirenvironments to learn optimal behaviors, improving over time through trial and error. A mathematical principled framework for experience-driven autonomous learning is reinforcement learning, but they are inherently limited to low-dimensional problems,but the deep learning boom has provided new tools to overcome these problems. For deep reinforcement learning teaching, we do not have an appropriate platform for making optimal labs. In the article, after studying the theoretical foundations and the requirements of the main platforms, we selected two open source platforms, according to their characteristics: robotic simulators platforms for teaching and benchmarking deep reinforcement learning algorithms. The first platform was <I>Gym and V-REP</I> and the second one, <I>KNIME Deeplearning4J Integration supports and Teaching-Box.</I> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART96288640&target=NART&cn=NART96288640",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms Open Source Robotic Simulators Platforms for Teaching Deep Reinforcement Learning Algorithms <P><B>Abstract</B></P>  <P>One of the primary goals of the artificial intelligence field is to produce fully autonomous agents that interact with theirenvironments to learn optimal behaviors, improving over time through trial and error. A mathematical principled framework for experience-driven autonomous learning is reinforcement learning, but they are inherently limited to low-dimensional problems,but the deep learning boom has provided new tools to overcome these problems. For deep reinforcement learning teaching, we do not have an appropriate platform for making optimal labs. In the article, after studying the theoretical foundations and the requirements of the main platforms, we selected two open source platforms, according to their characteristics: robotic simulators platforms for teaching and benchmarking deep reinforcement learning algorithms. The first platform was <I>Gym and V-REP</I> and the second one, <I>KNIME Deeplearning4J Integration supports and Teaching-Box.</I> </P>"
        },
        {
          "rank": 15,
          "score": 0.720056414604187,
          "doc_id": "JAKO202013261023095",
          "title": "딥러닝을 위한 경사하강법 비교",
          "abstract": "본 논문에서는 신경망을 학습하는 데 가장 많이 사용되고 있는 경사하강법에 대해 분석하였다. 학습이란 손실함수가 최소값이 되도록 매개변수를 갱신하는 것이다. 손실함수는 실제값과 예측값의 차이를 수치화 해주는 함수이다. 경사하강법은 오차가 최소화되도록 매개변수를 갱신하는데 손실함수의 기울기를 사용하는 것으로 현재 최고의 딥러닝 학습알고리즘을 제공하는 라이브러리에서 사용되고 있다. 그러나 이 알고리즘들은 블랙박스형태로 제공되고 있어서 다양한 경사하강법들의 장단점을 파악하는 것이 쉽지 않다. 경사하강법에서 현재 대표적으로 사용되고 있는 확률적 경사하강법(Stochastic Gradient Descent method), 모멘텀법(Momentum method), AdaGrad법 그리고 Adadelta법의 특성에 대하여 분석하였다. 실험 데이터는 신경망을 검증하는 데 널리 사용되는 MNIST 데이터 셋을 사용하였다. 은닉층은 2개의 층으로 첫 번째 층은 500개 그리고 두 번째 층은 300개의 뉴런으로 구성하였다. 출력 층의 활성화함수는 소프트 맥스함수이고 나머지 입력 층과 은닉 층의 활성화함수는 ReLu함수를 사용하였다. 그리고 손실함수는 교차 엔트로피 오차를 사용하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202013261023095&target=NART&cn=JAKO202013261023095",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 위한 경사하강법 비교 딥러닝을 위한 경사하강법 비교 딥러닝을 위한 경사하강법 비교 본 논문에서는 신경망을 학습하는 데 가장 많이 사용되고 있는 경사하강법에 대해 분석하였다. 학습이란 손실함수가 최소값이 되도록 매개변수를 갱신하는 것이다. 손실함수는 실제값과 예측값의 차이를 수치화 해주는 함수이다. 경사하강법은 오차가 최소화되도록 매개변수를 갱신하는데 손실함수의 기울기를 사용하는 것으로 현재 최고의 딥러닝 학습알고리즘을 제공하는 라이브러리에서 사용되고 있다. 그러나 이 알고리즘들은 블랙박스형태로 제공되고 있어서 다양한 경사하강법들의 장단점을 파악하는 것이 쉽지 않다. 경사하강법에서 현재 대표적으로 사용되고 있는 확률적 경사하강법(Stochastic Gradient Descent method), 모멘텀법(Momentum method), AdaGrad법 그리고 Adadelta법의 특성에 대하여 분석하였다. 실험 데이터는 신경망을 검증하는 데 널리 사용되는 MNIST 데이터 셋을 사용하였다. 은닉층은 2개의 층으로 첫 번째 층은 500개 그리고 두 번째 층은 300개의 뉴런으로 구성하였다. 출력 층의 활성화함수는 소프트 맥스함수이고 나머지 입력 층과 은닉 층의 활성화함수는 ReLu함수를 사용하였다. 그리고 손실함수는 교차 엔트로피 오차를 사용하였다."
        },
        {
          "rank": 16,
          "score": 0.718543291091919,
          "doc_id": "DIKO0009828582",
          "title": "오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석",
          "abstract": "객체 지향 기술과 관계형 기술은 대부분의 기업에서 어플리케이션을 개발할 때 공통적으로 사용되는 기술이다. 객체 지향 기술은 데이터와 행위를 가진 객체를 통해 어플리케이션 구축을 지원하며, 관계형 기술은 데이터 저장과 프로시저나 SQL를 통한 데이터 조작을 지원한다. 하지만 명확하게 두 기술은 서로 다르다. 이처럼 객체 기술과 관계형 기술을 같이 사용했을 매 발생하는 두 기술간의 불일치를 'object-relational impedance mismatch'라고 한다. 이러한 문제를 해결하기 위해 등장한 기술중의 하나가 바로 ORM(Object/Relational Mapping)이다. 본 논문에서는 ORM 기술을 지원하는 ORM 툴로서의 오픈 소스 자바 프레임 워크를 성능이나 코드 복잡성, 관리 용이성 등 다양한 측면에서 비교 분석하였다. 현재 30여가지가 넘는 다양한 오픈 소스 자바 프레임워크가 개발되어 배포되고 있지만, 본 논문에서는 Hibernate, iBatis SqlMaps, Apache OJB 이렇게 새 개의 프래임워크를 현재 객체 지속성을 위해 가장 많이 사용되는 JDBC 기술을 기준으로 비교 분석하였다. 데이터에 대한 CRUD(저장,추출,수정,삭제)를 수행하는 시간을 통해 성능 분석을 실시하였으며, 사례 어플리케이션 구현을 통해 각 프레임워크 별로 CRUD를 수행하는 메소드 구현 시 코드량 분석을 통해 코드 복잡성을, 요구 사항 변경 시 어떻게 각 프레임워크가 이를 반영하는지를 통해 관리 용이성을 분석하였다. 이러한 분석을 통해 각 프레임워크가 어떠한 서비스를 제공하는지, 각 프레임워크의 성능은 어떠한지 쉽게 알 수 있다. 따라서 기업은 좀더 명확한 근거를 통해 어플리케이션 개발에 적절한 퍼시스턴스 프레임워크를 선택할 수 있을 것이다. 또한 상용과 오픈 소스 기반의 프레임워크 중 어떠한 것을 도입해야 할지 결정해야 할 경우 중요한 참고 자료로 활용할 수 있을 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0009828582&target=NART&cn=DIKO0009828582",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 객체 지향 기술과 관계형 기술은 대부분의 기업에서 어플리케이션을 개발할 때 공통적으로 사용되는 기술이다. 객체 지향 기술은 데이터와 행위를 가진 객체를 통해 어플리케이션 구축을 지원하며, 관계형 기술은 데이터 저장과 프로시저나 SQL를 통한 데이터 조작을 지원한다. 하지만 명확하게 두 기술은 서로 다르다. 이처럼 객체 기술과 관계형 기술을 같이 사용했을 매 발생하는 두 기술간의 불일치를 'object-relational impedance mismatch'라고 한다. 이러한 문제를 해결하기 위해 등장한 기술중의 하나가 바로 ORM(Object/Relational Mapping)이다. 본 논문에서는 ORM 기술을 지원하는 ORM 툴로서의 오픈 소스 자바 프레임 워크를 성능이나 코드 복잡성, 관리 용이성 등 다양한 측면에서 비교 분석하였다. 현재 30여가지가 넘는 다양한 오픈 소스 자바 프레임워크가 개발되어 배포되고 있지만, 본 논문에서는 Hibernate, iBatis SqlMaps, Apache OJB 이렇게 새 개의 프래임워크를 현재 객체 지속성을 위해 가장 많이 사용되는 JDBC 기술을 기준으로 비교 분석하였다. 데이터에 대한 CRUD(저장,추출,수정,삭제)를 수행하는 시간을 통해 성능 분석을 실시하였으며, 사례 어플리케이션 구현을 통해 각 프레임워크 별로 CRUD를 수행하는 메소드 구현 시 코드량 분석을 통해 코드 복잡성을, 요구 사항 변경 시 어떻게 각 프레임워크가 이를 반영하는지를 통해 관리 용이성을 분석하였다. 이러한 분석을 통해 각 프레임워크가 어떠한 서비스를 제공하는지, 각 프레임워크의 성능은 어떠한지 쉽게 알 수 있다. 따라서 기업은 좀더 명확한 근거를 통해 어플리케이션 개발에 적절한 퍼시스턴스 프레임워크를 선택할 수 있을 것이다. 또한 상용과 오픈 소스 기반의 프레임워크 중 어떠한 것을 도입해야 할지 결정해야 할 경우 중요한 참고 자료로 활용할 수 있을 것이다."
        },
        {
          "rank": 17,
          "score": 0.7110514640808105,
          "doc_id": "ATN0038661375",
          "title": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험",
          "abstract": "Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0038661375&target=NART&cn=ATN0038661375",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model."
        },
        {
          "rank": 18,
          "score": 0.7107077836990356,
          "doc_id": "DIKO0008948393",
          "title": "소프트웨어 컴포넌트 프레임워크 성능 비교",
          "abstract": "프레임워크는 포함하는 컴포넌트의 영역적 특성에 따라 시스템의 하부 구조를 중심으로 정의된 시스템 프레임워크와 특정 응용 영역을 위한 응용 애플리케이션 프레임워크로 나뉠 수 있다. 본 논문에서는 각각의 프레임워크 사용 제품에 대해 비교 분석하였다. 시스템 프레임워크는 시스템의 기본 성능인 '객체모델', '객체 버스', '언어의 독립성', '위치 투명성'등을 기준으로 비교 하였다. CORBA는 컴포넌트 간 상호 운용성이 뛰어나며, 다양한 서비스와 응용 컴포넌트를 지원한다. 이에 비하여 DCOM은 윈도우 사용자를 기반으로 하고 있으며 MTS를 기반으로 하는 분산 트랜젝션 기능을 지원한다. EJB는 구현언어의 독립성을 갖는 CORBA와 DCOM과 달리 자바라는 단일 언어를 기반으로 한다. 이로 인해 언어 독립적이지는 않지만 컨테이너 차원에서 데이터베이스, 트랜잭션, 보안문제, 데이터베이스의 커넥션 플링, 쓰레딩과 같은 기능을 제공한다 특정 비즈니스 영역을 위한 Steelpia, SanFrancisco의 경우에는 각 프레임워크의 재사용 컴포넌트 레이어에 대한 비교를 하였다 Steelpia는 철강 업무를 지원하며, 핵심 비즈니스 컴포넌트를 크기에 맞게 특화하여 기존의 개발 방식보다 30-50% 정도의 기간비용 효과를 얻을 수 있다. SanFrancisco 는 비즈니스 컴포넌트를 지원하여 특화함으로 애플리케이션 개발시 전체 노력의 40%정도의 시간적, 비용적 이익을 가져다 준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0008948393&target=NART&cn=DIKO0008948393",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "소프트웨어 컴포넌트 프레임워크 성능 비교 소프트웨어 컴포넌트 프레임워크 성능 비교 소프트웨어 컴포넌트 프레임워크 성능 비교 프레임워크는 포함하는 컴포넌트의 영역적 특성에 따라 시스템의 하부 구조를 중심으로 정의된 시스템 프레임워크와 특정 응용 영역을 위한 응용 애플리케이션 프레임워크로 나뉠 수 있다. 본 논문에서는 각각의 프레임워크 사용 제품에 대해 비교 분석하였다. 시스템 프레임워크는 시스템의 기본 성능인 '객체모델', '객체 버스', '언어의 독립성', '위치 투명성'등을 기준으로 비교 하였다. CORBA는 컴포넌트 간 상호 운용성이 뛰어나며, 다양한 서비스와 응용 컴포넌트를 지원한다. 이에 비하여 DCOM은 윈도우 사용자를 기반으로 하고 있으며 MTS를 기반으로 하는 분산 트랜젝션 기능을 지원한다. EJB는 구현언어의 독립성을 갖는 CORBA와 DCOM과 달리 자바라는 단일 언어를 기반으로 한다. 이로 인해 언어 독립적이지는 않지만 컨테이너 차원에서 데이터베이스, 트랜잭션, 보안문제, 데이터베이스의 커넥션 플링, 쓰레딩과 같은 기능을 제공한다 특정 비즈니스 영역을 위한 Steelpia, SanFrancisco의 경우에는 각 프레임워크의 재사용 컴포넌트 레이어에 대한 비교를 하였다 Steelpia는 철강 업무를 지원하며, 핵심 비즈니스 컴포넌트를 크기에 맞게 특화하여 기존의 개발 방식보다 30-50% 정도의 기간비용 효과를 얻을 수 있다. SanFrancisco 는 비즈니스 컴포넌트를 지원하여 특화함으로 애플리케이션 개발시 전체 노력의 40%정도의 시간적, 비용적 이익을 가져다 준다."
        },
        {
          "rank": 19,
          "score": 0.7105157375335693,
          "doc_id": "JAKO202433861648179",
          "title": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교",
          "abstract": "본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202433861648179&target=NART&cn=JAKO202433861648179",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다."
        },
        {
          "rank": 20,
          "score": 0.6985523700714111,
          "doc_id": "JAKO202404861562091",
          "title": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교",
          "abstract": "대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202404861562091&target=NART&cn=JAKO202404861562091",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다."
        },
        {
          "rank": 21,
          "score": 0.6971399784088135,
          "doc_id": "JAKO202514154005683",
          "title": "RAG 시스템 성능 평가를 위한 자동 데이터 셋 생성 프레임워크 비교 분석 연구",
          "abstract": "본 논문은 최근 주목받고 있는 검색 증강 생성(RAG) 시스템의 성능 평가를 위한 테스트 데이터셋 생성 방법을 비교 분석하였다. 대규모 언어 모델(LLM)의 한계를 극복하는 RAG 기술의 필요성과 중요성을 설명하고, 수동 생성 방식과 LLM을 활용한 자동 생성 방식의 특징과 장단점을 정리하였다. 또한 자동화된 데이터셋 구축 프레임워크 중 RAGAS, AutoRAG, DeepEval을 선정하여 의료,금융,법률 문서를 입력으로 각각 100개의 질문-답변 세트를 생성한 후 정확성을 평가하였다. 평가 결과, AutoRAG가 한국어 문장 표현의 자연성과 컨텍스트 기반의 정확성 측면에서 가장 뛰어난 성능을 보였으며, RAGAS는 문서 처리 과정에서 불필요한 정보 포함 등의 오류가 많았고, DeepEval은 한국어 지원 부족으로 인해 성능이 상대적으로 낮았다. 향후 연구에서는 LLM을 활용하여 사용자의 의도와 컨텍스트를 더욱 정확히 반영하는 고급 프롬프팅 기법과 자동화된 데이터 품질 평가 및 개선 전략을 중점적으로 탐색할 계획이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202514154005683&target=NART&cn=JAKO202514154005683",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RAG 시스템 성능 평가를 위한 자동 데이터 셋 생성 프레임워크 비교 분석 연구 RAG 시스템 성능 평가를 위한 자동 데이터 셋 생성 프레임워크 비교 분석 연구 RAG 시스템 성능 평가를 위한 자동 데이터 셋 생성 프레임워크 비교 분석 연구 본 논문은 최근 주목받고 있는 검색 증강 생성(RAG) 시스템의 성능 평가를 위한 테스트 데이터셋 생성 방법을 비교 분석하였다. 대규모 언어 모델(LLM)의 한계를 극복하는 RAG 기술의 필요성과 중요성을 설명하고, 수동 생성 방식과 LLM을 활용한 자동 생성 방식의 특징과 장단점을 정리하였다. 또한 자동화된 데이터셋 구축 프레임워크 중 RAGAS, AutoRAG, DeepEval을 선정하여 의료,금융,법률 문서를 입력으로 각각 100개의 질문-답변 세트를 생성한 후 정확성을 평가하였다. 평가 결과, AutoRAG가 한국어 문장 표현의 자연성과 컨텍스트 기반의 정확성 측면에서 가장 뛰어난 성능을 보였으며, RAGAS는 문서 처리 과정에서 불필요한 정보 포함 등의 오류가 많았고, DeepEval은 한국어 지원 부족으로 인해 성능이 상대적으로 낮았다. 향후 연구에서는 LLM을 활용하여 사용자의 의도와 컨텍스트를 더욱 정확히 반영하는 고급 프롬프팅 기법과 자동화된 데이터 품질 평가 및 개선 전략을 중점적으로 탐색할 계획이다."
        },
        {
          "rank": 22,
          "score": 0.6964774131774902,
          "doc_id": "JAKO201620853199880",
          "title": "딥러닝의 모형과 응용사례",
          "abstract": "딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수 있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201620853199880&target=NART&cn=JAKO201620853199880",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝의 모형과 응용사례 딥러닝의 모형과 응용사례 딥러닝의 모형과 응용사례 딥러닝은 인공신경망(neural network)이라는 인공지능분야의 모형이 발전된 형태로서, 계층구조로 이루어진 인공신경망의 내부계층(hidden layer)이 여러 단계로 이루어진 구조이다. 딥러닝에서의 주요 모형은 합성곱신경망(convolutional neural network), 순환신경망(recurrent neural network), 그리고 심층신뢰신경망(deep belief network)의 세가지라고 할 수 있다. 그 중에서 현재 흥미로운 연구가 많이 발표되어서 관심이 집중되고 있는 모형은 지도학습(supervised learning)모형인 처음 두 개의 모형이다. 따라서 본 논문에서는 지도학습모형의 가중치를 최적화하는 기본적인 방법인 오류역전파 알고리즘을 살펴본 뒤에 합성곱신경망과 순환신경망의 구조와 응용사례 등을 살펴보고자 한다. 본문에서 다루지 않은 모형인 심층신뢰신경망은 아직까지는 합성곱신경망 이나 순환신경망보다는 상대적으로 주목을 덜 받고 있다. 그러나 심층신뢰신경망은 CNN이나 RNN과는 달리 비지도학습(unsupervised learning)모형이며, 사람이나 동물은 관찰을 통해서 스스로 학습한다는 점에서 궁극적으로는 비지도학습모형이 더 많이 연구되어야 할 주제가 될 것이다."
        },
        {
          "rank": 23,
          "score": 0.6945513486862183,
          "doc_id": "DIKO0017011976",
          "title": "대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형",
          "abstract": "본 연구는 온라인 리뷰의 유용성을 예측하기 위한 모델을 제안하며, 이를 위해 대형 언어 모델과 다양한 딥러닝 기법을 통합적으로 활용하였다. 연구의 시작에서는 온라인 리뷰 및 리뷰 유용성에 대한 이론적 배경을 탐구하였으며, 여러 기존 연구들을 통해 리뷰 유용성에 영향을 미치는 요인들을 정리하였다. 특히, 통계기법, 머신러닝, 딥러닝, 그리고 대형 언어 모델을 중심으로 한 기존의 리뷰 유용성 예측 모형들을 비교 및 분석하였다. 이후, KoBERT와 KoGPT2와 같은 한국어 대형 언어 모델을 기반으로 한 리뷰 유용성 예측모형을 구축하였으며, K-NN 알고리즘으로 통합하여 모델의 성능을 향상시켰다. 실증분석 결과, 본 연구에서 제안한 모델은 기존의 모델들에 비해 높은 예측 성능을 보여주었고, 특히 대형 언어 모델의 통합은 리뷰 유용성 예측의 정확도를 크게 향상시켰다. 이러한 결과는 온라인 리뷰의 품질 및 유용성 평가에 큰 도움을 제공할 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017011976&target=NART&cn=DIKO0017011976",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형 대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형 대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형 본 연구는 온라인 리뷰의 유용성을 예측하기 위한 모델을 제안하며, 이를 위해 대형 언어 모델과 다양한 딥러닝 기법을 통합적으로 활용하였다. 연구의 시작에서는 온라인 리뷰 및 리뷰 유용성에 대한 이론적 배경을 탐구하였으며, 여러 기존 연구들을 통해 리뷰 유용성에 영향을 미치는 요인들을 정리하였다. 특히, 통계기법, 머신러닝, 딥러닝, 그리고 대형 언어 모델을 중심으로 한 기존의 리뷰 유용성 예측 모형들을 비교 및 분석하였다. 이후, KoBERT와 KoGPT2와 같은 한국어 대형 언어 모델을 기반으로 한 리뷰 유용성 예측모형을 구축하였으며, K-NN 알고리즘으로 통합하여 모델의 성능을 향상시켰다. 실증분석 결과, 본 연구에서 제안한 모델은 기존의 모델들에 비해 높은 예측 성능을 보여주었고, 특히 대형 언어 모델의 통합은 리뷰 유용성 예측의 정확도를 크게 향상시켰다. 이러한 결과는 온라인 리뷰의 품질 및 유용성 평가에 큰 도움을 제공할 것으로 기대된다."
        },
        {
          "rank": 24,
          "score": 0.6944317817687988,
          "doc_id": "JAKO202116047225054",
          "title": "신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어",
          "abstract": "최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형 교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은 고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를 분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에 적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은 내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기 위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와 더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의 정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116047225054&target=NART&cn=JAKO202116047225054",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어 신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어 신뢰성있는 딥러닝 기반 분석 모델을 참조하기 위한 딥러닝 기술 언어 최근 딥러닝은 하드웨어 성능이 향상됨에 따라 자연어 처리, 영상 인식 등의 다양한 기술에 접목되어 활용되고 있다. 이러한 기술들을 활용해 지능형 교통 시스템(ITS), 스마트홈, 헬스케어 등의 산업분야에서 데이터를 분석하여 고속도로 속도위반 차량 검출, 에너지 사용량 제어, 응급상황 등과 같은 고품질의 서비스를 제공하며, 고품질의 서비스를 제공하기 위해서는 정확도가 향상된 딥러닝 모델이 적용되어야 한다. 이를 위해 서비스 환경의 데이터를 분석하기 위한 딥러닝 모델을 개발할 때, 개발자는 신뢰성이 검증된 최신의 딥러닝 모델을 적용할 수 있어야 한다. 이는 개발자가 참조하는 딥러닝 모델에 적용된 학습 데이터셋의 정확도를 측정하여 검증할 수 있다. 이러한 검증을 위해서 개발자는 학습 데이터셋, 딥러닝의 계층구조 및 개발 환경 등과 같은 내용을 포함하는 딥러닝 모델을 문서화하여 적용하기 위한 구조적인 정보가 필요하다. 본 논문에서는 신뢰성있는 딥러닝 기반 데이터 분석 모델을 참조하기 위한 딥러닝 기술 언어를 제안한다. 제안하는 기술 언어는 신뢰성 있는 딥러닝 모델을 개발하는데 필요한 학습데이터셋, 개발 환경 및 설정 등의 정보와 더불어 딥러닝 모델의 계층구조를 표현할 수 있다. 제안하는 딥러닝 기술 언어를 이용하여 개발자는 지능형 교통 시스템에서 참조하는 분석 모델의 정확도를 검증할 수 있다. 실험에서는 제안하는 언어의 유효성을 검증하기 위해, 번호판 인식 모델을 중심으로 딥러닝 기술 문서의 적용과정을 보인다."
        },
        {
          "rank": 25,
          "score": 0.6893310546875,
          "doc_id": "DIKO0014861002",
          "title": "딥 러닝기반 고객평점 예측모델",
          "abstract": "인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014861002&target=NART&cn=DIKO0014861002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다."
        },
        {
          "rank": 26,
          "score": 0.6892210245132446,
          "doc_id": "JAKO202218262151224",
          "title": "딥러닝 기반 단일 이미지 생성적 적대 신경망 기법 비교 분석",
          "abstract": "생성적 적대 신경망(GAN, Generative Adversarial Networks)는 이미지 생성 분야에서 주목할 만한 발전을 이루었다. 하지만 큰 데이터 셋에서 불안정한 모습을 보인다는 한계 때문에 다양한 응용 분야에 쉽게 적용하기 어렵다. 단일 이미지 생성적 적대 신경망은 한장의 이미지의 내부 분포를 잘 학습하여 다양한 영상을 생성하는 분야이다. 큰 데이터셋이 아닌 단 한장만 학습함으로써 안정적인 학습이 가능하며 이미지 리타겟팅, 이미지 조작, super resolution 등 다양한 분야에 활용 가능하다. 본 논문에서는 SinGAN, ConSinGAN, InGAN, DeepSIM, 그리고 One-Shot GAN 총 다섯 개의 단일 이미지 생성적 적대 신경망을 살펴본다. 우리는 각각의 단일 이미지 생성적 적대 신경망 모델들의 성능을 비교하고 장단점을 분석한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202218262151224&target=NART&cn=JAKO202218262151224",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 단일 이미지 생성적 적대 신경망 기법 비교 분석 딥러닝 기반 단일 이미지 생성적 적대 신경망 기법 비교 분석 딥러닝 기반 단일 이미지 생성적 적대 신경망 기법 비교 분석 생성적 적대 신경망(GAN, Generative Adversarial Networks)는 이미지 생성 분야에서 주목할 만한 발전을 이루었다. 하지만 큰 데이터 셋에서 불안정한 모습을 보인다는 한계 때문에 다양한 응용 분야에 쉽게 적용하기 어렵다. 단일 이미지 생성적 적대 신경망은 한장의 이미지의 내부 분포를 잘 학습하여 다양한 영상을 생성하는 분야이다. 큰 데이터셋이 아닌 단 한장만 학습함으로써 안정적인 학습이 가능하며 이미지 리타겟팅, 이미지 조작, super resolution 등 다양한 분야에 활용 가능하다. 본 논문에서는 SinGAN, ConSinGAN, InGAN, DeepSIM, 그리고 One-Shot GAN 총 다섯 개의 단일 이미지 생성적 적대 신경망을 살펴본다. 우리는 각각의 단일 이미지 생성적 적대 신경망 모델들의 성능을 비교하고 장단점을 분석한다."
        },
        {
          "rank": 27,
          "score": 0.6876915097236633,
          "doc_id": "NART108328574",
          "title": "A fully open-source framework for deep learning protein real-valued distances",
          "abstract": "<P>As deep learning algorithms drive the progress in protein structure prediction, a lot remains to be studied at this merging superhighway of deep learning and protein structure prediction. Recent findings show that inter-residue distance prediction, a more granular version of the well-known contact prediction problem, is a key to predicting accurate models. However, deep learning methods that predict these distances are still in the early stages of their development. To advance these methods and develop other novel methods, a need exists for a small and representative dataset packaged for faster development and testing. In this work, we introduce protein distance net (PDNET), a framework that consists of one such representative dataset along with the scripts for training and testing deep learning methods. The framework also includes all the scripts that were used to curate the dataset, and generate the input features and distance maps. Deep learning models can also be trained and tested in a web browser using free platforms such as Google Colab. We discuss how PDNET can be used to predict contacts, distance intervals, and real-valued distances.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART108328574&target=NART&cn=NART108328574",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A fully open-source framework for deep learning protein real-valued distances A fully open-source framework for deep learning protein real-valued distances A fully open-source framework for deep learning protein real-valued distances <P>As deep learning algorithms drive the progress in protein structure prediction, a lot remains to be studied at this merging superhighway of deep learning and protein structure prediction. Recent findings show that inter-residue distance prediction, a more granular version of the well-known contact prediction problem, is a key to predicting accurate models. However, deep learning methods that predict these distances are still in the early stages of their development. To advance these methods and develop other novel methods, a need exists for a small and representative dataset packaged for faster development and testing. In this work, we introduce protein distance net (PDNET), a framework that consists of one such representative dataset along with the scripts for training and testing deep learning methods. The framework also includes all the scripts that were used to curate the dataset, and generate the input features and distance maps. Deep learning models can also be trained and tested in a web browser using free platforms such as Google Colab. We discuss how PDNET can be used to predict contacts, distance intervals, and real-valued distances.</P>"
        },
        {
          "rank": 28,
          "score": 0.6875465512275696,
          "doc_id": "JAKO202320150299733",
          "title": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가",
          "abstract": "본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202320150299733&target=NART&cn=JAKO202320150299733",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다."
        },
        {
          "rank": 29,
          "score": 0.6866655349731445,
          "doc_id": "JAKO199911921528980",
          "title": "다층회귀예측신경망의 음성인식성능에 관한 연구",
          "abstract": "4층구조의 다층퍼셉트론을 변형하여 3 종류의 다층회귀예측신경망을 구성하고, 예측차수, 두 은닉층의 뉴런개수, 연결세기의 초기치 및 전달함수 변화에 따른 각 망의 음성인식성능을 실험을 통해 각각 비교 분석한다. 실험결과에 의하면, 다층회귀신경망이 다층퍼셉트론에 비해 음성인식성능이 우수하다. 그리고 구조적으로는 상위은닉층의 출력을 하위은닉층으로 회귀할 때 인식성능이 가장 우수하며, 각 망 공히 상, 하위은닉층의 뉴런 10 혹은 15개, 예측차수 3 혹은 4차일 때 인식률이 양호하다. 학습시 연결세기의 초기치를 -0.5에서 0.5사이로 설정하고, 하위은닉층에서 단극성 시그모이드 전달함수를 사용할 때 인식성능이 더욱 향상된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199911921528980&target=NART&cn=JAKO199911921528980",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀예측신경망의 음성인식성능에 관한 연구 다층회귀예측신경망의 음성인식성능에 관한 연구 다층회귀예측신경망의 음성인식성능에 관한 연구 4층구조의 다층퍼셉트론을 변형하여 3 종류의 다층회귀예측신경망을 구성하고, 예측차수, 두 은닉층의 뉴런개수, 연결세기의 초기치 및 전달함수 변화에 따른 각 망의 음성인식성능을 실험을 통해 각각 비교 분석한다. 실험결과에 의하면, 다층회귀신경망이 다층퍼셉트론에 비해 음성인식성능이 우수하다. 그리고 구조적으로는 상위은닉층의 출력을 하위은닉층으로 회귀할 때 인식성능이 가장 우수하며, 각 망 공히 상, 하위은닉층의 뉴런 10 혹은 15개, 예측차수 3 혹은 4차일 때 인식률이 양호하다. 학습시 연결세기의 초기치를 -0.5에서 0.5사이로 설정하고, 하위은닉층에서 단극성 시그모이드 전달함수를 사용할 때 인식성능이 더욱 향상된다."
        },
        {
          "rank": 30,
          "score": 0.6858712434768677,
          "doc_id": "JAKO199215875841266",
          "title": "음성 인식 신경망을 위한 음성 파라키터들의 성능 비교",
          "abstract": "음성 인식에 신경망 모델을 적용하는 많은 연구들이 있었지만, 주된 관심은 음성인식에 적합한 구조와 학습 방법이었다.  그러나 음성인식에 신경망 모델을 적용한 시스템의 효율 향상은 모델 자체의 구조뿐 아니라, 신경망 모델의 입력으로 어떤 음성 파라미터를 사용하는가에 따라서도 큰 영향을 받는다.  본 논문은 기존 음성인식에 신경망 모델을 적용한 많은 연구들에서 사용한 음성 파라미터를 살펴보고, 대표적인 음성 파라미터 6개를 선정하여, 같은 데이타와 같은 신경망 모델 하에서 어떻게 성능이 달라지는지를 분석한다.  인식 실험에 있어서는 한국어 파열음 9개에 대한 8개 데이터 집합과 모음 8개에 대한 18개 데이터 집합을 음성 파라미터로 하고 신경망 모델은 순환 신경망 모델을 사용하여 노드의 수를 일정하게 한뒤 다양한 입력 파라미터의 성능을 비교하였다.  그 결과 선형 예측 계수로부터 얻어진 delta cepstrum의 음성 파라미터가 가장 좋은 성능을 보였으며 이때 인식률은 같은 학습 데이터에 대해 파열음 100.0%, 모음 95.1%이었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199215875841266&target=NART&cn=JAKO199215875841266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식에 신경망 모델을 적용하는 많은 연구들이 있었지만, 주된 관심은 음성인식에 적합한 구조와 학습 방법이었다.  그러나 음성인식에 신경망 모델을 적용한 시스템의 효율 향상은 모델 자체의 구조뿐 아니라, 신경망 모델의 입력으로 어떤 음성 파라미터를 사용하는가에 따라서도 큰 영향을 받는다.  본 논문은 기존 음성인식에 신경망 모델을 적용한 많은 연구들에서 사용한 음성 파라미터를 살펴보고, 대표적인 음성 파라미터 6개를 선정하여, 같은 데이타와 같은 신경망 모델 하에서 어떻게 성능이 달라지는지를 분석한다.  인식 실험에 있어서는 한국어 파열음 9개에 대한 8개 데이터 집합과 모음 8개에 대한 18개 데이터 집합을 음성 파라미터로 하고 신경망 모델은 순환 신경망 모델을 사용하여 노드의 수를 일정하게 한뒤 다양한 입력 파라미터의 성능을 비교하였다.  그 결과 선형 예측 계수로부터 얻어진 delta cepstrum의 음성 파라미터가 가장 좋은 성능을 보였으며 이때 인식률은 같은 학습 데이터에 대해 파열음 100.0%, 모음 95.1%이었다."
        },
        {
          "rank": 31,
          "score": 0.6843762397766113,
          "doc_id": "ATN0037463572",
          "title": "서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가",
          "abstract": "Serverless computing is a new computing paradigm which can develop and execute the application program without server management overhead. Recently, as the enterprise application architectures are evolved to container and micro-services, serverless-based cloud services make it easy to expand and distribute the micro-services. In this regard, we compared the technologies and performed an experiment to measure average response time and response success rate of distributed functions for the three typical serverless open source frameworks: OpenFaaS, Kubeless and Fission.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037463572&target=NART&cn=ATN0037463572",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 Serverless computing is a new computing paradigm which can develop and execute the application program without server management overhead. Recently, as the enterprise application architectures are evolved to container and micro-services, serverless-based cloud services make it easy to expand and distribute the micro-services. In this regard, we compared the technologies and performed an experiment to measure average response time and response success rate of distributed functions for the three typical serverless open source frameworks: OpenFaaS, Kubeless and Fission."
        },
        {
          "rank": 32,
          "score": 0.683463454246521,
          "doc_id": "NART95625496",
          "title": "Open source column : deep learning with Keras",
          "abstract": "<P>Following the last column on MatConvNet, let us continue to look at open source frameworks for deep learning. In this column we are going to check Keras, a Python API that allows to use several different backends like Tensorflow and CNTK. Actually, it also supports Theano, although the development of this framework has been halted by the original developers in 2017.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART95625496&target=NART&cn=NART95625496",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Open source column : deep learning with Keras Open source column : deep learning with Keras Open source column : deep learning with Keras <P>Following the last column on MatConvNet, let us continue to look at open source frameworks for deep learning. In this column we are going to check Keras, a Python API that allows to use several different backends like Tensorflow and CNTK. Actually, it also supports Theano, although the development of this framework has been halted by the original developers in 2017.</P>"
        },
        {
          "rank": 33,
          "score": 0.6823350191116333,
          "doc_id": "NART120007501",
          "title": "Open Source Assessment of Deep Learning Visual Object Detection",
          "abstract": "<P>This paper introduces Detection Metrics, an open-source scientific software for the assessment of deep learning neural network models for visual object detection. This software provides objective performance metrics such as mean average precision and mean inference time. The most relevant international object detection datasets are supported along with the most widely used deep learning frameworks. Different network models, even those built from different frameworks, can be fairly compared in this way. This is very useful when developing deep learning applications or research. A set of tools is provided to manage and work with different datasets and models, including visualization and conversion into several common formats. Detection Metrics may also be used in automatic batch processing for large experimental tests, saving researchers time, and new domain-specific datasets can be easily created from videos or webcams. It is open-source, can be audited, extended, and adapted to particular requirements. It has been experimentally validated. The performance of the most relevant state-of-the-art neural models for object detection has been experimentally compared. In addition, it has been used in several research projects, guiding in selecting the most suitable network model architectures and training procedures. The performance of the different models and training alternatives can be easily measured, even on large datasets.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART120007501&target=NART&cn=NART120007501",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Open Source Assessment of Deep Learning Visual Object Detection Open Source Assessment of Deep Learning Visual Object Detection Open Source Assessment of Deep Learning Visual Object Detection <P>This paper introduces Detection Metrics, an open-source scientific software for the assessment of deep learning neural network models for visual object detection. This software provides objective performance metrics such as mean average precision and mean inference time. The most relevant international object detection datasets are supported along with the most widely used deep learning frameworks. Different network models, even those built from different frameworks, can be fairly compared in this way. This is very useful when developing deep learning applications or research. A set of tools is provided to manage and work with different datasets and models, including visualization and conversion into several common formats. Detection Metrics may also be used in automatic batch processing for large experimental tests, saving researchers time, and new domain-specific datasets can be easily created from videos or webcams. It is open-source, can be audited, extended, and adapted to particular requirements. It has been experimentally validated. The performance of the most relevant state-of-the-art neural models for object detection has been experimentally compared. In addition, it has been used in several research projects, guiding in selecting the most suitable network model architectures and training procedures. The performance of the different models and training alternatives can be easily measured, even on large datasets.</P>"
        },
        {
          "rank": 34,
          "score": 0.6822346448898315,
          "doc_id": "JAKO201723840540692",
          "title": "빅데이터 통합모형 비교분석",
          "abstract": "빅데이터가 4차 산업혁명의 핵심으로 자리하면서 빅데이터 기반 처리 및 분석 능력이 기업의 미래 경쟁력을 좌우할 전망이다. 빅데이터 처리 및 분석을 위한 RHadoop과 RHIPE 모형은 R과 Hadoop의 통합모형으로 지금까지 각각의 모형에 대해서는 연구가 많이 진행되어 왔으나 두 모형간 비교 연구는 거의 이루어 지지 않았다. 본 논문에서는 대용량의 실제 데이터와 모의실험 데이터에서 다중 회귀 (multiple regression)와 로지스틱 회귀 (logistic regression) 추정을 위한 머신러닝 (machine learning) 알고리즘을 MapReduce 프로그램 구현을 통해 RHadoop과 RHIPE 간의 비교 분석하고자 한다. 구축된 분산 클러스터 (distributed cluster) 하에서 두 모형간 성능 실험 결과, RHIPE은 RHadoop에 비해 대체로 빠른 처리속도를 보인 반면에 설치, 사용면에서 어려움을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201723840540692&target=NART&cn=JAKO201723840540692",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "빅데이터 통합모형 비교분석 빅데이터 통합모형 비교분석 빅데이터 통합모형 비교분석 빅데이터가 4차 산업혁명의 핵심으로 자리하면서 빅데이터 기반 처리 및 분석 능력이 기업의 미래 경쟁력을 좌우할 전망이다. 빅데이터 처리 및 분석을 위한 RHadoop과 RHIPE 모형은 R과 Hadoop의 통합모형으로 지금까지 각각의 모형에 대해서는 연구가 많이 진행되어 왔으나 두 모형간 비교 연구는 거의 이루어 지지 않았다. 본 논문에서는 대용량의 실제 데이터와 모의실험 데이터에서 다중 회귀 (multiple regression)와 로지스틱 회귀 (logistic regression) 추정을 위한 머신러닝 (machine learning) 알고리즘을 MapReduce 프로그램 구현을 통해 RHadoop과 RHIPE 간의 비교 분석하고자 한다. 구축된 분산 클러스터 (distributed cluster) 하에서 두 모형간 성능 실험 결과, RHIPE은 RHadoop에 비해 대체로 빠른 처리속도를 보인 반면에 설치, 사용면에서 어려움을 보였다."
        },
        {
          "rank": 35,
          "score": 0.6809267997741699,
          "doc_id": "DIKO0016938177",
          "title": "분산학습 환경을 적용한 연속학습",
          "abstract": "본 논문은 의료 분야에 특화된 연속 학습과 분할 학습 기법을 통합한 혁신적인 프레임워크를 제안한다. 이 프레임워크는 의료 인공지능 모델이 학습할 때 실제 환경에서 발생하는 데이터 프라이버시, 보안, 그리고 변화하는 임상 조건과 같은 주요 과제를 해결한다. 제안된 프레임워크를 통해 의료 클라이언트는 민감한 데이터를 공유하지 않으면서 분산 컴퓨팅 리소스에서 딥러닝 모델을 공동으로 학습할 수 있으며, 연산 비용(computational cost)을 줄일 수 있다. 또한, 제안된 프레임워크는 변이와 새로운 질병과 같은 의료 환경의 증분 특성(incremental characteristic)을 고려함으로써 실제 임상 시나리오에서 의료 인공지능 모델의 질병 진단 능력을 향상할 수 있다. 본 논문은 여러 가지의 의료 이미지 데이터셋과 일반 이미지 데이터셋을 사용하여 프레임워크의 성능을 검증했다. 그 결과 제안된 프레임워크는 분할 학습 상황에서 다른 연속 학습 방법보다 평균 정확도(average accuracy)와 평균 망각(average forgetting) 성능이 우수함을 보였으며, 핵심 요소 분석을 통해 성능을 여러 방면에서 분석했다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016938177&target=NART&cn=DIKO0016938177",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "분산학습 환경을 적용한 연속학습 분산학습 환경을 적용한 연속학습 분산학습 환경을 적용한 연속학습 본 논문은 의료 분야에 특화된 연속 학습과 분할 학습 기법을 통합한 혁신적인 프레임워크를 제안한다. 이 프레임워크는 의료 인공지능 모델이 학습할 때 실제 환경에서 발생하는 데이터 프라이버시, 보안, 그리고 변화하는 임상 조건과 같은 주요 과제를 해결한다. 제안된 프레임워크를 통해 의료 클라이언트는 민감한 데이터를 공유하지 않으면서 분산 컴퓨팅 리소스에서 딥러닝 모델을 공동으로 학습할 수 있으며, 연산 비용(computational cost)을 줄일 수 있다. 또한, 제안된 프레임워크는 변이와 새로운 질병과 같은 의료 환경의 증분 특성(incremental characteristic)을 고려함으로써 실제 임상 시나리오에서 의료 인공지능 모델의 질병 진단 능력을 향상할 수 있다. 본 논문은 여러 가지의 의료 이미지 데이터셋과 일반 이미지 데이터셋을 사용하여 프레임워크의 성능을 검증했다. 그 결과 제안된 프레임워크는 분할 학습 상황에서 다른 연속 학습 방법보다 평균 정확도(average accuracy)와 평균 망각(average forgetting) 성능이 우수함을 보였으며, 핵심 요소 분석을 통해 성능을 여러 방면에서 분석했다."
        },
        {
          "rank": 36,
          "score": 0.6807548999786377,
          "doc_id": "JAKO202325543363508",
          "title": "딥러닝 영상분석 시스템의 성능평가 산정식 개발",
          "abstract": "도시부 교통정보 수집은 VDS, DSRC, 레이더 등 다양한 시스템에 의해 수집되고 있다. 최근 딥러닝 기술의 발전으로 스마트교차로시스템이 확대 보급되고 있으며 교통량, 속도, 차종 등 다양한 정보수집이 가능하다. 그러나 관련 문헌을 고찰한 결과 지금까지의 성능평가 기준은 딥러닝 영역을 고려하지 않은 RBS기반 평가체계로 '기준값-측정값'의 퍼센트 오차만 고려하고 있어 기존 평가방식으로는 딥러닝 부분의 평가를 수행할 수 없어 새로운 성능평가 방법이 필요하다. 따라서, 본 연구에서는 데이터 비율 및 가중치를 고려하여 Precision과 Recall 등 딥러닝 성능지표를 고려한 오차산정식을 개발하여 개별오차와 구간 오차, 전체오차를 산정하였다. 연구결과, 측정값 1의 오차율은 3.99와 3.54, 측정값 2는 5.34와 5.07로 기존 산정식과 오차율에 차이가 있는 것으로 나타났으며, 반복측정 분석결과 개발 산정식이 우수한 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202325543363508&target=NART&cn=JAKO202325543363508",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 영상분석 시스템의 성능평가 산정식 개발 딥러닝 영상분석 시스템의 성능평가 산정식 개발 딥러닝 영상분석 시스템의 성능평가 산정식 개발 도시부 교통정보 수집은 VDS, DSRC, 레이더 등 다양한 시스템에 의해 수집되고 있다. 최근 딥러닝 기술의 발전으로 스마트교차로시스템이 확대 보급되고 있으며 교통량, 속도, 차종 등 다양한 정보수집이 가능하다. 그러나 관련 문헌을 고찰한 결과 지금까지의 성능평가 기준은 딥러닝 영역을 고려하지 않은 RBS기반 평가체계로 '기준값-측정값'의 퍼센트 오차만 고려하고 있어 기존 평가방식으로는 딥러닝 부분의 평가를 수행할 수 없어 새로운 성능평가 방법이 필요하다. 따라서, 본 연구에서는 데이터 비율 및 가중치를 고려하여 Precision과 Recall 등 딥러닝 성능지표를 고려한 오차산정식을 개발하여 개별오차와 구간 오차, 전체오차를 산정하였다. 연구결과, 측정값 1의 오차율은 3.99와 3.54, 측정값 2는 5.34와 5.07로 기존 산정식과 오차율에 차이가 있는 것으로 나타났으며, 반복측정 분석결과 개발 산정식이 우수한 것으로 나타났다."
        },
        {
          "rank": 37,
          "score": 0.6804394721984863,
          "doc_id": "JAKO202225948452506",
          "title": "딥러닝 기법을 사용하는 소프트웨어 결함 예측 모델",
          "abstract": "수십년간 매우 많은 소프트웨어 결함 예측 모델에 관한 연구들이 수행되었으며, 그들 중 기계학습 기법을 사용한 모델들이 가장 좋은 성능을 보였다. 딥러닝 기법은 기계학습 분야에서 가장 각광받는 기술이 되었지만 결함 예측 모델의 분류기로 사용된 연구는 거의 없었다. 몇몇 연구들은 모델의 입력 소스나 구문 데이터로부터 시맨틱 정보를 얻어내는데 딥러닝을 사용하였다. 본 논문은 3개 이상의 은닉층을 갖는 MLP를 이용하여 모델 구조와 하이퍼 파라미터를 변경하여 여러 모델들을 제작하였다. 모델 평가 실험 결과 MLP 기반 딥러닝 모델들은 기존 결함 예측 모델들과 Accuracy는 비슷한 성능을 보였으나 AUC는 유의미하게 더 우수한 성능을 보였다. 또한 또다른 딥러닝 모델인 CNN 모델보다도 더 나은 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202225948452506&target=NART&cn=JAKO202225948452506",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기법을 사용하는 소프트웨어 결함 예측 모델 딥러닝 기법을 사용하는 소프트웨어 결함 예측 모델 딥러닝 기법을 사용하는 소프트웨어 결함 예측 모델 수십년간 매우 많은 소프트웨어 결함 예측 모델에 관한 연구들이 수행되었으며, 그들 중 기계학습 기법을 사용한 모델들이 가장 좋은 성능을 보였다. 딥러닝 기법은 기계학습 분야에서 가장 각광받는 기술이 되었지만 결함 예측 모델의 분류기로 사용된 연구는 거의 없었다. 몇몇 연구들은 모델의 입력 소스나 구문 데이터로부터 시맨틱 정보를 얻어내는데 딥러닝을 사용하였다. 본 논문은 3개 이상의 은닉층을 갖는 MLP를 이용하여 모델 구조와 하이퍼 파라미터를 변경하여 여러 모델들을 제작하였다. 모델 평가 실험 결과 MLP 기반 딥러닝 모델들은 기존 결함 예측 모델들과 Accuracy는 비슷한 성능을 보였으나 AUC는 유의미하게 더 우수한 성능을 보였다. 또한 또다른 딥러닝 모델인 CNN 모델보다도 더 나은 성능을 보였다."
        },
        {
          "rank": 38,
          "score": 0.679287314414978,
          "doc_id": "JAKO202313933270962",
          "title": "딥 러닝 기반 이미지 압축 기법의 성능 비교 분석",
          "abstract": "Image compression is a fundamental technique in the field of digital image processing, which will help to decrease the storage space and to transmit the files efficiently. Recently many deep learning techniques have been proposed to promise results on image compression field. Since many image compression techniques have artifact problems, this paper has compared two deep learning approaches to verify their performance experimentally to solve the problems. One of the approaches is a deep autoencoder technique, and another is a deep convolutional neural network (CNN). For those results in the performance of peak signal-to-noise and root mean square error, this paper shows that deep autoencoder method has more advantages than deep CNN approach.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202313933270962&target=NART&cn=JAKO202313933270962",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 기반 이미지 압축 기법의 성능 비교 분석 딥 러닝 기반 이미지 압축 기법의 성능 비교 분석 딥 러닝 기반 이미지 압축 기법의 성능 비교 분석 Image compression is a fundamental technique in the field of digital image processing, which will help to decrease the storage space and to transmit the files efficiently. Recently many deep learning techniques have been proposed to promise results on image compression field. Since many image compression techniques have artifact problems, this paper has compared two deep learning approaches to verify their performance experimentally to solve the problems. One of the approaches is a deep autoencoder technique, and another is a deep convolutional neural network (CNN). For those results in the performance of peak signal-to-noise and root mean square error, this paper shows that deep autoencoder method has more advantages than deep CNN approach."
        },
        {
          "rank": 39,
          "score": 0.6792206168174744,
          "doc_id": "JAKO202334662554660",
          "title": "증강형 딥러닝 기반 미세먼지 예측 시스템",
          "abstract": "딥러닝은 심층신경망(Deep Neural Network)을 구축하고 대량의 훈련 데이터를 수집한 후, 구축된 신경망을 오랫동안 학습 시켜야 한다. 만약, 학습이 제대로 진행되지 않거나 과적합이 발생하면, 학습은 실패하게 된다. 현재까지 개발되고 있는 딥러닝 도구들을 사용할 경우, 훈련데이터 수집과 학습에 많은 시간이 소요된다. 하지만, 모바일 환경의 급격한 도래와 센서 데이터의 증가로 인해, 신경망 학습에 걸리는 시간을 획기적으로 줄일 수 있는 실시간 증강형 딥러닝 기술에 대한 요구가 급격하게 증가하고 있다. 본 연구에서는 미세먼지 센서를 장착한 아두이노 시스템을 사용하여 실시간 증강형 딥러닝 시스템을 구현 하였다. 구현된 시스템에서는 미세먼지 데이터를 5초마다 측정하고 최대 120개가 축적이 되면, 기존에 축적된 데이터와 새로이 축적된 데이터를 데이터셋으로 사용하여 학습을 수행하도록 하였다. 학습 수행을 위한 신경망은 입력층 1개, 은닉층 1개, 출력등 1개로 구성하였다. 구현된 시스템에 대한 성능을 평가하기 위해 학습 시간과 평균 제곱근 오차(root mean square error, RMSE)를 측정 하였다. 실험 결과, 평균 학습 오차는 0.04053796이었으며, 학습주기당(1 에포크) 평균 학습 시간은 3,447 초 정도의 시간이 걸렸다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202334662554660&target=NART&cn=JAKO202334662554660",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "증강형 딥러닝 기반 미세먼지 예측 시스템 증강형 딥러닝 기반 미세먼지 예측 시스템 증강형 딥러닝 기반 미세먼지 예측 시스템 딥러닝은 심층신경망(Deep Neural Network)을 구축하고 대량의 훈련 데이터를 수집한 후, 구축된 신경망을 오랫동안 학습 시켜야 한다. 만약, 학습이 제대로 진행되지 않거나 과적합이 발생하면, 학습은 실패하게 된다. 현재까지 개발되고 있는 딥러닝 도구들을 사용할 경우, 훈련데이터 수집과 학습에 많은 시간이 소요된다. 하지만, 모바일 환경의 급격한 도래와 센서 데이터의 증가로 인해, 신경망 학습에 걸리는 시간을 획기적으로 줄일 수 있는 실시간 증강형 딥러닝 기술에 대한 요구가 급격하게 증가하고 있다. 본 연구에서는 미세먼지 센서를 장착한 아두이노 시스템을 사용하여 실시간 증강형 딥러닝 시스템을 구현 하였다. 구현된 시스템에서는 미세먼지 데이터를 5초마다 측정하고 최대 120개가 축적이 되면, 기존에 축적된 데이터와 새로이 축적된 데이터를 데이터셋으로 사용하여 학습을 수행하도록 하였다. 학습 수행을 위한 신경망은 입력층 1개, 은닉층 1개, 출력등 1개로 구성하였다. 구현된 시스템에 대한 성능을 평가하기 위해 학습 시간과 평균 제곱근 오차(root mean square error, RMSE)를 측정 하였다. 실험 결과, 평균 학습 오차는 0.04053796이었으며, 학습주기당(1 에포크) 평균 학습 시간은 3,447 초 정도의 시간이 걸렸다."
        },
        {
          "rank": 40,
          "score": 0.678185224533081,
          "doc_id": "JAKO202408557654430",
          "title": "회전된 객체 분류를 위한 CNN 기법들의 성능 비교 분석",
          "abstract": "이미지 공간에서 무작위로 회전된 객체에 대한 분류 성능이 우수한 기법으로는 군 등변 CNN과 steerable 필터를 이용한 CNN 등이 있다. 본 논문에서는 이들의 수학적 구조를 설명하고 구현 방법을 소개한다. 기존의 CNN을 포함한 세 개의 모델에 대하여 동일한 필터 수를 갖도록 구현한 다음, 무작위로 회전된 MNIST를 이용하여 실험하고 이들의 성능을 비교분석한다. 실험 결과에 의하면 steerable CNN은 CNN보다 6.5% 이상의 인식률 향상을 보여준다. 특히, steerable CNN은 학습할 파라미터의 수가 상대적으로 적어서 훈련 데이터셋의 크기를 줄여도 성능 열화가 비교적 크지 않음을 실험 결과로 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202408557654430&target=NART&cn=JAKO202408557654430",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회전된 객체 분류를 위한 CNN 기법들의 성능 비교 분석 회전된 객체 분류를 위한 CNN 기법들의 성능 비교 분석 회전된 객체 분류를 위한 CNN 기법들의 성능 비교 분석 이미지 공간에서 무작위로 회전된 객체에 대한 분류 성능이 우수한 기법으로는 군 등변 CNN과 steerable 필터를 이용한 CNN 등이 있다. 본 논문에서는 이들의 수학적 구조를 설명하고 구현 방법을 소개한다. 기존의 CNN을 포함한 세 개의 모델에 대하여 동일한 필터 수를 갖도록 구현한 다음, 무작위로 회전된 MNIST를 이용하여 실험하고 이들의 성능을 비교분석한다. 실험 결과에 의하면 steerable CNN은 CNN보다 6.5% 이상의 인식률 향상을 보여준다. 특히, steerable CNN은 학습할 파라미터의 수가 상대적으로 적어서 훈련 데이터셋의 크기를 줄여도 성능 열화가 비교적 크지 않음을 실험 결과로 확인한다."
        },
        {
          "rank": 41,
          "score": 0.6772293448448181,
          "doc_id": "JAKO202011161035249",
          "title": "객체 검출을 위한 CNN과 YOLO 성능 비교 실험",
          "abstract": "Object detection plays a critical role in the field of computer vision, and various researches have rapidly increased along with applying convolutional neural network and its modified structures since 2012. There are representative object detection algorithms, which are convolutional neural networks and YOLO. This paper presents two representative algorithm series, based on CNN and YOLO which solves the problem of CNN bounding box. We compare the performance of algorithm series in terms of accuracy, speed and cost. Compared with the latest advanced solution, YOLO v3 achieves a good trade-off between speed and accuracy.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202011161035249&target=NART&cn=JAKO202011161035249",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "객체 검출을 위한 CNN과 YOLO 성능 비교 실험 객체 검출을 위한 CNN과 YOLO 성능 비교 실험 객체 검출을 위한 CNN과 YOLO 성능 비교 실험 Object detection plays a critical role in the field of computer vision, and various researches have rapidly increased along with applying convolutional neural network and its modified structures since 2012. There are representative object detection algorithms, which are convolutional neural networks and YOLO. This paper presents two representative algorithm series, based on CNN and YOLO which solves the problem of CNN bounding box. We compare the performance of algorithm series in terms of accuracy, speed and cost. Compared with the latest advanced solution, YOLO v3 achieves a good trade-off between speed and accuracy."
        },
        {
          "rank": 42,
          "score": 0.6772260069847107,
          "doc_id": "JAKO202231159469541",
          "title": "고속도로 자율주행 시 보상을 최대화하기 위한 강화 학습 활성화 함수 비교",
          "abstract": "자율주행 기술은 최근 심층 강화학습의 도입으로 큰 발전을 이루고 있다. 심층 강화 학습을 효과적으로 사용하기 위해서는 적절한 활성화 함수를 선택하는 것이 중요하다. 그 동안 많은 활성화 함수가 제시되었으나 적용할 환경에 따라 다른 성능을 보여주었다. 본 논문은 고속도로에서 자율주행을 학습하기 위해 강화 학습을 사용할 때 어떤 활성화 함수를 사용하는 것이 효과적인지 12개의 활성화 함수 성능을 비교 평가한다. 이를 위한 성능 평가 방법을 제시하였고 각 활성화 함수의 평균 보상 값을 비교하였다. 그 결과 GELU를 사용할 경우 가장 높은 평균 보상을 얻을 수 있었으며 SiLU는 가장 낮은 성능을 보여주었다. 두 활성화 함수의 평균 보상 차이는 20%였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202231159469541&target=NART&cn=JAKO202231159469541",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "고속도로 자율주행 시 보상을 최대화하기 위한 강화 학습 활성화 함수 비교 고속도로 자율주행 시 보상을 최대화하기 위한 강화 학습 활성화 함수 비교 고속도로 자율주행 시 보상을 최대화하기 위한 강화 학습 활성화 함수 비교 자율주행 기술은 최근 심층 강화학습의 도입으로 큰 발전을 이루고 있다. 심층 강화 학습을 효과적으로 사용하기 위해서는 적절한 활성화 함수를 선택하는 것이 중요하다. 그 동안 많은 활성화 함수가 제시되었으나 적용할 환경에 따라 다른 성능을 보여주었다. 본 논문은 고속도로에서 자율주행을 학습하기 위해 강화 학습을 사용할 때 어떤 활성화 함수를 사용하는 것이 효과적인지 12개의 활성화 함수 성능을 비교 평가한다. 이를 위한 성능 평가 방법을 제시하였고 각 활성화 함수의 평균 보상 값을 비교하였다. 그 결과 GELU를 사용할 경우 가장 높은 평균 보상을 얻을 수 있었으며 SiLU는 가장 낮은 성능을 보여주었다. 두 활성화 함수의 평균 보상 차이는 20%였다."
        },
        {
          "rank": 43,
          "score": 0.6756912469863892,
          "doc_id": "NART104701803",
          "title": "Improved Feature Learning: A Maximum-Average-Out Deep Neural Network for the Game Go",
          "abstract": "<P>Computer game-playing programs based on deep reinforcement learning have surpassed the performance of even the best human players. However, the huge analysis space of such neural networks and their numerous parameters require extensive computing power. Hence, in this study, we aimed to increase the network learning efficiency by modifying the neural network structure, which should reduce the number of learning iterations and the required computing power. A convolutional neural network with a maximum-average-out (MAO) unit structure based on piecewise function thinking is proposed, through which features can be effectively learned and the expression ability of hidden layer features can be enhanced. To verify the performance of the MAO structure, we compared it with the ResNet18 network by applying them both to the framework of AlphaGo Zero, which was developed for playing the game Go. The two network structures were trained from scratch using a low-cost server environment. MAO unit won eight out of ten games against the ResNet18 network. The superior performance of the MAO unit compared with the ResNet18 network is significant for the further development of game algorithms that require less computing power than those currently in use.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART104701803&target=NART&cn=NART104701803",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Improved Feature Learning: A Maximum-Average-Out Deep Neural Network for the Game Go Improved Feature Learning: A Maximum-Average-Out Deep Neural Network for the Game Go Improved Feature Learning: A Maximum-Average-Out Deep Neural Network for the Game Go <P>Computer game-playing programs based on deep reinforcement learning have surpassed the performance of even the best human players. However, the huge analysis space of such neural networks and their numerous parameters require extensive computing power. Hence, in this study, we aimed to increase the network learning efficiency by modifying the neural network structure, which should reduce the number of learning iterations and the required computing power. A convolutional neural network with a maximum-average-out (MAO) unit structure based on piecewise function thinking is proposed, through which features can be effectively learned and the expression ability of hidden layer features can be enhanced. To verify the performance of the MAO structure, we compared it with the ResNet18 network by applying them both to the framework of AlphaGo Zero, which was developed for playing the game Go. The two network structures were trained from scratch using a low-cost server environment. MAO unit won eight out of ten games against the ResNet18 network. The superior performance of the MAO unit compared with the ResNet18 network is significant for the further development of game algorithms that require less computing power than those currently in use.</P>"
        },
        {
          "rank": 44,
          "score": 0.6735955476760864,
          "doc_id": "JAKO201974757494930",
          "title": "심층강화학습 라이브러리 기술동향",
          "abstract": "Reinforcement learning is a type of machine learning paradigm that forces agents to repeat the observation-action-reward process to assess and predict the values of possible future action sequences. This allows the agents to incrementally reinforce the desired behavior for a given observation. Thanks to the recent advancements of deep learning, reinforcement learning has evolved into deep reinforcement learning that introduces promising results in various control and optimization domains, such as games, robotics, autonomous vehicles, computing, industrial control, and so on. In addition to this trend, a number of programming libraries have been developed for importing deep reinforcement learning into a variety of applications. In this article, we briefly review and summarize 10 representative deep reinforcement learning libraries and compare them from a development project perspective.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201974757494930&target=NART&cn=JAKO201974757494930",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층강화학습 라이브러리 기술동향 심층강화학습 라이브러리 기술동향 심층강화학습 라이브러리 기술동향 Reinforcement learning is a type of machine learning paradigm that forces agents to repeat the observation-action-reward process to assess and predict the values of possible future action sequences. This allows the agents to incrementally reinforce the desired behavior for a given observation. Thanks to the recent advancements of deep learning, reinforcement learning has evolved into deep reinforcement learning that introduces promising results in various control and optimization domains, such as games, robotics, autonomous vehicles, computing, industrial control, and so on. In addition to this trend, a number of programming libraries have been developed for importing deep reinforcement learning into a variety of applications. In this article, we briefly review and summarize 10 representative deep reinforcement learning libraries and compare them from a development project perspective."
        },
        {
          "rank": 45,
          "score": 0.6717106103897095,
          "doc_id": "JAKO202102153821210",
          "title": "교차로에서 자율주행을 위한 심층 강화 학습 활성화 함수 비교 분석",
          "abstract": "자율주행은 자동차가 사람 없이 운전할 수 있도록 해 주며 최근 인공지능 기술의 발전에 힘입어 매우 활발히 연구되고 있다. 인공지능 기술 중에서도 특히 심층 강화 학습이 가장 효과적으로 사용되는데 이를 위해서는 적절한 활성화 함수를 이용한 신경망 구축이 필수적이다. 여태껏 많은 활성화 함수가 제시됐으나 적용 분야에 따라 서로 다른 성능을 보여주었다. 본 논문은 교차로에서 자율주행을 학습하기 위해 심층 강화 학습을 사용할 때 어떤 활성화 함수를 사용하는 것이 효과적인지 성능을 비교 평가한다. 이를 위해 평가에서 사용할 성능 메트릭을 정의하고 각 활성화 함수에 따른 메트릭의 값을 그래프로 비교하였다. 그 결과 Mish를 사용할 경우 보상이 다른 활성화 함수보다 평균적으로 높은 것을 알 수 있었고 보상이 가장 낮은 활성화 함수와의 차이는 9.8%였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202102153821210&target=NART&cn=JAKO202102153821210",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "교차로에서 자율주행을 위한 심층 강화 학습 활성화 함수 비교 분석 교차로에서 자율주행을 위한 심층 강화 학습 활성화 함수 비교 분석 교차로에서 자율주행을 위한 심층 강화 학습 활성화 함수 비교 분석 자율주행은 자동차가 사람 없이 운전할 수 있도록 해 주며 최근 인공지능 기술의 발전에 힘입어 매우 활발히 연구되고 있다. 인공지능 기술 중에서도 특히 심층 강화 학습이 가장 효과적으로 사용되는데 이를 위해서는 적절한 활성화 함수를 이용한 신경망 구축이 필수적이다. 여태껏 많은 활성화 함수가 제시됐으나 적용 분야에 따라 서로 다른 성능을 보여주었다. 본 논문은 교차로에서 자율주행을 학습하기 위해 심층 강화 학습을 사용할 때 어떤 활성화 함수를 사용하는 것이 효과적인지 성능을 비교 평가한다. 이를 위해 평가에서 사용할 성능 메트릭을 정의하고 각 활성화 함수에 따른 메트릭의 값을 그래프로 비교하였다. 그 결과 Mish를 사용할 경우 보상이 다른 활성화 함수보다 평균적으로 높은 것을 알 수 있었고 보상이 가장 낮은 활성화 함수와의 차이는 9.8%였다."
        },
        {
          "rank": 46,
          "score": 0.6716818809509277,
          "doc_id": "JAKO201905653788881",
          "title": "실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안",
          "abstract": "오늘날의 기술 환경에서 대다수의 빅 데이터 기반 애플리케이션 및 솔루션은 스트리밍 데이터의 실시간 처리를 기반으로 한다. 빅 데이터 스트림의 실시간 처리 및 분석은 빅 데이터 기반 애플리케이션 및 솔루션 개발에서 중요한 역할을 한다. 특히 해사 분야 데이터 처리 환경에서도 데이터의 폭발적 증대에 따른 대용량 실시간 데이터를 빠르게 처리 및 분석할 수 있는 기술 개발의 필요성이 가속화되고 있다. 따라서 본 논문에서는 다양한 빅 데이터 처리를 위한 오픈소스 기술 중에 적합한 오픈소스로 NiFi, Kafka, Druid의 특징을 분석하여 한국형 e-Navigation 서비스에서 해사 분야 서비스 분석에 필요한 외부 연계 필요 정보들을 상시 최신 정보로 제공할 수 있도록 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 기술 적용의 기초를 마련하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201905653788881&target=NART&cn=JAKO201905653788881",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 적용 방안 오늘날의 기술 환경에서 대다수의 빅 데이터 기반 애플리케이션 및 솔루션은 스트리밍 데이터의 실시간 처리를 기반으로 한다. 빅 데이터 스트림의 실시간 처리 및 분석은 빅 데이터 기반 애플리케이션 및 솔루션 개발에서 중요한 역할을 한다. 특히 해사 분야 데이터 처리 환경에서도 데이터의 폭발적 증대에 따른 대용량 실시간 데이터를 빠르게 처리 및 분석할 수 있는 기술 개발의 필요성이 가속화되고 있다. 따라서 본 논문에서는 다양한 빅 데이터 처리를 위한 오픈소스 기술 중에 적합한 오픈소스로 NiFi, Kafka, Druid의 특징을 분석하여 한국형 e-Navigation 서비스에서 해사 분야 서비스 분석에 필요한 외부 연계 필요 정보들을 상시 최신 정보로 제공할 수 있도록 실시간 데이터 처리를 위한 개방형 데이터 프레임워크 기술 적용의 기초를 마련하고자 한다."
        },
        {
          "rank": 47,
          "score": 0.6702706813812256,
          "doc_id": "ART003222390",
          "title": "Comparison of CNN-based deep learning architectures for unsteady CFD acceleration on small datasets",
          "abstract": "CFD acceleration for virtual nuclear reactors or digital twin technology is a primary goal in the nuclear industry.This study compares advanced convolutional neural network (CNN) architectures for accelerating unsteady computational fluid dynamics (CFD) simulations using small datasets based on a challenging natural convection flow dataset. The advanced architectures such as autoencoders, UNet, and ConvLSTM-UNet, were evaluated under identical conditions to determine their predictive accuracy and robustness in autoregressive time-series predictions. ConvLSTM-UNet consistently outperformed other models, particularly in difference value calculation, achieving lower maximum errors and stable residuals. However, error accumulation remains a challenge, limiting reliable predictions to approximately 10 timesteps. This highlights the need for enhanced strategies to improve long-term prediction stability. The novelty of this work lies in its fair comparison of state-of-the-art CNN models within the RePIT framework, demonstrating their potential for accelerating CFD simulations while identifying limitations under small data conditions. Future research will focus on exploring alternative models, such as graph neural networks and implicit neural representations. These efforts aim to develop a robust hybrid approach for long-term unsteady CFD acceleration, contributing to practical applications in virtual nuclear reactors.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART003222390&target=NART&cn=ART003222390",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Comparison of CNN-based deep learning architectures for unsteady CFD acceleration on small datasets Comparison of CNN-based deep learning architectures for unsteady CFD acceleration on small datasets Comparison of CNN-based deep learning architectures for unsteady CFD acceleration on small datasets CFD acceleration for virtual nuclear reactors or digital twin technology is a primary goal in the nuclear industry.This study compares advanced convolutional neural network (CNN) architectures for accelerating unsteady computational fluid dynamics (CFD) simulations using small datasets based on a challenging natural convection flow dataset. The advanced architectures such as autoencoders, UNet, and ConvLSTM-UNet, were evaluated under identical conditions to determine their predictive accuracy and robustness in autoregressive time-series predictions. ConvLSTM-UNet consistently outperformed other models, particularly in difference value calculation, achieving lower maximum errors and stable residuals. However, error accumulation remains a challenge, limiting reliable predictions to approximately 10 timesteps. This highlights the need for enhanced strategies to improve long-term prediction stability. The novelty of this work lies in its fair comparison of state-of-the-art CNN models within the RePIT framework, demonstrating their potential for accelerating CFD simulations while identifying limitations under small data conditions. Future research will focus on exploring alternative models, such as graph neural networks and implicit neural representations. These efforts aim to develop a robust hybrid approach for long-term unsteady CFD acceleration, contributing to practical applications in virtual nuclear reactors."
        },
        {
          "rank": 48,
          "score": 0.669703483581543,
          "doc_id": "JAKO202201856714620",
          "title": "자율주행 자동차의 주차를 위한 강화학습 활성화 함수 비교 분석",
          "abstract": "주차 공간의 부족함을 획기적으로 해결할 수 있는 자율주행 자동차는 심층 강화 학습을 통해 큰 발전을 이루고 있다. 심층 강화 학습에는 활성화 함수가 사용되는데, 그동안 다양한 활성화 함수가 제안되어 왔으나 적용 환경에 따라 그 성능 편차가 심했다. 따라서 환경에 따라 최적의 활성화 함수를 찾는 것이 효과적인 학습을 위해 중요하다. 본 논문은 자율주행 자동차가 주차를 학습하기 위해 심층 강화 학습을 사용할 때 어떤 활성화 함수를 사용하는 것이 가장 효과적인지 비교 평가하기 위해 강화 학습에 주로 사용되는 12개의 함수를 분석하였다. 이를 위해 성능 평가 환경을 구축하고 각 활성화 함수의 평균 보상을 성공률, 에피소드 길이, 자동차 속도와 비교하였다. 그 결과 가장 높은 보상은 GELU를 사용한 경우였고, ELU는 가장 낮았다. 두 활성화 함수의 보상 차이는 35.2%였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202201856714620&target=NART&cn=JAKO202201856714620",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "자율주행 자동차의 주차를 위한 강화학습 활성화 함수 비교 분석 자율주행 자동차의 주차를 위한 강화학습 활성화 함수 비교 분석 자율주행 자동차의 주차를 위한 강화학습 활성화 함수 비교 분석 주차 공간의 부족함을 획기적으로 해결할 수 있는 자율주행 자동차는 심층 강화 학습을 통해 큰 발전을 이루고 있다. 심층 강화 학습에는 활성화 함수가 사용되는데, 그동안 다양한 활성화 함수가 제안되어 왔으나 적용 환경에 따라 그 성능 편차가 심했다. 따라서 환경에 따라 최적의 활성화 함수를 찾는 것이 효과적인 학습을 위해 중요하다. 본 논문은 자율주행 자동차가 주차를 학습하기 위해 심층 강화 학습을 사용할 때 어떤 활성화 함수를 사용하는 것이 가장 효과적인지 비교 평가하기 위해 강화 학습에 주로 사용되는 12개의 함수를 분석하였다. 이를 위해 성능 평가 환경을 구축하고 각 활성화 함수의 평균 보상을 성공률, 에피소드 길이, 자동차 속도와 비교하였다. 그 결과 가장 높은 보상은 GELU를 사용한 경우였고, ELU는 가장 낮았다. 두 활성화 함수의 보상 차이는 35.2%였다."
        },
        {
          "rank": 49,
          "score": 0.6685903072357178,
          "doc_id": "JAKO201912758458868",
          "title": "딥러닝 개념을 위한 인공지능 교육 프로그램",
          "abstract": "본 연구는 초등학생의 딥러닝 개념 학습을 위한 교육 프로그램을 개발하는 것이다. 교육 프로그램의 모델은 CT요소 중심 모델을 토대로 딥러닝 교수학습모델을 개발하였다. 개발한 프로그램의 주제는 인공지능의 이미지 인식 CNN알고리즘으로 정하고, 9개 차시 교육프로그램을 개발하였다. 프로그램은 6학년을 대상으로 2주간에 걸쳐 적용을 하였다. 프로그램에 대한 학습 적합도 검사는 전문가 타당도 분석 결과로 CVR이 타당하게 나왔다. 학습자 수준 적합도와 교사 지도 수준의 적합도 문항의 경우 .80이하로 나타났으며 .96이 넘은 학습 환경과 매체의 적합도 문항에서는 높게 나타났다. 학생들의 만족도 분석 결과 학습의 이해도와 유익성, 흥미도, 학습자료 등에 대해서 평균 4.0이상을 보여 긍정적인 평가를 하여 본 연구의 가치를 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201912758458868&target=NART&cn=JAKO201912758458868",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 개념을 위한 인공지능 교육 프로그램 딥러닝 개념을 위한 인공지능 교육 프로그램 딥러닝 개념을 위한 인공지능 교육 프로그램 본 연구는 초등학생의 딥러닝 개념 학습을 위한 교육 프로그램을 개발하는 것이다. 교육 프로그램의 모델은 CT요소 중심 모델을 토대로 딥러닝 교수학습모델을 개발하였다. 개발한 프로그램의 주제는 인공지능의 이미지 인식 CNN알고리즘으로 정하고, 9개 차시 교육프로그램을 개발하였다. 프로그램은 6학년을 대상으로 2주간에 걸쳐 적용을 하였다. 프로그램에 대한 학습 적합도 검사는 전문가 타당도 분석 결과로 CVR이 타당하게 나왔다. 학습자 수준 적합도와 교사 지도 수준의 적합도 문항의 경우 .80이하로 나타났으며 .96이 넘은 학습 환경과 매체의 적합도 문항에서는 높게 나타났다. 학생들의 만족도 분석 결과 학습의 이해도와 유익성, 흥미도, 학습자료 등에 대해서 평균 4.0이상을 보여 긍정적인 평가를 하여 본 연구의 가치를 확인할 수 있었다."
        },
        {
          "rank": 50,
          "score": 0.6678239107131958,
          "doc_id": "JAKO202201253148351",
          "title": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰",
          "abstract": "위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202201253148351&target=NART&cn=JAKO202201253148351",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다."
        }
      ]
    },
    {
      "query": "이러한 성능 비교 결과의 의의는 무엇인가요?",
      "query_meta": {
        "type": "single_hop",
        "index": 2
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.6868300437927246,
          "doc_id": "JAKO201718054814596",
          "title": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석",
          "abstract": "딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201718054814596&target=NART&cn=JAKO201718054814596",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다."
        },
        {
          "rank": 2,
          "score": 0.6750084757804871,
          "doc_id": "JAKO201129362563090",
          "title": "스타 스키마 조인 처리에 대한 세로-지향 데이터베이스 시스템과 가로-지향 데이터베이스 시스템의 성능 비교",
          "abstract": "세로-지향 데이터베이스 시스템은 기존의 가로-지향 데이터베이스 시스템과 달리 데이터를 가로(row) 위주가 아닌 세로(column) 위주로 저장한다. 최근에는 데이터 웨어하우스나 의사 결정 시스템 같은 대용량 데이터를 갖는 읽기 위주의 응용들에서 세로-지향데이터베이스의 우수성이 관찰되었다. 본 논문에서는 세로-지향데이터베이스에서의 조인 전략을 구체적으로 분석하고 데이터 웨어하우스 시스템에서 세로-지향 데이터베이스의 우수성을 검증하고자 한다. 두 시스템간의 객관적인 비교를 위해 데이터 웨어하우스 분석 모델인 스타 스키마 벤치마크를 통해 스타스키마조인 질의에 대한 성능분석을 실시하고자 한다. 또한 세로-지향 데이터베이스의 조인 전략으로 조기 실체화(early materialization)와 지연 실체화(late materialization)를 고려하였다. 성능 분석을 통해 스타 스키마 조인 질의처리에 있어 가로-지향 시스템보다는 세로-지향 시스템에서 디스크 I/O 비용이 더 효율적인 결과를 확인할 수 있었다. 세로-지향 데이터베이스 시스템 측면에서는 조기 실체화보다는 지연 실체화 조인전략이 훨씬 우수한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201129362563090&target=NART&cn=JAKO201129362563090",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스타 스키마 조인 처리에 대한 세로-지향 데이터베이스 시스템과 가로-지향 데이터베이스 시스템의 성능 비교 스타 스키마 조인 처리에 대한 세로-지향 데이터베이스 시스템과 가로-지향 데이터베이스 시스템의 성능 비교 스타 스키마 조인 처리에 대한 세로-지향 데이터베이스 시스템과 가로-지향 데이터베이스 시스템의 성능 비교 세로-지향 데이터베이스 시스템은 기존의 가로-지향 데이터베이스 시스템과 달리 데이터를 가로(row) 위주가 아닌 세로(column) 위주로 저장한다. 최근에는 데이터 웨어하우스나 의사 결정 시스템 같은 대용량 데이터를 갖는 읽기 위주의 응용들에서 세로-지향데이터베이스의 우수성이 관찰되었다. 본 논문에서는 세로-지향데이터베이스에서의 조인 전략을 구체적으로 분석하고 데이터 웨어하우스 시스템에서 세로-지향 데이터베이스의 우수성을 검증하고자 한다. 두 시스템간의 객관적인 비교를 위해 데이터 웨어하우스 분석 모델인 스타 스키마 벤치마크를 통해 스타스키마조인 질의에 대한 성능분석을 실시하고자 한다. 또한 세로-지향 데이터베이스의 조인 전략으로 조기 실체화(early materialization)와 지연 실체화(late materialization)를 고려하였다. 성능 분석을 통해 스타 스키마 조인 질의처리에 있어 가로-지향 시스템보다는 세로-지향 시스템에서 디스크 I/O 비용이 더 효율적인 결과를 확인할 수 있었다. 세로-지향 데이터베이스 시스템 측면에서는 조기 실체화보다는 지연 실체화 조인전략이 훨씬 우수한 성능을 보였다."
        },
        {
          "rank": 3,
          "score": 0.6708970069885254,
          "doc_id": "JAKO202322957802897",
          "title": "R과 텐서플로우 딥러닝 성능 비교",
          "abstract": "본 연구에서는 무료 딥러닝 도구인 R과 텐서플로우에 대한 성능 비교를 수행하였다. 실험에서는 각 도구를 사용하여 6종류의 심층 신경망을 구축하고 10년간의 한국 온도 데이터셋을 사용하여 신경망을 학습시켰다. 구축된 신경망의 입력층 노드 갯수는 10개, 출력층은 5개로 설정 하였으며, 은닉층은 5, 10, 20개로 설정하여 실험을 진행 하였다. 학습 데이터는 2013년 3월 1일부터 2023년 3월 29일까지 서울시 강남구에서 수집된 온도 데이터 3681건을 사용하였다. 성능 비교를 위해, 학습된 신경망을 사용하여, 5일간의 온도를 예측하고 예측된 값과 실제값을 사용하여 평균 제곱근 오차(root mean square error, RMSE)값을 측정하였다. 실험결과, 은닉층이 1개인 경우, R의 학습 오차는 0.04731176이었으며, 텐서플로우는 0.06677193으로 측정되었으며, 은닉층이 2개인 경우에는 R이 0.04782134, 텐서플로 우는 0.05799060로 측정되었다. 전체적으로 R이 더 우수한 성능을 보였다. 우리는 기계학습을 처음 접하는 사용자들에게 두 도구에 대한 정량적 성능 정보를 제공함으로써, 도구 선택에서 발생하는 어려움을 해소하고자 하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202322957802897&target=NART&cn=JAKO202322957802897",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "R과 텐서플로우 딥러닝 성능 비교 R과 텐서플로우 딥러닝 성능 비교 R과 텐서플로우 딥러닝 성능 비교 본 연구에서는 무료 딥러닝 도구인 R과 텐서플로우에 대한 성능 비교를 수행하였다. 실험에서는 각 도구를 사용하여 6종류의 심층 신경망을 구축하고 10년간의 한국 온도 데이터셋을 사용하여 신경망을 학습시켰다. 구축된 신경망의 입력층 노드 갯수는 10개, 출력층은 5개로 설정 하였으며, 은닉층은 5, 10, 20개로 설정하여 실험을 진행 하였다. 학습 데이터는 2013년 3월 1일부터 2023년 3월 29일까지 서울시 강남구에서 수집된 온도 데이터 3681건을 사용하였다. 성능 비교를 위해, 학습된 신경망을 사용하여, 5일간의 온도를 예측하고 예측된 값과 실제값을 사용하여 평균 제곱근 오차(root mean square error, RMSE)값을 측정하였다. 실험결과, 은닉층이 1개인 경우, R의 학습 오차는 0.04731176이었으며, 텐서플로우는 0.06677193으로 측정되었으며, 은닉층이 2개인 경우에는 R이 0.04782134, 텐서플로 우는 0.05799060로 측정되었다. 전체적으로 R이 더 우수한 성능을 보였다. 우리는 기계학습을 처음 접하는 사용자들에게 두 도구에 대한 정량적 성능 정보를 제공함으로써, 도구 선택에서 발생하는 어려움을 해소하고자 하였다."
        },
        {
          "rank": 4,
          "score": 0.6652636528015137,
          "doc_id": "DIKO0015889140",
          "title": "딥 러닝 프레임워크 성능 비교 및 개선 방안",
          "abstract": "현 시대는 4차 산업혁명이 대두되는 시대로 요소 기술들 중 인공지능의 중 요성은 아무리 강조하더라도 지나치지 않으며, 기업들 경쟁력의 척도라고 불 릴만큼 모든 산업에서 활용되고있다. 2016년 경 DeepMind 의 AlphaGo 와 이 세돌 선수의 경기로 국내에서는 처음으로 인공지능의 위력과 Deep Learning 이라는 단어가 대중들에게 알려지게 되었다.&amp;#xD; 특정 IT 산업이 발전하게 되면 해당 분야의 개발자들의 생산성과 접근성을 높이기 위해 Framework 들이 등장, 발전하게 된다. 통신기술과 스마트폰의 출현으로 WEB 붐이 이르렀을 때, Server-side 에서는 Spring, django, Ruby on Rails 등이 출현하였고, Client-side 에서는 Angular, React, jQuery 와 같이 다양한 Framework 들이 등장 발전하였다. 컴퓨터 성능의 발전과 다양 한 컴퓨팅 기술의 발전으로 현 시대는 인공지능 3차 붐으로 Machine Learning 과 Deep Learning 의 시대로 불리고있다.&amp;#xD; 이와 같이 Deep Learning 분야에서도 다양한 Framework 들이 개발되었다. 이런 다양한 Framework 제품들의 목적은 개발자들의 생산성을 향상시키기 위 해 내부 알고리즘이나 메커니즘을 Black Box 형식으로 감추고 High Level API 를 제공하기 때문에, 내부적인 구현 방식은 Framework 별로 다르다. 본 논문에서는 현 시대에 가장 많이 사용하는 대표적인 Framework 들을 선정한 다. 그리고 선정된 Framework 들을 이용하여 Convolutional Neural Network 알고리즘을 구현, 동일한 Training Data 를 이용하여 학습 Model 을 만들어 낸다. 그리고 동일한 Cloud 환경에서 각 Framework 별 학습을 수행하여 성 능을 비교한다. 성능 비교 환경은 총 3가지로 CPU, GPU 1 Core, Multi GPU Core 환경에서 각 Framework 별 성능 지표를 추출한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015889140&target=NART&cn=DIKO0015889140",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 프레임워크 성능 비교 및 개선 방안 딥 러닝 프레임워크 성능 비교 및 개선 방안 딥 러닝 프레임워크 성능 비교 및 개선 방안 현 시대는 4차 산업혁명이 대두되는 시대로 요소 기술들 중 인공지능의 중 요성은 아무리 강조하더라도 지나치지 않으며, 기업들 경쟁력의 척도라고 불 릴만큼 모든 산업에서 활용되고있다. 2016년 경 DeepMind 의 AlphaGo 와 이 세돌 선수의 경기로 국내에서는 처음으로 인공지능의 위력과 Deep Learning 이라는 단어가 대중들에게 알려지게 되었다.&amp;#xD; 특정 IT 산업이 발전하게 되면 해당 분야의 개발자들의 생산성과 접근성을 높이기 위해 Framework 들이 등장, 발전하게 된다. 통신기술과 스마트폰의 출현으로 WEB 붐이 이르렀을 때, Server-side 에서는 Spring, django, Ruby on Rails 등이 출현하였고, Client-side 에서는 Angular, React, jQuery 와 같이 다양한 Framework 들이 등장 발전하였다. 컴퓨터 성능의 발전과 다양 한 컴퓨팅 기술의 발전으로 현 시대는 인공지능 3차 붐으로 Machine Learning 과 Deep Learning 의 시대로 불리고있다.&amp;#xD; 이와 같이 Deep Learning 분야에서도 다양한 Framework 들이 개발되었다. 이런 다양한 Framework 제품들의 목적은 개발자들의 생산성을 향상시키기 위 해 내부 알고리즘이나 메커니즘을 Black Box 형식으로 감추고 High Level API 를 제공하기 때문에, 내부적인 구현 방식은 Framework 별로 다르다. 본 논문에서는 현 시대에 가장 많이 사용하는 대표적인 Framework 들을 선정한 다. 그리고 선정된 Framework 들을 이용하여 Convolutional Neural Network 알고리즘을 구현, 동일한 Training Data 를 이용하여 학습 Model 을 만들어 낸다. 그리고 동일한 Cloud 환경에서 각 Framework 별 학습을 수행하여 성 능을 비교한다. 성능 비교 환경은 총 3가지로 CPU, GPU 1 Core, Multi GPU Core 환경에서 각 Framework 별 성능 지표를 추출한다."
        },
        {
          "rank": 5,
          "score": 0.6591448783874512,
          "doc_id": "NART110448816",
          "title": "PERFORMANCE ANALYSIS OF OPEN SOURCE STORAGE CLOUDS IN CLOUD COMPUTING",
          "abstract": "<P>Cloud computing is one of the latest research area that helps in storing the information permanently on the servers and manages the different resources for the requested users to provide on-demand services. In order to create the more usable and economic value based cloud computing, the principles, goals and structure of the cloud engineering is of vital importance. The objective of this study is to analyze the CPU and memory performance of different open source clouds. We will use different open source cloud to measure the different performance metrics like CPU time for downloading and uploading of file, memory usage while downloading and uploading the file, standard deviation of CPU usage and standard deviation of memory usage.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART110448816&target=NART&cn=NART110448816",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PERFORMANCE ANALYSIS OF OPEN SOURCE STORAGE CLOUDS IN CLOUD COMPUTING PERFORMANCE ANALYSIS OF OPEN SOURCE STORAGE CLOUDS IN CLOUD COMPUTING PERFORMANCE ANALYSIS OF OPEN SOURCE STORAGE CLOUDS IN CLOUD COMPUTING <P>Cloud computing is one of the latest research area that helps in storing the information permanently on the servers and manages the different resources for the requested users to provide on-demand services. In order to create the more usable and economic value based cloud computing, the principles, goals and structure of the cloud engineering is of vital importance. The objective of this study is to analyze the CPU and memory performance of different open source clouds. We will use different open source cloud to measure the different performance metrics like CPU time for downloading and uploading of file, memory usage while downloading and uploading the file, standard deviation of CPU usage and standard deviation of memory usage.</P>"
        },
        {
          "rank": 6,
          "score": 0.656951904296875,
          "doc_id": "JAKO199215875841266",
          "title": "음성 인식 신경망을 위한 음성 파라키터들의 성능 비교",
          "abstract": "음성 인식에 신경망 모델을 적용하는 많은 연구들이 있었지만, 주된 관심은 음성인식에 적합한 구조와 학습 방법이었다.  그러나 음성인식에 신경망 모델을 적용한 시스템의 효율 향상은 모델 자체의 구조뿐 아니라, 신경망 모델의 입력으로 어떤 음성 파라미터를 사용하는가에 따라서도 큰 영향을 받는다.  본 논문은 기존 음성인식에 신경망 모델을 적용한 많은 연구들에서 사용한 음성 파라미터를 살펴보고, 대표적인 음성 파라미터 6개를 선정하여, 같은 데이타와 같은 신경망 모델 하에서 어떻게 성능이 달라지는지를 분석한다.  인식 실험에 있어서는 한국어 파열음 9개에 대한 8개 데이터 집합과 모음 8개에 대한 18개 데이터 집합을 음성 파라미터로 하고 신경망 모델은 순환 신경망 모델을 사용하여 노드의 수를 일정하게 한뒤 다양한 입력 파라미터의 성능을 비교하였다.  그 결과 선형 예측 계수로부터 얻어진 delta cepstrum의 음성 파라미터가 가장 좋은 성능을 보였으며 이때 인식률은 같은 학습 데이터에 대해 파열음 100.0%, 모음 95.1%이었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199215875841266&target=NART&cn=JAKO199215875841266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식에 신경망 모델을 적용하는 많은 연구들이 있었지만, 주된 관심은 음성인식에 적합한 구조와 학습 방법이었다.  그러나 음성인식에 신경망 모델을 적용한 시스템의 효율 향상은 모델 자체의 구조뿐 아니라, 신경망 모델의 입력으로 어떤 음성 파라미터를 사용하는가에 따라서도 큰 영향을 받는다.  본 논문은 기존 음성인식에 신경망 모델을 적용한 많은 연구들에서 사용한 음성 파라미터를 살펴보고, 대표적인 음성 파라미터 6개를 선정하여, 같은 데이타와 같은 신경망 모델 하에서 어떻게 성능이 달라지는지를 분석한다.  인식 실험에 있어서는 한국어 파열음 9개에 대한 8개 데이터 집합과 모음 8개에 대한 18개 데이터 집합을 음성 파라미터로 하고 신경망 모델은 순환 신경망 모델을 사용하여 노드의 수를 일정하게 한뒤 다양한 입력 파라미터의 성능을 비교하였다.  그 결과 선형 예측 계수로부터 얻어진 delta cepstrum의 음성 파라미터가 가장 좋은 성능을 보였으며 이때 인식률은 같은 학습 데이터에 대해 파열음 100.0%, 모음 95.1%이었다."
        },
        {
          "rank": 7,
          "score": 0.6500295400619507,
          "doc_id": "DIKO0008948393",
          "title": "소프트웨어 컴포넌트 프레임워크 성능 비교",
          "abstract": "프레임워크는 포함하는 컴포넌트의 영역적 특성에 따라 시스템의 하부 구조를 중심으로 정의된 시스템 프레임워크와 특정 응용 영역을 위한 응용 애플리케이션 프레임워크로 나뉠 수 있다. 본 논문에서는 각각의 프레임워크 사용 제품에 대해 비교 분석하였다. 시스템 프레임워크는 시스템의 기본 성능인 '객체모델', '객체 버스', '언어의 독립성', '위치 투명성'등을 기준으로 비교 하였다. CORBA는 컴포넌트 간 상호 운용성이 뛰어나며, 다양한 서비스와 응용 컴포넌트를 지원한다. 이에 비하여 DCOM은 윈도우 사용자를 기반으로 하고 있으며 MTS를 기반으로 하는 분산 트랜젝션 기능을 지원한다. EJB는 구현언어의 독립성을 갖는 CORBA와 DCOM과 달리 자바라는 단일 언어를 기반으로 한다. 이로 인해 언어 독립적이지는 않지만 컨테이너 차원에서 데이터베이스, 트랜잭션, 보안문제, 데이터베이스의 커넥션 플링, 쓰레딩과 같은 기능을 제공한다 특정 비즈니스 영역을 위한 Steelpia, SanFrancisco의 경우에는 각 프레임워크의 재사용 컴포넌트 레이어에 대한 비교를 하였다 Steelpia는 철강 업무를 지원하며, 핵심 비즈니스 컴포넌트를 크기에 맞게 특화하여 기존의 개발 방식보다 30-50% 정도의 기간비용 효과를 얻을 수 있다. SanFrancisco 는 비즈니스 컴포넌트를 지원하여 특화함으로 애플리케이션 개발시 전체 노력의 40%정도의 시간적, 비용적 이익을 가져다 준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0008948393&target=NART&cn=DIKO0008948393",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "소프트웨어 컴포넌트 프레임워크 성능 비교 소프트웨어 컴포넌트 프레임워크 성능 비교 소프트웨어 컴포넌트 프레임워크 성능 비교 프레임워크는 포함하는 컴포넌트의 영역적 특성에 따라 시스템의 하부 구조를 중심으로 정의된 시스템 프레임워크와 특정 응용 영역을 위한 응용 애플리케이션 프레임워크로 나뉠 수 있다. 본 논문에서는 각각의 프레임워크 사용 제품에 대해 비교 분석하였다. 시스템 프레임워크는 시스템의 기본 성능인 '객체모델', '객체 버스', '언어의 독립성', '위치 투명성'등을 기준으로 비교 하였다. CORBA는 컴포넌트 간 상호 운용성이 뛰어나며, 다양한 서비스와 응용 컴포넌트를 지원한다. 이에 비하여 DCOM은 윈도우 사용자를 기반으로 하고 있으며 MTS를 기반으로 하는 분산 트랜젝션 기능을 지원한다. EJB는 구현언어의 독립성을 갖는 CORBA와 DCOM과 달리 자바라는 단일 언어를 기반으로 한다. 이로 인해 언어 독립적이지는 않지만 컨테이너 차원에서 데이터베이스, 트랜잭션, 보안문제, 데이터베이스의 커넥션 플링, 쓰레딩과 같은 기능을 제공한다 특정 비즈니스 영역을 위한 Steelpia, SanFrancisco의 경우에는 각 프레임워크의 재사용 컴포넌트 레이어에 대한 비교를 하였다 Steelpia는 철강 업무를 지원하며, 핵심 비즈니스 컴포넌트를 크기에 맞게 특화하여 기존의 개발 방식보다 30-50% 정도의 기간비용 효과를 얻을 수 있다. SanFrancisco 는 비즈니스 컴포넌트를 지원하여 특화함으로 애플리케이션 개발시 전체 노력의 40%정도의 시간적, 비용적 이익을 가져다 준다."
        },
        {
          "rank": 8,
          "score": 0.6485028266906738,
          "doc_id": "JAKO199911921528980",
          "title": "다층회귀예측신경망의 음성인식성능에 관한 연구",
          "abstract": "4층구조의 다층퍼셉트론을 변형하여 3 종류의 다층회귀예측신경망을 구성하고, 예측차수, 두 은닉층의 뉴런개수, 연결세기의 초기치 및 전달함수 변화에 따른 각 망의 음성인식성능을 실험을 통해 각각 비교 분석한다. 실험결과에 의하면, 다층회귀신경망이 다층퍼셉트론에 비해 음성인식성능이 우수하다. 그리고 구조적으로는 상위은닉층의 출력을 하위은닉층으로 회귀할 때 인식성능이 가장 우수하며, 각 망 공히 상, 하위은닉층의 뉴런 10 혹은 15개, 예측차수 3 혹은 4차일 때 인식률이 양호하다. 학습시 연결세기의 초기치를 -0.5에서 0.5사이로 설정하고, 하위은닉층에서 단극성 시그모이드 전달함수를 사용할 때 인식성능이 더욱 향상된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199911921528980&target=NART&cn=JAKO199911921528980",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀예측신경망의 음성인식성능에 관한 연구 다층회귀예측신경망의 음성인식성능에 관한 연구 다층회귀예측신경망의 음성인식성능에 관한 연구 4층구조의 다층퍼셉트론을 변형하여 3 종류의 다층회귀예측신경망을 구성하고, 예측차수, 두 은닉층의 뉴런개수, 연결세기의 초기치 및 전달함수 변화에 따른 각 망의 음성인식성능을 실험을 통해 각각 비교 분석한다. 실험결과에 의하면, 다층회귀신경망이 다층퍼셉트론에 비해 음성인식성능이 우수하다. 그리고 구조적으로는 상위은닉층의 출력을 하위은닉층으로 회귀할 때 인식성능이 가장 우수하며, 각 망 공히 상, 하위은닉층의 뉴런 10 혹은 15개, 예측차수 3 혹은 4차일 때 인식률이 양호하다. 학습시 연결세기의 초기치를 -0.5에서 0.5사이로 설정하고, 하위은닉층에서 단극성 시그모이드 전달함수를 사용할 때 인식성능이 더욱 향상된다."
        },
        {
          "rank": 9,
          "score": 0.6458823680877686,
          "doc_id": "JAKO201723840540692",
          "title": "빅데이터 통합모형 비교분석",
          "abstract": "빅데이터가 4차 산업혁명의 핵심으로 자리하면서 빅데이터 기반 처리 및 분석 능력이 기업의 미래 경쟁력을 좌우할 전망이다. 빅데이터 처리 및 분석을 위한 RHadoop과 RHIPE 모형은 R과 Hadoop의 통합모형으로 지금까지 각각의 모형에 대해서는 연구가 많이 진행되어 왔으나 두 모형간 비교 연구는 거의 이루어 지지 않았다. 본 논문에서는 대용량의 실제 데이터와 모의실험 데이터에서 다중 회귀 (multiple regression)와 로지스틱 회귀 (logistic regression) 추정을 위한 머신러닝 (machine learning) 알고리즘을 MapReduce 프로그램 구현을 통해 RHadoop과 RHIPE 간의 비교 분석하고자 한다. 구축된 분산 클러스터 (distributed cluster) 하에서 두 모형간 성능 실험 결과, RHIPE은 RHadoop에 비해 대체로 빠른 처리속도를 보인 반면에 설치, 사용면에서 어려움을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201723840540692&target=NART&cn=JAKO201723840540692",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "빅데이터 통합모형 비교분석 빅데이터 통합모형 비교분석 빅데이터 통합모형 비교분석 빅데이터가 4차 산업혁명의 핵심으로 자리하면서 빅데이터 기반 처리 및 분석 능력이 기업의 미래 경쟁력을 좌우할 전망이다. 빅데이터 처리 및 분석을 위한 RHadoop과 RHIPE 모형은 R과 Hadoop의 통합모형으로 지금까지 각각의 모형에 대해서는 연구가 많이 진행되어 왔으나 두 모형간 비교 연구는 거의 이루어 지지 않았다. 본 논문에서는 대용량의 실제 데이터와 모의실험 데이터에서 다중 회귀 (multiple regression)와 로지스틱 회귀 (logistic regression) 추정을 위한 머신러닝 (machine learning) 알고리즘을 MapReduce 프로그램 구현을 통해 RHadoop과 RHIPE 간의 비교 분석하고자 한다. 구축된 분산 클러스터 (distributed cluster) 하에서 두 모형간 성능 실험 결과, RHIPE은 RHadoop에 비해 대체로 빠른 처리속도를 보인 반면에 설치, 사용면에서 어려움을 보였다."
        },
        {
          "rank": 10,
          "score": 0.6437795162200928,
          "doc_id": "DIKO0014373092",
          "title": "Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서",
          "abstract": "통신 기술의 발달로 인한 사람과 사람, 장치와 장치, 사람과 장치 간의 연결성의 증가와 저장 매체 기술의 발달, 그리고 데이터 저장 비용의 감소로 인해 데이터의 양이 폭발적으로 증가했다. 이에 따라 다양한 형태의 대규모의 데이터를 빠른 시간 내에 효율적으로 처리할 수 있는 Cloud Computing 기술이 주목 받고 있고, Cloud Computing을 위한 오픈소스 기반의 솔루션 또한 많이 나타나게 되었다. &amp;#xD; 2012년부터 주목받기 시작한 Deep Learning은 전 세계적으로 가장 많은 관심을 받는 연구 분야 중 하나이며, 이중 CNN(Convolution Neural Network)은 가장 대표적 알고리즘이다. Deep Learning은 미래사회를 이끌어갈 분야로 평가받고 있으며, 이에 따라 많은 연구들이 진행되고 있고, Deep Learning을 쉽게 활용할 수 있도록 하는 많은 프레임워크가 개발되었다. &amp;#xD; 이에 따라 많은 사람들이 쉽게 Deep Learning을 접할 수 있게 되었지만 특정 환경에서 어떤 프레임워크가 더 우수한 성능을 보이는지에 대한 연구는 부족한 실정이다. 본 논문에서는 특정 환경에서의 성능 비교가 부족하다는 기존 연구의 한계점을 개선하고자 가장 대표적인 Cloud Computing용 오픈소스 소프트웨어 중 하나인 OpenStack을 이용하여 Cloud Computing 환경에서 어떤 Deep Learning 프레임워크가 더 우수한 성능을 보이는지 비교해 보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014373092&target=NART&cn=DIKO0014373092",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 Deep Learning 프레임워크 성능 비교 연구 : Cloud Computing 환경에서 통신 기술의 발달로 인한 사람과 사람, 장치와 장치, 사람과 장치 간의 연결성의 증가와 저장 매체 기술의 발달, 그리고 데이터 저장 비용의 감소로 인해 데이터의 양이 폭발적으로 증가했다. 이에 따라 다양한 형태의 대규모의 데이터를 빠른 시간 내에 효율적으로 처리할 수 있는 Cloud Computing 기술이 주목 받고 있고, Cloud Computing을 위한 오픈소스 기반의 솔루션 또한 많이 나타나게 되었다. &amp;#xD; 2012년부터 주목받기 시작한 Deep Learning은 전 세계적으로 가장 많은 관심을 받는 연구 분야 중 하나이며, 이중 CNN(Convolution Neural Network)은 가장 대표적 알고리즘이다. Deep Learning은 미래사회를 이끌어갈 분야로 평가받고 있으며, 이에 따라 많은 연구들이 진행되고 있고, Deep Learning을 쉽게 활용할 수 있도록 하는 많은 프레임워크가 개발되었다. &amp;#xD; 이에 따라 많은 사람들이 쉽게 Deep Learning을 접할 수 있게 되었지만 특정 환경에서 어떤 프레임워크가 더 우수한 성능을 보이는지에 대한 연구는 부족한 실정이다. 본 논문에서는 특정 환경에서의 성능 비교가 부족하다는 기존 연구의 한계점을 개선하고자 가장 대표적인 Cloud Computing용 오픈소스 소프트웨어 중 하나인 OpenStack을 이용하여 Cloud Computing 환경에서 어떤 Deep Learning 프레임워크가 더 우수한 성능을 보이는지 비교해 보고자 한다."
        },
        {
          "rank": 11,
          "score": 0.6405637860298157,
          "doc_id": "JAKO201719950757340",
          "title": "딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로",
          "abstract": "딥러닝 프레임워크의 대표적인 기능으로는 '자동미분'과 'GPU의 활용' 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각 프레임워크의 실행속도에 대한 평가는 '큰 차이는 없다'는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만 빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데, 위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로 구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201719950757340&target=NART&cn=JAKO201719950757340",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 비교: 티아노, 텐서플로, CNTK를 중심으로 딥러닝 프레임워크의 대표적인 기능으로는 '자동미분'과 'GPU의 활용' 등을 들 수 있다. 본 논문은 파이썬의 라이브러리 형태로 사용 가능한 프레임워크 중에서 구글의 텐서플로와 마이크로소프트의 CNTK, 그리고 텐서플로의 원조라고 할 수 있는 티아노를 비교하였다. 본문에서는 자동미분의 개념과 GPU의 활용형태를 간단히 설명하고, 그 다음에 logistic regression을 실행하는 예를 통하여 각 프레임워크의 문법을 알아본 뒤에, 마지막으로 대표적인 딥러닝 응용인 CNN의 예제를 실행시켜보고 코딩의 편의성과 실행속도 등을 확인해 보았다. 그 결과, 편의성의 관점에서 보면 티아노가 가장 코딩 하기가 어렵고, CNTK와 텐서플로는 많은 부분이 비슷하게 추상화 되어 있어서 코딩이 비슷하지만 가중치와 편향을 직접 정의하느냐의 여부에서 차이를 보였다. 그리고 각 프레임워크의 실행속도에 대한 평가는 '큰 차이는 없다'는 것이다. 텐서플로는 티아노에 비하여 속도가 느리다는 평가가 있어왔는데, 본 연구의 실험에 의하면, 비록 CNN 모형에 국한되었지만, 텐서플로가 아주 조금이지만 빠른 것으로 나타났다. CNTK의 경우에도, 비록 실험환경이 달랐지만, 실험환경의 차이에 의한 속도의 차이의 편차범위 이내에 있는 것으로 판단이 되었다. 본 연구에서는 세 종류의 딥러닝 프레임워크만을 살펴보았는데, 위키피디아에 따르면 딥러닝 프레임워크의 종류는 12가지가 있으며, 각 프레임워크의 특징을 15가지 속성으로 구분하여 차이를 특정하고 있다. 그 많은 속성 중에서 사용자의 입장에서 볼 때 중요한 속성은 어떤 언어(파이썬, C++, Java, 등)로 사용가능한지, 어떤 딥러닝 모형에 대한 라이브러리가 잘 구현되어 있는지 등일 것이다. 그리고 사용자가 대규모의 딥러닝 모형을 구축한다면, 다중 GPU 혹은 다중 서버를 지원하는지의 여부도 중요할 것이다. 또한 딥러닝 모형을 처음 학습하는 경우에는 사용설명서가 많은지 예제 프로그램이 많은지 여부도 중요한 기준이 될 것이다."
        },
        {
          "rank": 12,
          "score": 0.638893723487854,
          "doc_id": "ATN0037463572",
          "title": "서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가",
          "abstract": "Serverless computing is a new computing paradigm which can develop and execute the application program without server management overhead. Recently, as the enterprise application architectures are evolved to container and micro-services, serverless-based cloud services make it easy to expand and distribute the micro-services. In this regard, we compared the technologies and performed an experiment to measure average response time and response success rate of distributed functions for the three typical serverless open source frameworks: OpenFaaS, Kubeless and Fission.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037463572&target=NART&cn=ATN0037463572",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 서버리스 컴퓨팅 오픈소스 플랫폼 기술 및 성능 평가 Serverless computing is a new computing paradigm which can develop and execute the application program without server management overhead. Recently, as the enterprise application architectures are evolved to container and micro-services, serverless-based cloud services make it easy to expand and distribute the micro-services. In this regard, we compared the technologies and performed an experiment to measure average response time and response success rate of distributed functions for the three typical serverless open source frameworks: OpenFaaS, Kubeless and Fission."
        },
        {
          "rank": 13,
          "score": 0.6378195881843567,
          "doc_id": "JAKO202433861648179",
          "title": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교",
          "abstract": "본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202433861648179&target=NART&cn=JAKO202433861648179",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다."
        },
        {
          "rank": 14,
          "score": 0.636302649974823,
          "doc_id": "JAKO201722163438668",
          "title": "통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가",
          "abstract": "최근, 딥러닝을 사용 가능한 임베디드 디바이스가 상용화됨에 따라 임베디드 시스템 영역에서도 딥러닝 활용에 대한 다양한 연구가 진행되고 있다. 그러나 임베디드 시스템을 고성능 PC 환경과 비교하면 상대적으로 저사양의 CPU/GPU 프로세서와 메모리를 탑재하고 있으므로 딥러닝 기술의 적용에 있어서 많은 제약이 있다. 본 논문에서는 다양한 최신 딥러닝 네트워크들을 임베디드 디바이스에 적용했을때의 성능을 시간과 전력이라는 관점에서 실험적으로 평가한다. 또한, 호스트 CPU와 GPU 디바이스간의 메모리를 공유하는 임베디드 시스템들의 아키텍처적인 특성을 이용하여 메모리 복사를 줄임으로써 실시간 성능과 저전력성을 높이는 방법을 제시한다. 제안된 방법은 대표적인 공개 딥러닝 프레임워크인 Caffe를 수정하여 구현되었으며, 임베디드 GPU를 탑재한 NVIDIA Jetson TK1에서 성능평가 되었다. 실험결과, 대부분의 딥러닝 네트워크에서 뚜렷한 성능향상을 관찰할 수 있었다. 특히, 메모리 사용량이 높은 AlexNet에서 약 33%의 이미지 인식 속도 단축과 50%의 소비 전력량 감소를 관찰할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201722163438668&target=NART&cn=JAKO201722163438668",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 통합메모리를 이용한 임베디드 환경에서의 딥러닝 프레임워크 성능 개선과 평가 최근, 딥러닝을 사용 가능한 임베디드 디바이스가 상용화됨에 따라 임베디드 시스템 영역에서도 딥러닝 활용에 대한 다양한 연구가 진행되고 있다. 그러나 임베디드 시스템을 고성능 PC 환경과 비교하면 상대적으로 저사양의 CPU/GPU 프로세서와 메모리를 탑재하고 있으므로 딥러닝 기술의 적용에 있어서 많은 제약이 있다. 본 논문에서는 다양한 최신 딥러닝 네트워크들을 임베디드 디바이스에 적용했을때의 성능을 시간과 전력이라는 관점에서 실험적으로 평가한다. 또한, 호스트 CPU와 GPU 디바이스간의 메모리를 공유하는 임베디드 시스템들의 아키텍처적인 특성을 이용하여 메모리 복사를 줄임으로써 실시간 성능과 저전력성을 높이는 방법을 제시한다. 제안된 방법은 대표적인 공개 딥러닝 프레임워크인 Caffe를 수정하여 구현되었으며, 임베디드 GPU를 탑재한 NVIDIA Jetson TK1에서 성능평가 되었다. 실험결과, 대부분의 딥러닝 네트워크에서 뚜렷한 성능향상을 관찰할 수 있었다. 특히, 메모리 사용량이 높은 AlexNet에서 약 33%의 이미지 인식 속도 단축과 50%의 소비 전력량 감소를 관찰할 수 있었다."
        },
        {
          "rank": 15,
          "score": 0.6362158060073853,
          "doc_id": "ATN0038661375",
          "title": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험",
          "abstract": "Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0038661375&target=NART&cn=ATN0038661375",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model."
        },
        {
          "rank": 16,
          "score": 0.6344814300537109,
          "doc_id": "NPAP12915394",
          "title": "u-IT Convergence 기기의 성능평가기준과 표준화 연구",
          "abstract": "u-IT 기술의 발달로 인해 통신, 방송, 포털, 콘텐츠, 장비 및 솔루션 등 사업의 모든 분야에 u-IT Convergence 기기들이 연구 개발되고 있다. 본 논문에서는 u-IT Convergence를 정의하고, u-IT Convergence 기기들과 단말에서 정보를 감지하는 온도, 압력, 자기, 광, 가스, 습도 센서들에 대한 정의와 평가 기준으로 기술성, 경제성, 경영성 등의 평가 기준을 제시한다. 또한 u-IT Convergence 기기들에 대한 평가 기준에 따르는 u-IT Convergence 네트워크 기기의 표준화 기준을 연구하였다. 본 연구를 통해 유비쿼터스 정보화 사회에서 기술적인 발전과 기기들에 대한 정확한 인증 평가를 통해서 안정성과 신뢰성을 갖춘 평가 제품으로서 가치를 정립하고 표준화를 이루는데 기여하게 될 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12915394&target=NART&cn=NPAP12915394",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "u-IT Convergence 기기의 성능평가기준과 표준화 연구 u-IT Convergence 기기의 성능평가기준과 표준화 연구 u-IT Convergence 기기의 성능평가기준과 표준화 연구 u-IT 기술의 발달로 인해 통신, 방송, 포털, 콘텐츠, 장비 및 솔루션 등 사업의 모든 분야에 u-IT Convergence 기기들이 연구 개발되고 있다. 본 논문에서는 u-IT Convergence를 정의하고, u-IT Convergence 기기들과 단말에서 정보를 감지하는 온도, 압력, 자기, 광, 가스, 습도 센서들에 대한 정의와 평가 기준으로 기술성, 경제성, 경영성 등의 평가 기준을 제시한다. 또한 u-IT Convergence 기기들에 대한 평가 기준에 따르는 u-IT Convergence 네트워크 기기의 표준화 기준을 연구하였다. 본 연구를 통해 유비쿼터스 정보화 사회에서 기술적인 발전과 기기들에 대한 정확한 인증 평가를 통해서 안정성과 신뢰성을 갖춘 평가 제품으로서 가치를 정립하고 표준화를 이루는데 기여하게 될 것이다."
        },
        {
          "rank": 17,
          "score": 0.6303461790084839,
          "doc_id": "JAKO202011161035249",
          "title": "객체 검출을 위한 CNN과 YOLO 성능 비교 실험",
          "abstract": "Object detection plays a critical role in the field of computer vision, and various researches have rapidly increased along with applying convolutional neural network and its modified structures since 2012. There are representative object detection algorithms, which are convolutional neural networks and YOLO. This paper presents two representative algorithm series, based on CNN and YOLO which solves the problem of CNN bounding box. We compare the performance of algorithm series in terms of accuracy, speed and cost. Compared with the latest advanced solution, YOLO v3 achieves a good trade-off between speed and accuracy.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202011161035249&target=NART&cn=JAKO202011161035249",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "객체 검출을 위한 CNN과 YOLO 성능 비교 실험 객체 검출을 위한 CNN과 YOLO 성능 비교 실험 객체 검출을 위한 CNN과 YOLO 성능 비교 실험 Object detection plays a critical role in the field of computer vision, and various researches have rapidly increased along with applying convolutional neural network and its modified structures since 2012. There are representative object detection algorithms, which are convolutional neural networks and YOLO. This paper presents two representative algorithm series, based on CNN and YOLO which solves the problem of CNN bounding box. We compare the performance of algorithm series in terms of accuracy, speed and cost. Compared with the latest advanced solution, YOLO v3 achieves a good trade-off between speed and accuracy."
        },
        {
          "rank": 18,
          "score": 0.6284099817276001,
          "doc_id": "JAKO202018853212134",
          "title": "딥러닝 기반의 수중 IoT 네트워크 BER 예측 모델",
          "abstract": "수중 IoT 네트워크에서 센서 노드는 지속적인 전력 공급이 어렵기 때문에 제한된 상황에서 소비 전력과 네트워크 처리량의 효율성이 매우 중요하다. 이를 위해 기존의 무선 네트워크에서는 SNR(Signal Noise Rate)과 BER(Bit Error Rate)의 높은 연관성을 기반으로 적응적으로 통신 파라미터를 선택하는 AMC(Adaptive Modulation and Coding) 기술을 적용한다. 하지만 본 논문의 실험 결과, 수중에서 SNR과 BER 사이의 상관 관계가 상대적으로 감소함을 확인하였다. 따라서 본 논문에서는 SNR과 함께 다중 파라미터를 동시에 사용하는 딥러닝 기반 BER 예측 모델(MLP, Multi-Layer Perceptron)을 적용한다. 제안하는 BER 예측 모델은 처리량이 가장 높은 통신 방법을 찾아낼 수 있고, 시뮬레이션 결과 85.2%의 높은 정확도와 네트워크 처리량은 기존 처리량보다 4.4배 높은 성능을 보여주는 우수한 성능을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202018853212134&target=NART&cn=JAKO202018853212134",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반의 수중 IoT 네트워크 BER 예측 모델 딥러닝 기반의 수중 IoT 네트워크 BER 예측 모델 딥러닝 기반의 수중 IoT 네트워크 BER 예측 모델 수중 IoT 네트워크에서 센서 노드는 지속적인 전력 공급이 어렵기 때문에 제한된 상황에서 소비 전력과 네트워크 처리량의 효율성이 매우 중요하다. 이를 위해 기존의 무선 네트워크에서는 SNR(Signal Noise Rate)과 BER(Bit Error Rate)의 높은 연관성을 기반으로 적응적으로 통신 파라미터를 선택하는 AMC(Adaptive Modulation and Coding) 기술을 적용한다. 하지만 본 논문의 실험 결과, 수중에서 SNR과 BER 사이의 상관 관계가 상대적으로 감소함을 확인하였다. 따라서 본 논문에서는 SNR과 함께 다중 파라미터를 동시에 사용하는 딥러닝 기반 BER 예측 모델(MLP, Multi-Layer Perceptron)을 적용한다. 제안하는 BER 예측 모델은 처리량이 가장 높은 통신 방법을 찾아낼 수 있고, 시뮬레이션 결과 85.2%의 높은 정확도와 네트워크 처리량은 기존 처리량보다 4.4배 높은 성능을 보여주는 우수한 성능을 확인하였다."
        },
        {
          "rank": 19,
          "score": 0.6280890703201294,
          "doc_id": "JAKO201722163435084",
          "title": "원심압축기 공력성능 해석 프레임워크 구축",
          "abstract": "A framework for the aerodynamic performance analysis of a centrifugal compressor was constructed using the non-dimensional performance. Because of the much time and efforts for the new design of centrifugal compressors, it is widely used in the industry to assess the performance of the compressor under the new operating conditions based on the existing performance results and to apply some design changes to the existing compressors. When checking the performance of centrifugal compressors for different operating conditions by using the existing results, it is easy to utilize the non-dimensional performance head coefficient. If the framework of the non-dimensional performance coefficient is utilized, it is convenient to input design variables and performance curve data. Also it is easy to perform the non-dimensional analysis and performance review for off-design points. The accuracy of the framework was verified by comparison with the results of two 1-stage centrifugal compressors at off-design conditions.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201722163435084&target=NART&cn=JAKO201722163435084",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "원심압축기 공력성능 해석 프레임워크 구축 원심압축기 공력성능 해석 프레임워크 구축 원심압축기 공력성능 해석 프레임워크 구축 A framework for the aerodynamic performance analysis of a centrifugal compressor was constructed using the non-dimensional performance. Because of the much time and efforts for the new design of centrifugal compressors, it is widely used in the industry to assess the performance of the compressor under the new operating conditions based on the existing performance results and to apply some design changes to the existing compressors. When checking the performance of centrifugal compressors for different operating conditions by using the existing results, it is easy to utilize the non-dimensional performance head coefficient. If the framework of the non-dimensional performance coefficient is utilized, it is convenient to input design variables and performance curve data. Also it is easy to perform the non-dimensional analysis and performance review for off-design points. The accuracy of the framework was verified by comparison with the results of two 1-stage centrifugal compressors at off-design conditions."
        },
        {
          "rank": 20,
          "score": 0.6279988288879395,
          "doc_id": "JAKO200634718251368",
          "title": "IT 및 BT 산업별 기술관련 기업성과 결정요인 비교 분석 연구",
          "abstract": "본 논문은 첨단 기술기반 기업의 여러 성과결정 요인 중 기술과 직접 관련된 요인들이 IT 및 BT 산업별로 어떻게 다르게 기업들의 성과에 영향을 미치는 가를 연구의 목적으로 하고 있다. 본 연구에서 비교 분석을 위해 선택한 기술관련 성과 결정요인은 기업의 기술개발 전략, R&D 집단 특성, 그리고 지식자산 관리역량 등 3가지이다. 그리고 기업의 성과는 전년대비 매출액 성장률의 2개년 평균을 주로 사용하였다. 성과 결정 요인들을 독립변수로 하고 매출액 성장률을 종속변수로 하여 전 산업을 대상으로 한 회귀분석 모델과 IT 및 BT 산업만을 대상으로 한 회귀분석 모텔을 설정하여 연구목적을 규명하였다. 분석결과, 기술관련 성과 결정요인들이 기업의 성과에 전반적으로 영향을 미칠 뿐만 아니라, IT 및 BT 산업별로 기술관련 성과결정 요인이 기업성과에 상이한 영향을 미치는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200634718251368&target=NART&cn=JAKO200634718251368",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "IT 및 BT 산업별 기술관련 기업성과 결정요인 비교 분석 연구 IT 및 BT 산업별 기술관련 기업성과 결정요인 비교 분석 연구 IT 및 BT 산업별 기술관련 기업성과 결정요인 비교 분석 연구 본 논문은 첨단 기술기반 기업의 여러 성과결정 요인 중 기술과 직접 관련된 요인들이 IT 및 BT 산업별로 어떻게 다르게 기업들의 성과에 영향을 미치는 가를 연구의 목적으로 하고 있다. 본 연구에서 비교 분석을 위해 선택한 기술관련 성과 결정요인은 기업의 기술개발 전략, R&D 집단 특성, 그리고 지식자산 관리역량 등 3가지이다. 그리고 기업의 성과는 전년대비 매출액 성장률의 2개년 평균을 주로 사용하였다. 성과 결정 요인들을 독립변수로 하고 매출액 성장률을 종속변수로 하여 전 산업을 대상으로 한 회귀분석 모델과 IT 및 BT 산업만을 대상으로 한 회귀분석 모텔을 설정하여 연구목적을 규명하였다. 분석결과, 기술관련 성과 결정요인들이 기업의 성과에 전반적으로 영향을 미칠 뿐만 아니라, IT 및 BT 산업별로 기술관련 성과결정 요인이 기업성과에 상이한 영향을 미치는 것으로 나타났다."
        },
        {
          "rank": 21,
          "score": 0.6278191208839417,
          "doc_id": "ART001624002",
          "title": "Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model",
          "abstract": "In this paper, we introduce an integrated performance evaluation model defined by the three indices (evaluation index for each evaluation stage, performance viewpoint, and performance type) and analyze the utilization and satisfaction of the seven public u-IT services implemented from 2008 to 2009 using the proposed model. From the performance results of the public u-IT services, it was found out that most of the research goals initially set by the public u-IT service support projects were satisfied. Furthermore, we suggest improvement directions and promotion strategies of future projects.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART001624002&target=NART&cn=ART001624002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model In this paper, we introduce an integrated performance evaluation model defined by the three indices (evaluation index for each evaluation stage, performance viewpoint, and performance type) and analyze the utilization and satisfaction of the seven public u-IT services implemented from 2008 to 2009 using the proposed model. From the performance results of the public u-IT services, it was found out that most of the research goals initially set by the public u-IT service support projects were satisfied. Furthermore, we suggest improvement directions and promotion strategies of future projects."
        },
        {
          "rank": 22,
          "score": 0.6269981265068054,
          "doc_id": "JAKO202514154005683",
          "title": "RAG 시스템 성능 평가를 위한 자동 데이터 셋 생성 프레임워크 비교 분석 연구",
          "abstract": "본 논문은 최근 주목받고 있는 검색 증강 생성(RAG) 시스템의 성능 평가를 위한 테스트 데이터셋 생성 방법을 비교 분석하였다. 대규모 언어 모델(LLM)의 한계를 극복하는 RAG 기술의 필요성과 중요성을 설명하고, 수동 생성 방식과 LLM을 활용한 자동 생성 방식의 특징과 장단점을 정리하였다. 또한 자동화된 데이터셋 구축 프레임워크 중 RAGAS, AutoRAG, DeepEval을 선정하여 의료,금융,법률 문서를 입력으로 각각 100개의 질문-답변 세트를 생성한 후 정확성을 평가하였다. 평가 결과, AutoRAG가 한국어 문장 표현의 자연성과 컨텍스트 기반의 정확성 측면에서 가장 뛰어난 성능을 보였으며, RAGAS는 문서 처리 과정에서 불필요한 정보 포함 등의 오류가 많았고, DeepEval은 한국어 지원 부족으로 인해 성능이 상대적으로 낮았다. 향후 연구에서는 LLM을 활용하여 사용자의 의도와 컨텍스트를 더욱 정확히 반영하는 고급 프롬프팅 기법과 자동화된 데이터 품질 평가 및 개선 전략을 중점적으로 탐색할 계획이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202514154005683&target=NART&cn=JAKO202514154005683",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RAG 시스템 성능 평가를 위한 자동 데이터 셋 생성 프레임워크 비교 분석 연구 RAG 시스템 성능 평가를 위한 자동 데이터 셋 생성 프레임워크 비교 분석 연구 RAG 시스템 성능 평가를 위한 자동 데이터 셋 생성 프레임워크 비교 분석 연구 본 논문은 최근 주목받고 있는 검색 증강 생성(RAG) 시스템의 성능 평가를 위한 테스트 데이터셋 생성 방법을 비교 분석하였다. 대규모 언어 모델(LLM)의 한계를 극복하는 RAG 기술의 필요성과 중요성을 설명하고, 수동 생성 방식과 LLM을 활용한 자동 생성 방식의 특징과 장단점을 정리하였다. 또한 자동화된 데이터셋 구축 프레임워크 중 RAGAS, AutoRAG, DeepEval을 선정하여 의료,금융,법률 문서를 입력으로 각각 100개의 질문-답변 세트를 생성한 후 정확성을 평가하였다. 평가 결과, AutoRAG가 한국어 문장 표현의 자연성과 컨텍스트 기반의 정확성 측면에서 가장 뛰어난 성능을 보였으며, RAGAS는 문서 처리 과정에서 불필요한 정보 포함 등의 오류가 많았고, DeepEval은 한국어 지원 부족으로 인해 성능이 상대적으로 낮았다. 향후 연구에서는 LLM을 활용하여 사용자의 의도와 컨텍스트를 더욱 정확히 반영하는 고급 프롬프팅 기법과 자동화된 데이터 품질 평가 및 개선 전략을 중점적으로 탐색할 계획이다."
        },
        {
          "rank": 23,
          "score": 0.6259748935699463,
          "doc_id": "JAKO202231159469541",
          "title": "고속도로 자율주행 시 보상을 최대화하기 위한 강화 학습 활성화 함수 비교",
          "abstract": "자율주행 기술은 최근 심층 강화학습의 도입으로 큰 발전을 이루고 있다. 심층 강화 학습을 효과적으로 사용하기 위해서는 적절한 활성화 함수를 선택하는 것이 중요하다. 그 동안 많은 활성화 함수가 제시되었으나 적용할 환경에 따라 다른 성능을 보여주었다. 본 논문은 고속도로에서 자율주행을 학습하기 위해 강화 학습을 사용할 때 어떤 활성화 함수를 사용하는 것이 효과적인지 12개의 활성화 함수 성능을 비교 평가한다. 이를 위한 성능 평가 방법을 제시하였고 각 활성화 함수의 평균 보상 값을 비교하였다. 그 결과 GELU를 사용할 경우 가장 높은 평균 보상을 얻을 수 있었으며 SiLU는 가장 낮은 성능을 보여주었다. 두 활성화 함수의 평균 보상 차이는 20%였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202231159469541&target=NART&cn=JAKO202231159469541",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "고속도로 자율주행 시 보상을 최대화하기 위한 강화 학습 활성화 함수 비교 고속도로 자율주행 시 보상을 최대화하기 위한 강화 학습 활성화 함수 비교 고속도로 자율주행 시 보상을 최대화하기 위한 강화 학습 활성화 함수 비교 자율주행 기술은 최근 심층 강화학습의 도입으로 큰 발전을 이루고 있다. 심층 강화 학습을 효과적으로 사용하기 위해서는 적절한 활성화 함수를 선택하는 것이 중요하다. 그 동안 많은 활성화 함수가 제시되었으나 적용할 환경에 따라 다른 성능을 보여주었다. 본 논문은 고속도로에서 자율주행을 학습하기 위해 강화 학습을 사용할 때 어떤 활성화 함수를 사용하는 것이 효과적인지 12개의 활성화 함수 성능을 비교 평가한다. 이를 위한 성능 평가 방법을 제시하였고 각 활성화 함수의 평균 보상 값을 비교하였다. 그 결과 GELU를 사용할 경우 가장 높은 평균 보상을 얻을 수 있었으며 SiLU는 가장 낮은 성능을 보여주었다. 두 활성화 함수의 평균 보상 차이는 20%였다."
        },
        {
          "rank": 24,
          "score": 0.6248210668563843,
          "doc_id": "JAKO201216349186631",
          "title": "조합된 서비스의 성능 평가를 위한 Aspect 기반 테스팅 프레임워크",
          "abstract": "최근 서비스 기반의 소프트웨어 개발이 사용자의 다양한 요구를 충족시킬 수 있는 하나의 솔루션으로 부각되면서, 안정적인 서비스의 조합을 통하여 보다 큰 서비스를 제공하려는 시도가 증가하고 있다. 그러나 조합된 서비스의 개발시 고려되어야 하는 사항중의 하나는 사용자의 입장에서 서비스의 정확성과 함께 신속성을 제공해야 한다는 것이다. 왜냐하면 사용자가 서비스의 요청 과정에서 늦은 응답으로 인하여 실행 버튼을 중복적으로 클릭하는 행동을 보이기 때문이다. 본 논문에서는 조합된 서비스의 성능을 측정하기 위한 테스팅 프레임워크를 제시한다. 즉, 조합된 서비스의 실행 시간을 측정함으로써, 개발자에게 서비스의 성능을 분석할 수 있는 도구를 제공한다. 이러한 실행시간 측정을 위하여 본 연구에서는 Aspect 컴포넌트를 이용하는 타이머 서비스를 개발하여 기존 웹 서비스들과 연동할 수 있도록 하였다. 또한 몇 실험을 통하여 조합된 서비스의 성능 테스트가 가능함을 확인하였다. 제시한 프레임워크는 조합된 서비스를 구성하는 단위 서비스 중에서 가장 많은 시간이 소요되는 서비스를 식별하고 필요에 따라 다른 서비스로 대체할 수 있는 서비스 개발을 가능하게 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201216349186631&target=NART&cn=JAKO201216349186631",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "조합된 서비스의 성능 평가를 위한 Aspect 기반 테스팅 프레임워크 조합된 서비스의 성능 평가를 위한 Aspect 기반 테스팅 프레임워크 조합된 서비스의 성능 평가를 위한 Aspect 기반 테스팅 프레임워크 최근 서비스 기반의 소프트웨어 개발이 사용자의 다양한 요구를 충족시킬 수 있는 하나의 솔루션으로 부각되면서, 안정적인 서비스의 조합을 통하여 보다 큰 서비스를 제공하려는 시도가 증가하고 있다. 그러나 조합된 서비스의 개발시 고려되어야 하는 사항중의 하나는 사용자의 입장에서 서비스의 정확성과 함께 신속성을 제공해야 한다는 것이다. 왜냐하면 사용자가 서비스의 요청 과정에서 늦은 응답으로 인하여 실행 버튼을 중복적으로 클릭하는 행동을 보이기 때문이다. 본 논문에서는 조합된 서비스의 성능을 측정하기 위한 테스팅 프레임워크를 제시한다. 즉, 조합된 서비스의 실행 시간을 측정함으로써, 개발자에게 서비스의 성능을 분석할 수 있는 도구를 제공한다. 이러한 실행시간 측정을 위하여 본 연구에서는 Aspect 컴포넌트를 이용하는 타이머 서비스를 개발하여 기존 웹 서비스들과 연동할 수 있도록 하였다. 또한 몇 실험을 통하여 조합된 서비스의 성능 테스트가 가능함을 확인하였다. 제시한 프레임워크는 조합된 서비스를 구성하는 단위 서비스 중에서 가장 많은 시간이 소요되는 서비스를 식별하고 필요에 따라 다른 서비스로 대체할 수 있는 서비스 개발을 가능하게 한다."
        },
        {
          "rank": 25,
          "score": 0.6242656111717224,
          "doc_id": "ATN0043397445",
          "title": "클라우드 환경에서 단일 자원 스케줄링 연구",
          "abstract": "Clouds execute very heterogeneous applications such as webservers and video streaming that have different characteristics. For example, one application burns mostly CPU whereas another uses network heavily. Some applications vary their characteristics with time. Yet, resource scheduling in clouds focuses on a single resource -- CPU scheduling or network scheduling. We call it a separate scheduling. Through extensive performance evaluation and profiling, this paper points out the limitations of the separate scheduling, which results in the performance degradation of the time-varying application. Evaluation results show that the separate scheduling only offers 67% and 73% of maximum performance on average. Based on the evaluation results, we argue that it is necessary to develop a new scheduling technique that can overcome the limitation of the separate scheduling and support cloud applications with time-varying characteristics.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0043397445&target=NART&cn=ATN0043397445",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 환경에서 단일 자원 스케줄링 연구 클라우드 환경에서 단일 자원 스케줄링 연구 클라우드 환경에서 단일 자원 스케줄링 연구 Clouds execute very heterogeneous applications such as webservers and video streaming that have different characteristics. For example, one application burns mostly CPU whereas another uses network heavily. Some applications vary their characteristics with time. Yet, resource scheduling in clouds focuses on a single resource -- CPU scheduling or network scheduling. We call it a separate scheduling. Through extensive performance evaluation and profiling, this paper points out the limitations of the separate scheduling, which results in the performance degradation of the time-varying application. Evaluation results show that the separate scheduling only offers 67% and 73% of maximum performance on average. Based on the evaluation results, we argue that it is necessary to develop a new scheduling technique that can overcome the limitation of the separate scheduling and support cloud applications with time-varying characteristics."
        },
        {
          "rank": 26,
          "score": 0.6235885620117188,
          "doc_id": "NART69876154",
          "title": "다양한 잡음 환경하에서의 PSO 정규화를 적용한 음성인식 성능평가",
          "abstract": "<P>&amp;nbsp;&amp;nbsp;음성만이 가지고 있는 편리함으로 인하여 매우 오랜 기간 자동 음성 인식에 관한 많은 연구가 지속되어 왔다. 그러나 현재의 음성 인식 기술을 실제 환경에서 적용하기는 쉽지 않다. 입력된 음성은 주변 소음과 마이크 특성 등에 따라 왜곡되며, 이 왜곡된 음성으로 인하여 음성인식 성능은 감소하게 된다. 본 논문에서는 잡음 음성인식에 성공적으로 적용된 PSO(particle swarm optimization) 정규화를 이용하여 다양한 잡음 환경에서의 인식성능을 평가하였다. 신호대 잡음비(SNR)에 따라 음소 신뢰도를 정규화하였고, 화자 18명의 음성 DB를 대상으로 실험을 수행하였다. 실험 결과, 각 잡음 환경에서 인식 성능이 향상됨을 알 수 있으며, 특히 SNR 15dB에서 5~7%의 인식 성능이 향상되었다.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART69876154&target=NART&cn=NART69876154",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다양한 잡음 환경하에서의 PSO 정규화를 적용한 음성인식 성능평가 다양한 잡음 환경하에서의 PSO 정규화를 적용한 음성인식 성능평가 다양한 잡음 환경하에서의 PSO 정규화를 적용한 음성인식 성능평가 <P>&amp;nbsp;&amp;nbsp;음성만이 가지고 있는 편리함으로 인하여 매우 오랜 기간 자동 음성 인식에 관한 많은 연구가 지속되어 왔다. 그러나 현재의 음성 인식 기술을 실제 환경에서 적용하기는 쉽지 않다. 입력된 음성은 주변 소음과 마이크 특성 등에 따라 왜곡되며, 이 왜곡된 음성으로 인하여 음성인식 성능은 감소하게 된다. 본 논문에서는 잡음 음성인식에 성공적으로 적용된 PSO(particle swarm optimization) 정규화를 이용하여 다양한 잡음 환경에서의 인식성능을 평가하였다. 신호대 잡음비(SNR)에 따라 음소 신뢰도를 정규화하였고, 화자 18명의 음성 DB를 대상으로 실험을 수행하였다. 실험 결과, 각 잡음 환경에서 인식 성능이 향상됨을 알 수 있으며, 특히 SNR 15dB에서 5~7%의 인식 성능이 향상되었다.</P>"
        },
        {
          "rank": 27,
          "score": 0.6230782866477966,
          "doc_id": "JAKO202408075073596",
          "title": "인공지능 학습데이터 라벨링 정확도에 따른 인공지능 성능",
          "abstract": "본 연구는 데이터의 품질이 인공지능(AI) 성능에 미치는 영향을 검토한다. 이를 위해, 데이터 특성변수(Feature)의 유사도와 클래스(Class) 구성의 불균형을 고려한 모의실험(Simulation)을 통해 라벨링 오류 수준이 인공지능의 성능에 미치는 영향을 비교 분석하였다. 그 결과, 특성변수 간 유사성이 높은 데이터에서는 특성 변수 간 유사성이 낮은 데이터에 비해 라벨링 정확도에 더 민감하게 반응하였으며, 클래스 불균형이 증가함에 따라 인공지능 정확도가 급격히 감소되는 경향을 관찰하였다. 이는 인공지능 학습데이터의 품질평가 기준 및 관련 연구를 위한 기초자료가 될 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202408075073596&target=NART&cn=JAKO202408075073596",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "인공지능 학습데이터 라벨링 정확도에 따른 인공지능 성능 인공지능 학습데이터 라벨링 정확도에 따른 인공지능 성능 인공지능 학습데이터 라벨링 정확도에 따른 인공지능 성능 본 연구는 데이터의 품질이 인공지능(AI) 성능에 미치는 영향을 검토한다. 이를 위해, 데이터 특성변수(Feature)의 유사도와 클래스(Class) 구성의 불균형을 고려한 모의실험(Simulation)을 통해 라벨링 오류 수준이 인공지능의 성능에 미치는 영향을 비교 분석하였다. 그 결과, 특성변수 간 유사성이 높은 데이터에서는 특성 변수 간 유사성이 낮은 데이터에 비해 라벨링 정확도에 더 민감하게 반응하였으며, 클래스 불균형이 증가함에 따라 인공지능 정확도가 급격히 감소되는 경향을 관찰하였다. 이는 인공지능 학습데이터의 품질평가 기준 및 관련 연구를 위한 기초자료가 될 것이다."
        },
        {
          "rank": 28,
          "score": 0.6204883456230164,
          "doc_id": "JAKO202305758375548",
          "title": "머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교",
          "abstract": "전국 대학생의 중도 탈락 비율의 증가는 학생 개인 뿐만 아니라 대학과 사회에 심각한 부정적 영향을 끼친다. 본 연구에서는 중도 탈락이 예상되는 학생을 사전에 식별하기 위하여, 각 대학의 학사관리 시스템에서 손쉽게 얻을 수 있는 학적 데이터를 기반으로 머신러닝 분야의 결정트리, 랜덤 포레스트, 로지스틱 회귀 및 딥러닝 기반의 중도 탈락 예측 모델을 구축하고, 그 성능을 비교&#x00B7;분석하였다. 분석 결과 로지스틱 회귀 기반 예측 모델의 재현율이 가장 높았으나 f-1 및 auc 값이 낮은 한계를 보였고, 랜덤 포레스트 기반의 예측 모델의 경우 재현율을 제외한 다른 모든 지표에서 가장 우수한 성능을 보였다. 또한 예측 기간에 따른 예측 모델의 성능을 확인하기 위하여 예측 기간을 단기(1개 학기 이내), 중기(2개 학기 이내) 및 장기(3개 학기 이내)로 나누어 분석해 본 결과, 장기 예측 시 가장 높은 예측력을 보였다. 본 연구를 통해 각 대학은 중도 탈락이 예상되는 학생들을 조기에 식별하고, 이들에 대한 집중 관리를 통해 중도 탈락 비율을 줄이며 나아가 대학 재정 안정화에 기여할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202305758375548&target=NART&cn=JAKO202305758375548",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교 머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교 머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교 전국 대학생의 중도 탈락 비율의 증가는 학생 개인 뿐만 아니라 대학과 사회에 심각한 부정적 영향을 끼친다. 본 연구에서는 중도 탈락이 예상되는 학생을 사전에 식별하기 위하여, 각 대학의 학사관리 시스템에서 손쉽게 얻을 수 있는 학적 데이터를 기반으로 머신러닝 분야의 결정트리, 랜덤 포레스트, 로지스틱 회귀 및 딥러닝 기반의 중도 탈락 예측 모델을 구축하고, 그 성능을 비교&#x00B7;분석하였다. 분석 결과 로지스틱 회귀 기반 예측 모델의 재현율이 가장 높았으나 f-1 및 auc 값이 낮은 한계를 보였고, 랜덤 포레스트 기반의 예측 모델의 경우 재현율을 제외한 다른 모든 지표에서 가장 우수한 성능을 보였다. 또한 예측 기간에 따른 예측 모델의 성능을 확인하기 위하여 예측 기간을 단기(1개 학기 이내), 중기(2개 학기 이내) 및 장기(3개 학기 이내)로 나누어 분석해 본 결과, 장기 예측 시 가장 높은 예측력을 보였다. 본 연구를 통해 각 대학은 중도 탈락이 예상되는 학생들을 조기에 식별하고, 이들에 대한 집중 관리를 통해 중도 탈락 비율을 줄이며 나아가 대학 재정 안정화에 기여할 수 있을 것으로 기대된다."
        },
        {
          "rank": 29,
          "score": 0.6196191310882568,
          "doc_id": "DIKO0014041997",
          "title": "클라우드 서비스 품질 측정 시스템 프레임워크 개발",
          "abstract": "클라우드 서비스는 IT 자원 운영관리 및 에너지 소비 효율성 극대화를 위한 그린 IT의 차세대 핵심 서비스이다. 클라우드 서비스는 이용자 중심의 편리성 제공과, IT 인프라의 운영관리 효율화, 산업 간의 융합 등을 통해 높은 파급효과가 기대되는 분야로 지속적으로 시장규모가 성장할 것으로 전망되고 있다. 그럼에도 불구하고, ICT 이용자들은 클라우드 서비스 도입 및 이용 시, 보안과 비용 이외에도 통제권 상실, 신뢰성, 성능 등에 대한 불확실성 등을 고려사항으로 꼽고 있다. 본 연구에서는 이용자에게 클라우드 서비스에 대한 객관적이고 정량적인 평가 결과를 제공하여 요구에 맞는 클라우드 서비스를 신뢰하고 선택할 수 있는 기준을 제공하기 위한 클라우드 서비스 성능 평가를 위한 모델링과, 모델링을 기반으로 한 성능 평가 시스템을 제안한다. 특히, 본 연구에서는 다양한 클라우드 서비스 분야 중 인프라 서비스(IaaS)를 대상으로 가상머신(VM)의 성능 평가를 위한 범위를 정의하고, 요구사항을 분석하여 성능 평가 항목을 도출한다. 제안한 시스템에서는 성능 평가 대상을 시스템 성능, 네트워크 성능, 서비스 성능, 가용성, 보안 성능 5개 항목으로 모듈화하여 구성한다. 또한 이를 바탕으로 가상머신(VM)의 성능 평가를 위한 시스템을 개발하여 클라우드 인프라 서비스의 성능 관리에 활용 할 수 있도록 도움을 주고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014041997&target=NART&cn=DIKO0014041997",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "클라우드 서비스 품질 측정 시스템 프레임워크 개발 클라우드 서비스 품질 측정 시스템 프레임워크 개발 클라우드 서비스 품질 측정 시스템 프레임워크 개발 클라우드 서비스는 IT 자원 운영관리 및 에너지 소비 효율성 극대화를 위한 그린 IT의 차세대 핵심 서비스이다. 클라우드 서비스는 이용자 중심의 편리성 제공과, IT 인프라의 운영관리 효율화, 산업 간의 융합 등을 통해 높은 파급효과가 기대되는 분야로 지속적으로 시장규모가 성장할 것으로 전망되고 있다. 그럼에도 불구하고, ICT 이용자들은 클라우드 서비스 도입 및 이용 시, 보안과 비용 이외에도 통제권 상실, 신뢰성, 성능 등에 대한 불확실성 등을 고려사항으로 꼽고 있다. 본 연구에서는 이용자에게 클라우드 서비스에 대한 객관적이고 정량적인 평가 결과를 제공하여 요구에 맞는 클라우드 서비스를 신뢰하고 선택할 수 있는 기준을 제공하기 위한 클라우드 서비스 성능 평가를 위한 모델링과, 모델링을 기반으로 한 성능 평가 시스템을 제안한다. 특히, 본 연구에서는 다양한 클라우드 서비스 분야 중 인프라 서비스(IaaS)를 대상으로 가상머신(VM)의 성능 평가를 위한 범위를 정의하고, 요구사항을 분석하여 성능 평가 항목을 도출한다. 제안한 시스템에서는 성능 평가 대상을 시스템 성능, 네트워크 성능, 서비스 성능, 가용성, 보안 성능 5개 항목으로 모듈화하여 구성한다. 또한 이를 바탕으로 가상머신(VM)의 성능 평가를 위한 시스템을 개발하여 클라우드 인프라 서비스의 성능 관리에 활용 할 수 있도록 도움을 주고자 한다."
        },
        {
          "rank": 30,
          "score": 0.6189768314361572,
          "doc_id": "NPAP12690195",
          "title": "전기차 배터리의 실시간 주행 데이터 취득과 주행경로별 비교",
          "abstract": "As the number of electric vehicles (EV) increases, there is an increasing interest in the post-vehicle application of the EV batteries. For the second use application of EV batteries, the state of health (SOH) at the end of automotive service has to be evaluated differently from the automotive perspective. It will be helpful to consider the driving conditions of EVs in understanding the performance deterioration trend of the battery. In this paper, we acquired the battery status information in real time during driving and compared the characteristics by the driving routes. The SOH from the BMS can be rescaled to percentage ratio to give a more general idea about the performance degradation.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12690195&target=NART&cn=NPAP12690195",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "전기차 배터리의 실시간 주행 데이터 취득과 주행경로별 비교 전기차 배터리의 실시간 주행 데이터 취득과 주행경로별 비교 전기차 배터리의 실시간 주행 데이터 취득과 주행경로별 비교 As the number of electric vehicles (EV) increases, there is an increasing interest in the post-vehicle application of the EV batteries. For the second use application of EV batteries, the state of health (SOH) at the end of automotive service has to be evaluated differently from the automotive perspective. It will be helpful to consider the driving conditions of EVs in understanding the performance deterioration trend of the battery. In this paper, we acquired the battery status information in real time during driving and compared the characteristics by the driving routes. The SOH from the BMS can be rescaled to percentage ratio to give a more general idea about the performance degradation."
        },
        {
          "rank": 31,
          "score": 0.6149165630340576,
          "doc_id": "NART76077564",
          "title": "Performance analysis and framework optimization of open source cloud storage system",
          "abstract": "<P>More and more embedded devices, such as mobile phones, tablet PCs and laptops, are used in every field, so huge files need to be stored or backed up into cloud storage. Optimizing the performance of cloud storage is very important for Internet development. This paper presents the performance evaluation of the open source distributed storage system, a highly available, distributed, eventually consistent object/blob store from OpenStack cloud computing components. This paper mainly focuses on the mechanism of cloud storage as well as the optimization methods to process different sized files. This work provides two major contributions through comprehensive performance evaluations. First, it provides different configurations for OpenStack Swift system and an analysis of how every component affects the performance. Second, it presents the detailed optimization methods to improve the performance in processing different sized files. The experimental results show that our method improves the performance and the structure. We give the methods to optimize the object-based cloud storage system to deploy the readily available storage system.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART76077564&target=NART&cn=NART76077564",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Performance analysis and framework optimization of open source cloud storage system Performance analysis and framework optimization of open source cloud storage system Performance analysis and framework optimization of open source cloud storage system <P>More and more embedded devices, such as mobile phones, tablet PCs and laptops, are used in every field, so huge files need to be stored or backed up into cloud storage. Optimizing the performance of cloud storage is very important for Internet development. This paper presents the performance evaluation of the open source distributed storage system, a highly available, distributed, eventually consistent object/blob store from OpenStack cloud computing components. This paper mainly focuses on the mechanism of cloud storage as well as the optimization methods to process different sized files. This work provides two major contributions through comprehensive performance evaluations. First, it provides different configurations for OpenStack Swift system and an analysis of how every component affects the performance. Second, it presents the detailed optimization methods to improve the performance in processing different sized files. The experimental results show that our method improves the performance and the structure. We give the methods to optimize the object-based cloud storage system to deploy the readily available storage system.</P>"
        },
        {
          "rank": 32,
          "score": 0.6144440174102783,
          "doc_id": "JAKO201713056893580",
          "title": "딥 러닝 프레임워크의 비교 및 분석",
          "abstract": "딥 러닝은 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 딥 러닝은 세상을 이해하고 감지하는 인공지능을 개발하는데 가장 촉망받는 기술이 되고 있으며, 구글, 바이두, 페이스북 등이 가장 앞서서 개발을 하고 있다. 본 논문에서는 딥 러닝을 구현하는 딥 러닝 프레임워크의 종류에 대해 논의하고, 딥 러닝 프레임워크의 영상과 음성 인식 분야의 효율성에 대해 비교, 분석하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201713056893580&target=NART&cn=JAKO201713056893580",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 프레임워크의 비교 및 분석 딥 러닝 프레임워크의 비교 및 분석 딥 러닝 프레임워크의 비교 및 분석 딥 러닝은 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 딥 러닝은 세상을 이해하고 감지하는 인공지능을 개발하는데 가장 촉망받는 기술이 되고 있으며, 구글, 바이두, 페이스북 등이 가장 앞서서 개발을 하고 있다. 본 논문에서는 딥 러닝을 구현하는 딥 러닝 프레임워크의 종류에 대해 논의하고, 딥 러닝 프레임워크의 영상과 음성 인식 분야의 효율성에 대해 비교, 분석하고자 한다."
        },
        {
          "rank": 33,
          "score": 0.6142317056655884,
          "doc_id": "JAKO201014035212851",
          "title": "반송장치에서의 방향제어를 위한 DC모터와 STEP모터 제어기의 성능비교에 관한 연구",
          "abstract": "영구자석의 반발력을 이용한 반송장치에서 캐리어를 받아 다음 스테이션으로 방향을 전환해주기 위한 회전판의 제어를 수행하기 위한 제어기로서 적당한 모터의 선정을 위하여 두 가지 형태의 제어기를 만들고 이를 비교하여 최적의 제어기를 선정하고자 하였다. 즉 스텝모터를 사용한 마이크로스텝방식의 오픈루프 제어와 DC모터를 사용한 폐루프 제어방식의 DC서보 방향제어기를 설계하고 제작하였다. 이 두 방식의 방향제어기의 성능과 제어특성을 비교하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201014035212851&target=NART&cn=JAKO201014035212851",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "반송장치에서의 방향제어를 위한 DC모터와 STEP모터 제어기의 성능비교에 관한 연구 반송장치에서의 방향제어를 위한 DC모터와 STEP모터 제어기의 성능비교에 관한 연구 반송장치에서의 방향제어를 위한 DC모터와 STEP모터 제어기의 성능비교에 관한 연구 영구자석의 반발력을 이용한 반송장치에서 캐리어를 받아 다음 스테이션으로 방향을 전환해주기 위한 회전판의 제어를 수행하기 위한 제어기로서 적당한 모터의 선정을 위하여 두 가지 형태의 제어기를 만들고 이를 비교하여 최적의 제어기를 선정하고자 하였다. 즉 스텝모터를 사용한 마이크로스텝방식의 오픈루프 제어와 DC모터를 사용한 폐루프 제어방식의 DC서보 방향제어기를 설계하고 제작하였다. 이 두 방식의 방향제어기의 성능과 제어특성을 비교하였다."
        },
        {
          "rank": 34,
          "score": 0.6141984462738037,
          "doc_id": "JAKO202512254006340",
          "title": "컨테이너 환경에서 딥러닝 워크로드의 성능 분석",
          "abstract": "최근 딥러닝 워크로드가 컨테이너 환경에서 실행되는 사례가 늘고 있다. 컨테이너는 가상머신에 비해 낮은 오버 헤드와 높은 이식성을 제공하지만, 딥러닝 워크로드의 실행 시 시스템 자원의 비효율적 활용 문제가 발생할 수 있다. 본 논문에서는 컨테이너 환경에서 딥러닝 워크로드 실행으로 인한 오버헤드와 비효율성을 분석하기 위해 시스템콜 및 이벤트 추적 트레이스를 수집 및 분석하였다. 특히, 동일한 워크로드를 호스트 머신에서 직접 실행한 경우와 컨테이너 환경에서 실행한 경우를 비교하여 자원 소비 및 간섭과 관련된 컨테이너 환경의 오버헤드를 정량적으로 확인하였다. 분석 결과 딥러닝 워크로드의 컨테이너 실행 시 성능 병목을 초래하는 주요 원인으로 주기적인 스토리지 플러시 작업이 확인되었으며, 다중 테넌트 환경에서는 자원 경합으로 인해 이러한 문제가 더욱 심화됨을 확인하였다. 본 연구의 결과는 컨테이너 환경에서 딥러닝 워크로드를 효율적으로 실행하기 위한 클라우드 및 엣지 시스템 설계에 중요한 인사이트를 제공할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202512254006340&target=NART&cn=JAKO202512254006340",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "컨테이너 환경에서 딥러닝 워크로드의 성능 분석 컨테이너 환경에서 딥러닝 워크로드의 성능 분석 컨테이너 환경에서 딥러닝 워크로드의 성능 분석 최근 딥러닝 워크로드가 컨테이너 환경에서 실행되는 사례가 늘고 있다. 컨테이너는 가상머신에 비해 낮은 오버 헤드와 높은 이식성을 제공하지만, 딥러닝 워크로드의 실행 시 시스템 자원의 비효율적 활용 문제가 발생할 수 있다. 본 논문에서는 컨테이너 환경에서 딥러닝 워크로드 실행으로 인한 오버헤드와 비효율성을 분석하기 위해 시스템콜 및 이벤트 추적 트레이스를 수집 및 분석하였다. 특히, 동일한 워크로드를 호스트 머신에서 직접 실행한 경우와 컨테이너 환경에서 실행한 경우를 비교하여 자원 소비 및 간섭과 관련된 컨테이너 환경의 오버헤드를 정량적으로 확인하였다. 분석 결과 딥러닝 워크로드의 컨테이너 실행 시 성능 병목을 초래하는 주요 원인으로 주기적인 스토리지 플러시 작업이 확인되었으며, 다중 테넌트 환경에서는 자원 경합으로 인해 이러한 문제가 더욱 심화됨을 확인하였다. 본 연구의 결과는 컨테이너 환경에서 딥러닝 워크로드를 효율적으로 실행하기 위한 클라우드 및 엣지 시스템 설계에 중요한 인사이트를 제공할 수 있을 것으로 기대된다."
        },
        {
          "rank": 35,
          "score": 0.6141797304153442,
          "doc_id": "JAKO202325543363508",
          "title": "딥러닝 영상분석 시스템의 성능평가 산정식 개발",
          "abstract": "도시부 교통정보 수집은 VDS, DSRC, 레이더 등 다양한 시스템에 의해 수집되고 있다. 최근 딥러닝 기술의 발전으로 스마트교차로시스템이 확대 보급되고 있으며 교통량, 속도, 차종 등 다양한 정보수집이 가능하다. 그러나 관련 문헌을 고찰한 결과 지금까지의 성능평가 기준은 딥러닝 영역을 고려하지 않은 RBS기반 평가체계로 '기준값-측정값'의 퍼센트 오차만 고려하고 있어 기존 평가방식으로는 딥러닝 부분의 평가를 수행할 수 없어 새로운 성능평가 방법이 필요하다. 따라서, 본 연구에서는 데이터 비율 및 가중치를 고려하여 Precision과 Recall 등 딥러닝 성능지표를 고려한 오차산정식을 개발하여 개별오차와 구간 오차, 전체오차를 산정하였다. 연구결과, 측정값 1의 오차율은 3.99와 3.54, 측정값 2는 5.34와 5.07로 기존 산정식과 오차율에 차이가 있는 것으로 나타났으며, 반복측정 분석결과 개발 산정식이 우수한 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202325543363508&target=NART&cn=JAKO202325543363508",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 영상분석 시스템의 성능평가 산정식 개발 딥러닝 영상분석 시스템의 성능평가 산정식 개발 딥러닝 영상분석 시스템의 성능평가 산정식 개발 도시부 교통정보 수집은 VDS, DSRC, 레이더 등 다양한 시스템에 의해 수집되고 있다. 최근 딥러닝 기술의 발전으로 스마트교차로시스템이 확대 보급되고 있으며 교통량, 속도, 차종 등 다양한 정보수집이 가능하다. 그러나 관련 문헌을 고찰한 결과 지금까지의 성능평가 기준은 딥러닝 영역을 고려하지 않은 RBS기반 평가체계로 '기준값-측정값'의 퍼센트 오차만 고려하고 있어 기존 평가방식으로는 딥러닝 부분의 평가를 수행할 수 없어 새로운 성능평가 방법이 필요하다. 따라서, 본 연구에서는 데이터 비율 및 가중치를 고려하여 Precision과 Recall 등 딥러닝 성능지표를 고려한 오차산정식을 개발하여 개별오차와 구간 오차, 전체오차를 산정하였다. 연구결과, 측정값 1의 오차율은 3.99와 3.54, 측정값 2는 5.34와 5.07로 기존 산정식과 오차율에 차이가 있는 것으로 나타났으며, 반복측정 분석결과 개발 산정식이 우수한 것으로 나타났다."
        },
        {
          "rank": 36,
          "score": 0.6140454411506653,
          "doc_id": "NART57626890",
          "title": "PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化",
          "abstract": "<P>This study presents an application of the Particle Swarm Optimization (PSO) algorithm to the parameter optimization of rainfall-runoff models. Six global optimization algorithms, the shuffled complex evolution method (SCE-UA), modified SCE-UA, modified SCE-UA with initial value, PSO, modified PSO and modified PSO with initial value, were applied to parameter optimization on four kinds of series tank models. Performance comparison of there algorithms was evaluated and it can be concluded that SCE-UA and PSO show comparable performance in most cases. In addition, PSO is more effective than SCE-UA under the following conditions. 1) the model has large number of parameters, 2) the model has wide range of parameters, 3) calibration period is too short, 4) observation data contains large uncertainty. The modified PSO with initial value shows the most effective and stable performance.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART57626890&target=NART&cn=NART57626890",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化 PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化 PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化 <P>This study presents an application of the Particle Swarm Optimization (PSO) algorithm to the parameter optimization of rainfall-runoff models. Six global optimization algorithms, the shuffled complex evolution method (SCE-UA), modified SCE-UA, modified SCE-UA with initial value, PSO, modified PSO and modified PSO with initial value, were applied to parameter optimization on four kinds of series tank models. Performance comparison of there algorithms was evaluated and it can be concluded that SCE-UA and PSO show comparable performance in most cases. In addition, PSO is more effective than SCE-UA under the following conditions. 1) the model has large number of parameters, 2) the model has wide range of parameters, 3) calibration period is too short, 4) observation data contains large uncertainty. The modified PSO with initial value shows the most effective and stable performance.</P>"
        },
        {
          "rank": 37,
          "score": 0.613818883895874,
          "doc_id": "DIKO0011486112",
          "title": "IT서비스산업 경쟁력 요인이 기업 성과에 미치는 영향에 관한 연구",
          "abstract": "IT서비스산업은 국가 경제사회 전반의 생산성과 효율성을 제고시키는 기반 산업으로서 유비쿼터스 시대의 선도 산업이자, 미래 성장산업으로의 도약과 차세대 해외진출의 주력산업으로 부각되고 있다. IT서비스산업이 성공적으로 발전하기 위해서는 우선 IT서비스의 특성을 충분히 살릴 수 있는 전략과 경쟁력을 갖추는 것이 필요하다. IT서비스 기업뿐만 아니라 IT서비스산업 정책 관점에서도 동 산업의 질적인 발전을 위한 경쟁력 요인을 탐색해 내는 일은 매우 중요하다.&amp;#xD; 본 연구는 IT서비스의 유형을 시스템통합, IT컨설팅, IT아웃소싱 세 영역으로 분류하고 IT서비스의 유형에 따라 경쟁력 요인들이 무엇이며, 또 이들은 재무적, 비재무적 성과와 어떤 연관 관계를 보이는지에 대해 연구하였다. IT서비스 경쟁력에 영향을 주는 독립변수로는 마이클 포터의 다이아몬드 모델을 참조하여 요소조건(Factor Conditions), 수요조건 (Demand Conditions), 연관 및 지원 분야 (Related and Supporting Sectors), 경영 여건(Management environment), 제도 환경(Institutions Eenvironment)으로 구성하여 측정하였다. 총 117부의 설문지를 분석하여 가설을 검증한 결과 8개의 가설이 모두 채택되었다&amp;#xD; 이 결과를 통해 재무적 성과와 비재무적 성과의 연관 관계를 규명한바, 분석 결과를 종합적인 관점 보면, 시스템통합부분의 경쟁력은 비재무적 성과 및 재무적 성과 모두 제도 환경 경쟁력과 관련 지원 경쟁력 제고가 중요한 것으로 볼 수 있다. IT컨설팅부분의 경쟁력은 비재무적 성과 및 재무적 성과 모두 제도 환경 경쟁력, 수요조건 경쟁력, 경영여건 경쟁력이 중요한 요인으로 나타났다. IT아웃소싱 부문에서는 비재무적 성과와 재무적 성과에 동시에 중요한 요인은 관련 지원 경쟁력과 경영여건 경쟁력인 것으로 밝혀졌다.&amp;#xD; 비재무적 성과에 유의미한 영향을 미치는 경쟁력을 살펴보면, 시스템통합 서비스 경쟁력은 관련 지원 경쟁력과 제도 환경 경쟁력, IT컨설팅 서비스 경쟁력에서는 수요조건 경쟁력, 경영여건 경쟁력, 제도 환경 경쟁력이 중요한 요인으로 나타났다. 마지막으로 IT아웃소싱 서비스 경쟁력은 요소조건 경쟁력, 관련 지원 경쟁력, 경영여건 경쟁력, 제도환경 경쟁력으서, 수요조건 경쟁력을 제외하고 모두 비재무적 성과에 유의미한 긍정적 영향을 미치는 것으로 나타났으며, 비재무적 성과는 재무적 성과에 의미 있는 긍정적 영향을 미치는 것으로 파악되었다&amp;#xD; 본 연구에서는 경쟁력 부문 중 요소조건에 포함하는 기술 부문 경쟁력에 대한 심층 분석이 부족했다. 이는 기술을 중심으로 변화하는 산업 특성과 새로운 기술의 개발과 확보가 성장과 발전의 핵심적 동인이 된다는 점에서 향후 기술 경쟁력 부문에 대한 전반적인 연구가 필요하다고 본다.&amp;#xD; 결론적으로 현재 국내 IT서비스 산업 경쟁력은 원천 경쟁력이라 볼 수 있는 기술, 품질, 노하우, 고급인력의 경쟁우위보다는 제도적 환경이 경쟁력 중요하고 분석되었다. 이러한 결과는 우리나라의 IT서비스산업의 성숙도와 무관하지 않다고 본다. 일천한 산업 역사와 정부 정책에 의한 진입장벽, 가격 규제, 그룹사 내부 수주 관행 등 외부 환경의 영향력이 IT서비스 수행능력 보다 기업 성과에 직접적인 영향을 미치는 것으로 풀이된다. 따라서 IT서비스 산업이 건전하게 육성되고 보다 합리적인 시장경쟁을 추구키 위해서는 제도적 환경의 개선과 본원적 경쟁력인 프로젝트 수행 능력이 우선시되는 환경조성이 요구된다 하겠다. 따라서 본 연구는 기업 성과 극대화를 위한 경쟁력 제고 전략 수립과 IT서비스산업 육성 정책 수립에 중요한 자료로 활용될 수 있을 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0011486112&target=NART&cn=DIKO0011486112",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "IT서비스산업 경쟁력 요인이 기업 성과에 미치는 영향에 관한 연구 IT서비스산업 경쟁력 요인이 기업 성과에 미치는 영향에 관한 연구 IT서비스산업 경쟁력 요인이 기업 성과에 미치는 영향에 관한 연구 IT서비스산업은 국가 경제사회 전반의 생산성과 효율성을 제고시키는 기반 산업으로서 유비쿼터스 시대의 선도 산업이자, 미래 성장산업으로의 도약과 차세대 해외진출의 주력산업으로 부각되고 있다. IT서비스산업이 성공적으로 발전하기 위해서는 우선 IT서비스의 특성을 충분히 살릴 수 있는 전략과 경쟁력을 갖추는 것이 필요하다. IT서비스 기업뿐만 아니라 IT서비스산업 정책 관점에서도 동 산업의 질적인 발전을 위한 경쟁력 요인을 탐색해 내는 일은 매우 중요하다.&amp;#xD; 본 연구는 IT서비스의 유형을 시스템통합, IT컨설팅, IT아웃소싱 세 영역으로 분류하고 IT서비스의 유형에 따라 경쟁력 요인들이 무엇이며, 또 이들은 재무적, 비재무적 성과와 어떤 연관 관계를 보이는지에 대해 연구하였다. IT서비스 경쟁력에 영향을 주는 독립변수로는 마이클 포터의 다이아몬드 모델을 참조하여 요소조건(Factor Conditions), 수요조건 (Demand Conditions), 연관 및 지원 분야 (Related and Supporting Sectors), 경영 여건(Management environment), 제도 환경(Institutions Eenvironment)으로 구성하여 측정하였다. 총 117부의 설문지를 분석하여 가설을 검증한 결과 8개의 가설이 모두 채택되었다&amp;#xD; 이 결과를 통해 재무적 성과와 비재무적 성과의 연관 관계를 규명한바, 분석 결과를 종합적인 관점 보면, 시스템통합부분의 경쟁력은 비재무적 성과 및 재무적 성과 모두 제도 환경 경쟁력과 관련 지원 경쟁력 제고가 중요한 것으로 볼 수 있다. IT컨설팅부분의 경쟁력은 비재무적 성과 및 재무적 성과 모두 제도 환경 경쟁력, 수요조건 경쟁력, 경영여건 경쟁력이 중요한 요인으로 나타났다. IT아웃소싱 부문에서는 비재무적 성과와 재무적 성과에 동시에 중요한 요인은 관련 지원 경쟁력과 경영여건 경쟁력인 것으로 밝혀졌다.&amp;#xD; 비재무적 성과에 유의미한 영향을 미치는 경쟁력을 살펴보면, 시스템통합 서비스 경쟁력은 관련 지원 경쟁력과 제도 환경 경쟁력, IT컨설팅 서비스 경쟁력에서는 수요조건 경쟁력, 경영여건 경쟁력, 제도 환경 경쟁력이 중요한 요인으로 나타났다. 마지막으로 IT아웃소싱 서비스 경쟁력은 요소조건 경쟁력, 관련 지원 경쟁력, 경영여건 경쟁력, 제도환경 경쟁력으서, 수요조건 경쟁력을 제외하고 모두 비재무적 성과에 유의미한 긍정적 영향을 미치는 것으로 나타났으며, 비재무적 성과는 재무적 성과에 의미 있는 긍정적 영향을 미치는 것으로 파악되었다&amp;#xD; 본 연구에서는 경쟁력 부문 중 요소조건에 포함하는 기술 부문 경쟁력에 대한 심층 분석이 부족했다. 이는 기술을 중심으로 변화하는 산업 특성과 새로운 기술의 개발과 확보가 성장과 발전의 핵심적 동인이 된다는 점에서 향후 기술 경쟁력 부문에 대한 전반적인 연구가 필요하다고 본다.&amp;#xD; 결론적으로 현재 국내 IT서비스 산업 경쟁력은 원천 경쟁력이라 볼 수 있는 기술, 품질, 노하우, 고급인력의 경쟁우위보다는 제도적 환경이 경쟁력 중요하고 분석되었다. 이러한 결과는 우리나라의 IT서비스산업의 성숙도와 무관하지 않다고 본다. 일천한 산업 역사와 정부 정책에 의한 진입장벽, 가격 규제, 그룹사 내부 수주 관행 등 외부 환경의 영향력이 IT서비스 수행능력 보다 기업 성과에 직접적인 영향을 미치는 것으로 풀이된다. 따라서 IT서비스 산업이 건전하게 육성되고 보다 합리적인 시장경쟁을 추구키 위해서는 제도적 환경의 개선과 본원적 경쟁력인 프로젝트 수행 능력이 우선시되는 환경조성이 요구된다 하겠다. 따라서 본 연구는 기업 성과 극대화를 위한 경쟁력 제고 전략 수립과 IT서비스산업 육성 정책 수립에 중요한 자료로 활용될 수 있을 것이다."
        },
        {
          "rank": 38,
          "score": 0.6135397553443909,
          "doc_id": "NPAP00072266",
          "title": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models",
          "abstract": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP00072266&target=NART&cn=NPAP00072266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error."
        },
        {
          "rank": 39,
          "score": 0.6134915351867676,
          "doc_id": "JAKO202029462558904",
          "title": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식",
          "abstract": "특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202029462558904&target=NART&cn=JAKO202029462558904",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다."
        },
        {
          "rank": 40,
          "score": 0.6133898496627808,
          "doc_id": "ATN0037496660",
          "title": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교",
          "abstract": "Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037496660&target=NART&cn=ATN0037496660",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model."
        },
        {
          "rank": 41,
          "score": 0.6133032441139221,
          "doc_id": "JAKO201518564243648",
          "title": "CC와 ISO 표준에 따른 침입방지시스템의 융합 성능평가 모델",
          "abstract": "침입방지시스템은 네트워크에서 공격 서명을 찾아내어 자동으로 조치를 취하여 비정상적인 트래픽을 중단시키는 보안시스템이다. 수동적인 방어를 하는 침입차단시스템이나 침입탐지시스템과 달리 침입경고 이전에 침입을 중단시키는 개념의 솔루션이다. 침입방지시스템의 보안성 성능은 보안감사, 사용자 데이터 보호, 보안 인증 등에 좌우되며 성능은 탐지시간, 처리량, 공격차단 성능 등에 좌우된다. 본 연구에서는 이러한 침입방지시스템의 보안성 성능평가를 위한 모델을 구축하기 위해 CC(Common Criteria : ISO/IEC 15408)와 소프트웨어 제품평가에 관한 ISO 국제표준을 근간으로 하여 융합 성능평가 모델을 구성하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201518564243648&target=NART&cn=JAKO201518564243648",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "CC와 ISO 표준에 따른 침입방지시스템의 융합 성능평가 모델 CC와 ISO 표준에 따른 침입방지시스템의 융합 성능평가 모델 CC와 ISO 표준에 따른 침입방지시스템의 융합 성능평가 모델 침입방지시스템은 네트워크에서 공격 서명을 찾아내어 자동으로 조치를 취하여 비정상적인 트래픽을 중단시키는 보안시스템이다. 수동적인 방어를 하는 침입차단시스템이나 침입탐지시스템과 달리 침입경고 이전에 침입을 중단시키는 개념의 솔루션이다. 침입방지시스템의 보안성 성능은 보안감사, 사용자 데이터 보호, 보안 인증 등에 좌우되며 성능은 탐지시간, 처리량, 공격차단 성능 등에 좌우된다. 본 연구에서는 이러한 침입방지시스템의 보안성 성능평가를 위한 모델을 구축하기 위해 CC(Common Criteria : ISO/IEC 15408)와 소프트웨어 제품평가에 관한 ISO 국제표준을 근간으로 하여 융합 성능평가 모델을 구성하였다."
        },
        {
          "rank": 42,
          "score": 0.6129319667816162,
          "doc_id": "JAKO200520152774483",
          "title": "컨버전스 제품에 대한 소비자 평가",
          "abstract": "컨버전스 제품들이 기업과 소비자들로부터 관심을 끌고 있는 현실에 비해 이에 대한 학문적인 연구는 부족한 상황이다. 본 연구는 컨버전스 제품을 구성제품간의 다양한 관계를 바탕으로 한 하이테크 단일 복합제품으로 정의하였으며, 컨버전스 제품의 어떤 특성들이 소비자들의 태도와 구매의도에 영향을 미칠 수 있는가에 대해 연구를 시도하였다. 이를 위해 기존의 혁신적인 신제품연구와 묶음제품에 대한 소비자 연구들을 바탕으로 컨버전스 제품관련 혁신특성 변수, 구성특성 변수, 제품 수용자 특성변수를 도출하였으며, 가상의 컨버전스 제품을 대상으로 한 소비자 서베이를 통해 각각의 특성변수들의 영향력을 실증적으로 살펴보았다. 연구결과, 컨버전스 제품에 대한 소비자들의 태도와 구매의도는 컨버전스 제품을 구성하는 각 제품들간의 관계(지각된 일치성, 상대적 이점)에 따라 가장 큰 영향을 받는 것으로 나타났다. 이 외에도 소비자들이 지각하는 기능적&#x00B7;감성적 위험, 지각된 편리성, 그리고 해당 소비자의 기술에 대한 성향(테크노그래픽스) 등의 변수들이 컨버전스 제품에 대한 소비자 태도 및 구매의도에 유의적인 영향을 미치는 것으로 나타났다. 이러한 연구결과와 함께 학문적&#x00B7;실무적인 시사점들이 논의된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200520152774483&target=NART&cn=JAKO200520152774483",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "컨버전스 제품에 대한 소비자 평가 컨버전스 제품에 대한 소비자 평가 컨버전스 제품에 대한 소비자 평가 컨버전스 제품들이 기업과 소비자들로부터 관심을 끌고 있는 현실에 비해 이에 대한 학문적인 연구는 부족한 상황이다. 본 연구는 컨버전스 제품을 구성제품간의 다양한 관계를 바탕으로 한 하이테크 단일 복합제품으로 정의하였으며, 컨버전스 제품의 어떤 특성들이 소비자들의 태도와 구매의도에 영향을 미칠 수 있는가에 대해 연구를 시도하였다. 이를 위해 기존의 혁신적인 신제품연구와 묶음제품에 대한 소비자 연구들을 바탕으로 컨버전스 제품관련 혁신특성 변수, 구성특성 변수, 제품 수용자 특성변수를 도출하였으며, 가상의 컨버전스 제품을 대상으로 한 소비자 서베이를 통해 각각의 특성변수들의 영향력을 실증적으로 살펴보았다. 연구결과, 컨버전스 제품에 대한 소비자들의 태도와 구매의도는 컨버전스 제품을 구성하는 각 제품들간의 관계(지각된 일치성, 상대적 이점)에 따라 가장 큰 영향을 받는 것으로 나타났다. 이 외에도 소비자들이 지각하는 기능적&#x00B7;감성적 위험, 지각된 편리성, 그리고 해당 소비자의 기술에 대한 성향(테크노그래픽스) 등의 변수들이 컨버전스 제품에 대한 소비자 태도 및 구매의도에 유의적인 영향을 미치는 것으로 나타났다. 이러한 연구결과와 함께 학문적&#x00B7;실무적인 시사점들이 논의된다."
        },
        {
          "rank": 43,
          "score": 0.6120675206184387,
          "doc_id": "DIKO0009828582",
          "title": "오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석",
          "abstract": "객체 지향 기술과 관계형 기술은 대부분의 기업에서 어플리케이션을 개발할 때 공통적으로 사용되는 기술이다. 객체 지향 기술은 데이터와 행위를 가진 객체를 통해 어플리케이션 구축을 지원하며, 관계형 기술은 데이터 저장과 프로시저나 SQL를 통한 데이터 조작을 지원한다. 하지만 명확하게 두 기술은 서로 다르다. 이처럼 객체 기술과 관계형 기술을 같이 사용했을 매 발생하는 두 기술간의 불일치를 'object-relational impedance mismatch'라고 한다. 이러한 문제를 해결하기 위해 등장한 기술중의 하나가 바로 ORM(Object/Relational Mapping)이다. 본 논문에서는 ORM 기술을 지원하는 ORM 툴로서의 오픈 소스 자바 프레임 워크를 성능이나 코드 복잡성, 관리 용이성 등 다양한 측면에서 비교 분석하였다. 현재 30여가지가 넘는 다양한 오픈 소스 자바 프레임워크가 개발되어 배포되고 있지만, 본 논문에서는 Hibernate, iBatis SqlMaps, Apache OJB 이렇게 새 개의 프래임워크를 현재 객체 지속성을 위해 가장 많이 사용되는 JDBC 기술을 기준으로 비교 분석하였다. 데이터에 대한 CRUD(저장,추출,수정,삭제)를 수행하는 시간을 통해 성능 분석을 실시하였으며, 사례 어플리케이션 구현을 통해 각 프레임워크 별로 CRUD를 수행하는 메소드 구현 시 코드량 분석을 통해 코드 복잡성을, 요구 사항 변경 시 어떻게 각 프레임워크가 이를 반영하는지를 통해 관리 용이성을 분석하였다. 이러한 분석을 통해 각 프레임워크가 어떠한 서비스를 제공하는지, 각 프레임워크의 성능은 어떠한지 쉽게 알 수 있다. 따라서 기업은 좀더 명확한 근거를 통해 어플리케이션 개발에 적절한 퍼시스턴스 프레임워크를 선택할 수 있을 것이다. 또한 상용과 오픈 소스 기반의 프레임워크 중 어떠한 것을 도입해야 할지 결정해야 할 경우 중요한 참고 자료로 활용할 수 있을 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0009828582&target=NART&cn=DIKO0009828582",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 오픈 소스 자바 퍼시스턴스 프레임워크 비교 분석 객체 지향 기술과 관계형 기술은 대부분의 기업에서 어플리케이션을 개발할 때 공통적으로 사용되는 기술이다. 객체 지향 기술은 데이터와 행위를 가진 객체를 통해 어플리케이션 구축을 지원하며, 관계형 기술은 데이터 저장과 프로시저나 SQL를 통한 데이터 조작을 지원한다. 하지만 명확하게 두 기술은 서로 다르다. 이처럼 객체 기술과 관계형 기술을 같이 사용했을 매 발생하는 두 기술간의 불일치를 'object-relational impedance mismatch'라고 한다. 이러한 문제를 해결하기 위해 등장한 기술중의 하나가 바로 ORM(Object/Relational Mapping)이다. 본 논문에서는 ORM 기술을 지원하는 ORM 툴로서의 오픈 소스 자바 프레임 워크를 성능이나 코드 복잡성, 관리 용이성 등 다양한 측면에서 비교 분석하였다. 현재 30여가지가 넘는 다양한 오픈 소스 자바 프레임워크가 개발되어 배포되고 있지만, 본 논문에서는 Hibernate, iBatis SqlMaps, Apache OJB 이렇게 새 개의 프래임워크를 현재 객체 지속성을 위해 가장 많이 사용되는 JDBC 기술을 기준으로 비교 분석하였다. 데이터에 대한 CRUD(저장,추출,수정,삭제)를 수행하는 시간을 통해 성능 분석을 실시하였으며, 사례 어플리케이션 구현을 통해 각 프레임워크 별로 CRUD를 수행하는 메소드 구현 시 코드량 분석을 통해 코드 복잡성을, 요구 사항 변경 시 어떻게 각 프레임워크가 이를 반영하는지를 통해 관리 용이성을 분석하였다. 이러한 분석을 통해 각 프레임워크가 어떠한 서비스를 제공하는지, 각 프레임워크의 성능은 어떠한지 쉽게 알 수 있다. 따라서 기업은 좀더 명확한 근거를 통해 어플리케이션 개발에 적절한 퍼시스턴스 프레임워크를 선택할 수 있을 것이다. 또한 상용과 오픈 소스 기반의 프레임워크 중 어떠한 것을 도입해야 할지 결정해야 할 경우 중요한 참고 자료로 활용할 수 있을 것이다."
        },
        {
          "rank": 44,
          "score": 0.6110343933105469,
          "doc_id": "DIKO0014477767",
          "title": "머신러닝에 기반한 k-최근접 이웃과 서포트벡터머신 분류기 비교실험 및 평가",
          "abstract": "최근 들어 정보기술의 발전과 더불어 수많은 다양하고 형태의 데이터들이 기하급수적으로 방대하게 늘어나고 있어 많은 복잡한 데이터의 분석에 대한 필요성이 여러 분야에서 대두 되고 있다. 다량의 복잡한 데이터를 분석하고 원하는 결과로 나타내기위한 방법으로 머신러닝이 각광받고 있다. 머신러닝이란 어떠한 명시적인 프로그램 없이도 스스로 학습하는 능력을 갖춘 컴퓨터로 제공되는 데이터 인공지능 분야의 한 종류이며 새로운 데이터가 입력되면 스스로 학습하여 자신을 향상시키고 변화시키는 컴퓨터 프로그램을 바탕으로 동작한다. 머신러닝은 크게 지도학습과 비지도학습으로 분류할 수 있고, 지도학습은 분류와 회귀 등의 알고리즘들이 있고, 비지도학습은 군집화와 밀도추정 등의 알고리즘들이 있다. &amp;#xD; 본 논문에서는 분류기중에서도 많이 사용되어 비교되어지는 k-최근접 이웃(k-Nearest Neighbor: k-NN)과 서포트벡터머신(Support Vector Machine: SVM) 분류기(Classifier) 알고리즘 두 개를 실험하여 비교 평가하였다. 두 알고리즘을 실험하기 위해서 표본 데이터로 어린이집에서 보유한 영유아 건강검진 신체 데이터 중에 나이, 성별, 신장, 몸무게 정보만을 따로 분류 수집하여 이용하였다. 그중에 분류를 위한 수치정보는 신장과 몸무게를 이용하였고 데이터집합의 항목 값으로는 나이를 사용하였다. &amp;#xD; k-NN은 비모수적 밀도추정에 기반을 두고 가장 가까운 k개의 인접 데이터를 취하여 다수결을 통해 해당 데이터집합의 항목 값을 분류할 데이터의 항목 값으로 결정하는 방식이다. SVM은 기본적으로 이진선형분류에 사용되어지며 분류를 원하는 집합 사이의 가장 큰 폭의 경계에 위치한 서포트벡터 데이터를 기준으로 선형경계를 이용하여 두 집단을 분리시키는 알고리즘이다. SVM은 선형분리뿐 아니라 비선형분리에도 사용 할 수 있지만 주어진 데이터를 고차원으로 사상시키는 커널(Kernel) 방법을 사용해야한다. &amp;#xD; 원천수집데이터는 100개뿐이라서 본 실험에서 사용되어지는 500개, 1,000개 그리고 10,000개의 실험을 하기위해서는 데이터를 늘릴 필요가 있었다. 데이터개수를 늘리는 방법으로는 모든 각각의 데이터 임의의 반경 내에서 100의 배수만큼 무작위로 생성시키는 방법을 취하였다. &amp;#xD; k-NN의 알고리즘은 분류할 데이터가 입력 될 때마다 다시 모든 데이터를 학습하는 방식이고 SVM은 미리 학습하여 미리 분리할 경계를 찾고 분류할 대상인 데이터가 입력되면 학습만 별도로 수행이 가능한 알고리즘 방식이다. 비교실험은 두 분류기의 분류하는 소요시간과 분류 정확도를 실험데이터 크기별로 비교 실험 하였다. &amp;#xD; 본 논문을 통하여 두 분류기의 이론적인 내용을 심화 학습하고 분류기라는 공통적인 머신러닝 알고리즘을 각각 실험하여 비교 평가하였다. 비교 평가한 결과물은 다른 종류의 데이터를 분류하고자 할 때 적당한 분류기를 선택 할 수 있는 기준을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014477767&target=NART&cn=DIKO0014477767",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝에 기반한 k-최근접 이웃과 서포트벡터머신 분류기 비교실험 및 평가 머신러닝에 기반한 k-최근접 이웃과 서포트벡터머신 분류기 비교실험 및 평가 머신러닝에 기반한 k-최근접 이웃과 서포트벡터머신 분류기 비교실험 및 평가 최근 들어 정보기술의 발전과 더불어 수많은 다양하고 형태의 데이터들이 기하급수적으로 방대하게 늘어나고 있어 많은 복잡한 데이터의 분석에 대한 필요성이 여러 분야에서 대두 되고 있다. 다량의 복잡한 데이터를 분석하고 원하는 결과로 나타내기위한 방법으로 머신러닝이 각광받고 있다. 머신러닝이란 어떠한 명시적인 프로그램 없이도 스스로 학습하는 능력을 갖춘 컴퓨터로 제공되는 데이터 인공지능 분야의 한 종류이며 새로운 데이터가 입력되면 스스로 학습하여 자신을 향상시키고 변화시키는 컴퓨터 프로그램을 바탕으로 동작한다. 머신러닝은 크게 지도학습과 비지도학습으로 분류할 수 있고, 지도학습은 분류와 회귀 등의 알고리즘들이 있고, 비지도학습은 군집화와 밀도추정 등의 알고리즘들이 있다. &amp;#xD; 본 논문에서는 분류기중에서도 많이 사용되어 비교되어지는 k-최근접 이웃(k-Nearest Neighbor: k-NN)과 서포트벡터머신(Support Vector Machine: SVM) 분류기(Classifier) 알고리즘 두 개를 실험하여 비교 평가하였다. 두 알고리즘을 실험하기 위해서 표본 데이터로 어린이집에서 보유한 영유아 건강검진 신체 데이터 중에 나이, 성별, 신장, 몸무게 정보만을 따로 분류 수집하여 이용하였다. 그중에 분류를 위한 수치정보는 신장과 몸무게를 이용하였고 데이터집합의 항목 값으로는 나이를 사용하였다. &amp;#xD; k-NN은 비모수적 밀도추정에 기반을 두고 가장 가까운 k개의 인접 데이터를 취하여 다수결을 통해 해당 데이터집합의 항목 값을 분류할 데이터의 항목 값으로 결정하는 방식이다. SVM은 기본적으로 이진선형분류에 사용되어지며 분류를 원하는 집합 사이의 가장 큰 폭의 경계에 위치한 서포트벡터 데이터를 기준으로 선형경계를 이용하여 두 집단을 분리시키는 알고리즘이다. SVM은 선형분리뿐 아니라 비선형분리에도 사용 할 수 있지만 주어진 데이터를 고차원으로 사상시키는 커널(Kernel) 방법을 사용해야한다. &amp;#xD; 원천수집데이터는 100개뿐이라서 본 실험에서 사용되어지는 500개, 1,000개 그리고 10,000개의 실험을 하기위해서는 데이터를 늘릴 필요가 있었다. 데이터개수를 늘리는 방법으로는 모든 각각의 데이터 임의의 반경 내에서 100의 배수만큼 무작위로 생성시키는 방법을 취하였다. &amp;#xD; k-NN의 알고리즘은 분류할 데이터가 입력 될 때마다 다시 모든 데이터를 학습하는 방식이고 SVM은 미리 학습하여 미리 분리할 경계를 찾고 분류할 대상인 데이터가 입력되면 학습만 별도로 수행이 가능한 알고리즘 방식이다. 비교실험은 두 분류기의 분류하는 소요시간과 분류 정확도를 실험데이터 크기별로 비교 실험 하였다. &amp;#xD; 본 논문을 통하여 두 분류기의 이론적인 내용을 심화 학습하고 분류기라는 공통적인 머신러닝 알고리즘을 각각 실험하여 비교 평가하였다. 비교 평가한 결과물은 다른 종류의 데이터를 분류하고자 할 때 적당한 분류기를 선택 할 수 있는 기준을 제시한다."
        },
        {
          "rank": 45,
          "score": 0.6086039543151855,
          "doc_id": "JAKO201124359117833",
          "title": "강하비행시의 연료소모량 비교분석",
          "abstract": "A Continuous Descent Approach(CDA) is defined as a simple, cost-effective, noise and emission abatement technique for any powered aircraft on approach. CDA also can be optimised within energy, speed and safety constraints by avoiding unnecessary flap, air brake and engine thrust. This study includes comparison on fuel consumption between Continuous Descent type and Step Down type by using flight data. In particular, we investigated fuel flow per hour, calibrated airspeed and pressure altitude for all flight time. During descent flight, the fuel consumption of Continuous Descent type was less than the fuel consumption of Step Down type.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201124359117833&target=NART&cn=JAKO201124359117833",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "강하비행시의 연료소모량 비교분석 강하비행시의 연료소모량 비교분석 강하비행시의 연료소모량 비교분석 A Continuous Descent Approach(CDA) is defined as a simple, cost-effective, noise and emission abatement technique for any powered aircraft on approach. CDA also can be optimised within energy, speed and safety constraints by avoiding unnecessary flap, air brake and engine thrust. This study includes comparison on fuel consumption between Continuous Descent type and Step Down type by using flight data. In particular, we investigated fuel flow per hour, calibrated airspeed and pressure altitude for all flight time. During descent flight, the fuel consumption of Continuous Descent type was less than the fuel consumption of Step Down type."
        },
        {
          "rank": 46,
          "score": 0.6068414449691772,
          "doc_id": "JAKO202205843203328",
          "title": "머신러닝과 딥러닝을 이용한 영산강의 Chlorophyll-a 예측 성능 비교 및 변화 요인 분석",
          "abstract": "The Yeongsan River, one of the four largest rivers in South Korea, has been facing difficulties with water quality management with respect to algal bloom. The algal bloom menace has become bigger, especially after the construction of two weirs in the mainstream of the Yeongsan River. Therefore, the prediction and factor analysis of Chlorophyll-a (Chl-a) concentration is needed for effective water quality management. In this study, Chl-a prediction model was developed, and the performance evaluated using machine and deep learning methods, such as Deep Neural Network (DNN), Random Forest (RF), and eXtreme Gradient Boosting (XGBoost). Moreover, the correlation analysis and the feature importance results were compared to identify the major factors affecting the concentration of Chl-a. All models showed high prediction performance with an R<sup>2</sup> value of 0.9 or higher. In particular, XGBoost showed the highest prediction accuracy of 0.95 in the test data.The results of feature importance suggested that Ammonia (NH<sub>3</sub>-N) and Phosphate (PO<sub>4</sub>-P) were common major factors for the three models to manage Chl-a concentration. From the results, it was confirmed that three machine learning methods, DNN, RF, and XGBoost are powerful methods for predicting water quality parameters. Also, the comparison between feature importance and correlation analysis would present a more accurate assessment of the important major factors.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202205843203328&target=NART&cn=JAKO202205843203328",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝과 딥러닝을 이용한 영산강의 Chlorophyll-a 예측 성능 비교 및 변화 요인 분석 머신러닝과 딥러닝을 이용한 영산강의 Chlorophyll-a 예측 성능 비교 및 변화 요인 분석 머신러닝과 딥러닝을 이용한 영산강의 Chlorophyll-a 예측 성능 비교 및 변화 요인 분석 The Yeongsan River, one of the four largest rivers in South Korea, has been facing difficulties with water quality management with respect to algal bloom. The algal bloom menace has become bigger, especially after the construction of two weirs in the mainstream of the Yeongsan River. Therefore, the prediction and factor analysis of Chlorophyll-a (Chl-a) concentration is needed for effective water quality management. In this study, Chl-a prediction model was developed, and the performance evaluated using machine and deep learning methods, such as Deep Neural Network (DNN), Random Forest (RF), and eXtreme Gradient Boosting (XGBoost). Moreover, the correlation analysis and the feature importance results were compared to identify the major factors affecting the concentration of Chl-a. All models showed high prediction performance with an R<sup>2</sup> value of 0.9 or higher. In particular, XGBoost showed the highest prediction accuracy of 0.95 in the test data.The results of feature importance suggested that Ammonia (NH<sub>3</sub>-N) and Phosphate (PO<sub>4</sub>-P) were common major factors for the three models to manage Chl-a concentration. From the results, it was confirmed that three machine learning methods, DNN, RF, and XGBoost are powerful methods for predicting water quality parameters. Also, the comparison between feature importance and correlation analysis would present a more accurate assessment of the important major factors."
        },
        {
          "rank": 47,
          "score": 0.6068016290664673,
          "doc_id": "JAKO202313933270962",
          "title": "딥 러닝 기반 이미지 압축 기법의 성능 비교 분석",
          "abstract": "Image compression is a fundamental technique in the field of digital image processing, which will help to decrease the storage space and to transmit the files efficiently. Recently many deep learning techniques have been proposed to promise results on image compression field. Since many image compression techniques have artifact problems, this paper has compared two deep learning approaches to verify their performance experimentally to solve the problems. One of the approaches is a deep autoencoder technique, and another is a deep convolutional neural network (CNN). For those results in the performance of peak signal-to-noise and root mean square error, this paper shows that deep autoencoder method has more advantages than deep CNN approach.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202313933270962&target=NART&cn=JAKO202313933270962",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 기반 이미지 압축 기법의 성능 비교 분석 딥 러닝 기반 이미지 압축 기법의 성능 비교 분석 딥 러닝 기반 이미지 압축 기법의 성능 비교 분석 Image compression is a fundamental technique in the field of digital image processing, which will help to decrease the storage space and to transmit the files efficiently. Recently many deep learning techniques have been proposed to promise results on image compression field. Since many image compression techniques have artifact problems, this paper has compared two deep learning approaches to verify their performance experimentally to solve the problems. One of the approaches is a deep autoencoder technique, and another is a deep convolutional neural network (CNN). For those results in the performance of peak signal-to-noise and root mean square error, this paper shows that deep autoencoder method has more advantages than deep CNN approach."
        },
        {
          "rank": 48,
          "score": 0.606031596660614,
          "doc_id": "NART111572458",
          "title": "광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교",
          "abstract": "This study examines the classification characteristics of various machine learning classifiers for pain assessment using photoplethysmogram. The presence of pain was assessed using waveform characteristics derived from photoplethysmogram obtained from 73 patients before and after surgery. Classification performance was evaluated using logistic regression, random forest, multi-layer perceptron, and 1-D convolutional neural network, and was validated with nested k-fold cross validation. As a result, pain classification accuracy was highest in order of logistic regression, convolutional neural network, multi-layer perceptron, and random forest classifier. In addition, logistic regression, random forest, multi-layer perceptron, and convolutional neural network were shown to be robust to overfitting in order.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART111572458&target=NART&cn=NART111572458",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교 광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교 광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교 This study examines the classification characteristics of various machine learning classifiers for pain assessment using photoplethysmogram. The presence of pain was assessed using waveform characteristics derived from photoplethysmogram obtained from 73 patients before and after surgery. Classification performance was evaluated using logistic regression, random forest, multi-layer perceptron, and 1-D convolutional neural network, and was validated with nested k-fold cross validation. As a result, pain classification accuracy was highest in order of logistic regression, convolutional neural network, multi-layer perceptron, and random forest classifier. In addition, logistic regression, random forest, multi-layer perceptron, and convolutional neural network were shown to be robust to overfitting in order."
        },
        {
          "rank": 49,
          "score": 0.6049996614456177,
          "doc_id": "JAKO202517036003495",
          "title": "사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크",
          "abstract": "본 연구는 사이버 복원력을 정량적으로 평가하기 위한 지표 선정 프레임워크를 제시한다. 정보보호 및 개인정보보호 관리체계 인증기관을 기반으로 다양한 서비스 유형의 특성을 분석하고 복원력 측정을 위한 지표 후보군을 도출한다. 선정된 후보 지표는 본 연구에서 정의한 객관성, 재현성, 확장성, 실용성, 복원력 반영성의 다섯가지 핵심 특성을 기준으로 타당성을 검증한다. 또한 지표간 상호 배타성(ME)과 전체 포괄성(CE) 원칙을 보완 기준으로 적용하여 중복성을 제거하고 평가 체계의 범용성을 확보하였다. 최종적으로 12개의 정량적 평가 지표를 선정하였으며 이 중 웹서비스와 같은 트랜잭션 기반 시스템에서의 초당 처리 건수를 나타내는 TPS 지표를 중심으로, TCP SYN flooding 공격 상황에서의 지표의 변화와 해석방법에 대한 실증적 분석을 수행하였다. 제안한 지표선정 프레임워크와 정량적 평가지표는 사이버 복원력을 객관적으로 측정할 수 있는 평가 체계 수립에 기여하며, 지속적으로 변화하는 사이버 위협에 효과적인 대응과 개선을 위한 기준을 제공한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202517036003495&target=NART&cn=JAKO202517036003495",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크 사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크 사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크 본 연구는 사이버 복원력을 정량적으로 평가하기 위한 지표 선정 프레임워크를 제시한다. 정보보호 및 개인정보보호 관리체계 인증기관을 기반으로 다양한 서비스 유형의 특성을 분석하고 복원력 측정을 위한 지표 후보군을 도출한다. 선정된 후보 지표는 본 연구에서 정의한 객관성, 재현성, 확장성, 실용성, 복원력 반영성의 다섯가지 핵심 특성을 기준으로 타당성을 검증한다. 또한 지표간 상호 배타성(ME)과 전체 포괄성(CE) 원칙을 보완 기준으로 적용하여 중복성을 제거하고 평가 체계의 범용성을 확보하였다. 최종적으로 12개의 정량적 평가 지표를 선정하였으며 이 중 웹서비스와 같은 트랜잭션 기반 시스템에서의 초당 처리 건수를 나타내는 TPS 지표를 중심으로, TCP SYN flooding 공격 상황에서의 지표의 변화와 해석방법에 대한 실증적 분석을 수행하였다. 제안한 지표선정 프레임워크와 정량적 평가지표는 사이버 복원력을 객관적으로 측정할 수 있는 평가 체계 수립에 기여하며, 지속적으로 변화하는 사이버 위협에 효과적인 대응과 개선을 위한 기준을 제공한다."
        },
        {
          "rank": 50,
          "score": 0.6048649549484253,
          "doc_id": "JAKO202231363876883",
          "title": "인공지능 서비스 운영을 위한 시스템 측면에서의 연구",
          "abstract": "AI 기술을 활용한 다양한 서비스가 개발되면서, AI 서비스 운영에 많은 관심이 집중되고 있다. 최근에는 AI 기술도 하나의 ICT 서비스를 보고, 범용적인 AI 서비스 운영을 위한 연구가 많이 진행되고 있다. 본 논문에서는 일반적인 기계학습 개발 절차의 마지막 단계인 기계학습 모델 배포 및 운영에 초점을 두고 AI 서비스 운영을 위한 시스템 측면에서의 연구 결과를 기술하였다. 3대의 서로 다른 Ubuntu 시스템을 구축하고, 이 시스템상에서 서로 다른 AI 모델(RFCN, SSD-Mobilenet)과 서로 다른 통신 방식(gRPC, REST)의 조합으로 2017 validation COCO dataset의 데이터를 이용하여 객체 검출 서비스를 Tensorflow serving을 통하여 AI 서비스를 요청하는 부분과 AI 서비스를 수행하는 부분으로 나누어 실험하였다. 다양한 실험을 통하여 AI 모델의 종류가 AI 머신의 통신 방식보다 AI 서비스 추론 시간에 더 큰 영향을 미치고, 객체 검출 AI 서비스의 경우 검출하려는 이미지의 파일 크기보다는 이미지 내의 객체 개수와 복잡도에 따라 AI 서비스 추론 시간이 더 큰 영향을 받는다는 것을 알 수 있었다. 그리고, AI 서비스를 로컬이 아닌 원격에서 수행하면 성능이 좋은 머신이라고 하더라도 로컬에서 수행하는 경우보다 AI 서비스 추론 시간이 더 걸린다는 것을 확인할 수 있었다. 본 연구 결과를 통하여 서비스 목표에 적합한 시스템 설계와 AI 모델 개발 및 효율적인 AI 서비스 운영이 가능해질 것으로 본다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202231363876883&target=NART&cn=JAKO202231363876883",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "인공지능 서비스 운영을 위한 시스템 측면에서의 연구 인공지능 서비스 운영을 위한 시스템 측면에서의 연구 인공지능 서비스 운영을 위한 시스템 측면에서의 연구 AI 기술을 활용한 다양한 서비스가 개발되면서, AI 서비스 운영에 많은 관심이 집중되고 있다. 최근에는 AI 기술도 하나의 ICT 서비스를 보고, 범용적인 AI 서비스 운영을 위한 연구가 많이 진행되고 있다. 본 논문에서는 일반적인 기계학습 개발 절차의 마지막 단계인 기계학습 모델 배포 및 운영에 초점을 두고 AI 서비스 운영을 위한 시스템 측면에서의 연구 결과를 기술하였다. 3대의 서로 다른 Ubuntu 시스템을 구축하고, 이 시스템상에서 서로 다른 AI 모델(RFCN, SSD-Mobilenet)과 서로 다른 통신 방식(gRPC, REST)의 조합으로 2017 validation COCO dataset의 데이터를 이용하여 객체 검출 서비스를 Tensorflow serving을 통하여 AI 서비스를 요청하는 부분과 AI 서비스를 수행하는 부분으로 나누어 실험하였다. 다양한 실험을 통하여 AI 모델의 종류가 AI 머신의 통신 방식보다 AI 서비스 추론 시간에 더 큰 영향을 미치고, 객체 검출 AI 서비스의 경우 검출하려는 이미지의 파일 크기보다는 이미지 내의 객체 개수와 복잡도에 따라 AI 서비스 추론 시간이 더 큰 영향을 받는다는 것을 알 수 있었다. 그리고, AI 서비스를 로컬이 아닌 원격에서 수행하면 성능이 좋은 머신이라고 하더라도 로컬에서 수행하는 경우보다 AI 서비스 추론 시간이 더 걸린다는 것을 확인할 수 있었다. 본 연구 결과를 통하여 서비스 목표에 적합한 시스템 설계와 AI 모델 개발 및 효율적인 AI 서비스 운영이 가능해질 것으로 본다."
        }
      ]
    }
  ],
  "meta": {
    "model": "gemini-2.5-flash",
    "temperature": 0.2
  }
}