{
  "id": "row_000021",
  "model_name": "Alibaba-NLP/gte-multilingual-base",
  "timestamp_kst": "2025-09-08T23:55:34.962841+09:00",
  "trial_id": "3900048b",
  "queries": [
    {
      "query": "Could you summarize how deep learning techniques are envisioned to enhance radar imaging and the main challenges and proposed network architecture discussed?",
      "query_meta": {
        "type": "original"
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8218663930892944,
          "doc_id": "NPAP12546494",
          "title": "Deep learning for radar",
          "abstract": "<P>Motivated by the recent advances in deep learning, we lay out a vision of how deep learning techniques can be used in radar. Specifically, our discussion focuses on the use of deep learning to advance the state-of-the-art in radar imaging. While deep learning can be directly applied to automatic target recognition (ATR), the relevance of these techniques in other radar problems is not obvious. We argue that deep learning can play a central role in advancing the state-of-the-art in a wide range of radar imaging problems, discuss the challenges associated with applying these methods, and the potential advancements that are expected. We lay out an approach to design a network architecture based on the specific structure of the synthetic aperture radar (SAR) imaging problem that augments learning with traditional SAR modelling. This framework allows for capture of the non-linearity of the SAR forward model. Furthermore, we demonstrate how this process can be used to learn and compensate for trajectory based phase error for the autofocus problem.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12546494&target=NART&cn=NPAP12546494",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning for radar Deep learning for radar Deep learning for radar <P>Motivated by the recent advances in deep learning, we lay out a vision of how deep learning techniques can be used in radar. Specifically, our discussion focuses on the use of deep learning to advance the state-of-the-art in radar imaging. While deep learning can be directly applied to automatic target recognition (ATR), the relevance of these techniques in other radar problems is not obvious. We argue that deep learning can play a central role in advancing the state-of-the-art in a wide range of radar imaging problems, discuss the challenges associated with applying these methods, and the potential advancements that are expected. We lay out an approach to design a network architecture based on the specific structure of the synthetic aperture radar (SAR) imaging problem that augments learning with traditional SAR modelling. This framework allows for capture of the non-linearity of the SAR forward model. Furthermore, we demonstrate how this process can be used to learn and compensate for trajectory based phase error for the autofocus problem.</P>"
        },
        {
          "rank": 2,
          "score": 0.7698138952255249,
          "doc_id": "NART116403822",
          "title": "Deep-Learning for Radar: A Survey",
          "abstract": "<P>A comprehensive and well-structured review on the application of deep learning (DL) based algorithms, such as convolutional neural networks (CNN) and long-short term memory (LSTM), in radar signal processing is given. The following DL application areas are covered: i) radar waveform and antenna array design; ii) passive or low probability of interception (LPI) radar waveform recognition; iii) automatic target recognition (ATR) based on high range resolution profiles (HRRPs), Doppler signatures, and synthetic aperture radar (SAR) images; and iv) radar jamming/clutter recognition and suppression. Although DL is unanimously praised as the ultimate solution to many bottleneck problems in most of existing works on similar topics, both the positive and the negative sides of stories about DL are checked in this work. Specifically, two limiting factors of the real-life performance of deep neural networks (DNNs), limited training samples and adversarial examples, are thoroughly examined. By investigating the relationship between the DL-based algorithms proposed in various papers and linking them together to form a full picture, this work serves as a valuable source for researchers who are seeking potential research opportunities in this promising research field.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART116403822&target=NART&cn=NART116403822",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep-Learning for Radar: A Survey Deep-Learning for Radar: A Survey Deep-Learning for Radar: A Survey <P>A comprehensive and well-structured review on the application of deep learning (DL) based algorithms, such as convolutional neural networks (CNN) and long-short term memory (LSTM), in radar signal processing is given. The following DL application areas are covered: i) radar waveform and antenna array design; ii) passive or low probability of interception (LPI) radar waveform recognition; iii) automatic target recognition (ATR) based on high range resolution profiles (HRRPs), Doppler signatures, and synthetic aperture radar (SAR) images; and iv) radar jamming/clutter recognition and suppression. Although DL is unanimously praised as the ultimate solution to many bottleneck problems in most of existing works on similar topics, both the positive and the negative sides of stories about DL are checked in this work. Specifically, two limiting factors of the real-life performance of deep neural networks (DNNs), limited training samples and adversarial examples, are thoroughly examined. By investigating the relationship between the DL-based algorithms proposed in various papers and linking them together to form a full picture, this work serves as a valuable source for researchers who are seeking potential research opportunities in this promising research field.</P>"
        },
        {
          "rank": 3,
          "score": 0.76008141040802,
          "doc_id": "NART121030945",
          "title": "MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning",
          "abstract": "<P>Transmitting orthogonal waveforms are the basis for giving full play to the advantages of MIMO radar imaging technology, but the commonly used waveforms with the same frequency cannot meet the orthogonality requirement, resulting in serious coupling noise in traditional imaging methods and affecting the imaging effect. In order to effectively suppress the mutual coupling interference caused by non-orthogonal waveforms, a new non-orthogonal waveform MIMO radar imaging method based on deep learning is proposed in this paper: with the powerful nonlinear fitting ability of deep learning, the mapping relationship between the non-orthogonal waveform MIMO radar echo and ideal target image is automatically learned by constructing a deep imaging network and training on a large number of simulated training data. The learned imaging network can effectively suppress the coupling interference between non-ideal orthogonal waveforms and improve the imaging quality of MIMO radar. Finally, the effectiveness of the proposed method is verified by experiments with point scattering model data and electromagnetic scattering calculation data.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART121030945&target=NART&cn=NART121030945",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning <P>Transmitting orthogonal waveforms are the basis for giving full play to the advantages of MIMO radar imaging technology, but the commonly used waveforms with the same frequency cannot meet the orthogonality requirement, resulting in serious coupling noise in traditional imaging methods and affecting the imaging effect. In order to effectively suppress the mutual coupling interference caused by non-orthogonal waveforms, a new non-orthogonal waveform MIMO radar imaging method based on deep learning is proposed in this paper: with the powerful nonlinear fitting ability of deep learning, the mapping relationship between the non-orthogonal waveform MIMO radar echo and ideal target image is automatically learned by constructing a deep imaging network and training on a large number of simulated training data. The learned imaging network can effectively suppress the coupling interference between non-ideal orthogonal waveforms and improve the imaging quality of MIMO radar. Finally, the effectiveness of the proposed method is verified by experiments with point scattering model data and electromagnetic scattering calculation data.</P>"
        },
        {
          "rank": 4,
          "score": 0.7459375262260437,
          "doc_id": "NART106334316",
          "title": "Deep learning for waveform estimation and imaging in passive radar",
          "abstract": "<P>The authors consider a bistatic configuration with a stationary transmitter transmitting unknown waveforms of opportunity and a single moving receiver and present a deep learning (DL) framework for passive synthetic aperture radar (SAR) imaging. They approach DL from an optimisation based perspective and formulate image reconstruction as a machine learning task. By unfolding the iterations of a proximal gradient descent algorithm, they construct a deep recurrent neural network (RNN) that is parameterised by the transmitted waveforms. They cascade the RNN structure with a decoder stage to form a recurrent auto&#x2010;encoder architecture. They then use backpropagation to learn transmitted waveforms by training the network in an unsupervised manner using SAR measurements. The highly non&#x2010;convex problem of backpropagation is guided to a feasible solution over the parameter space by initialising the network with the known components of the SAR forward model. Moreover, prior information regarding the waveform structure is incorporated during initialisation and backpropagation. They demonstrate the effectiveness of the DL&#x2010;based approach through numerical simulations that show focused, high contrast imagery using a single receiver antenna at realistic signal&#x2010;to&#x2010;noise&#x2010;ratio levels.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106334316&target=NART&cn=NART106334316",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning for waveform estimation and imaging in passive radar Deep learning for waveform estimation and imaging in passive radar Deep learning for waveform estimation and imaging in passive radar <P>The authors consider a bistatic configuration with a stationary transmitter transmitting unknown waveforms of opportunity and a single moving receiver and present a deep learning (DL) framework for passive synthetic aperture radar (SAR) imaging. They approach DL from an optimisation based perspective and formulate image reconstruction as a machine learning task. By unfolding the iterations of a proximal gradient descent algorithm, they construct a deep recurrent neural network (RNN) that is parameterised by the transmitted waveforms. They cascade the RNN structure with a decoder stage to form a recurrent auto&#x2010;encoder architecture. They then use backpropagation to learn transmitted waveforms by training the network in an unsupervised manner using SAR measurements. The highly non&#x2010;convex problem of backpropagation is guided to a feasible solution over the parameter space by initialising the network with the known components of the SAR forward model. Moreover, prior information regarding the waveform structure is incorporated during initialisation and backpropagation. They demonstrate the effectiveness of the DL&#x2010;based approach through numerical simulations that show focused, high contrast imagery using a single receiver antenna at realistic signal&#x2010;to&#x2010;noise&#x2010;ratio levels.</P>"
        },
        {
          "rank": 5,
          "score": 0.740979790687561,
          "doc_id": "NART127041948",
          "title": "Deep Learning Techniques in Radar Emitter Identification",
          "abstract": "<P>In the field of electronic warfare (EW), one of the crucial roles of electronic intelligence is the identification of radar signals. In an operational environment, it is very essential to identify radar emitters whether friend or foe so that appropriate radar countermeasures can be taken against them. With the electromagnetic environment becoming increasingly complex and the diversity of signal features, radar emitter identification with high recognition accuracy has become a significantly challenging task. Traditional radar identification methods have shown some limitations in this complex electromagnetic scenario. Several radar classification and identification methods based on artificial neural networks have emerged with the emergence of artificial neural networks, notably deep learning approaches. Machine learning and deep learning algorithms are now frequently utilized to extract various types of information from radar signals more accurately and robustly. This paper illustrates the use of Deep Neural Networks (DNN) in radar applications for emitter classification and identification. Since deep learning approaches are capable of accurately classifying complicated patterns in radar signals, they have demonstrated significant promise for identifying radar emitters. By offering a thorough literature analysis of deep learning-based methodologies, the study intends to assist researchers and practitioners in better understanding the application of deep learning techniques to challenges related to the classification and identification of radar emitters. The study demonstrates that DNN can be used successfully in applications for radar classification and identification.&amp;#xD; &amp;#xD; </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART127041948&target=NART&cn=NART127041948",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning Techniques in Radar Emitter Identification Deep Learning Techniques in Radar Emitter Identification Deep Learning Techniques in Radar Emitter Identification <P>In the field of electronic warfare (EW), one of the crucial roles of electronic intelligence is the identification of radar signals. In an operational environment, it is very essential to identify radar emitters whether friend or foe so that appropriate radar countermeasures can be taken against them. With the electromagnetic environment becoming increasingly complex and the diversity of signal features, radar emitter identification with high recognition accuracy has become a significantly challenging task. Traditional radar identification methods have shown some limitations in this complex electromagnetic scenario. Several radar classification and identification methods based on artificial neural networks have emerged with the emergence of artificial neural networks, notably deep learning approaches. Machine learning and deep learning algorithms are now frequently utilized to extract various types of information from radar signals more accurately and robustly. This paper illustrates the use of Deep Neural Networks (DNN) in radar applications for emitter classification and identification. Since deep learning approaches are capable of accurately classifying complicated patterns in radar signals, they have demonstrated significant promise for identifying radar emitters. By offering a thorough literature analysis of deep learning-based methodologies, the study intends to assist researchers and practitioners in better understanding the application of deep learning techniques to challenges related to the classification and identification of radar emitters. The study demonstrates that DNN can be used successfully in applications for radar classification and identification.&amp;#xD; &amp;#xD; </P>"
        },
        {
          "rank": 6,
          "score": 0.7375428676605225,
          "doc_id": "DIKO0016954237",
          "title": "밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구",
          "abstract": "본 논문에서는 합성개구레이더 (Synthetic Aperture Radar, SAR) 시스템의 데이터를 획득하는 과정에서 발생할 수 있는 위상 오차를 보상하기 위해, 딥러닝 기반 요동 보상 방법으로 Unsupervised Image-to-image Translation (UNIT) 네트워크를 제안한다. 일반적으로 SAR 시스템을 이용한 데이터 취득 과정에서 레이더가 부착된 플랫폼의 비이상적인 경로나 불안정한 자세로 인해 위상 오차가 포함된 데이터를 얻는 문제가 발생할 수 있다. 이러한 위상 오차는 주변 환경 인식 및 표적 탐지 성능을 감소시키며, 군사 목적의 감시, 정찰을 위한 SAR 시스템과 자율주행 분야에서 지능형 차량의 경로 계획 및 사고 위협 회피를 위해 주변 환경 표현이 필수적이다. 따라서, 본 연구에서는 딥러닝 기반 요동 보상 방법을 제안하고, 밀리미터파 레이더 센서를 이용한 실험을 통해 제안된 방법의 성능을 검증한다. 제안된 방법은 Peak Signal-to-Noise Ratio (PSNR)와 Structural Similarity Index Measure (SSIM) 측면에서 기존의 요동 보상 기법들과 성능 평가 및 비교가 수행된다. 실제 측정 데이터를 기반으로 성능을 비교한 결과, 제안된 UNIT 네트워크는 기존 요동 보상 기법들 대비 PSNR은 평균 10.17%, SSIM은 9.4% 향상되는 것을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016954237&target=NART&cn=DIKO0016954237",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 본 논문에서는 합성개구레이더 (Synthetic Aperture Radar, SAR) 시스템의 데이터를 획득하는 과정에서 발생할 수 있는 위상 오차를 보상하기 위해, 딥러닝 기반 요동 보상 방법으로 Unsupervised Image-to-image Translation (UNIT) 네트워크를 제안한다. 일반적으로 SAR 시스템을 이용한 데이터 취득 과정에서 레이더가 부착된 플랫폼의 비이상적인 경로나 불안정한 자세로 인해 위상 오차가 포함된 데이터를 얻는 문제가 발생할 수 있다. 이러한 위상 오차는 주변 환경 인식 및 표적 탐지 성능을 감소시키며, 군사 목적의 감시, 정찰을 위한 SAR 시스템과 자율주행 분야에서 지능형 차량의 경로 계획 및 사고 위협 회피를 위해 주변 환경 표현이 필수적이다. 따라서, 본 연구에서는 딥러닝 기반 요동 보상 방법을 제안하고, 밀리미터파 레이더 센서를 이용한 실험을 통해 제안된 방법의 성능을 검증한다. 제안된 방법은 Peak Signal-to-Noise Ratio (PSNR)와 Structural Similarity Index Measure (SSIM) 측면에서 기존의 요동 보상 기법들과 성능 평가 및 비교가 수행된다. 실제 측정 데이터를 기반으로 성능을 비교한 결과, 제안된 UNIT 네트워크는 기존 요동 보상 기법들 대비 PSNR은 평균 10.17%, SSIM은 9.4% 향상되는 것을 확인하였다."
        },
        {
          "rank": 7,
          "score": 0.7281338572502136,
          "doc_id": "JAKO201923233204235",
          "title": "딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구",
          "abstract": "Classification of radar signals in the field of electronic warfare is a problem of discriminating threat types by analyzing enemy threat radar signals such as aircraft, radar, and missile received through electronic warfare equipment. Recent radar systems have adopted a variety of modulation schemes that are different from those used in conventional systems, and are often difficult to analyze using existing algorithms. Also, it is necessary to design a robust algorithm for the signal received in the real environment due to the environmental influence and the measurement error due to the characteristics of the hardware. In this paper, we propose a radar signal classification method which are not affected by radar signal modulation methods and noise generation by using deep learning techniques.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201923233204235&target=NART&cn=JAKO201923233204235",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 Classification of radar signals in the field of electronic warfare is a problem of discriminating threat types by analyzing enemy threat radar signals such as aircraft, radar, and missile received through electronic warfare equipment. Recent radar systems have adopted a variety of modulation schemes that are different from those used in conventional systems, and are often difficult to analyze using existing algorithms. Also, it is necessary to design a robust algorithm for the signal received in the real environment due to the environmental influence and the measurement error due to the characteristics of the hardware. In this paper, we propose a radar signal classification method which are not affected by radar signal modulation methods and noise generation by using deep learning techniques."
        },
        {
          "rank": 8,
          "score": 0.7238141298294067,
          "doc_id": "NART110796699",
          "title": "Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network",
          "abstract": "<P>As compared with traditional ISAR imaging methods, the compressive sensing (CS)&#x2010;based imaging methods can obtain high&#x2010;quality images using much less under&#x2010;sampled data. However, the availability or appropriateness of the sparse representation of the target scene and the relatively low computational efficiency of image reconstruction algorithms limit the performance and application of the CS&#x2010;based ISAR imaging methods. In recent years, the deep learning technology has been applied in many fields and achieved outstanding performance in image classification, image reconstruction etc. DL implements the tasks using the deep neural network (DNN), which composes multiple hidden layers and non&#x2010;linear activation layer. In this study, a novel ISAR imaging method that uses a complex&#x2010;value deep neural network (CV&#x2010;DNN) to perform the image formation using under&#x2010;sampled data is proposed. The CV&#x2010;DNN architecture can extract and exploit the sparse feature of the target image extremely well by multilayer non&#x2010;linear processing. The experimental results show that the proposed CV&#x2010;DNN&#x2010;based ISAR imaging method can provide better shape reconstruction of target with less data than state&#x2010;of&#x2010;the&#x2010;art CS reconstruction algorithms and improve the imaging efficiency obviously.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART110796699&target=NART&cn=NART110796699",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network <P>As compared with traditional ISAR imaging methods, the compressive sensing (CS)&#x2010;based imaging methods can obtain high&#x2010;quality images using much less under&#x2010;sampled data. However, the availability or appropriateness of the sparse representation of the target scene and the relatively low computational efficiency of image reconstruction algorithms limit the performance and application of the CS&#x2010;based ISAR imaging methods. In recent years, the deep learning technology has been applied in many fields and achieved outstanding performance in image classification, image reconstruction etc. DL implements the tasks using the deep neural network (DNN), which composes multiple hidden layers and non&#x2010;linear activation layer. In this study, a novel ISAR imaging method that uses a complex&#x2010;value deep neural network (CV&#x2010;DNN) to perform the image formation using under&#x2010;sampled data is proposed. The CV&#x2010;DNN architecture can extract and exploit the sparse feature of the target image extremely well by multilayer non&#x2010;linear processing. The experimental results show that the proposed CV&#x2010;DNN&#x2010;based ISAR imaging method can provide better shape reconstruction of target with less data than state&#x2010;of&#x2010;the&#x2010;art CS reconstruction algorithms and improve the imaging efficiency obviously.</P>"
        },
        {
          "rank": 9,
          "score": 0.7202968001365662,
          "doc_id": "NART106334309",
          "title": "Cognitive radar antenna selection via deep learning",
          "abstract": "<P>Direction&#x2010;of&#x2010;arrival (DoA) estimation of targets improves with the number of elements employed by a phased array radar antenna. Since larger arrays have high associated cost, area and computational load, there is a recent interest in thinning the antenna arrays without loss of far&#x2010;field DoA accuracy. In this context, a cognitive radar may deploy a full array and then select an optimal subarray to transmit and receive the signals in response to changes in the target environment. Prior works have used optimisation and greedy search methods to pick the best subarrays cognitively. In this study, deep learning is leveraged to address the antenna selection problem. Specifically, they construct a convolutional neural network (CNN) as a multi&#x2010;class classification framework, where each class designates a different subarray. The proposed network determines a new array every time data is received by the radar, thereby making antenna selection a cognitive operation. Their numerical experiments show that the proposed CNN structure provides 22% better classification performance than a support vector machine and the resulting subarrays yield 72% more accurate DoA estimates than random array selections.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106334309&target=NART&cn=NART106334309",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Cognitive radar antenna selection via deep learning Cognitive radar antenna selection via deep learning Cognitive radar antenna selection via deep learning <P>Direction&#x2010;of&#x2010;arrival (DoA) estimation of targets improves with the number of elements employed by a phased array radar antenna. Since larger arrays have high associated cost, area and computational load, there is a recent interest in thinning the antenna arrays without loss of far&#x2010;field DoA accuracy. In this context, a cognitive radar may deploy a full array and then select an optimal subarray to transmit and receive the signals in response to changes in the target environment. Prior works have used optimisation and greedy search methods to pick the best subarrays cognitively. In this study, deep learning is leveraged to address the antenna selection problem. Specifically, they construct a convolutional neural network (CNN) as a multi&#x2010;class classification framework, where each class designates a different subarray. The proposed network determines a new array every time data is received by the radar, thereby making antenna selection a cognitive operation. Their numerical experiments show that the proposed CNN structure provides 22% better classification performance than a support vector machine and the resulting subarrays yield 72% more accurate DoA estimates than random array selections.</P>"
        },
        {
          "rank": 10,
          "score": 0.7137684226036072,
          "doc_id": "NART124851615",
          "title": "Radar Spectrum Image Classification Based on Deep Learning",
          "abstract": "<P>With the continuous development and progress of science and technology, the increasingly complex electromagnetic environment and the research and development of new radar systems have led to the emergence of various radar signals. Traditional methods of radar emitter identification cannot meet the needs of current practical applications. For the purpose of classification and recognition of radar emitter signals, this paper proposes an improved EfficientNetv2-s classification method based on deep learning for more precise classification and recognition of radar radiation source signals. Using 16 different types of radar signal parameters from the signal parameter setting table, the proposed method generates random data sets consisting of spectrum images with varying amplitude. The proposed method replaces two-dimensional convolution in EfficientNetV2 with one-dimensional convolution. Additionally, the channel attention mechanism of the EfficientNetv2-s is optimized and modified to obtain attention weights without dimensional reduction, resulting in superior accuracy. Compared with other deep-learning image-classification methods, the test results of this method have better classification accuracy on the test set: the top1 accuracy reaches 98.12%, which is 0.17~3.12% higher than other methods. Furthermore, the proposed method has lower complexity compared to most methods.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART124851615&target=NART&cn=NART124851615",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Radar Spectrum Image Classification Based on Deep Learning Radar Spectrum Image Classification Based on Deep Learning Radar Spectrum Image Classification Based on Deep Learning <P>With the continuous development and progress of science and technology, the increasingly complex electromagnetic environment and the research and development of new radar systems have led to the emergence of various radar signals. Traditional methods of radar emitter identification cannot meet the needs of current practical applications. For the purpose of classification and recognition of radar emitter signals, this paper proposes an improved EfficientNetv2-s classification method based on deep learning for more precise classification and recognition of radar radiation source signals. Using 16 different types of radar signal parameters from the signal parameter setting table, the proposed method generates random data sets consisting of spectrum images with varying amplitude. The proposed method replaces two-dimensional convolution in EfficientNetV2 with one-dimensional convolution. Additionally, the channel attention mechanism of the EfficientNetv2-s is optimized and modified to obtain attention weights without dimensional reduction, resulting in superior accuracy. Compared with other deep-learning image-classification methods, the test results of this method have better classification accuracy on the test set: the top1 accuracy reaches 98.12%, which is 0.17~3.12% higher than other methods. Furthermore, the proposed method has lower complexity compared to most methods.</P>"
        },
        {
          "rank": 11,
          "score": 0.7132049202919006,
          "doc_id": "NART125907540",
          "title": "Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review",
          "abstract": "<P>Radar automatic target recognition (RATR) technology is fundamental but complicated system engineering that combines sensor, target, environment, and signal processing technology, etc. It plays a significant role in improving the level and capabilities of military and civilian automation. Although RATR has been successfully applied in some aspects, the complete theoretical system has not been established. At present, deep learning algorithms have received a lot of attention and have emerged as potential and feasible solutions in RATR. This paper mainly reviews related articles published between 2010 and 2022, which corresponds to the period when deep learning methods were introduced into RATR research. In this paper, the current research status of radar target characteristics is summarized, including motion, micro-motion, one-dimensional, and two-dimensional characteristics, etc. This paper reviews the progress of deep learning methods in the feature extraction and recognition of radar target characteristics in recent years, including space, air, ground, sea-surface targets, etc. Due to more and more attention and research results published in the past few years, it is hoped that this review can provide potential guidance for future research and application of deep learning in fields related to RATR.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART125907540&target=NART&cn=NART125907540",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review <P>Radar automatic target recognition (RATR) technology is fundamental but complicated system engineering that combines sensor, target, environment, and signal processing technology, etc. It plays a significant role in improving the level and capabilities of military and civilian automation. Although RATR has been successfully applied in some aspects, the complete theoretical system has not been established. At present, deep learning algorithms have received a lot of attention and have emerged as potential and feasible solutions in RATR. This paper mainly reviews related articles published between 2010 and 2022, which corresponds to the period when deep learning methods were introduced into RATR research. In this paper, the current research status of radar target characteristics is summarized, including motion, micro-motion, one-dimensional, and two-dimensional characteristics, etc. This paper reviews the progress of deep learning methods in the feature extraction and recognition of radar target characteristics in recent years, including space, air, ground, sea-surface targets, etc. Due to more and more attention and research results published in the past few years, it is hoped that this review can provide potential guidance for future research and application of deep learning in fields related to RATR.</P>"
        },
        {
          "rank": 12,
          "score": 0.7104924917221069,
          "doc_id": "NART84975182",
          "title": "Deep Learning for Passive Synthetic Aperture Radar",
          "abstract": "<P>We introduce a deep learning (DL) framework for inverse problems in imaging, and demonstrate the advantages and applicability of this approach in passive synthetic aperture radar (SAR) image reconstruction. We interpret image reconstruction as a machine learning task and utilize deep networks as forward and inverse solvers for imaging. Specifically, we design a recurrent neural network (RNN) architecture as an inverse solver based on the iterations of proximal gradient descent optimization methods. We further adapt the RNN architecture to image reconstruction problems by transforming the network into a recurrent auto-encoder, thereby allowing for unsupervised training. Our DL based inverse solver is particularly suitable for a class of image formation problems in which the forward model is only partially known. The ability to learn forward models and hyper parameters combined with unsupervised training approach establish our recurrent auto-encoder suitable for real world applications. We demonstrate the performance of our method in passive SAR image reconstruction. In this regime a source of opportunity, with unknown location and transmitted waveform, is used to illuminate a scene of interest. We investigate recurrent auto-encoder architecture based on the <TEX>$\\ell _1$</TEX> and <TEX>$\\ell _0$</TEX> constrained least-squares problem. We present a projected stochastic gradient descent based training scheme which incorporates constraints of the unknown model parameters. We demonstrate through extensive numerical simulations that our DL based approach out performs conventional sparse coding methods in terms of computation and reconstructed image quality, specifically, when no information about the transmitter is available.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART84975182&target=NART&cn=NART84975182",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning for Passive Synthetic Aperture Radar Deep Learning for Passive Synthetic Aperture Radar Deep Learning for Passive Synthetic Aperture Radar <P>We introduce a deep learning (DL) framework for inverse problems in imaging, and demonstrate the advantages and applicability of this approach in passive synthetic aperture radar (SAR) image reconstruction. We interpret image reconstruction as a machine learning task and utilize deep networks as forward and inverse solvers for imaging. Specifically, we design a recurrent neural network (RNN) architecture as an inverse solver based on the iterations of proximal gradient descent optimization methods. We further adapt the RNN architecture to image reconstruction problems by transforming the network into a recurrent auto-encoder, thereby allowing for unsupervised training. Our DL based inverse solver is particularly suitable for a class of image formation problems in which the forward model is only partially known. The ability to learn forward models and hyper parameters combined with unsupervised training approach establish our recurrent auto-encoder suitable for real world applications. We demonstrate the performance of our method in passive SAR image reconstruction. In this regime a source of opportunity, with unknown location and transmitted waveform, is used to illuminate a scene of interest. We investigate recurrent auto-encoder architecture based on the <TEX>$\\ell _1$</TEX> and <TEX>$\\ell _0$</TEX> constrained least-squares problem. We present a projected stochastic gradient descent based training scheme which incorporates constraints of the unknown model parameters. We demonstrate through extensive numerical simulations that our DL based approach out performs conventional sparse coding methods in terms of computation and reconstructed image quality, specifically, when no information about the transmitter is available.</P>"
        },
        {
          "rank": 13,
          "score": 0.7069627046585083,
          "doc_id": "NART117063882",
          "title": "Advancing Radar Nowcasting Through Deep Transfer Learning",
          "abstract": "<P>Deep learning is emerging as a powerful tool in scientific applications, such as radar-based convective storm nowcasting. However, it is still a challenge to extend the application of a well-trained deep learning nowcasting model, which demands to incorporate the learned knowledge at a certain location to other locations characterized by different precipitation features. This article designs a transfer learning framework to tackle this problem. A convolutional neural network (CNN)-based nowcasting method is utilized as the benchmark, based on which two transfer learning models are constructed through fine-tune and maximum mean discrepancy (MMD) minimization. The base CNN model is trained using radar data in the source study domain near Beijing, China, whereas the transferred models are applied to the target domain near Guangzhou, China, with only a small amount of data in the target area. The influence of a varying number of target data samples on the nowcasting performance is quantified. The experimental results demonstrate that the deep transfer learning models can improve the nowcasting skills.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART117063882&target=NART&cn=NART117063882",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Advancing Radar Nowcasting Through Deep Transfer Learning Advancing Radar Nowcasting Through Deep Transfer Learning Advancing Radar Nowcasting Through Deep Transfer Learning <P>Deep learning is emerging as a powerful tool in scientific applications, such as radar-based convective storm nowcasting. However, it is still a challenge to extend the application of a well-trained deep learning nowcasting model, which demands to incorporate the learned knowledge at a certain location to other locations characterized by different precipitation features. This article designs a transfer learning framework to tackle this problem. A convolutional neural network (CNN)-based nowcasting method is utilized as the benchmark, based on which two transfer learning models are constructed through fine-tune and maximum mean discrepancy (MMD) minimization. The base CNN model is trained using radar data in the source study domain near Beijing, China, whereas the transferred models are applied to the target domain near Guangzhou, China, with only a small amount of data in the target area. The influence of a varying number of target data samples on the nowcasting performance is quantified. The experimental results demonstrate that the deep transfer learning models can improve the nowcasting skills.</P>"
        },
        {
          "rank": 14,
          "score": 0.7049200534820557,
          "doc_id": "DIKO0017187917",
          "title": "레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구",
          "abstract": "자율주행의 안정적인 운행을 보장하기 위해서는 도로 상황에 대한 깊이 있는 이해가 필수적이다. 이에 본 논문에서는 단일 딥러닝(deep learning, DL) 네트워크 구조를 활용해 차량, 사이클리스트, 보행자 등 도로에서 자주 마주하는 객체를 분류하고 동시에 이들의 이동 방향을 추정하는 방법을 제안한다. 먼저, 4차원 이미징 레이더를 이용해 대상의 거리, 속도, 방위각, 고도각과 같은 정보를 획득한다. 이후, 검출 결과를 포인트 클라우드 데이터로 변환하여 3차원 공간 좌표계로 표현한다. 다음으로, 포인트 클라우드 데이터를 XY 평면에 정사영하여 대상의 분류와 이동 방향 추정을 수행한다. XY 평면에서 밀도 기반의 클러스터링(density-based spatial clustering) 기법을 적용해 검출 결과에서 잡음을 제거하고 객체를 클러스터링하여, 이를 이미지 데이터로 변환하는 전처리 과정을 거친다. 그런 후, 이미지 데이터를 이용해 객체 분류와 이동 방향 예측을 동시에 수행할 수 있는 다중 출력 DL 네트워크를 학습시킨다. 제안된 방법의 성능 평가 결과, 객체 분류 정확도는 96.10%로 나타났고, 이동 방향 예측의 평균 제곱근 오차(root mean square error, RMSE)는 차량, 사이클리스트, 보행자에 대해 각각 5.54°, 3.89°, 15.35°로 측정되었으며, 실행시간은 0.1초로 측정되어 효율성을 입증하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017187917&target=NART&cn=DIKO0017187917",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 자율주행의 안정적인 운행을 보장하기 위해서는 도로 상황에 대한 깊이 있는 이해가 필수적이다. 이에 본 논문에서는 단일 딥러닝(deep learning, DL) 네트워크 구조를 활용해 차량, 사이클리스트, 보행자 등 도로에서 자주 마주하는 객체를 분류하고 동시에 이들의 이동 방향을 추정하는 방법을 제안한다. 먼저, 4차원 이미징 레이더를 이용해 대상의 거리, 속도, 방위각, 고도각과 같은 정보를 획득한다. 이후, 검출 결과를 포인트 클라우드 데이터로 변환하여 3차원 공간 좌표계로 표현한다. 다음으로, 포인트 클라우드 데이터를 XY 평면에 정사영하여 대상의 분류와 이동 방향 추정을 수행한다. XY 평면에서 밀도 기반의 클러스터링(density-based spatial clustering) 기법을 적용해 검출 결과에서 잡음을 제거하고 객체를 클러스터링하여, 이를 이미지 데이터로 변환하는 전처리 과정을 거친다. 그런 후, 이미지 데이터를 이용해 객체 분류와 이동 방향 예측을 동시에 수행할 수 있는 다중 출력 DL 네트워크를 학습시킨다. 제안된 방법의 성능 평가 결과, 객체 분류 정확도는 96.10%로 나타났고, 이동 방향 예측의 평균 제곱근 오차(root mean square error, RMSE)는 차량, 사이클리스트, 보행자에 대해 각각 5.54°, 3.89°, 15.35°로 측정되었으며, 실행시간은 0.1초로 측정되어 효율성을 입증하였다."
        },
        {
          "rank": 15,
          "score": 0.7035968899726868,
          "doc_id": "NART123168643",
          "title": "Synthetic Aperture Radar (SAR) Meets Deep Learning",
          "abstract": "<P>Synthetic aperture radar (SAR) is an important active microwave imaging sensor [...]</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART123168643&target=NART&cn=NART123168643",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Synthetic Aperture Radar (SAR) Meets Deep Learning Synthetic Aperture Radar (SAR) Meets Deep Learning Synthetic Aperture Radar (SAR) Meets Deep Learning <P>Synthetic aperture radar (SAR) is an important active microwave imaging sensor [...]</P>"
        },
        {
          "rank": 16,
          "score": 0.7018324136734009,
          "doc_id": "JAKO202201253148351",
          "title": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰",
          "abstract": "위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202201253148351&target=NART&cn=JAKO202201253148351",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다."
        },
        {
          "rank": 17,
          "score": 0.6680072546005249,
          "doc_id": "JAKO202116954704821",
          "title": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측",
          "abstract": "본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116954704821&target=NART&cn=JAKO202116954704821",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다."
        },
        {
          "rank": 18,
          "score": 0.6658749580383301,
          "doc_id": "ART002342492",
          "title": "Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning",
          "abstract": "Heavy rainfall has frequently caused serious flooding and landslides, increasing traffic delays in most parts of the world. Consequently, the people in areas battered by heavy rainfall face many hardships. Thus, the negative effects of torrential rainfall always remind researchers to keep seeking the ways to prevent such damage. Therefore, we designed a system for short-term prediction of localized heavy downpours by using radar images coupled with a machine learning method. Here, we introduce a new approach, named dual k-nearest neighbor (dual-kNN), for shortterm rainfall prediction by upgrading the ordinary classification routines of classical k-nearest neighbors (k-NN). dual-kNN is able to maintain highly robust classification of various K values with an advanced simple dual consideration, where observation of a targeted object can be found not only in the specified region but also in other related regions. We conducted experimentations using 2011, 2013, and 2014 data sets collected from the WITH small-dish aviation radar installed on the rooftop of Information Engineering, University of the Ryukyus. Then, we compared the prediction accuracy of our new approach with classical k-NN. It was experimentally confirmed with test cases and simulations that the performance of dual-kNN is more effective than classical k- NN.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002342492&target=NART&cn=ART002342492",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Heavy rainfall has frequently caused serious flooding and landslides, increasing traffic delays in most parts of the world. Consequently, the people in areas battered by heavy rainfall face many hardships. Thus, the negative effects of torrential rainfall always remind researchers to keep seeking the ways to prevent such damage. Therefore, we designed a system for short-term prediction of localized heavy downpours by using radar images coupled with a machine learning method. Here, we introduce a new approach, named dual k-nearest neighbor (dual-kNN), for shortterm rainfall prediction by upgrading the ordinary classification routines of classical k-nearest neighbors (k-NN). dual-kNN is able to maintain highly robust classification of various K values with an advanced simple dual consideration, where observation of a targeted object can be found not only in the specified region but also in other related regions. We conducted experimentations using 2011, 2013, and 2014 data sets collected from the WITH small-dish aviation radar installed on the rooftop of Information Engineering, University of the Ryukyus. Then, we compared the prediction accuracy of our new approach with classical k-NN. It was experimentally confirmed with test cases and simulations that the performance of dual-kNN is more effective than classical k- NN."
        },
        {
          "rank": 19,
          "score": 0.665817141532898,
          "doc_id": "JAKO202007552827199",
          "title": "심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측",
          "abstract": "본 연구에서는 강우예측을 위해 U-Net과 SegNet에 기반한 합성곱 신경망 네트워크 구조에 장기간의 국내 기상레이더 자료를 활용하여 심층학습기반의 강우예측을 수행하였다. 또한, 기존 외삽기반의 강우예측 기법인 이류모델의 결과와 비교 평가하였다. 심층신경망의 학습 및 검정을 위해 2010부터 2016년 동안의 기상청 관악산과 광덕산 레이더의 원자료를 수집, 1 km 공간해상도를 갖는 480 &#215; 480의 픽셀의 회색조 영상으로 변환하여 HDF5 형태의 데이터를 구축하였다. 구축된 데이터로 30분 전부터 현재까지 10분 간격의 연속된 레이더 영상 4개를 이용하여 10분 후의 강수량을 예측하도록 심층신경망 모델을 학습하였으며, 학습된 심층신경망 모델로 60분의 선행예측을 수행하기 위해 예측값을 반복 사용하는 재귀적 방식을 적용하였다. 심층신경망 예측모델의 성능 평가를 위해 2017년에 발생한 24개의 호우사례에 대해 선행 60분까지 강우예측을 수행하였다. 임계강우강도 0.1, 1, 5 mm/hr에서 평균절대오차와 임계성공지수를 산정하여 예측성능을 평가한 결과, 강우강도 임계 값 0.1, 1 mm/hr의 경우 MAE는 60분 선행예측까지, CSI는 선행예측 50분까지 참조 예측모델인 이류모델이 보다 우수한 성능을 보였다. 특히, 5 mm/hr 이하의 약한 강우에 대해서는 심층신경망 예측모델이 이류모델보다 대체적으로 좋은 성능을 보였지만, 5 mm/hr의 임계 값에 대한 평가결과 심층신경망 예측모델은 고강도의 뚜렷한 강수 특징을 예측하는 데 한계가 있었다. 심층신경망 예측모델은 예측시간이 길어질수록 공간 평활화되는 경향이 뚜렷해지며, 이로 인해 강우 예측의 정확도가 저하되었다. 이류모델은 뚜렷한 강수 특성을 보존하기 때문에 강한 강도 (>5 mm/hr)에 대해 심층신경망 예측모델을 능가하지만, 강우 위치가 잘못 이동하는 경향이 있다. 본 연구결과는 이후 심층신경망을 이용한 레이더 강우 예측기술의 개발과 개선에 도움이 될 수 있을 것으로 판단된다. 또한, 본 연구에서 구축한 대용량 기상레이더 자료는 향후 후속연구에 활용될 수 있도록 개방형 저장소를 통해 제공될 예정이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202007552827199&target=NART&cn=JAKO202007552827199",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 본 연구에서는 강우예측을 위해 U-Net과 SegNet에 기반한 합성곱 신경망 네트워크 구조에 장기간의 국내 기상레이더 자료를 활용하여 심층학습기반의 강우예측을 수행하였다. 또한, 기존 외삽기반의 강우예측 기법인 이류모델의 결과와 비교 평가하였다. 심층신경망의 학습 및 검정을 위해 2010부터 2016년 동안의 기상청 관악산과 광덕산 레이더의 원자료를 수집, 1 km 공간해상도를 갖는 480 &#215; 480의 픽셀의 회색조 영상으로 변환하여 HDF5 형태의 데이터를 구축하였다. 구축된 데이터로 30분 전부터 현재까지 10분 간격의 연속된 레이더 영상 4개를 이용하여 10분 후의 강수량을 예측하도록 심층신경망 모델을 학습하였으며, 학습된 심층신경망 모델로 60분의 선행예측을 수행하기 위해 예측값을 반복 사용하는 재귀적 방식을 적용하였다. 심층신경망 예측모델의 성능 평가를 위해 2017년에 발생한 24개의 호우사례에 대해 선행 60분까지 강우예측을 수행하였다. 임계강우강도 0.1, 1, 5 mm/hr에서 평균절대오차와 임계성공지수를 산정하여 예측성능을 평가한 결과, 강우강도 임계 값 0.1, 1 mm/hr의 경우 MAE는 60분 선행예측까지, CSI는 선행예측 50분까지 참조 예측모델인 이류모델이 보다 우수한 성능을 보였다. 특히, 5 mm/hr 이하의 약한 강우에 대해서는 심층신경망 예측모델이 이류모델보다 대체적으로 좋은 성능을 보였지만, 5 mm/hr의 임계 값에 대한 평가결과 심층신경망 예측모델은 고강도의 뚜렷한 강수 특징을 예측하는 데 한계가 있었다. 심층신경망 예측모델은 예측시간이 길어질수록 공간 평활화되는 경향이 뚜렷해지며, 이로 인해 강우 예측의 정확도가 저하되었다. 이류모델은 뚜렷한 강수 특성을 보존하기 때문에 강한 강도 (>5 mm/hr)에 대해 심층신경망 예측모델을 능가하지만, 강우 위치가 잘못 이동하는 경향이 있다. 본 연구결과는 이후 심층신경망을 이용한 레이더 강우 예측기술의 개발과 개선에 도움이 될 수 있을 것으로 판단된다. 또한, 본 연구에서 구축한 대용량 기상레이더 자료는 향후 후속연구에 활용될 수 있도록 개방형 저장소를 통해 제공될 예정이다."
        },
        {
          "rank": 20,
          "score": 0.6657363176345825,
          "doc_id": "NART66897872",
          "title": "Image enhancement in forward imaging radar using modified apodisation technique",
          "abstract": "<P>Forward imaging radar using an UWB signal has been developed by many researchers. The image quality in this radar is not satisfactory because of the limitation of aperture length. Proposed is an image enhancement method using a modified apodisation technique in forward imaging radar. An experiment is carried out to validate the proposed method. The azimuth resolution and peak-to-sidelobe ratio are improved by the proposed method by up to about 11.7% and 10 dB, respectively.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART66897872&target=NART&cn=NART66897872",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Image enhancement in forward imaging radar using modified apodisation technique Image enhancement in forward imaging radar using modified apodisation technique Image enhancement in forward imaging radar using modified apodisation technique <P>Forward imaging radar using an UWB signal has been developed by many researchers. The image quality in this radar is not satisfactory because of the limitation of aperture length. Proposed is an image enhancement method using a modified apodisation technique in forward imaging radar. An experiment is carried out to validate the proposed method. The azimuth resolution and peak-to-sidelobe ratio are improved by the proposed method by up to about 11.7% and 10 dB, respectively.</P>"
        },
        {
          "rank": 21,
          "score": 0.6595445275306702,
          "doc_id": "DIKO0015644673",
          "title": "M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지",
          "abstract": "해상 교통량의 증가로 인해 해상 선박관리의 필요성이 늘어남에 따라 선박을 탐지하기 위한 연구들이 꾸준히 수행되어왔다. 특히 위성레이더 영상은 시간과 기후에 영향을 받지 않고 촬영할 수 있다는 장점으로 인해 선박탐지를 위한 많은 연구에서 활용되어왔다. 최근에는 딥러닝 기법의 발전으로 인해 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구들이 꾸준히 수행되고 있다. 그런데 위성레이더 영상은 값의 분포범위가 매우 넓고, 많은 스펙클 노이즈가 존재한다. 이러한 요소들은 딥러닝 모델의 학습에 부정적인 영향을 끼칠 수 있으므로 전처리를 통해 해당 요소들을 저감해줄 필요가 있다. 본 연구에서는 전처리된 위성레이더 영상으로부터 딥러닝 선박탐지를 수행하고, 영상의 전처리가 딥러닝 선박탐지에 미치는 요소를 비교분석 하고자 한다.&amp;#xD; 본 연구를 위해 TerraSAR-X와 COSMO-SkyMed 위성레이더 영상을 이용했다. 영상을 딥러닝 학습에 이용하기 전에 먼저 총 세 가지 다른 방법으로 전처리를 수행했다. 첫 번째는 위성레이더 영상에서 강도 값만을 추출한 강도 영상을 생성하는 방법이다. 강도 영상은 값의 범위가 매우 넓을 뿐만 아니라 많은 스펙클 노이즈를 가지고 있다. 두 번째는 강도영상에서 값의 단위를 데시벨로 변환한 데시벨 영상을 생성하는 방법이다. 데시벨 영상은 강도영상과 마찬가지로 많은 스펙클 노이즈를 가지고 있으나 값의 범위가 줄어들어, 더 안정적인 학습을 할 수 있다. 세 번째는 본 연구에서 제안하는 위성레이더 전처리방법으로써, 강도차분과 거칠기영상을 생성하는 방법이다. 두 영상은 중간값 필터링을 이용해 스펙클 노이즈를 줄이고, 값의 분포 대역을 좁힘으로써 빠른 학습이 가능하다.&amp;#xD; 각 전처리된 위성레이더 영상을 이용해 딥러닝 학습을 하기 위해 본 연구에서는 M2Det 객체탐지 모델을 사용했다. 객체탐지 모델을 학습시킨 뒤 테스트 영상을 이용해 선박탐지를 수행했으며, 테스트 결과는 정밀도(Precision), 재현율(Recall)을 이용해 나타냈으며, 두 지수를 하나의 값으로 표현하기 위해 AP(Average Precision)와 F1 점수(F1-score)를 이용해 나타냈다. 각 영상의 정밀도, 재현율, AP, F1 점수는 강도 영상 93.18%, 91.11%, 89.78%, 92.13%, 데시벨 영상 94.16%, 94.16%, 92.34%, 94.16%, 강도차분과 거칠기 영상 97.40%, 94.94%, 95.55%, 96.15%로 계산되었다. 강도 영상을 이용한 경우 미탐지와 오탐지 선박이 많았으며, 전처리된 영상을 이용한 경우 강도 영상에 비해 미탐지와 오탐지 선박이 줄어든 것을 확인할 수 있었다. 데시벨 영상과 강도차분, 거칠기 영상의 결과를 비교했을 때, 두 영상의 오탐지율은 유사했다. 하지만 강도차분, 거칠기 영상을 이용했을 때 강도 영상에 비해 미탐지 선박의 비율이 4% 줄어든 것을 확인할 수 있었다. 이 결과를 통해 위성레이더 영상을 전처리함으로써 딥러닝 학습을 돕고 선박탐지 결과를 향상시킬 수 있다는 것을 알 수 있다. 본 연구결과는 향후 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구의 발전에 이바지할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015644673&target=NART&cn=DIKO0015644673",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 해상 교통량의 증가로 인해 해상 선박관리의 필요성이 늘어남에 따라 선박을 탐지하기 위한 연구들이 꾸준히 수행되어왔다. 특히 위성레이더 영상은 시간과 기후에 영향을 받지 않고 촬영할 수 있다는 장점으로 인해 선박탐지를 위한 많은 연구에서 활용되어왔다. 최근에는 딥러닝 기법의 발전으로 인해 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구들이 꾸준히 수행되고 있다. 그런데 위성레이더 영상은 값의 분포범위가 매우 넓고, 많은 스펙클 노이즈가 존재한다. 이러한 요소들은 딥러닝 모델의 학습에 부정적인 영향을 끼칠 수 있으므로 전처리를 통해 해당 요소들을 저감해줄 필요가 있다. 본 연구에서는 전처리된 위성레이더 영상으로부터 딥러닝 선박탐지를 수행하고, 영상의 전처리가 딥러닝 선박탐지에 미치는 요소를 비교분석 하고자 한다.&amp;#xD; 본 연구를 위해 TerraSAR-X와 COSMO-SkyMed 위성레이더 영상을 이용했다. 영상을 딥러닝 학습에 이용하기 전에 먼저 총 세 가지 다른 방법으로 전처리를 수행했다. 첫 번째는 위성레이더 영상에서 강도 값만을 추출한 강도 영상을 생성하는 방법이다. 강도 영상은 값의 범위가 매우 넓을 뿐만 아니라 많은 스펙클 노이즈를 가지고 있다. 두 번째는 강도영상에서 값의 단위를 데시벨로 변환한 데시벨 영상을 생성하는 방법이다. 데시벨 영상은 강도영상과 마찬가지로 많은 스펙클 노이즈를 가지고 있으나 값의 범위가 줄어들어, 더 안정적인 학습을 할 수 있다. 세 번째는 본 연구에서 제안하는 위성레이더 전처리방법으로써, 강도차분과 거칠기영상을 생성하는 방법이다. 두 영상은 중간값 필터링을 이용해 스펙클 노이즈를 줄이고, 값의 분포 대역을 좁힘으로써 빠른 학습이 가능하다.&amp;#xD; 각 전처리된 위성레이더 영상을 이용해 딥러닝 학습을 하기 위해 본 연구에서는 M2Det 객체탐지 모델을 사용했다. 객체탐지 모델을 학습시킨 뒤 테스트 영상을 이용해 선박탐지를 수행했으며, 테스트 결과는 정밀도(Precision), 재현율(Recall)을 이용해 나타냈으며, 두 지수를 하나의 값으로 표현하기 위해 AP(Average Precision)와 F1 점수(F1-score)를 이용해 나타냈다. 각 영상의 정밀도, 재현율, AP, F1 점수는 강도 영상 93.18%, 91.11%, 89.78%, 92.13%, 데시벨 영상 94.16%, 94.16%, 92.34%, 94.16%, 강도차분과 거칠기 영상 97.40%, 94.94%, 95.55%, 96.15%로 계산되었다. 강도 영상을 이용한 경우 미탐지와 오탐지 선박이 많았으며, 전처리된 영상을 이용한 경우 강도 영상에 비해 미탐지와 오탐지 선박이 줄어든 것을 확인할 수 있었다. 데시벨 영상과 강도차분, 거칠기 영상의 결과를 비교했을 때, 두 영상의 오탐지율은 유사했다. 하지만 강도차분, 거칠기 영상을 이용했을 때 강도 영상에 비해 미탐지 선박의 비율이 4% 줄어든 것을 확인할 수 있었다. 이 결과를 통해 위성레이더 영상을 전처리함으로써 딥러닝 학습을 돕고 선박탐지 결과를 향상시킬 수 있다는 것을 알 수 있다. 본 연구결과는 향후 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구의 발전에 이바지할 수 있을 것으로 기대된다."
        },
        {
          "rank": 22,
          "score": 0.6581299304962158,
          "doc_id": "JAKO202300957609703",
          "title": "딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합",
          "abstract": "위성 시계열 데이터가 증가함에 따라 원격탐사 자료의 활용도가 높아지고 있다. 시계열 자료를 통한 분석에 있어 영상 간의 상대적인 위치 정확도는 결과에 큰 영향을 미치기 때문에 이를 보정하기 위한 영상 정합 과정은 필수적으로 선행되어야 한다. 최근에는 기존 알고리즘의 성능을 상회하는 딥러닝 기반 영상 정합 연구의 사례가 증가하고 있다. 딥러닝 기반 정합 모델을 학습하기 위해서는 수 많은 영상 쌍이 필요하다. 또한, 기존 딥러닝 모델의 데이터 간의 상관도 map을 제작하고, 이에 추가적인 연산을 적용하여 정합점을 추출는데 이는 비효율적이다. 이러한 문제를 해결하기 위해 본 연구에서는 영상 정합 모델 학습을 위한 데이터 증강 기법을 구축하여 데이터셋을 제작하였고, 이를 오프셋(offset) 양 자체를 예측하는 정합 모델인 OffsetNet에 적용하여 KOMSAT-2, -3, -3A 영상 정합을 수행하였다. 모델 학습 결과, OffsetNet은 평가 데이터에 대해 높은 정확도로 오프셋 양을 예측하였고, 이를 통해 주영상과 부영상을 효과적으로 정합하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202300957609703&target=NART&cn=JAKO202300957609703",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 위성 시계열 데이터가 증가함에 따라 원격탐사 자료의 활용도가 높아지고 있다. 시계열 자료를 통한 분석에 있어 영상 간의 상대적인 위치 정확도는 결과에 큰 영향을 미치기 때문에 이를 보정하기 위한 영상 정합 과정은 필수적으로 선행되어야 한다. 최근에는 기존 알고리즘의 성능을 상회하는 딥러닝 기반 영상 정합 연구의 사례가 증가하고 있다. 딥러닝 기반 정합 모델을 학습하기 위해서는 수 많은 영상 쌍이 필요하다. 또한, 기존 딥러닝 모델의 데이터 간의 상관도 map을 제작하고, 이에 추가적인 연산을 적용하여 정합점을 추출는데 이는 비효율적이다. 이러한 문제를 해결하기 위해 본 연구에서는 영상 정합 모델 학습을 위한 데이터 증강 기법을 구축하여 데이터셋을 제작하였고, 이를 오프셋(offset) 양 자체를 예측하는 정합 모델인 OffsetNet에 적용하여 KOMSAT-2, -3, -3A 영상 정합을 수행하였다. 모델 학습 결과, OffsetNet은 평가 데이터에 대해 높은 정확도로 오프셋 양을 예측하였고, 이를 통해 주영상과 부영상을 효과적으로 정합하였다."
        },
        {
          "rank": 23,
          "score": 0.6561884880065918,
          "doc_id": "DIKO0015551607",
          "title": "데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법",
          "abstract": "오늘날 딥 러닝(Deep Learning)이란 머신러닝의 세부적인 방법과 개념&amp;#xD; 및 기법들을 통칭한다. 딥 러닝은 크게는 컴퓨터 비전(Computer vision)으&amp;#xD; 로부터 시작하여 패턴 인식(Pattern recognition), 색상 및 픽셀 복원, 추청&amp;#xD; 과 진단 등 다양한 곳에 사용이 되고 있다. 그 중 대게 객체 및 사람을 인&amp;#xD; 식하는 단계 및 추적을 더불어 대상의 안면 인식을 할 수 있는 단계까지&amp;#xD; 발달했다. 기본적인 네트워크인 컨볼루션 뉴럴 네트워크(CNN :&amp;#xD; convolutional neural network)를 시작으로 순환신경망(RNN : Recurrent&amp;#xD; Neural Network), 볼츠만 머신(RBM : Restricted Boltzmann Machine), 생&amp;#xD; 성 대립 신경망(GAN : Generative Adversarial Network) 그리고 Google의&amp;#xD; 딥 마인드에서 개발한 관계형 네트워크(RL : Relation Networks)등이 존재&amp;#xD; 한다. 이와 같은 네트워크 모델들은 다양한 강점들을 가지고 있는데 그 중&amp;#xD; 데이터를 이용한 요인 추출(feature extraction)이나 학습을 통한 결과 추론&amp;#xD; 이라고 볼 수 있다. 위와 같은 요인들을 성공적으로 학습시키기 위해서는&amp;#xD; 적합한 환경에 맞는 데이터 세트인지 판단하고, 모델에 관한 특징들을 파악&amp;#xD; 하여 가장 적합한 형태의 모델을 구현하여 효과적으로 학습 할 수 있도록&amp;#xD; 진행한다. 하지만 위 과정 중에서 데이터 세트들은 손쉽게 만들어지지 않는&amp;#xD; 다. 그 이유는 여러 다양한 방법으로 디자인되고 환경에 맞게 제작이 되어&amp;#xD; 야하기 때문이다.&amp;#xD; 본 논문에서는 기존 데이터 세트들을 이용하여 여러 다양한 방법을 이&amp;#xD; 용하여 데이터를 증강(data augmentation)시키는 연구를 진행한다. 객체 인&amp;#xD; 식 및 판단을 목적으로 딥 러닝을 학습 시킬 경우에는 이미지의 데이터 정&amp;#xD; 보들을 통해 학습을 진행한다. 학습하는 데이터 정보는 관심이 있는 영역이&amp;#xD; 나 혹은 주요 지정된 객체의 정보를 학습하는 것을 목표로 한다. 이것을 달&amp;#xD; 성하기 위해 데이터 세트를 이용하여 유용한 정보를 추출하고 학습 후 객&amp;#xD; 체에 관한 인식을 할 수 있게 진행했다. 여기에서 데이터 세트들은 대부분&amp;#xD; ILSVRC (Image Large Scale Visual Recognition Challenges) 및 PASCAL&amp;#xD; VOC (Visual Object Classes) 같은 것으로 이루어져 있다. 하지만 이와 같&amp;#xD; 은 데이터 세트는 특수한 상황이나 제한된 상황에서 사용하기가 매우 어렵&amp;#xD; 다. 상황에 맞게 데이터 세트들을 제작을 해야 하는 경우 이는 매우 많은&amp;#xD; 시간이 걸린다. 또한 만들어진 데이터 세트들을 테스트해야 하는 시간 또한&amp;#xD; 오래 걸린다. 본 논문에서는 제안된 방법을 사용하여 이를 해결한다. 기본&amp;#xD; 적인 영상처리부터 시작하여 알고리즘 및 3D 환경에서까지의 방법을 설명&amp;#xD; 한다. 이 방법들을 통해 생성된 데이터들은 성능 검증을 위해 실시간 모델&amp;#xD; 인 YOLO ver2(You Only Look Once)를 사용한다. 그리고 이미지 생성 후&amp;#xD; 분류에 사용할 CNN과 VGGNet(Very Deep Convolutional Networks for&amp;#xD; Large-Scale Image Recognition)을 이용한다. 최종적으로 제시한 방법을&amp;#xD; 통해 데이터 세트의 수를 수백 배 이상 생성했으며, 객체 간의 정확도는 5&amp;#xD; ∼ 10% 이상 증가시켰다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015551607&target=NART&cn=DIKO0015551607",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법 데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법 데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법 오늘날 딥 러닝(Deep Learning)이란 머신러닝의 세부적인 방법과 개념&amp;#xD; 및 기법들을 통칭한다. 딥 러닝은 크게는 컴퓨터 비전(Computer vision)으&amp;#xD; 로부터 시작하여 패턴 인식(Pattern recognition), 색상 및 픽셀 복원, 추청&amp;#xD; 과 진단 등 다양한 곳에 사용이 되고 있다. 그 중 대게 객체 및 사람을 인&amp;#xD; 식하는 단계 및 추적을 더불어 대상의 안면 인식을 할 수 있는 단계까지&amp;#xD; 발달했다. 기본적인 네트워크인 컨볼루션 뉴럴 네트워크(CNN :&amp;#xD; convolutional neural network)를 시작으로 순환신경망(RNN : Recurrent&amp;#xD; Neural Network), 볼츠만 머신(RBM : Restricted Boltzmann Machine), 생&amp;#xD; 성 대립 신경망(GAN : Generative Adversarial Network) 그리고 Google의&amp;#xD; 딥 마인드에서 개발한 관계형 네트워크(RL : Relation Networks)등이 존재&amp;#xD; 한다. 이와 같은 네트워크 모델들은 다양한 강점들을 가지고 있는데 그 중&amp;#xD; 데이터를 이용한 요인 추출(feature extraction)이나 학습을 통한 결과 추론&amp;#xD; 이라고 볼 수 있다. 위와 같은 요인들을 성공적으로 학습시키기 위해서는&amp;#xD; 적합한 환경에 맞는 데이터 세트인지 판단하고, 모델에 관한 특징들을 파악&amp;#xD; 하여 가장 적합한 형태의 모델을 구현하여 효과적으로 학습 할 수 있도록&amp;#xD; 진행한다. 하지만 위 과정 중에서 데이터 세트들은 손쉽게 만들어지지 않는&amp;#xD; 다. 그 이유는 여러 다양한 방법으로 디자인되고 환경에 맞게 제작이 되어&amp;#xD; 야하기 때문이다.&amp;#xD; 본 논문에서는 기존 데이터 세트들을 이용하여 여러 다양한 방법을 이&amp;#xD; 용하여 데이터를 증강(data augmentation)시키는 연구를 진행한다. 객체 인&amp;#xD; 식 및 판단을 목적으로 딥 러닝을 학습 시킬 경우에는 이미지의 데이터 정&amp;#xD; 보들을 통해 학습을 진행한다. 학습하는 데이터 정보는 관심이 있는 영역이&amp;#xD; 나 혹은 주요 지정된 객체의 정보를 학습하는 것을 목표로 한다. 이것을 달&amp;#xD; 성하기 위해 데이터 세트를 이용하여 유용한 정보를 추출하고 학습 후 객&amp;#xD; 체에 관한 인식을 할 수 있게 진행했다. 여기에서 데이터 세트들은 대부분&amp;#xD; ILSVRC (Image Large Scale Visual Recognition Challenges) 및 PASCAL&amp;#xD; VOC (Visual Object Classes) 같은 것으로 이루어져 있다. 하지만 이와 같&amp;#xD; 은 데이터 세트는 특수한 상황이나 제한된 상황에서 사용하기가 매우 어렵&amp;#xD; 다. 상황에 맞게 데이터 세트들을 제작을 해야 하는 경우 이는 매우 많은&amp;#xD; 시간이 걸린다. 또한 만들어진 데이터 세트들을 테스트해야 하는 시간 또한&amp;#xD; 오래 걸린다. 본 논문에서는 제안된 방법을 사용하여 이를 해결한다. 기본&amp;#xD; 적인 영상처리부터 시작하여 알고리즘 및 3D 환경에서까지의 방법을 설명&amp;#xD; 한다. 이 방법들을 통해 생성된 데이터들은 성능 검증을 위해 실시간 모델&amp;#xD; 인 YOLO ver2(You Only Look Once)를 사용한다. 그리고 이미지 생성 후&amp;#xD; 분류에 사용할 CNN과 VGGNet(Very Deep Convolutional Networks for&amp;#xD; Large-Scale Image Recognition)을 이용한다. 최종적으로 제시한 방법을&amp;#xD; 통해 데이터 세트의 수를 수백 배 이상 생성했으며, 객체 간의 정확도는 5&amp;#xD; ∼ 10% 이상 증가시켰다."
        },
        {
          "rank": 24,
          "score": 0.6524646878242493,
          "doc_id": "NPAP13842123",
          "title": "레이더 영상 기반 딥러닝을 이용한 물체 인식",
          "abstract": "본 연구에서는 컴퓨터 비전 기반의 딥러닝 객체 인식 기술을 이용하여 속초해수욕장에서 수집한 레이더 이미지에서 선박, 섬 및 부유체에 대해 탐지(Detection), 인식(Recognition)하는 연구를 수행하였다. 2021년 8월에 수집한 레이더 영상을 이용하여 본 연구를 수행하였으며, 움직이는 물표와 섬 등을 구분하였다. 일부 환경적인 제약에 따라 에러 발생이 있었지만, 향후 현재까지 수집한 레이더 영상을 추가하여 정확도를 높일 예정이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP13842123&target=NART&cn=NPAP13842123",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "레이더 영상 기반 딥러닝을 이용한 물체 인식 레이더 영상 기반 딥러닝을 이용한 물체 인식 레이더 영상 기반 딥러닝을 이용한 물체 인식 본 연구에서는 컴퓨터 비전 기반의 딥러닝 객체 인식 기술을 이용하여 속초해수욕장에서 수집한 레이더 이미지에서 선박, 섬 및 부유체에 대해 탐지(Detection), 인식(Recognition)하는 연구를 수행하였다. 2021년 8월에 수집한 레이더 영상을 이용하여 본 연구를 수행하였으며, 움직이는 물표와 섬 등을 구분하였다. 일부 환경적인 제약에 따라 에러 발생이 있었지만, 향후 현재까지 수집한 레이더 영상을 추가하여 정확도를 높일 예정이다."
        },
        {
          "rank": 25,
          "score": 0.650148332118988,
          "doc_id": "JAKO202319937622688",
          "title": "머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구",
          "abstract": "본 논문에서는 유리섬유 직물 강화 복합재 소재위에 Cross-Dipole 패턴이 배치된 정형적 Circuit Analog(CA) 전파 흡수 구조 설계를 위한 머신러닝 및 딥러닝 모델을 제시하였다. 제시된 모델은 Cross-Dipole 패턴의 형상에 따라서 Ku-band (12-18 GHz)에서의 전파흡수성능을 3차원 전자파 수치해석 없이 바로 계산할 수 있다. 이를 위하여 다양한 머신러닝 및 딥러닝 기술을 적용한 최적 학습 모델을 도출하고, 학습 모델이 계산한 결과를 3차원 전자파 수치해석결과로 얻은 전파흡수특성과 비교함으로써 각각의 모델 간의 성능의 비교우위를 평가하였다. 개발된 모델들은 대부분 수치해석결과와 유사한 계산결과를 보여주었지만, 그 중 Fully-Connected 모델이 가장 유사한 계산결과를 제공할 수 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202319937622688&target=NART&cn=JAKO202319937622688",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구 머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구 머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구 본 논문에서는 유리섬유 직물 강화 복합재 소재위에 Cross-Dipole 패턴이 배치된 정형적 Circuit Analog(CA) 전파 흡수 구조 설계를 위한 머신러닝 및 딥러닝 모델을 제시하였다. 제시된 모델은 Cross-Dipole 패턴의 형상에 따라서 Ku-band (12-18 GHz)에서의 전파흡수성능을 3차원 전자파 수치해석 없이 바로 계산할 수 있다. 이를 위하여 다양한 머신러닝 및 딥러닝 기술을 적용한 최적 학습 모델을 도출하고, 학습 모델이 계산한 결과를 3차원 전자파 수치해석결과로 얻은 전파흡수특성과 비교함으로써 각각의 모델 간의 성능의 비교우위를 평가하였다. 개발된 모델들은 대부분 수치해석결과와 유사한 계산결과를 보여주었지만, 그 중 Fully-Connected 모델이 가장 유사한 계산결과를 제공할 수 있음을 확인하였다."
        },
        {
          "rank": 26,
          "score": 0.6426170468330383,
          "doc_id": "JAKO202407064802797",
          "title": "딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가",
          "abstract": "급격한 기후 변화로 인한 어획량 감소와 양식 기술의 발전으로 양식 생산물 수요가 전세계적으로 계속해서 증가하고 있다. 그러나 이에 따른 무분별한 시설물 확장이 연안 생태계와 어족 자원 가격 책정에 악영향을 미치기 때문에, 주기적인 연안 환경 모니터링을 통한 양식시설물 관리가 필수적이다. 본 연구에서는 Sentinel-2 광학 영상과 다양한 딥러닝 기반 탐지 기법을 활용하여 경상남도의 패류 양식시설물 탐지 정확도를 분석하였다. DeepLabv3+, ResUNet++ 그리고 Attention U-Net 모델을 적용하였으며, 실험 결과 Attention U-Net 모델이 F1 score 0.8708, Intersection over Union 0.7708로 가장 우수한 탐지 성능을 보였다. 연구에서 제시한 탐지 방법론은 조류 및 부유 물질에 영향을 받는 양식시설물을 주기적으로 관측할 수 있고, 다양한 양식 품종에 적용할 수 있어 넓은 지역으로의 확장 가능성이 높다. 따라서 본 연구 방법을 통해 도출된 양식 시설물 정보는 향후 해양 공간 활용에 관한 정책 결정에 유용하게 활용할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202407064802797&target=NART&cn=JAKO202407064802797",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 급격한 기후 변화로 인한 어획량 감소와 양식 기술의 발전으로 양식 생산물 수요가 전세계적으로 계속해서 증가하고 있다. 그러나 이에 따른 무분별한 시설물 확장이 연안 생태계와 어족 자원 가격 책정에 악영향을 미치기 때문에, 주기적인 연안 환경 모니터링을 통한 양식시설물 관리가 필수적이다. 본 연구에서는 Sentinel-2 광학 영상과 다양한 딥러닝 기반 탐지 기법을 활용하여 경상남도의 패류 양식시설물 탐지 정확도를 분석하였다. DeepLabv3+, ResUNet++ 그리고 Attention U-Net 모델을 적용하였으며, 실험 결과 Attention U-Net 모델이 F1 score 0.8708, Intersection over Union 0.7708로 가장 우수한 탐지 성능을 보였다. 연구에서 제시한 탐지 방법론은 조류 및 부유 물질에 영향을 받는 양식시설물을 주기적으로 관측할 수 있고, 다양한 양식 품종에 적용할 수 있어 넓은 지역으로의 확장 가능성이 높다. 따라서 본 연구 방법을 통해 도출된 양식 시설물 정보는 향후 해양 공간 활용에 관한 정책 결정에 유용하게 활용할 수 있을 것으로 기대된다."
        },
        {
          "rank": 27,
          "score": 0.6415311098098755,
          "doc_id": "JAKO202231363544671",
          "title": "입자 군집 최적화(PSO) 알고리즘 기반 다층 레이더 흡수 구조체 설계",
          "abstract": "본 논문에서는 입자 군집 최적화 (Particle Swarm Optimization: PSO) 알고리즘을 이용하여 다층 레이더 흡수 구조체를 설계하고, 다층 레이더 흡수 구조체의 특성을 분석하였다. 다층 레이더 흡수 구조체 설계에 PSO를 적용함으로써 빠르고 정확하게 설계 값을 도출할 수 있음을 보였으며, 특히 경사 입사에 대한 경우에 대해서도 최적의 다층 레이더 흡수 구조체를 설계할 수 있음을 보였다. 또한, 다양한 설계 파라미터의 조합에서도 성능 요구 조건에 부합하는 최적의 값이 결정될 수 있음을 보였다. 각 단계별로 필요한 방정식 및 모든 변수에 대한 자세한 설명을 포함해서 포괄적인 순서도를 통해 제시하였고 본 논문의 결과로부터 다층 레이더 흡수 구조체를 설계하기 위한 복잡하고 많은 계산을 생략할 수 있으며, 다양한 복합 재료를 활용한 다층 레이다 흡수 구조체 설계 및 개발에 활용할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202231363544671&target=NART&cn=JAKO202231363544671",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "입자 군집 최적화(PSO) 알고리즘 기반 다층 레이더 흡수 구조체 설계 입자 군집 최적화(PSO) 알고리즘 기반 다층 레이더 흡수 구조체 설계 입자 군집 최적화(PSO) 알고리즘 기반 다층 레이더 흡수 구조체 설계 본 논문에서는 입자 군집 최적화 (Particle Swarm Optimization: PSO) 알고리즘을 이용하여 다층 레이더 흡수 구조체를 설계하고, 다층 레이더 흡수 구조체의 특성을 분석하였다. 다층 레이더 흡수 구조체 설계에 PSO를 적용함으로써 빠르고 정확하게 설계 값을 도출할 수 있음을 보였으며, 특히 경사 입사에 대한 경우에 대해서도 최적의 다층 레이더 흡수 구조체를 설계할 수 있음을 보였다. 또한, 다양한 설계 파라미터의 조합에서도 성능 요구 조건에 부합하는 최적의 값이 결정될 수 있음을 보였다. 각 단계별로 필요한 방정식 및 모든 변수에 대한 자세한 설명을 포함해서 포괄적인 순서도를 통해 제시하였고 본 논문의 결과로부터 다층 레이더 흡수 구조체를 설계하기 위한 복잡하고 많은 계산을 생략할 수 있으며, 다양한 복합 재료를 활용한 다층 레이다 흡수 구조체 설계 및 개발에 활용할 수 있다."
        },
        {
          "rank": 28,
          "score": 0.6407886743545532,
          "doc_id": "NART108808939",
          "title": "Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출",
          "abstract": "This paper presents a novel remote infrared pedestrian detection method for night use by means of local projection-CNN. Conventional sliding window methods (HOG/ACF) or region proposal-based deep learning approaches (faster R-CNN, SSD, YOLO) either fail to detect small objects or generate many false positives. Multi-modal region proposal schemes (multi-scale contrast filters with local projection+ACF) are used to improve remote pedestrian detection. AlexNet-based CNN feature extraction and SVM classification can reduce false positives further. This paper's experimental evaluations indicate that the proposed method can improve remote IR pedestrian detection by 16%.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART108808939&target=NART&cn=NART108808939",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 This paper presents a novel remote infrared pedestrian detection method for night use by means of local projection-CNN. Conventional sliding window methods (HOG/ACF) or region proposal-based deep learning approaches (faster R-CNN, SSD, YOLO) either fail to detect small objects or generate many false positives. Multi-modal region proposal schemes (multi-scale contrast filters with local projection+ACF) are used to improve remote pedestrian detection. AlexNet-based CNN feature extraction and SVM classification can reduce false positives further. This paper's experimental evaluations indicate that the proposed method can improve remote IR pedestrian detection by 16%."
        },
        {
          "rank": 29,
          "score": 0.6396719813346863,
          "doc_id": "DIKO0017114224",
          "title": "딥러닝을 활용한 초음파 영상 개선",
          "abstract": "의료용 초음파 이미지(Clinical Ultrasonic Image) 기법은 인체 내부의 대한 영상을 비침습적, 안전적, 실시간적 있는 도구로, 의료 분야에서 사용되는 대표적인 진단 의료 영상 중 하나이다. 초고속 초음파(Ultra-fast Ultrasound)는 다수의 초음파 송수신을 통하여 상대적으로 고품질의 초음파 이미지를 얻을 수 있다. 그러나, 초음파 빔의 다양성, 복원 이미지의 해상도, 관심 영역(Region of Interest)의 크기 등은 실시간성과 절충 관계(Trade-off)에 있기에 초당 프레임 수(FPS)를 방어하기에 하드웨어적으로 어려움이 있다. 본 연구에서는 딥러닝(Deep Learning) 모델을 활용하여 단일 평면파(Single Plane-wave)의 저품질의 초음파 이미지를 고품질 다중 평면파(Multi-angle Plane-wave)의 고품질 초음파 이미지로 강화하는 것을 목표로 한다. U-Net 구조로 이루어진 딥러닝 모델은 다양한 크기의 합성곱 필터를 이용하여 복잡한 이미지의 세부 정보의 특징을 효과적으로 추출할 수 있다. 제안된 딥러닝 모델은 피크 대 잡음 비율(PSNR), 신호 대 잡음 비율(SNR), 스페클 신호 대 잡음 비율(SSNR) 등의 성능 지표를 통해 효과적인 잡음 감소 및 신호 보존을 보였으며, 상관계수(Correlation)를 통하여 강화된 이미지와 실제 이미지 간의 높은 유사성 및 정확성을 보였다. 향후 연구로는 본 작업에 영향을 줄 수 있는 세부 요인들을 조사하고, 모델 구조를 세밀하게 조정 및 최적화하여 강화되는 이미지의 품질을 더욱 향상시키고, 보다 다양한 부위에 대한 실험을 통해 일반화 성능을 확장할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017114224&target=NART&cn=DIKO0017114224",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 활용한 초음파 영상 개선 딥러닝을 활용한 초음파 영상 개선 딥러닝을 활용한 초음파 영상 개선 의료용 초음파 이미지(Clinical Ultrasonic Image) 기법은 인체 내부의 대한 영상을 비침습적, 안전적, 실시간적 있는 도구로, 의료 분야에서 사용되는 대표적인 진단 의료 영상 중 하나이다. 초고속 초음파(Ultra-fast Ultrasound)는 다수의 초음파 송수신을 통하여 상대적으로 고품질의 초음파 이미지를 얻을 수 있다. 그러나, 초음파 빔의 다양성, 복원 이미지의 해상도, 관심 영역(Region of Interest)의 크기 등은 실시간성과 절충 관계(Trade-off)에 있기에 초당 프레임 수(FPS)를 방어하기에 하드웨어적으로 어려움이 있다. 본 연구에서는 딥러닝(Deep Learning) 모델을 활용하여 단일 평면파(Single Plane-wave)의 저품질의 초음파 이미지를 고품질 다중 평면파(Multi-angle Plane-wave)의 고품질 초음파 이미지로 강화하는 것을 목표로 한다. U-Net 구조로 이루어진 딥러닝 모델은 다양한 크기의 합성곱 필터를 이용하여 복잡한 이미지의 세부 정보의 특징을 효과적으로 추출할 수 있다. 제안된 딥러닝 모델은 피크 대 잡음 비율(PSNR), 신호 대 잡음 비율(SNR), 스페클 신호 대 잡음 비율(SSNR) 등의 성능 지표를 통해 효과적인 잡음 감소 및 신호 보존을 보였으며, 상관계수(Correlation)를 통하여 강화된 이미지와 실제 이미지 간의 높은 유사성 및 정확성을 보였다. 향후 연구로는 본 작업에 영향을 줄 수 있는 세부 요인들을 조사하고, 모델 구조를 세밀하게 조정 및 최적화하여 강화되는 이미지의 품질을 더욱 향상시키고, 보다 다양한 부위에 대한 실험을 통해 일반화 성능을 확장할 수 있다."
        },
        {
          "rank": 30,
          "score": 0.6390950083732605,
          "doc_id": "JAKO199911921528980",
          "title": "다층회귀예측신경망의 음성인식성능에 관한 연구",
          "abstract": "4층구조의 다층퍼셉트론을 변형하여 3 종류의 다층회귀예측신경망을 구성하고, 예측차수, 두 은닉층의 뉴런개수, 연결세기의 초기치 및 전달함수 변화에 따른 각 망의 음성인식성능을 실험을 통해 각각 비교 분석한다. 실험결과에 의하면, 다층회귀신경망이 다층퍼셉트론에 비해 음성인식성능이 우수하다. 그리고 구조적으로는 상위은닉층의 출력을 하위은닉층으로 회귀할 때 인식성능이 가장 우수하며, 각 망 공히 상, 하위은닉층의 뉴런 10 혹은 15개, 예측차수 3 혹은 4차일 때 인식률이 양호하다. 학습시 연결세기의 초기치를 -0.5에서 0.5사이로 설정하고, 하위은닉층에서 단극성 시그모이드 전달함수를 사용할 때 인식성능이 더욱 향상된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199911921528980&target=NART&cn=JAKO199911921528980",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀예측신경망의 음성인식성능에 관한 연구 다층회귀예측신경망의 음성인식성능에 관한 연구 다층회귀예측신경망의 음성인식성능에 관한 연구 4층구조의 다층퍼셉트론을 변형하여 3 종류의 다층회귀예측신경망을 구성하고, 예측차수, 두 은닉층의 뉴런개수, 연결세기의 초기치 및 전달함수 변화에 따른 각 망의 음성인식성능을 실험을 통해 각각 비교 분석한다. 실험결과에 의하면, 다층회귀신경망이 다층퍼셉트론에 비해 음성인식성능이 우수하다. 그리고 구조적으로는 상위은닉층의 출력을 하위은닉층으로 회귀할 때 인식성능이 가장 우수하며, 각 망 공히 상, 하위은닉층의 뉴런 10 혹은 15개, 예측차수 3 혹은 4차일 때 인식률이 양호하다. 학습시 연결세기의 초기치를 -0.5에서 0.5사이로 설정하고, 하위은닉층에서 단극성 시그모이드 전달함수를 사용할 때 인식성능이 더욱 향상된다."
        },
        {
          "rank": 31,
          "score": 0.6377816796302795,
          "doc_id": "NART119629224",
          "title": "65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging",
          "abstract": "<P>High dynamic range (HDR) techniques have received significant attention in generating realistic, high&#x2010;quality images and videos and improving visual quality in new display systems. We have witnessed remarkable advances in HDR reconstruction using deep learning technologies in recent years. This review examines recent developments in HDR reconstruction using a deep learning approach, which takes a single low dynamic range (LDR) image as an input and aims to restore an HDR image featuring higher color gamut and a higher detail retention than the LDR image. We aim to provide a comprehensive survey in this field. Since there are numerous HDR algorithms, it is necessary to evaluate and organize theirperformance, therefore, we evaluate them using two objective evaluation metrics.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART119629224&target=NART&cn=NART119629224",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging 65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging 65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging <P>High dynamic range (HDR) techniques have received significant attention in generating realistic, high&#x2010;quality images and videos and improving visual quality in new display systems. We have witnessed remarkable advances in HDR reconstruction using deep learning technologies in recent years. This review examines recent developments in HDR reconstruction using a deep learning approach, which takes a single low dynamic range (LDR) image as an input and aims to restore an HDR image featuring higher color gamut and a higher detail retention than the LDR image. We aim to provide a comprehensive survey in this field. Since there are numerous HDR algorithms, it is necessary to evaluate and organize theirperformance, therefore, we evaluate them using two objective evaluation metrics.</P>"
        },
        {
          "rank": 32,
          "score": 0.6374272108078003,
          "doc_id": "NART119879737",
          "title": "Image Enhancement Method Based on Deep Learning",
          "abstract": "<P>Image enhancement and reconstruction are the basic processing steps of many real vision systems. Their purpose is to improve the visual quality of images and provide reliable information for subsequent visual decision-making. In this paper, convolution neural network, residual neural network, and generative countermeasure network are studied. A rain fog image enhancement generative countermeasure network model structure including a scalable auxiliary generation network is proposed. The objective loss function is defined, and the periodic consistency loss and periodic perceptual consistency loss analysis are introduced. The core problem of image layering is discussed, and a layering solution framework with a deep expansion structure is proposed. This method realizes multitasking through adaptive feature learning, which has a good theoretical guarantee. This paper can not only bring a pleasant visual experience to viewers but also help to improve the performance of computer vision applications. Through image enhancement technology, the quality of low illumination image can be effectively improved, so that the image has better definition, richer texture details, and lower image noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART119879737&target=NART&cn=NART119879737",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Image Enhancement Method Based on Deep Learning Image Enhancement Method Based on Deep Learning Image Enhancement Method Based on Deep Learning <P>Image enhancement and reconstruction are the basic processing steps of many real vision systems. Their purpose is to improve the visual quality of images and provide reliable information for subsequent visual decision-making. In this paper, convolution neural network, residual neural network, and generative countermeasure network are studied. A rain fog image enhancement generative countermeasure network model structure including a scalable auxiliary generation network is proposed. The objective loss function is defined, and the periodic consistency loss and periodic perceptual consistency loss analysis are introduced. The core problem of image layering is discussed, and a layering solution framework with a deep expansion structure is proposed. This method realizes multitasking through adaptive feature learning, which has a good theoretical guarantee. This paper can not only bring a pleasant visual experience to viewers but also help to improve the performance of computer vision applications. Through image enhancement technology, the quality of low illumination image can be effectively improved, so that the image has better definition, richer texture details, and lower image noise.</P>"
        },
        {
          "rank": 33,
          "score": 0.635930597782135,
          "doc_id": "JAKO202320150299733",
          "title": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가",
          "abstract": "본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202320150299733&target=NART&cn=JAKO202320150299733",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다."
        },
        {
          "rank": 34,
          "score": 0.6345157623291016,
          "doc_id": "DIKO0016929635",
          "title": "딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식",
          "abstract": "전자기전에서 레이더 파형의 변조 방식은 다른 레이더 파형 제원과 함께 적대적인 레이더를 분석하고 대응책을 마련하는 데 활용될 수 있어 사전정보 없이 레이더 파형의 변조 방식을 자동 인식하는 기술은 매우 중요한 기술이다. 특히, 미래의 전자기전에서 복합 변조 방식이 적용된 레이더 파형에 선제적으로 대응하기 위해서는 다양한 단일 및 복합 변조 방식을 인식할 수 있는 기술을 개발하는 것이 필수적이다.&amp;#xD; 본 논문에서는 31종의 단일 및 복합 변조 방식의 레이더 파형을 자동으로 인식하는 시퀀스 데이터 기반의 레이더 파형 변조 인식 기법을 제안한다. 제안하는 기법은 수신 레이더 파형을 바탕으로 시퀀스 데이터를 구성하고, 이를 딥러닝 네트워크에 입력으로 사용하여 변조 방식을 자동으로 인식한다. 이때 시퀀스 데이터로 레이더 파형 원본, 레이더 파형의 이산 푸리에 변환(Discrete Fourier transform, DFT), 레이더 파형의 자기 상관 함수 각각의 실수부 및 허수부를 이용해 구성한 데이터를 고려하며, 딥러닝 네트워크로는 CNN(Convolutional neural network), CLDNN(Convolutional long short-term deep neural network), ResNet(Residual network)-18, 34, 50을 고려한다. 컴퓨터 모의실험을 통해 시퀀스 데이터와 딥러닝 네트워크에 따른 제안하는 시퀀스 데이터 기반 레이더 파형 자동 변조 인식 기법의 인식 성능을 비교하여 레이더 파형의 DFT로 구성한 시퀀스 데이터를 딥러닝 네트워크의 입력으로 사용할 때 가장 효과적인 변조 인식이 가능함을 보인다. 또한 레이더 파형의 DFT로 구성한 시퀀스 데이터를 이용할 때, 딥러닝 네트워크로 ResNet을 사용하는 경우가 CNN, CLDNN을 사용하는 경우보다 우수한 성능을 보임을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016929635&target=NART&cn=DIKO0016929635",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 전자기전에서 레이더 파형의 변조 방식은 다른 레이더 파형 제원과 함께 적대적인 레이더를 분석하고 대응책을 마련하는 데 활용될 수 있어 사전정보 없이 레이더 파형의 변조 방식을 자동 인식하는 기술은 매우 중요한 기술이다. 특히, 미래의 전자기전에서 복합 변조 방식이 적용된 레이더 파형에 선제적으로 대응하기 위해서는 다양한 단일 및 복합 변조 방식을 인식할 수 있는 기술을 개발하는 것이 필수적이다.&amp;#xD; 본 논문에서는 31종의 단일 및 복합 변조 방식의 레이더 파형을 자동으로 인식하는 시퀀스 데이터 기반의 레이더 파형 변조 인식 기법을 제안한다. 제안하는 기법은 수신 레이더 파형을 바탕으로 시퀀스 데이터를 구성하고, 이를 딥러닝 네트워크에 입력으로 사용하여 변조 방식을 자동으로 인식한다. 이때 시퀀스 데이터로 레이더 파형 원본, 레이더 파형의 이산 푸리에 변환(Discrete Fourier transform, DFT), 레이더 파형의 자기 상관 함수 각각의 실수부 및 허수부를 이용해 구성한 데이터를 고려하며, 딥러닝 네트워크로는 CNN(Convolutional neural network), CLDNN(Convolutional long short-term deep neural network), ResNet(Residual network)-18, 34, 50을 고려한다. 컴퓨터 모의실험을 통해 시퀀스 데이터와 딥러닝 네트워크에 따른 제안하는 시퀀스 데이터 기반 레이더 파형 자동 변조 인식 기법의 인식 성능을 비교하여 레이더 파형의 DFT로 구성한 시퀀스 데이터를 딥러닝 네트워크의 입력으로 사용할 때 가장 효과적인 변조 인식이 가능함을 보인다. 또한 레이더 파형의 DFT로 구성한 시퀀스 데이터를 이용할 때, 딥러닝 네트워크로 ResNet을 사용하는 경우가 CNN, CLDNN을 사용하는 경우보다 우수한 성능을 보임을 확인한다."
        },
        {
          "rank": 35,
          "score": 0.6298070549964905,
          "doc_id": "JAKO202210351407855",
          "title": "심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적",
          "abstract": "Nowadays, the complexity of object tracking models among hardware applications has become a more in-demand duty to complete in various indeterminable environment tracking situations with multifunctional algorithm skills. In this paper, we propose a virtual city environment using AirSim (Aerial Informatics and Robotics Simulation - AirSim, CityEnvironment) and use the DQN (Deep Q-Learning) model of deep reinforcement learning model in the virtual environment. The proposed object tracking DQN network observes the environment using a deep reinforcement learning model that receives continuous images taken by a virtual environment simulation system as input to control the operation of a virtual drone. The deep reinforcement learning model is pre-trained using various existing continuous image sets. Since the existing various continuous image sets are image data of real environments and objects, it is implemented in 3D to track virtual environments and moving objects in them.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202210351407855&target=NART&cn=JAKO202210351407855",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 Nowadays, the complexity of object tracking models among hardware applications has become a more in-demand duty to complete in various indeterminable environment tracking situations with multifunctional algorithm skills. In this paper, we propose a virtual city environment using AirSim (Aerial Informatics and Robotics Simulation - AirSim, CityEnvironment) and use the DQN (Deep Q-Learning) model of deep reinforcement learning model in the virtual environment. The proposed object tracking DQN network observes the environment using a deep reinforcement learning model that receives continuous images taken by a virtual environment simulation system as input to control the operation of a virtual drone. The deep reinforcement learning model is pre-trained using various existing continuous image sets. Since the existing various continuous image sets are image data of real environments and objects, it is implemented in 3D to track virtual environments and moving objects in them."
        },
        {
          "rank": 36,
          "score": 0.6290154457092285,
          "doc_id": "NART98464294",
          "title": "Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging",
          "abstract": "<P><B>Abstract</B></P>  <P>Artificial intelligence (AI) in medical imaging is a potentially disruptive technology. An understanding of the principles and application of radiomics, artificial neural networks, machine learning, and deep learning is an essential foundation to weave design solutions that accommodate ethical and regulatory requirements, and to craft AI-based algorithms that enhance outcomes, quality, and efficiency. Moreover, a more holistic perspective of applications, opportunities, and challenges from a programmatic perspective contributes to ethical and sustainable implementation of AI solutions.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART98464294&target=NART&cn=NART98464294",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging <P><B>Abstract</B></P>  <P>Artificial intelligence (AI) in medical imaging is a potentially disruptive technology. An understanding of the principles and application of radiomics, artificial neural networks, machine learning, and deep learning is an essential foundation to weave design solutions that accommodate ethical and regulatory requirements, and to craft AI-based algorithms that enhance outcomes, quality, and efficiency. Moreover, a more holistic perspective of applications, opportunities, and challenges from a programmatic perspective contributes to ethical and sustainable implementation of AI solutions.</P>"
        },
        {
          "rank": 37,
          "score": 0.6280817985534668,
          "doc_id": "NART118990104",
          "title": "Hierarchical Image Object Search Based on Deep Reinforcement Learning",
          "abstract": "<P><B>Abstract</B></P><P>Object detection technology occupies a pivotal position in the field of modern computer vision research, its purpose is to accurately locate the object human beings are looking for in the image and classify the object. With the development of deep learning technology, convolutional neural networks are widely used because of their outstanding performance in feature extraction, which greatly improves the speed and accuracy of object detection. In recent years, reinforcement learning technology has emerged in the field of artificial intelligence, showing excellent decision-making ability to deal with problems. In order to combine the perception ability of deep learning technology with the decision-making ability of reinforcement learning technology, this paper incorporate reinforcement learning into the convolutional neural network, and propose a hierarchical deep reinforcement learning object detection model.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART118990104&target=NART&cn=NART118990104",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hierarchical Image Object Search Based on Deep Reinforcement Learning Hierarchical Image Object Search Based on Deep Reinforcement Learning Hierarchical Image Object Search Based on Deep Reinforcement Learning <P><B>Abstract</B></P><P>Object detection technology occupies a pivotal position in the field of modern computer vision research, its purpose is to accurately locate the object human beings are looking for in the image and classify the object. With the development of deep learning technology, convolutional neural networks are widely used because of their outstanding performance in feature extraction, which greatly improves the speed and accuracy of object detection. In recent years, reinforcement learning technology has emerged in the field of artificial intelligence, showing excellent decision-making ability to deal with problems. In order to combine the perception ability of deep learning technology with the decision-making ability of reinforcement learning technology, this paper incorporate reinforcement learning into the convolutional neural network, and propose a hierarchical deep reinforcement learning object detection model.</P>"
        },
        {
          "rank": 38,
          "score": 0.6276326179504395,
          "doc_id": "JAKO202126048601456",
          "title": "유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구",
          "abstract": "딥 러닝을 활용한 컴퓨터 비전 연구는 여전히 대규모의 학습 데이터와 컴퓨팅 파워가 필수적이며, 최적의 네트워크 구조를 도출하기 위해 많은 시행착오가 수반된다. 본 연구에서는 네트워크 최적화나 데이터를 보강하는 것과 무관하게 데이터 자체의 특성만을 고려한 CR(Confusion Rate)기반의 유사 이미지 분류 성능 향상 기법을 제안한다. 제안 방법은 유사한 이미지 데이터를 정확히 분류하기 위해 CR을 산출하고 이를 손실 함수의 가중치에 반영함으로서 딥 러닝 모델의 성능을 향상시키는 기법을 제안한다. 제안 방법은 네트워크 최적화 결과와 독립적으로 이미지 분류 성능의 향상을 가져올 수 있으며, 클래스 간의 유사성을 고려해 유사도가 높은 이미지 식별에 적합하다. 제안 방법의 평가결과 HanDB에서는 0.22%, Animal-10N에서는 3.38%의 성능향상을 보였다. 제안한 방법은 다양한 Noisy Labeled 데이터를 활용한 인공지능 연구에 기반이 될 것을 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202126048601456&target=NART&cn=JAKO202126048601456",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 딥 러닝을 활용한 컴퓨터 비전 연구는 여전히 대규모의 학습 데이터와 컴퓨팅 파워가 필수적이며, 최적의 네트워크 구조를 도출하기 위해 많은 시행착오가 수반된다. 본 연구에서는 네트워크 최적화나 데이터를 보강하는 것과 무관하게 데이터 자체의 특성만을 고려한 CR(Confusion Rate)기반의 유사 이미지 분류 성능 향상 기법을 제안한다. 제안 방법은 유사한 이미지 데이터를 정확히 분류하기 위해 CR을 산출하고 이를 손실 함수의 가중치에 반영함으로서 딥 러닝 모델의 성능을 향상시키는 기법을 제안한다. 제안 방법은 네트워크 최적화 결과와 독립적으로 이미지 분류 성능의 향상을 가져올 수 있으며, 클래스 간의 유사성을 고려해 유사도가 높은 이미지 식별에 적합하다. 제안 방법의 평가결과 HanDB에서는 0.22%, Animal-10N에서는 3.38%의 성능향상을 보였다. 제안한 방법은 다양한 Noisy Labeled 데이터를 활용한 인공지능 연구에 기반이 될 것을 기대한다."
        },
        {
          "rank": 39,
          "score": 0.6271713972091675,
          "doc_id": "NART115326214",
          "title": "Deep learning-based single image face depth data enhancement",
          "abstract": "<P><B>Abstract</B></P>  <P>Face recognition can benefit from the utilization of depth data captured using low-cost cameras, in particular for presentation attack detection purposes. Depth video output from these capture devices can however contain defects such as holes or general depth inaccuracies. This work proposes a deep learning face depth enhancement method in this context of facial biometrics, which adds a security aspect to the topic. U-Net-like architectures are utilized, and the networks are compared against hand-crafted enhancer types, as well as a similar depth enhancer network from related work trained for an adjacent application scenario. All tested enhancer types exclusively use depth data as input, which differs from methods that enhance depth based on additional input data such as visible light color images. Synthetic face depth ground truth images and degraded forms thereof are created with help of PRNet, to train multiple deep learning enhancer models with different network sizes and training configurations. Evaluations are carried out on the synthetic data, on Kinect v1 images from the KinectFaceDB, and on in-house RealSense D435 images. These evaluations include an assessment of the falsification for occluded face depth input, which is relevant to biometric security. The proposed deep learning enhancers yield noticeably better results than the tested preexisting enhancers, without overly falsifying depth data when non-face input is provided, and are shown to reduce the error of a simple landmark-based PAD method.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Pure depth image enhancement using deep learning is effective for facial biometrics. </LI> <LI>  Synthesis of realistic low detail face depth enhancer training data is viable. </LI> <LI>  Comparisons with more general enhancers favor the face-specific model. </LI> <LI>  Depth is not overly falsified for non-face input during enhancement. </LI> <LI>  Face depth enhancement can be used to aid real-time presentation attack detection. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART115326214&target=NART&cn=NART115326214",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning-based single image face depth data enhancement Deep learning-based single image face depth data enhancement Deep learning-based single image face depth data enhancement <P><B>Abstract</B></P>  <P>Face recognition can benefit from the utilization of depth data captured using low-cost cameras, in particular for presentation attack detection purposes. Depth video output from these capture devices can however contain defects such as holes or general depth inaccuracies. This work proposes a deep learning face depth enhancement method in this context of facial biometrics, which adds a security aspect to the topic. U-Net-like architectures are utilized, and the networks are compared against hand-crafted enhancer types, as well as a similar depth enhancer network from related work trained for an adjacent application scenario. All tested enhancer types exclusively use depth data as input, which differs from methods that enhance depth based on additional input data such as visible light color images. Synthetic face depth ground truth images and degraded forms thereof are created with help of PRNet, to train multiple deep learning enhancer models with different network sizes and training configurations. Evaluations are carried out on the synthetic data, on Kinect v1 images from the KinectFaceDB, and on in-house RealSense D435 images. These evaluations include an assessment of the falsification for occluded face depth input, which is relevant to biometric security. The proposed deep learning enhancers yield noticeably better results than the tested preexisting enhancers, without overly falsifying depth data when non-face input is provided, and are shown to reduce the error of a simple landmark-based PAD method.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Pure depth image enhancement using deep learning is effective for facial biometrics. </LI> <LI>  Synthesis of realistic low detail face depth enhancer training data is viable. </LI> <LI>  Comparisons with more general enhancers favor the face-specific model. </LI> <LI>  Depth is not overly falsified for non-face input during enhancement. </LI> <LI>  Face depth enhancement can be used to aid real-time presentation attack detection. </LI> </UL> </P>"
        },
        {
          "rank": 40,
          "score": 0.6252745389938354,
          "doc_id": "JAKO201962652079504",
          "title": "심층 강화학습 기술 동향",
          "abstract": "Recent trends in deep reinforcement learning (DRL) have revealed the considerable improvements to DRL algorithms in terms of performance, learning stability, and computational efficiency. DRL also enables the scenarios that it covers (e.g., partial observability; cooperation, competition, coexistence, and communications among multiple agents; multi-task; decentralized intelligence) to be vastly expanded. These features have cultivated multi-agent reinforcement learning research. DRL is also expanding its applications from robotics to natural language processing and computer vision into a wide array of fields such as finance, healthcare, chemistry, and even art. In this report, we briefly summarize various DRL techniques and research directions.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201962652079504&target=NART&cn=JAKO201962652079504",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층 강화학습 기술 동향 심층 강화학습 기술 동향 심층 강화학습 기술 동향 Recent trends in deep reinforcement learning (DRL) have revealed the considerable improvements to DRL algorithms in terms of performance, learning stability, and computational efficiency. DRL also enables the scenarios that it covers (e.g., partial observability; cooperation, competition, coexistence, and communications among multiple agents; multi-task; decentralized intelligence) to be vastly expanded. These features have cultivated multi-agent reinforcement learning research. DRL is also expanding its applications from robotics to natural language processing and computer vision into a wide array of fields such as finance, healthcare, chemistry, and even art. In this report, we briefly summarize various DRL techniques and research directions."
        },
        {
          "rank": 41,
          "score": 0.6237369775772095,
          "doc_id": "JAKO202220154416716",
          "title": "밀리미터파 대역 딥러닝 기반 다중빔 전송링크 성능 예측기법",
          "abstract": "차세대 와이파이 표준기술인 IEEE 802.11ay는 밀리미터파 대역에서 AP (Access Point)가 다수의 STA (Station)로 동시에 데이터를 전송하도록 MU-MIMO (Multiple User Multiple Input Multiple Output) 통신을 지원한다. 이를 위해, 주기적으로 MU-MIMO 빔포밍 훈련을 수행해야 하고, 효율적인 빔포밍 훈련을 위해서는 AP가 다수의 안테나로 다수의 빔을 동시에 전송할 때, 각 STA에서 측정되는 신호 세기를 정확히 예측하는 것이 중요하다. 본 논문에서는 딥러닝 기반 다중 빔 전송링크 성능 예측기법을 제안한다. 제안한 예측기법은 특정 실내 또는 실외 환경에서 미리 학습된 딥러닝 모델을 이용하여 다수의 빔이 동시에 전송될 때 STA에서 측정되는 신호 세기 예측의 정확성을 높인다. 이때, 딥러닝의 입력으로 개별 빔이 전송될 때 STA에서 측정되는 신호 세기 정보를 이용하고, 개별 빔의 신호 세기 정보를 얻는 과정은 이미 기존의 빔포밍 훈련에 포함되어 있으므로 정보 수집을 위해 추가적인 비용을 발생하지 않는다. 성능평가를 위해 NIST (National Institute of Standards and Technology)에 의해 개발된 Q-D 채널구현 (Quasi-Deterministic Channel Realization) 오픈소스 소프트웨어를 활용하였고 실측 데이터 기반으로 밀리미터파 채널을 구현하였다. 실험결과에서는 제안한 예측기법이 다른 비교기법보다 향상된 예측성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202220154416716&target=NART&cn=JAKO202220154416716",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "밀리미터파 대역 딥러닝 기반 다중빔 전송링크 성능 예측기법 밀리미터파 대역 딥러닝 기반 다중빔 전송링크 성능 예측기법 밀리미터파 대역 딥러닝 기반 다중빔 전송링크 성능 예측기법 차세대 와이파이 표준기술인 IEEE 802.11ay는 밀리미터파 대역에서 AP (Access Point)가 다수의 STA (Station)로 동시에 데이터를 전송하도록 MU-MIMO (Multiple User Multiple Input Multiple Output) 통신을 지원한다. 이를 위해, 주기적으로 MU-MIMO 빔포밍 훈련을 수행해야 하고, 효율적인 빔포밍 훈련을 위해서는 AP가 다수의 안테나로 다수의 빔을 동시에 전송할 때, 각 STA에서 측정되는 신호 세기를 정확히 예측하는 것이 중요하다. 본 논문에서는 딥러닝 기반 다중 빔 전송링크 성능 예측기법을 제안한다. 제안한 예측기법은 특정 실내 또는 실외 환경에서 미리 학습된 딥러닝 모델을 이용하여 다수의 빔이 동시에 전송될 때 STA에서 측정되는 신호 세기 예측의 정확성을 높인다. 이때, 딥러닝의 입력으로 개별 빔이 전송될 때 STA에서 측정되는 신호 세기 정보를 이용하고, 개별 빔의 신호 세기 정보를 얻는 과정은 이미 기존의 빔포밍 훈련에 포함되어 있으므로 정보 수집을 위해 추가적인 비용을 발생하지 않는다. 성능평가를 위해 NIST (National Institute of Standards and Technology)에 의해 개발된 Q-D 채널구현 (Quasi-Deterministic Channel Realization) 오픈소스 소프트웨어를 활용하였고 실측 데이터 기반으로 밀리미터파 채널을 구현하였다. 실험결과에서는 제안한 예측기법이 다른 비교기법보다 향상된 예측성능을 보였다."
        },
        {
          "rank": 42,
          "score": 0.6234363317489624,
          "doc_id": "DIKO0016837574",
          "title": "Research on Deep Spatio-Temporal Prediction Model for Traffic Prediction에 대한 연구",
          "abstract": "최근 차량의 급격한 증가는 다양한 도시 교통 문제를 야기하고 있으며 이를 해결하기 위해 많은 연구가 진행되고 있다. 도시 교통 상황은 시시각각 변화하므로 공간 및 시간적 측면에서 복잡하고 불규칙하며 비선형적인 시공간 관계를 갖는 데이터이다. 따라서 복잡한 시공간 의존성을 효율적으로 포착하고 적시에 정확한 교통 예측을 통한 교통 활용률을 개선하는 것은 매우 어려운 문제이다.&amp;#xD; 교통 예측 문제에 대한 연구는 전통적인 선형 방법, 기계 학습 방법, 딥 러닝을 활용하는 방법으로 발전하고 있지만 기존 방법에는 여전히 많은 문제가 있다. 예를 들어, 시공간 데이터에서 의존성을 획득하는 것만으로는 교통 상황의 급격한 시공간 변화를 반영할 수 없고, 시공간 멀티모달 (multimodal) 융합은 시공간의 복잡성에 적응할 수 없다. 또한 얕은 시공간 모델은 더 풍부한 교통 데이터 특징을 추출하는 데 어려움이 있고, 고정 교통 그래프 구조는 유연성이 부족하다.&amp;#xD; 이 논문은 시공간 교통 데이터를 대상으로 딥 러닝 이론과 방법에 기반한 일련의 혁신적인 시공간 시퀀스 예측 모델을 제시한다. 이를 위해 시공간 다중 연관 그래프 구축, 시공간 멀티모달 융합, 교통 그래프 네트워크 심화, 적응적인 시공간 그래프 등의 방법을 제안한다.&amp;#xD; 논문의 주요 혁신적 연구는 다음과 같습니다.&amp;#xD; 1. 교통망의 공간적 및 시간적, 동적 및 정적 특성을 얻기 위한 시공간적 다중 연관 그래프를 생성하는 새로운 방법을 제안한다. 이는 시공간 차원의 연관관계를 기반으로 여러 시공간 그래프를 생성하여 특징(feature)을 향상시키며 인접 시공간 그래프와 인접하지 않은 시공간 그래프 간의 의존 관계를 학습할 수 있다.&amp;#xD; 2. 시공간 멀티모달 데이터 융합 문제를 해결하기 위한 다단계 혼합 시공간 융합 방법을 제안한다. 이는 멀티헤드 주의(multi-headed attention) 메커니즘과 결합된 적응형 게이팅(adaptive gating) 메커니즘을 사용하여 보다 장기적이고 심층적인 복잡한 시공간 정보를 획득할 수 있다.&amp;#xD; 3. 시공간 그래프의 네트워크 계층의 심화로 인한 과도한 평활화 문제를 해결하기 위한 적응형 은닉층 연결 방법을 제안한다. 이는 특징 표현을 변환 및 분리한 후, 초기 및 은닉층 연결에 추가하고 파라미터를 통해 은닉층의 가중치를 적응적으로 조정한다. 이를 통해 노드의 수용성을 확장하고 더 깊은 교통 인접 노드의 특징을 종합적으로 획득할 수 있다.&amp;#xD; 4. 시공간 그래프의 유연성과 적응성을 향상시키기 위한 교통망의 복잡한 시공간 관계를 분석하기 위해 교통 예측을 위한 파라미터 공유 및 적응적 그래프 컨볼루션 방법을 제안한다. 이는 파라미터 공유를 위한 적응적 행렬을 구성하여 교통망 내의 공간 의존성 및 구조를 적응적으로 학습하고 조정한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016837574&target=NART&cn=DIKO0016837574",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Research on Deep Spatio-Temporal Prediction Model for Traffic Prediction에 대한 연구 Research on Deep Spatio-Temporal Prediction Model for Traffic Prediction에 대한 연구 Research on Deep Spatio-Temporal Prediction Model for Traffic Prediction에 대한 연구 최근 차량의 급격한 증가는 다양한 도시 교통 문제를 야기하고 있으며 이를 해결하기 위해 많은 연구가 진행되고 있다. 도시 교통 상황은 시시각각 변화하므로 공간 및 시간적 측면에서 복잡하고 불규칙하며 비선형적인 시공간 관계를 갖는 데이터이다. 따라서 복잡한 시공간 의존성을 효율적으로 포착하고 적시에 정확한 교통 예측을 통한 교통 활용률을 개선하는 것은 매우 어려운 문제이다.&amp;#xD; 교통 예측 문제에 대한 연구는 전통적인 선형 방법, 기계 학습 방법, 딥 러닝을 활용하는 방법으로 발전하고 있지만 기존 방법에는 여전히 많은 문제가 있다. 예를 들어, 시공간 데이터에서 의존성을 획득하는 것만으로는 교통 상황의 급격한 시공간 변화를 반영할 수 없고, 시공간 멀티모달 (multimodal) 융합은 시공간의 복잡성에 적응할 수 없다. 또한 얕은 시공간 모델은 더 풍부한 교통 데이터 특징을 추출하는 데 어려움이 있고, 고정 교통 그래프 구조는 유연성이 부족하다.&amp;#xD; 이 논문은 시공간 교통 데이터를 대상으로 딥 러닝 이론과 방법에 기반한 일련의 혁신적인 시공간 시퀀스 예측 모델을 제시한다. 이를 위해 시공간 다중 연관 그래프 구축, 시공간 멀티모달 융합, 교통 그래프 네트워크 심화, 적응적인 시공간 그래프 등의 방법을 제안한다.&amp;#xD; 논문의 주요 혁신적 연구는 다음과 같습니다.&amp;#xD; 1. 교통망의 공간적 및 시간적, 동적 및 정적 특성을 얻기 위한 시공간적 다중 연관 그래프를 생성하는 새로운 방법을 제안한다. 이는 시공간 차원의 연관관계를 기반으로 여러 시공간 그래프를 생성하여 특징(feature)을 향상시키며 인접 시공간 그래프와 인접하지 않은 시공간 그래프 간의 의존 관계를 학습할 수 있다.&amp;#xD; 2. 시공간 멀티모달 데이터 융합 문제를 해결하기 위한 다단계 혼합 시공간 융합 방법을 제안한다. 이는 멀티헤드 주의(multi-headed attention) 메커니즘과 결합된 적응형 게이팅(adaptive gating) 메커니즘을 사용하여 보다 장기적이고 심층적인 복잡한 시공간 정보를 획득할 수 있다.&amp;#xD; 3. 시공간 그래프의 네트워크 계층의 심화로 인한 과도한 평활화 문제를 해결하기 위한 적응형 은닉층 연결 방법을 제안한다. 이는 특징 표현을 변환 및 분리한 후, 초기 및 은닉층 연결에 추가하고 파라미터를 통해 은닉층의 가중치를 적응적으로 조정한다. 이를 통해 노드의 수용성을 확장하고 더 깊은 교통 인접 노드의 특징을 종합적으로 획득할 수 있다.&amp;#xD; 4. 시공간 그래프의 유연성과 적응성을 향상시키기 위한 교통망의 복잡한 시공간 관계를 분석하기 위해 교통 예측을 위한 파라미터 공유 및 적응적 그래프 컨볼루션 방법을 제안한다. 이는 파라미터 공유를 위한 적응적 행렬을 구성하여 교통망 내의 공간 의존성 및 구조를 적응적으로 학습하고 조정한다."
        },
        {
          "rank": 43,
          "score": 0.6226276159286499,
          "doc_id": "JAKO202131452704683",
          "title": "항공 및 위성영상을 활용한 토지피복 관련 인공지능 학습 데이터 구축 및 알고리즘 적용 연구",
          "abstract": "본 연구의 목적은 항공 및 위성영상을 활용한 토지피복 관련 인공지능 학습 데이터를 구축, 검증 및 알고리즘 적용의 효율화 방안을 연구하였다. 이를 위하여 토지피복 8개 항목에 대하여 고해상도의 항공영상 및 Sentinel-2 인공위성에서 얻은 이미지를 사용하여 0.51 m 및 10 m Multi-resolution 데이터셋을 구축하였다. 또한, 학습 데이터의 구성은 Fine data (총 17,000개) 와 Coarse data (총 33,000개)를 동시 구축 및 정밀한 변화 탐지 및 대규모 학습 데이터셋 구축이라는 2가지 목적을 달성하였다. 학습 데이터의 정확도를 위한 검수는 정제 데이터, 어노테이션 및 샘플링으로 3단계로 진행하였다. 최종적으로 검수가 완료된 학습데이터를 Semantic Segmentation 알고리즘 중 U-Net, DeeplabV3+에 적용하여, 결과를 분석하였다. 분석결과 항공영상 기반의 토지피복 평균 정확도는 U- Net 77.8%, Deeplab V3+ 76.3% 및 위성영상 기반의 토지피복에 대한 평균 정확도는 U-Net 91.4%, Deeplab V3+ 85.8%이다. 본 연구를 통하여 구축된 고해상도 항공영상 및 위성영상을 이용한 토지피복 인공지능 학습 데이터셋은 토지피복 변화 및 분류에 도움이 되는 참조자료로 활용이 가능하다. 향후 우리나라 전체를 대상으로 인공지능 학습 데이터셋 구축 시, 토지피복을 연구하는 다양한 인공지능 분야에 활용될 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202131452704683&target=NART&cn=JAKO202131452704683",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "항공 및 위성영상을 활용한 토지피복 관련 인공지능 학습 데이터 구축 및 알고리즘 적용 연구 항공 및 위성영상을 활용한 토지피복 관련 인공지능 학습 데이터 구축 및 알고리즘 적용 연구 항공 및 위성영상을 활용한 토지피복 관련 인공지능 학습 데이터 구축 및 알고리즘 적용 연구 본 연구의 목적은 항공 및 위성영상을 활용한 토지피복 관련 인공지능 학습 데이터를 구축, 검증 및 알고리즘 적용의 효율화 방안을 연구하였다. 이를 위하여 토지피복 8개 항목에 대하여 고해상도의 항공영상 및 Sentinel-2 인공위성에서 얻은 이미지를 사용하여 0.51 m 및 10 m Multi-resolution 데이터셋을 구축하였다. 또한, 학습 데이터의 구성은 Fine data (총 17,000개) 와 Coarse data (총 33,000개)를 동시 구축 및 정밀한 변화 탐지 및 대규모 학습 데이터셋 구축이라는 2가지 목적을 달성하였다. 학습 데이터의 정확도를 위한 검수는 정제 데이터, 어노테이션 및 샘플링으로 3단계로 진행하였다. 최종적으로 검수가 완료된 학습데이터를 Semantic Segmentation 알고리즘 중 U-Net, DeeplabV3+에 적용하여, 결과를 분석하였다. 분석결과 항공영상 기반의 토지피복 평균 정확도는 U- Net 77.8%, Deeplab V3+ 76.3% 및 위성영상 기반의 토지피복에 대한 평균 정확도는 U-Net 91.4%, Deeplab V3+ 85.8%이다. 본 연구를 통하여 구축된 고해상도 항공영상 및 위성영상을 이용한 토지피복 인공지능 학습 데이터셋은 토지피복 변화 및 분류에 도움이 되는 참조자료로 활용이 가능하다. 향후 우리나라 전체를 대상으로 인공지능 학습 데이터셋 구축 시, 토지피복을 연구하는 다양한 인공지능 분야에 활용될 것으로 기대된다."
        },
        {
          "rank": 44,
          "score": 0.6217144727706909,
          "doc_id": "NART121556950",
          "title": "Integrating deep learning and traditional image enhancement techniques for underwater image enhancement",
          "abstract": "<P><B>Abstract</B><P>Underwater images usually suffer from colour distortion, blur, and low contrast, which hinder the subsequent processing of underwater information. To address these problems, this paper proposes a novel approach for single underwater images enhancement by integrating data&#x2010;driven deep learning and hand&#x2010;crafted image enhancement techniques. First, a statistical analysis is made on the average deviation of each channel of input underwater images to that of its corresponding ground truths, and it is found that both the red channel and the green channel of an underwater image contribute to its colour distortion. Concretely, the red channel of an underwater image is usually seriously attenuated, and the green channel is usually over strengthened. Motivated by such an observation, an attention mechanism guided residual module for underwater image colour correction is proposed, where the colour of the red channel of the underwater image and that of the green channel is compensated in a different way, respectively. Coupled with an attention mechanism, the residual module can adaptively extract and integrate the most discriminative features for colour correction. For scene contrast enhancement and scene deblurring, the traditional image enhancement techniques such as CLAHE (contrast limited adaptive histogram equalization) and Gamma correction are coupled with a multi&#x2010;scale convolutional neural network (MSCNN), where CLAHE and Gamma correction are used as complement to deal with the complex and changeable underwater imaging environment. Experiments on synthetic and real underwater images demonstrate that the proposed method performs favourably against the state&#x2010;of&#x2010;the&#x2010;art underwater image enhancement methods.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART121556950&target=NART&cn=NART121556950",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Integrating deep learning and traditional image enhancement techniques for underwater image enhancement Integrating deep learning and traditional image enhancement techniques for underwater image enhancement Integrating deep learning and traditional image enhancement techniques for underwater image enhancement <P><B>Abstract</B><P>Underwater images usually suffer from colour distortion, blur, and low contrast, which hinder the subsequent processing of underwater information. To address these problems, this paper proposes a novel approach for single underwater images enhancement by integrating data&#x2010;driven deep learning and hand&#x2010;crafted image enhancement techniques. First, a statistical analysis is made on the average deviation of each channel of input underwater images to that of its corresponding ground truths, and it is found that both the red channel and the green channel of an underwater image contribute to its colour distortion. Concretely, the red channel of an underwater image is usually seriously attenuated, and the green channel is usually over strengthened. Motivated by such an observation, an attention mechanism guided residual module for underwater image colour correction is proposed, where the colour of the red channel of the underwater image and that of the green channel is compensated in a different way, respectively. Coupled with an attention mechanism, the residual module can adaptively extract and integrate the most discriminative features for colour correction. For scene contrast enhancement and scene deblurring, the traditional image enhancement techniques such as CLAHE (contrast limited adaptive histogram equalization) and Gamma correction are coupled with a multi&#x2010;scale convolutional neural network (MSCNN), where CLAHE and Gamma correction are used as complement to deal with the complex and changeable underwater imaging environment. Experiments on synthetic and real underwater images demonstrate that the proposed method performs favourably against the state&#x2010;of&#x2010;the&#x2010;art underwater image enhancement methods.</P></P>"
        },
        {
          "rank": 45,
          "score": 0.6205441951751709,
          "doc_id": "ART002483857",
          "title": "Deep Learning in MR Image Processing",
          "abstract": "Recently, deep learning methods have shown great potential in various tasks that involve handling large amounts of digital data. In the field of MR imaging research, deep learning methods are also rapidly being applied in a wide range of areas to complement or replace traditional model-based methods. Deep learning methods have shown remarkable improvements in several MR image processing areas such as image reconstruction, image quality improvement, parameter mapping, image contrast conversion, and image segmentation. With the current rapid development of deep learning technologies, the importance of the role of deep learning in MR imaging research appears to be growing. In this article, we introduce the basic concepts of deep learning and review recent studies on various MR image processing applications.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002483857&target=NART&cn=ART002483857",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning in MR Image Processing Deep Learning in MR Image Processing Deep Learning in MR Image Processing Recently, deep learning methods have shown great potential in various tasks that involve handling large amounts of digital data. In the field of MR imaging research, deep learning methods are also rapidly being applied in a wide range of areas to complement or replace traditional model-based methods. Deep learning methods have shown remarkable improvements in several MR image processing areas such as image reconstruction, image quality improvement, parameter mapping, image contrast conversion, and image segmentation. With the current rapid development of deep learning technologies, the importance of the role of deep learning in MR imaging research appears to be growing. In this article, we introduce the basic concepts of deep learning and review recent studies on various MR image processing applications."
        },
        {
          "rank": 46,
          "score": 0.6201061606407166,
          "doc_id": "NART111939444",
          "title": "Review of deep learning for photoacoustic imaging",
          "abstract": "<P>Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging.</P><P>The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART111939444&target=NART&cn=NART111939444",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Review of deep learning for photoacoustic imaging Review of deep learning for photoacoustic imaging Review of deep learning for photoacoustic imaging <P>Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging.</P><P>The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.</P>"
        },
        {
          "rank": 47,
          "score": 0.6196914911270142,
          "doc_id": "ART002885478",
          "title": "Detection of fake news using deep learning CNN–RNN based methods",
          "abstract": "Fake news is inaccurate information that is intentionally disseminated for a specific purpose. If allowed to spread, fake news can harm the political and social spheres, so several studies are conducted to detect fake news. This study uses a deep learning method with several architectures such as CNN, Bidirectional LSTM, and ResNet, combined with pre-trained word embedding, trained using four different datasets. Each data goes through a data augmentation process using the back-translation method to reduce data imbalances between classes. The results showed that the Bidirectional LSTM architecture outperformed CNN and ResNet on all tested datasets.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002885478&target=NART&cn=ART002885478",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Detection of fake news using deep learning CNN–RNN based methods Detection of fake news using deep learning CNN–RNN based methods Detection of fake news using deep learning CNN–RNN based methods Fake news is inaccurate information that is intentionally disseminated for a specific purpose. If allowed to spread, fake news can harm the political and social spheres, so several studies are conducted to detect fake news. This study uses a deep learning method with several architectures such as CNN, Bidirectional LSTM, and ResNet, combined with pre-trained word embedding, trained using four different datasets. Each data goes through a data augmentation process using the back-translation method to reduce data imbalances between classes. The results showed that the Bidirectional LSTM architecture outperformed CNN and ResNet on all tested datasets."
        },
        {
          "rank": 48,
          "score": 0.6196302175521851,
          "doc_id": "ART003167534",
          "title": "Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior",
          "abstract": "Thick pinhole imaging system is widely used for diagnosing intense pulsed radiation sources. However, owing to the trade-off among spatial resolution, field of view (FOV) and signal-to-noise ratio (SNR), the imaging system normally falls short in achieving high-precision spatial diagnosis. In this paper, we propose an unsupervised deep learning method for single image super-resolution (SISR) of the thick pinhole imaging system. The point spread function (PSF) of the imaging system is obtained by analytical calculation and Monte Carlo simulation methods, and the mathematical model of the imaging system is established using a linear equation. To solve the ill-posed inverse problem, we adopt randomly initialized deep convolutional neural networks (DCNNs) as an image prior without pre-training, which is named deep image prior (DIP). The results demonstrate that, by utilizing the SISR technique to increase the number of pixels in reconstructed images, the proposed DIP algorithm can mitigate the spatial resolution degradation caused by an insufficient spatial sampling frequency of the camera. Compared with various classical algorithms, the proposed DIP algorithm exhibits superior capabilities in recovering highfrequency signals and suppressing ringing artifacts. Furthermore, the convergence and robustness of the proposed DIP algorithm under different random seeds and SNR conditions are also verified.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART003167534&target=NART&cn=ART003167534",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Thick pinhole imaging system is widely used for diagnosing intense pulsed radiation sources. However, owing to the trade-off among spatial resolution, field of view (FOV) and signal-to-noise ratio (SNR), the imaging system normally falls short in achieving high-precision spatial diagnosis. In this paper, we propose an unsupervised deep learning method for single image super-resolution (SISR) of the thick pinhole imaging system. The point spread function (PSF) of the imaging system is obtained by analytical calculation and Monte Carlo simulation methods, and the mathematical model of the imaging system is established using a linear equation. To solve the ill-posed inverse problem, we adopt randomly initialized deep convolutional neural networks (DCNNs) as an image prior without pre-training, which is named deep image prior (DIP). The results demonstrate that, by utilizing the SISR technique to increase the number of pixels in reconstructed images, the proposed DIP algorithm can mitigate the spatial resolution degradation caused by an insufficient spatial sampling frequency of the camera. Compared with various classical algorithms, the proposed DIP algorithm exhibits superior capabilities in recovering highfrequency signals and suppressing ringing artifacts. Furthermore, the convergence and robustness of the proposed DIP algorithm under different random seeds and SNR conditions are also verified."
        },
        {
          "rank": 49,
          "score": 0.6193068623542786,
          "doc_id": "JAKO201914260902234",
          "title": "딥러닝을 이용한 차로이탈 경고 시스템",
          "abstract": "최근 인공지능 기술이 급격히 발전하면서 첨단 운전자 지원 시스템 분야에 딥러닝 기술을 접목하여 기존의 기술보다 뛰어난 성능을 보여주기 위한 여러 연구들이 진행 되고 있다. 이러한 동향에 맞춰 본 논문 또한 첨단 운전자 지원 시스템의 핵심 요소 중 하나인 차로이탈 경고시스템에 딥러닝 기술을 접목한 방법을 제안한다. 제안하는 방법과 기존의 차선검출 기반의 경고시스템과의 비교 실험을 통해 그 성능을 평가 하였다. 고속도로 주행영상과 시내 주행영상을 이용한 두 가지의 서로 다른 환경에서 모두 제안하는 방법이 정확도 및 정밀도 부분에서 더 높은 수치를 보여주었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201914260902234&target=NART&cn=JAKO201914260902234",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 이용한 차로이탈 경고 시스템 딥러닝을 이용한 차로이탈 경고 시스템 딥러닝을 이용한 차로이탈 경고 시스템 최근 인공지능 기술이 급격히 발전하면서 첨단 운전자 지원 시스템 분야에 딥러닝 기술을 접목하여 기존의 기술보다 뛰어난 성능을 보여주기 위한 여러 연구들이 진행 되고 있다. 이러한 동향에 맞춰 본 논문 또한 첨단 운전자 지원 시스템의 핵심 요소 중 하나인 차로이탈 경고시스템에 딥러닝 기술을 접목한 방법을 제안한다. 제안하는 방법과 기존의 차선검출 기반의 경고시스템과의 비교 실험을 통해 그 성능을 평가 하였다. 고속도로 주행영상과 시내 주행영상을 이용한 두 가지의 서로 다른 환경에서 모두 제안하는 방법이 정확도 및 정밀도 부분에서 더 높은 수치를 보여주었다."
        },
        {
          "rank": 50,
          "score": 0.6176087856292725,
          "doc_id": "ART002574280",
          "title": "A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis",
          "abstract": "The image analysis is an important and predominant task for classifying the different parts of the image. The analysis of complex image analysis like histopathological define a crucial factor in oncology due to its ability to help pathologists for interpretation of images and therefore various feature extraction techniques have been evolved from time to time for such analysis. Although deep reinforcement learning is a new and emerging technique but very less effort has been made to compare the deep learning and deep reinforcement learning for image analysis. The paper highlights how both techniques differ in feature extraction from complex images and discusses the potential pros and cons. The use of Convolution Neural Network (CNN) in image segmentation, detection and diagnosis of tumour, feature extraction is important but there are several challenges that need to be overcome before Deep Learning can be applied to digital pathology. The one being is the availability of sufficient training examples for medical image datasets, feature extraction from whole area of the image, ground truth localized annotations, adversarial effects of input representations and extremely large size of the digital pathological slides (in gigabytes).Even though formulating Histopathological Image Analysis (HIA) as Multi Instance Learning (MIL) problem is a remarkable step where histopathological image is divided into high resolution patches to make predictions for the patch and then combining them for overall slide predictions but it suffers from loss of contextual and spatial information. In such cases the deep reinforcement learning techniques can be used to learn feature from the limited data without losing contextual and spatial information.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002574280&target=NART&cn=ART002574280",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis The image analysis is an important and predominant task for classifying the different parts of the image. The analysis of complex image analysis like histopathological define a crucial factor in oncology due to its ability to help pathologists for interpretation of images and therefore various feature extraction techniques have been evolved from time to time for such analysis. Although deep reinforcement learning is a new and emerging technique but very less effort has been made to compare the deep learning and deep reinforcement learning for image analysis. The paper highlights how both techniques differ in feature extraction from complex images and discusses the potential pros and cons. The use of Convolution Neural Network (CNN) in image segmentation, detection and diagnosis of tumour, feature extraction is important but there are several challenges that need to be overcome before Deep Learning can be applied to digital pathology. The one being is the availability of sufficient training examples for medical image datasets, feature extraction from whole area of the image, ground truth localized annotations, adversarial effects of input representations and extremely large size of the digital pathological slides (in gigabytes).Even though formulating Histopathological Image Analysis (HIA) as Multi Instance Learning (MIL) problem is a remarkable step where histopathological image is divided into high resolution patches to make predictions for the patch and then combining them for overall slide predictions but it suffers from loss of contextual and spatial information. In such cases the deep reinforcement learning techniques can be used to learn feature from the limited data without losing contextual and spatial information."
        }
      ]
    },
    {
      "query": "How are deep learning techniques envisioned to enhance radar imaging?",
      "query_meta": {
        "type": "single_hop",
        "index": 0
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8031672239303589,
          "doc_id": "NPAP12546494",
          "title": "Deep learning for radar",
          "abstract": "<P>Motivated by the recent advances in deep learning, we lay out a vision of how deep learning techniques can be used in radar. Specifically, our discussion focuses on the use of deep learning to advance the state-of-the-art in radar imaging. While deep learning can be directly applied to automatic target recognition (ATR), the relevance of these techniques in other radar problems is not obvious. We argue that deep learning can play a central role in advancing the state-of-the-art in a wide range of radar imaging problems, discuss the challenges associated with applying these methods, and the potential advancements that are expected. We lay out an approach to design a network architecture based on the specific structure of the synthetic aperture radar (SAR) imaging problem that augments learning with traditional SAR modelling. This framework allows for capture of the non-linearity of the SAR forward model. Furthermore, we demonstrate how this process can be used to learn and compensate for trajectory based phase error for the autofocus problem.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12546494&target=NART&cn=NPAP12546494",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning for radar Deep learning for radar Deep learning for radar <P>Motivated by the recent advances in deep learning, we lay out a vision of how deep learning techniques can be used in radar. Specifically, our discussion focuses on the use of deep learning to advance the state-of-the-art in radar imaging. While deep learning can be directly applied to automatic target recognition (ATR), the relevance of these techniques in other radar problems is not obvious. We argue that deep learning can play a central role in advancing the state-of-the-art in a wide range of radar imaging problems, discuss the challenges associated with applying these methods, and the potential advancements that are expected. We lay out an approach to design a network architecture based on the specific structure of the synthetic aperture radar (SAR) imaging problem that augments learning with traditional SAR modelling. This framework allows for capture of the non-linearity of the SAR forward model. Furthermore, we demonstrate how this process can be used to learn and compensate for trajectory based phase error for the autofocus problem.</P>"
        },
        {
          "rank": 2,
          "score": 0.7866820096969604,
          "doc_id": "NART121030945",
          "title": "MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning",
          "abstract": "<P>Transmitting orthogonal waveforms are the basis for giving full play to the advantages of MIMO radar imaging technology, but the commonly used waveforms with the same frequency cannot meet the orthogonality requirement, resulting in serious coupling noise in traditional imaging methods and affecting the imaging effect. In order to effectively suppress the mutual coupling interference caused by non-orthogonal waveforms, a new non-orthogonal waveform MIMO radar imaging method based on deep learning is proposed in this paper: with the powerful nonlinear fitting ability of deep learning, the mapping relationship between the non-orthogonal waveform MIMO radar echo and ideal target image is automatically learned by constructing a deep imaging network and training on a large number of simulated training data. The learned imaging network can effectively suppress the coupling interference between non-ideal orthogonal waveforms and improve the imaging quality of MIMO radar. Finally, the effectiveness of the proposed method is verified by experiments with point scattering model data and electromagnetic scattering calculation data.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART121030945&target=NART&cn=NART121030945",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning <P>Transmitting orthogonal waveforms are the basis for giving full play to the advantages of MIMO radar imaging technology, but the commonly used waveforms with the same frequency cannot meet the orthogonality requirement, resulting in serious coupling noise in traditional imaging methods and affecting the imaging effect. In order to effectively suppress the mutual coupling interference caused by non-orthogonal waveforms, a new non-orthogonal waveform MIMO radar imaging method based on deep learning is proposed in this paper: with the powerful nonlinear fitting ability of deep learning, the mapping relationship between the non-orthogonal waveform MIMO radar echo and ideal target image is automatically learned by constructing a deep imaging network and training on a large number of simulated training data. The learned imaging network can effectively suppress the coupling interference between non-ideal orthogonal waveforms and improve the imaging quality of MIMO radar. Finally, the effectiveness of the proposed method is verified by experiments with point scattering model data and electromagnetic scattering calculation data.</P>"
        },
        {
          "rank": 3,
          "score": 0.7721282243728638,
          "doc_id": "JAKO201923233204235",
          "title": "딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구",
          "abstract": "Classification of radar signals in the field of electronic warfare is a problem of discriminating threat types by analyzing enemy threat radar signals such as aircraft, radar, and missile received through electronic warfare equipment. Recent radar systems have adopted a variety of modulation schemes that are different from those used in conventional systems, and are often difficult to analyze using existing algorithms. Also, it is necessary to design a robust algorithm for the signal received in the real environment due to the environmental influence and the measurement error due to the characteristics of the hardware. In this paper, we propose a radar signal classification method which are not affected by radar signal modulation methods and noise generation by using deep learning techniques.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201923233204235&target=NART&cn=JAKO201923233204235",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 Classification of radar signals in the field of electronic warfare is a problem of discriminating threat types by analyzing enemy threat radar signals such as aircraft, radar, and missile received through electronic warfare equipment. Recent radar systems have adopted a variety of modulation schemes that are different from those used in conventional systems, and are often difficult to analyze using existing algorithms. Also, it is necessary to design a robust algorithm for the signal received in the real environment due to the environmental influence and the measurement error due to the characteristics of the hardware. In this paper, we propose a radar signal classification method which are not affected by radar signal modulation methods and noise generation by using deep learning techniques."
        },
        {
          "rank": 4,
          "score": 0.7647056579589844,
          "doc_id": "NART116403822",
          "title": "Deep-Learning for Radar: A Survey",
          "abstract": "<P>A comprehensive and well-structured review on the application of deep learning (DL) based algorithms, such as convolutional neural networks (CNN) and long-short term memory (LSTM), in radar signal processing is given. The following DL application areas are covered: i) radar waveform and antenna array design; ii) passive or low probability of interception (LPI) radar waveform recognition; iii) automatic target recognition (ATR) based on high range resolution profiles (HRRPs), Doppler signatures, and synthetic aperture radar (SAR) images; and iv) radar jamming/clutter recognition and suppression. Although DL is unanimously praised as the ultimate solution to many bottleneck problems in most of existing works on similar topics, both the positive and the negative sides of stories about DL are checked in this work. Specifically, two limiting factors of the real-life performance of deep neural networks (DNNs), limited training samples and adversarial examples, are thoroughly examined. By investigating the relationship between the DL-based algorithms proposed in various papers and linking them together to form a full picture, this work serves as a valuable source for researchers who are seeking potential research opportunities in this promising research field.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART116403822&target=NART&cn=NART116403822",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep-Learning for Radar: A Survey Deep-Learning for Radar: A Survey Deep-Learning for Radar: A Survey <P>A comprehensive and well-structured review on the application of deep learning (DL) based algorithms, such as convolutional neural networks (CNN) and long-short term memory (LSTM), in radar signal processing is given. The following DL application areas are covered: i) radar waveform and antenna array design; ii) passive or low probability of interception (LPI) radar waveform recognition; iii) automatic target recognition (ATR) based on high range resolution profiles (HRRPs), Doppler signatures, and synthetic aperture radar (SAR) images; and iv) radar jamming/clutter recognition and suppression. Although DL is unanimously praised as the ultimate solution to many bottleneck problems in most of existing works on similar topics, both the positive and the negative sides of stories about DL are checked in this work. Specifically, two limiting factors of the real-life performance of deep neural networks (DNNs), limited training samples and adversarial examples, are thoroughly examined. By investigating the relationship between the DL-based algorithms proposed in various papers and linking them together to form a full picture, this work serves as a valuable source for researchers who are seeking potential research opportunities in this promising research field.</P>"
        },
        {
          "rank": 5,
          "score": 0.7636831402778625,
          "doc_id": "NART106334316",
          "title": "Deep learning for waveform estimation and imaging in passive radar",
          "abstract": "<P>The authors consider a bistatic configuration with a stationary transmitter transmitting unknown waveforms of opportunity and a single moving receiver and present a deep learning (DL) framework for passive synthetic aperture radar (SAR) imaging. They approach DL from an optimisation based perspective and formulate image reconstruction as a machine learning task. By unfolding the iterations of a proximal gradient descent algorithm, they construct a deep recurrent neural network (RNN) that is parameterised by the transmitted waveforms. They cascade the RNN structure with a decoder stage to form a recurrent auto&#x2010;encoder architecture. They then use backpropagation to learn transmitted waveforms by training the network in an unsupervised manner using SAR measurements. The highly non&#x2010;convex problem of backpropagation is guided to a feasible solution over the parameter space by initialising the network with the known components of the SAR forward model. Moreover, prior information regarding the waveform structure is incorporated during initialisation and backpropagation. They demonstrate the effectiveness of the DL&#x2010;based approach through numerical simulations that show focused, high contrast imagery using a single receiver antenna at realistic signal&#x2010;to&#x2010;noise&#x2010;ratio levels.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106334316&target=NART&cn=NART106334316",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning for waveform estimation and imaging in passive radar Deep learning for waveform estimation and imaging in passive radar Deep learning for waveform estimation and imaging in passive radar <P>The authors consider a bistatic configuration with a stationary transmitter transmitting unknown waveforms of opportunity and a single moving receiver and present a deep learning (DL) framework for passive synthetic aperture radar (SAR) imaging. They approach DL from an optimisation based perspective and formulate image reconstruction as a machine learning task. By unfolding the iterations of a proximal gradient descent algorithm, they construct a deep recurrent neural network (RNN) that is parameterised by the transmitted waveforms. They cascade the RNN structure with a decoder stage to form a recurrent auto&#x2010;encoder architecture. They then use backpropagation to learn transmitted waveforms by training the network in an unsupervised manner using SAR measurements. The highly non&#x2010;convex problem of backpropagation is guided to a feasible solution over the parameter space by initialising the network with the known components of the SAR forward model. Moreover, prior information regarding the waveform structure is incorporated during initialisation and backpropagation. They demonstrate the effectiveness of the DL&#x2010;based approach through numerical simulations that show focused, high contrast imagery using a single receiver antenna at realistic signal&#x2010;to&#x2010;noise&#x2010;ratio levels.</P>"
        },
        {
          "rank": 6,
          "score": 0.7610030174255371,
          "doc_id": "NART123168643",
          "title": "Synthetic Aperture Radar (SAR) Meets Deep Learning",
          "abstract": "<P>Synthetic aperture radar (SAR) is an important active microwave imaging sensor [...]</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART123168643&target=NART&cn=NART123168643",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Synthetic Aperture Radar (SAR) Meets Deep Learning Synthetic Aperture Radar (SAR) Meets Deep Learning Synthetic Aperture Radar (SAR) Meets Deep Learning <P>Synthetic aperture radar (SAR) is an important active microwave imaging sensor [...]</P>"
        },
        {
          "rank": 7,
          "score": 0.7601243257522583,
          "doc_id": "NART127041948",
          "title": "Deep Learning Techniques in Radar Emitter Identification",
          "abstract": "<P>In the field of electronic warfare (EW), one of the crucial roles of electronic intelligence is the identification of radar signals. In an operational environment, it is very essential to identify radar emitters whether friend or foe so that appropriate radar countermeasures can be taken against them. With the electromagnetic environment becoming increasingly complex and the diversity of signal features, radar emitter identification with high recognition accuracy has become a significantly challenging task. Traditional radar identification methods have shown some limitations in this complex electromagnetic scenario. Several radar classification and identification methods based on artificial neural networks have emerged with the emergence of artificial neural networks, notably deep learning approaches. Machine learning and deep learning algorithms are now frequently utilized to extract various types of information from radar signals more accurately and robustly. This paper illustrates the use of Deep Neural Networks (DNN) in radar applications for emitter classification and identification. Since deep learning approaches are capable of accurately classifying complicated patterns in radar signals, they have demonstrated significant promise for identifying radar emitters. By offering a thorough literature analysis of deep learning-based methodologies, the study intends to assist researchers and practitioners in better understanding the application of deep learning techniques to challenges related to the classification and identification of radar emitters. The study demonstrates that DNN can be used successfully in applications for radar classification and identification.&amp;#xD; &amp;#xD; </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART127041948&target=NART&cn=NART127041948",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning Techniques in Radar Emitter Identification Deep Learning Techniques in Radar Emitter Identification Deep Learning Techniques in Radar Emitter Identification <P>In the field of electronic warfare (EW), one of the crucial roles of electronic intelligence is the identification of radar signals. In an operational environment, it is very essential to identify radar emitters whether friend or foe so that appropriate radar countermeasures can be taken against them. With the electromagnetic environment becoming increasingly complex and the diversity of signal features, radar emitter identification with high recognition accuracy has become a significantly challenging task. Traditional radar identification methods have shown some limitations in this complex electromagnetic scenario. Several radar classification and identification methods based on artificial neural networks have emerged with the emergence of artificial neural networks, notably deep learning approaches. Machine learning and deep learning algorithms are now frequently utilized to extract various types of information from radar signals more accurately and robustly. This paper illustrates the use of Deep Neural Networks (DNN) in radar applications for emitter classification and identification. Since deep learning approaches are capable of accurately classifying complicated patterns in radar signals, they have demonstrated significant promise for identifying radar emitters. By offering a thorough literature analysis of deep learning-based methodologies, the study intends to assist researchers and practitioners in better understanding the application of deep learning techniques to challenges related to the classification and identification of radar emitters. The study demonstrates that DNN can be used successfully in applications for radar classification and identification.&amp;#xD; &amp;#xD; </P>"
        },
        {
          "rank": 8,
          "score": 0.7544536590576172,
          "doc_id": "NART110796699",
          "title": "Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network",
          "abstract": "<P>As compared with traditional ISAR imaging methods, the compressive sensing (CS)&#x2010;based imaging methods can obtain high&#x2010;quality images using much less under&#x2010;sampled data. However, the availability or appropriateness of the sparse representation of the target scene and the relatively low computational efficiency of image reconstruction algorithms limit the performance and application of the CS&#x2010;based ISAR imaging methods. In recent years, the deep learning technology has been applied in many fields and achieved outstanding performance in image classification, image reconstruction etc. DL implements the tasks using the deep neural network (DNN), which composes multiple hidden layers and non&#x2010;linear activation layer. In this study, a novel ISAR imaging method that uses a complex&#x2010;value deep neural network (CV&#x2010;DNN) to perform the image formation using under&#x2010;sampled data is proposed. The CV&#x2010;DNN architecture can extract and exploit the sparse feature of the target image extremely well by multilayer non&#x2010;linear processing. The experimental results show that the proposed CV&#x2010;DNN&#x2010;based ISAR imaging method can provide better shape reconstruction of target with less data than state&#x2010;of&#x2010;the&#x2010;art CS reconstruction algorithms and improve the imaging efficiency obviously.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART110796699&target=NART&cn=NART110796699",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network <P>As compared with traditional ISAR imaging methods, the compressive sensing (CS)&#x2010;based imaging methods can obtain high&#x2010;quality images using much less under&#x2010;sampled data. However, the availability or appropriateness of the sparse representation of the target scene and the relatively low computational efficiency of image reconstruction algorithms limit the performance and application of the CS&#x2010;based ISAR imaging methods. In recent years, the deep learning technology has been applied in many fields and achieved outstanding performance in image classification, image reconstruction etc. DL implements the tasks using the deep neural network (DNN), which composes multiple hidden layers and non&#x2010;linear activation layer. In this study, a novel ISAR imaging method that uses a complex&#x2010;value deep neural network (CV&#x2010;DNN) to perform the image formation using under&#x2010;sampled data is proposed. The CV&#x2010;DNN architecture can extract and exploit the sparse feature of the target image extremely well by multilayer non&#x2010;linear processing. The experimental results show that the proposed CV&#x2010;DNN&#x2010;based ISAR imaging method can provide better shape reconstruction of target with less data than state&#x2010;of&#x2010;the&#x2010;art CS reconstruction algorithms and improve the imaging efficiency obviously.</P>"
        },
        {
          "rank": 9,
          "score": 0.7527433037757874,
          "doc_id": "DIKO0016954237",
          "title": "밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구",
          "abstract": "본 논문에서는 합성개구레이더 (Synthetic Aperture Radar, SAR) 시스템의 데이터를 획득하는 과정에서 발생할 수 있는 위상 오차를 보상하기 위해, 딥러닝 기반 요동 보상 방법으로 Unsupervised Image-to-image Translation (UNIT) 네트워크를 제안한다. 일반적으로 SAR 시스템을 이용한 데이터 취득 과정에서 레이더가 부착된 플랫폼의 비이상적인 경로나 불안정한 자세로 인해 위상 오차가 포함된 데이터를 얻는 문제가 발생할 수 있다. 이러한 위상 오차는 주변 환경 인식 및 표적 탐지 성능을 감소시키며, 군사 목적의 감시, 정찰을 위한 SAR 시스템과 자율주행 분야에서 지능형 차량의 경로 계획 및 사고 위협 회피를 위해 주변 환경 표현이 필수적이다. 따라서, 본 연구에서는 딥러닝 기반 요동 보상 방법을 제안하고, 밀리미터파 레이더 센서를 이용한 실험을 통해 제안된 방법의 성능을 검증한다. 제안된 방법은 Peak Signal-to-Noise Ratio (PSNR)와 Structural Similarity Index Measure (SSIM) 측면에서 기존의 요동 보상 기법들과 성능 평가 및 비교가 수행된다. 실제 측정 데이터를 기반으로 성능을 비교한 결과, 제안된 UNIT 네트워크는 기존 요동 보상 기법들 대비 PSNR은 평균 10.17%, SSIM은 9.4% 향상되는 것을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016954237&target=NART&cn=DIKO0016954237",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 본 논문에서는 합성개구레이더 (Synthetic Aperture Radar, SAR) 시스템의 데이터를 획득하는 과정에서 발생할 수 있는 위상 오차를 보상하기 위해, 딥러닝 기반 요동 보상 방법으로 Unsupervised Image-to-image Translation (UNIT) 네트워크를 제안한다. 일반적으로 SAR 시스템을 이용한 데이터 취득 과정에서 레이더가 부착된 플랫폼의 비이상적인 경로나 불안정한 자세로 인해 위상 오차가 포함된 데이터를 얻는 문제가 발생할 수 있다. 이러한 위상 오차는 주변 환경 인식 및 표적 탐지 성능을 감소시키며, 군사 목적의 감시, 정찰을 위한 SAR 시스템과 자율주행 분야에서 지능형 차량의 경로 계획 및 사고 위협 회피를 위해 주변 환경 표현이 필수적이다. 따라서, 본 연구에서는 딥러닝 기반 요동 보상 방법을 제안하고, 밀리미터파 레이더 센서를 이용한 실험을 통해 제안된 방법의 성능을 검증한다. 제안된 방법은 Peak Signal-to-Noise Ratio (PSNR)와 Structural Similarity Index Measure (SSIM) 측면에서 기존의 요동 보상 기법들과 성능 평가 및 비교가 수행된다. 실제 측정 데이터를 기반으로 성능을 비교한 결과, 제안된 UNIT 네트워크는 기존 요동 보상 기법들 대비 PSNR은 평균 10.17%, SSIM은 9.4% 향상되는 것을 확인하였다."
        },
        {
          "rank": 10,
          "score": 0.7512218952178955,
          "doc_id": "NART106334309",
          "title": "Cognitive radar antenna selection via deep learning",
          "abstract": "<P>Direction&#x2010;of&#x2010;arrival (DoA) estimation of targets improves with the number of elements employed by a phased array radar antenna. Since larger arrays have high associated cost, area and computational load, there is a recent interest in thinning the antenna arrays without loss of far&#x2010;field DoA accuracy. In this context, a cognitive radar may deploy a full array and then select an optimal subarray to transmit and receive the signals in response to changes in the target environment. Prior works have used optimisation and greedy search methods to pick the best subarrays cognitively. In this study, deep learning is leveraged to address the antenna selection problem. Specifically, they construct a convolutional neural network (CNN) as a multi&#x2010;class classification framework, where each class designates a different subarray. The proposed network determines a new array every time data is received by the radar, thereby making antenna selection a cognitive operation. Their numerical experiments show that the proposed CNN structure provides 22% better classification performance than a support vector machine and the resulting subarrays yield 72% more accurate DoA estimates than random array selections.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106334309&target=NART&cn=NART106334309",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Cognitive radar antenna selection via deep learning Cognitive radar antenna selection via deep learning Cognitive radar antenna selection via deep learning <P>Direction&#x2010;of&#x2010;arrival (DoA) estimation of targets improves with the number of elements employed by a phased array radar antenna. Since larger arrays have high associated cost, area and computational load, there is a recent interest in thinning the antenna arrays without loss of far&#x2010;field DoA accuracy. In this context, a cognitive radar may deploy a full array and then select an optimal subarray to transmit and receive the signals in response to changes in the target environment. Prior works have used optimisation and greedy search methods to pick the best subarrays cognitively. In this study, deep learning is leveraged to address the antenna selection problem. Specifically, they construct a convolutional neural network (CNN) as a multi&#x2010;class classification framework, where each class designates a different subarray. The proposed network determines a new array every time data is received by the radar, thereby making antenna selection a cognitive operation. Their numerical experiments show that the proposed CNN structure provides 22% better classification performance than a support vector machine and the resulting subarrays yield 72% more accurate DoA estimates than random array selections.</P>"
        },
        {
          "rank": 11,
          "score": 0.7374878525733948,
          "doc_id": "NART66897872",
          "title": "Image enhancement in forward imaging radar using modified apodisation technique",
          "abstract": "<P>Forward imaging radar using an UWB signal has been developed by many researchers. The image quality in this radar is not satisfactory because of the limitation of aperture length. Proposed is an image enhancement method using a modified apodisation technique in forward imaging radar. An experiment is carried out to validate the proposed method. The azimuth resolution and peak-to-sidelobe ratio are improved by the proposed method by up to about 11.7% and 10 dB, respectively.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART66897872&target=NART&cn=NART66897872",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Image enhancement in forward imaging radar using modified apodisation technique Image enhancement in forward imaging radar using modified apodisation technique Image enhancement in forward imaging radar using modified apodisation technique <P>Forward imaging radar using an UWB signal has been developed by many researchers. The image quality in this radar is not satisfactory because of the limitation of aperture length. Proposed is an image enhancement method using a modified apodisation technique in forward imaging radar. An experiment is carried out to validate the proposed method. The azimuth resolution and peak-to-sidelobe ratio are improved by the proposed method by up to about 11.7% and 10 dB, respectively.</P>"
        },
        {
          "rank": 12,
          "score": 0.735914945602417,
          "doc_id": "NART125907540",
          "title": "Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review",
          "abstract": "<P>Radar automatic target recognition (RATR) technology is fundamental but complicated system engineering that combines sensor, target, environment, and signal processing technology, etc. It plays a significant role in improving the level and capabilities of military and civilian automation. Although RATR has been successfully applied in some aspects, the complete theoretical system has not been established. At present, deep learning algorithms have received a lot of attention and have emerged as potential and feasible solutions in RATR. This paper mainly reviews related articles published between 2010 and 2022, which corresponds to the period when deep learning methods were introduced into RATR research. In this paper, the current research status of radar target characteristics is summarized, including motion, micro-motion, one-dimensional, and two-dimensional characteristics, etc. This paper reviews the progress of deep learning methods in the feature extraction and recognition of radar target characteristics in recent years, including space, air, ground, sea-surface targets, etc. Due to more and more attention and research results published in the past few years, it is hoped that this review can provide potential guidance for future research and application of deep learning in fields related to RATR.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART125907540&target=NART&cn=NART125907540",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review <P>Radar automatic target recognition (RATR) technology is fundamental but complicated system engineering that combines sensor, target, environment, and signal processing technology, etc. It plays a significant role in improving the level and capabilities of military and civilian automation. Although RATR has been successfully applied in some aspects, the complete theoretical system has not been established. At present, deep learning algorithms have received a lot of attention and have emerged as potential and feasible solutions in RATR. This paper mainly reviews related articles published between 2010 and 2022, which corresponds to the period when deep learning methods were introduced into RATR research. In this paper, the current research status of radar target characteristics is summarized, including motion, micro-motion, one-dimensional, and two-dimensional characteristics, etc. This paper reviews the progress of deep learning methods in the feature extraction and recognition of radar target characteristics in recent years, including space, air, ground, sea-surface targets, etc. Due to more and more attention and research results published in the past few years, it is hoped that this review can provide potential guidance for future research and application of deep learning in fields related to RATR.</P>"
        },
        {
          "rank": 13,
          "score": 0.7320610284805298,
          "doc_id": "NART117063882",
          "title": "Advancing Radar Nowcasting Through Deep Transfer Learning",
          "abstract": "<P>Deep learning is emerging as a powerful tool in scientific applications, such as radar-based convective storm nowcasting. However, it is still a challenge to extend the application of a well-trained deep learning nowcasting model, which demands to incorporate the learned knowledge at a certain location to other locations characterized by different precipitation features. This article designs a transfer learning framework to tackle this problem. A convolutional neural network (CNN)-based nowcasting method is utilized as the benchmark, based on which two transfer learning models are constructed through fine-tune and maximum mean discrepancy (MMD) minimization. The base CNN model is trained using radar data in the source study domain near Beijing, China, whereas the transferred models are applied to the target domain near Guangzhou, China, with only a small amount of data in the target area. The influence of a varying number of target data samples on the nowcasting performance is quantified. The experimental results demonstrate that the deep transfer learning models can improve the nowcasting skills.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART117063882&target=NART&cn=NART117063882",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Advancing Radar Nowcasting Through Deep Transfer Learning Advancing Radar Nowcasting Through Deep Transfer Learning Advancing Radar Nowcasting Through Deep Transfer Learning <P>Deep learning is emerging as a powerful tool in scientific applications, such as radar-based convective storm nowcasting. However, it is still a challenge to extend the application of a well-trained deep learning nowcasting model, which demands to incorporate the learned knowledge at a certain location to other locations characterized by different precipitation features. This article designs a transfer learning framework to tackle this problem. A convolutional neural network (CNN)-based nowcasting method is utilized as the benchmark, based on which two transfer learning models are constructed through fine-tune and maximum mean discrepancy (MMD) minimization. The base CNN model is trained using radar data in the source study domain near Beijing, China, whereas the transferred models are applied to the target domain near Guangzhou, China, with only a small amount of data in the target area. The influence of a varying number of target data samples on the nowcasting performance is quantified. The experimental results demonstrate that the deep transfer learning models can improve the nowcasting skills.</P>"
        },
        {
          "rank": 14,
          "score": 0.7218596339225769,
          "doc_id": "NART124851615",
          "title": "Radar Spectrum Image Classification Based on Deep Learning",
          "abstract": "<P>With the continuous development and progress of science and technology, the increasingly complex electromagnetic environment and the research and development of new radar systems have led to the emergence of various radar signals. Traditional methods of radar emitter identification cannot meet the needs of current practical applications. For the purpose of classification and recognition of radar emitter signals, this paper proposes an improved EfficientNetv2-s classification method based on deep learning for more precise classification and recognition of radar radiation source signals. Using 16 different types of radar signal parameters from the signal parameter setting table, the proposed method generates random data sets consisting of spectrum images with varying amplitude. The proposed method replaces two-dimensional convolution in EfficientNetV2 with one-dimensional convolution. Additionally, the channel attention mechanism of the EfficientNetv2-s is optimized and modified to obtain attention weights without dimensional reduction, resulting in superior accuracy. Compared with other deep-learning image-classification methods, the test results of this method have better classification accuracy on the test set: the top1 accuracy reaches 98.12%, which is 0.17~3.12% higher than other methods. Furthermore, the proposed method has lower complexity compared to most methods.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART124851615&target=NART&cn=NART124851615",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Radar Spectrum Image Classification Based on Deep Learning Radar Spectrum Image Classification Based on Deep Learning Radar Spectrum Image Classification Based on Deep Learning <P>With the continuous development and progress of science and technology, the increasingly complex electromagnetic environment and the research and development of new radar systems have led to the emergence of various radar signals. Traditional methods of radar emitter identification cannot meet the needs of current practical applications. For the purpose of classification and recognition of radar emitter signals, this paper proposes an improved EfficientNetv2-s classification method based on deep learning for more precise classification and recognition of radar radiation source signals. Using 16 different types of radar signal parameters from the signal parameter setting table, the proposed method generates random data sets consisting of spectrum images with varying amplitude. The proposed method replaces two-dimensional convolution in EfficientNetV2 with one-dimensional convolution. Additionally, the channel attention mechanism of the EfficientNetv2-s is optimized and modified to obtain attention weights without dimensional reduction, resulting in superior accuracy. Compared with other deep-learning image-classification methods, the test results of this method have better classification accuracy on the test set: the top1 accuracy reaches 98.12%, which is 0.17~3.12% higher than other methods. Furthermore, the proposed method has lower complexity compared to most methods.</P>"
        },
        {
          "rank": 15,
          "score": 0.7216520309448242,
          "doc_id": "JAKO202201253148351",
          "title": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰",
          "abstract": "위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202201253148351&target=NART&cn=JAKO202201253148351",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다."
        },
        {
          "rank": 16,
          "score": 0.7184805870056152,
          "doc_id": "ART002483857",
          "title": "Deep Learning in MR Image Processing",
          "abstract": "Recently, deep learning methods have shown great potential in various tasks that involve handling large amounts of digital data. In the field of MR imaging research, deep learning methods are also rapidly being applied in a wide range of areas to complement or replace traditional model-based methods. Deep learning methods have shown remarkable improvements in several MR image processing areas such as image reconstruction, image quality improvement, parameter mapping, image contrast conversion, and image segmentation. With the current rapid development of deep learning technologies, the importance of the role of deep learning in MR imaging research appears to be growing. In this article, we introduce the basic concepts of deep learning and review recent studies on various MR image processing applications.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002483857&target=NART&cn=ART002483857",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning in MR Image Processing Deep Learning in MR Image Processing Deep Learning in MR Image Processing Recently, deep learning methods have shown great potential in various tasks that involve handling large amounts of digital data. In the field of MR imaging research, deep learning methods are also rapidly being applied in a wide range of areas to complement or replace traditional model-based methods. Deep learning methods have shown remarkable improvements in several MR image processing areas such as image reconstruction, image quality improvement, parameter mapping, image contrast conversion, and image segmentation. With the current rapid development of deep learning technologies, the importance of the role of deep learning in MR imaging research appears to be growing. In this article, we introduce the basic concepts of deep learning and review recent studies on various MR image processing applications."
        },
        {
          "rank": 17,
          "score": 0.7150101661682129,
          "doc_id": "NART84975182",
          "title": "Deep Learning for Passive Synthetic Aperture Radar",
          "abstract": "<P>We introduce a deep learning (DL) framework for inverse problems in imaging, and demonstrate the advantages and applicability of this approach in passive synthetic aperture radar (SAR) image reconstruction. We interpret image reconstruction as a machine learning task and utilize deep networks as forward and inverse solvers for imaging. Specifically, we design a recurrent neural network (RNN) architecture as an inverse solver based on the iterations of proximal gradient descent optimization methods. We further adapt the RNN architecture to image reconstruction problems by transforming the network into a recurrent auto-encoder, thereby allowing for unsupervised training. Our DL based inverse solver is particularly suitable for a class of image formation problems in which the forward model is only partially known. The ability to learn forward models and hyper parameters combined with unsupervised training approach establish our recurrent auto-encoder suitable for real world applications. We demonstrate the performance of our method in passive SAR image reconstruction. In this regime a source of opportunity, with unknown location and transmitted waveform, is used to illuminate a scene of interest. We investigate recurrent auto-encoder architecture based on the <TEX>$\\ell _1$</TEX> and <TEX>$\\ell _0$</TEX> constrained least-squares problem. We present a projected stochastic gradient descent based training scheme which incorporates constraints of the unknown model parameters. We demonstrate through extensive numerical simulations that our DL based approach out performs conventional sparse coding methods in terms of computation and reconstructed image quality, specifically, when no information about the transmitter is available.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART84975182&target=NART&cn=NART84975182",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning for Passive Synthetic Aperture Radar Deep Learning for Passive Synthetic Aperture Radar Deep Learning for Passive Synthetic Aperture Radar <P>We introduce a deep learning (DL) framework for inverse problems in imaging, and demonstrate the advantages and applicability of this approach in passive synthetic aperture radar (SAR) image reconstruction. We interpret image reconstruction as a machine learning task and utilize deep networks as forward and inverse solvers for imaging. Specifically, we design a recurrent neural network (RNN) architecture as an inverse solver based on the iterations of proximal gradient descent optimization methods. We further adapt the RNN architecture to image reconstruction problems by transforming the network into a recurrent auto-encoder, thereby allowing for unsupervised training. Our DL based inverse solver is particularly suitable for a class of image formation problems in which the forward model is only partially known. The ability to learn forward models and hyper parameters combined with unsupervised training approach establish our recurrent auto-encoder suitable for real world applications. We demonstrate the performance of our method in passive SAR image reconstruction. In this regime a source of opportunity, with unknown location and transmitted waveform, is used to illuminate a scene of interest. We investigate recurrent auto-encoder architecture based on the <TEX>$\\ell _1$</TEX> and <TEX>$\\ell _0$</TEX> constrained least-squares problem. We present a projected stochastic gradient descent based training scheme which incorporates constraints of the unknown model parameters. We demonstrate through extensive numerical simulations that our DL based approach out performs conventional sparse coding methods in terms of computation and reconstructed image quality, specifically, when no information about the transmitter is available.</P>"
        },
        {
          "rank": 18,
          "score": 0.7141252756118774,
          "doc_id": "DIKO0015644673",
          "title": "M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지",
          "abstract": "해상 교통량의 증가로 인해 해상 선박관리의 필요성이 늘어남에 따라 선박을 탐지하기 위한 연구들이 꾸준히 수행되어왔다. 특히 위성레이더 영상은 시간과 기후에 영향을 받지 않고 촬영할 수 있다는 장점으로 인해 선박탐지를 위한 많은 연구에서 활용되어왔다. 최근에는 딥러닝 기법의 발전으로 인해 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구들이 꾸준히 수행되고 있다. 그런데 위성레이더 영상은 값의 분포범위가 매우 넓고, 많은 스펙클 노이즈가 존재한다. 이러한 요소들은 딥러닝 모델의 학습에 부정적인 영향을 끼칠 수 있으므로 전처리를 통해 해당 요소들을 저감해줄 필요가 있다. 본 연구에서는 전처리된 위성레이더 영상으로부터 딥러닝 선박탐지를 수행하고, 영상의 전처리가 딥러닝 선박탐지에 미치는 요소를 비교분석 하고자 한다.&amp;#xD; 본 연구를 위해 TerraSAR-X와 COSMO-SkyMed 위성레이더 영상을 이용했다. 영상을 딥러닝 학습에 이용하기 전에 먼저 총 세 가지 다른 방법으로 전처리를 수행했다. 첫 번째는 위성레이더 영상에서 강도 값만을 추출한 강도 영상을 생성하는 방법이다. 강도 영상은 값의 범위가 매우 넓을 뿐만 아니라 많은 스펙클 노이즈를 가지고 있다. 두 번째는 강도영상에서 값의 단위를 데시벨로 변환한 데시벨 영상을 생성하는 방법이다. 데시벨 영상은 강도영상과 마찬가지로 많은 스펙클 노이즈를 가지고 있으나 값의 범위가 줄어들어, 더 안정적인 학습을 할 수 있다. 세 번째는 본 연구에서 제안하는 위성레이더 전처리방법으로써, 강도차분과 거칠기영상을 생성하는 방법이다. 두 영상은 중간값 필터링을 이용해 스펙클 노이즈를 줄이고, 값의 분포 대역을 좁힘으로써 빠른 학습이 가능하다.&amp;#xD; 각 전처리된 위성레이더 영상을 이용해 딥러닝 학습을 하기 위해 본 연구에서는 M2Det 객체탐지 모델을 사용했다. 객체탐지 모델을 학습시킨 뒤 테스트 영상을 이용해 선박탐지를 수행했으며, 테스트 결과는 정밀도(Precision), 재현율(Recall)을 이용해 나타냈으며, 두 지수를 하나의 값으로 표현하기 위해 AP(Average Precision)와 F1 점수(F1-score)를 이용해 나타냈다. 각 영상의 정밀도, 재현율, AP, F1 점수는 강도 영상 93.18%, 91.11%, 89.78%, 92.13%, 데시벨 영상 94.16%, 94.16%, 92.34%, 94.16%, 강도차분과 거칠기 영상 97.40%, 94.94%, 95.55%, 96.15%로 계산되었다. 강도 영상을 이용한 경우 미탐지와 오탐지 선박이 많았으며, 전처리된 영상을 이용한 경우 강도 영상에 비해 미탐지와 오탐지 선박이 줄어든 것을 확인할 수 있었다. 데시벨 영상과 강도차분, 거칠기 영상의 결과를 비교했을 때, 두 영상의 오탐지율은 유사했다. 하지만 강도차분, 거칠기 영상을 이용했을 때 강도 영상에 비해 미탐지 선박의 비율이 4% 줄어든 것을 확인할 수 있었다. 이 결과를 통해 위성레이더 영상을 전처리함으로써 딥러닝 학습을 돕고 선박탐지 결과를 향상시킬 수 있다는 것을 알 수 있다. 본 연구결과는 향후 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구의 발전에 이바지할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015644673&target=NART&cn=DIKO0015644673",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 해상 교통량의 증가로 인해 해상 선박관리의 필요성이 늘어남에 따라 선박을 탐지하기 위한 연구들이 꾸준히 수행되어왔다. 특히 위성레이더 영상은 시간과 기후에 영향을 받지 않고 촬영할 수 있다는 장점으로 인해 선박탐지를 위한 많은 연구에서 활용되어왔다. 최근에는 딥러닝 기법의 발전으로 인해 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구들이 꾸준히 수행되고 있다. 그런데 위성레이더 영상은 값의 분포범위가 매우 넓고, 많은 스펙클 노이즈가 존재한다. 이러한 요소들은 딥러닝 모델의 학습에 부정적인 영향을 끼칠 수 있으므로 전처리를 통해 해당 요소들을 저감해줄 필요가 있다. 본 연구에서는 전처리된 위성레이더 영상으로부터 딥러닝 선박탐지를 수행하고, 영상의 전처리가 딥러닝 선박탐지에 미치는 요소를 비교분석 하고자 한다.&amp;#xD; 본 연구를 위해 TerraSAR-X와 COSMO-SkyMed 위성레이더 영상을 이용했다. 영상을 딥러닝 학습에 이용하기 전에 먼저 총 세 가지 다른 방법으로 전처리를 수행했다. 첫 번째는 위성레이더 영상에서 강도 값만을 추출한 강도 영상을 생성하는 방법이다. 강도 영상은 값의 범위가 매우 넓을 뿐만 아니라 많은 스펙클 노이즈를 가지고 있다. 두 번째는 강도영상에서 값의 단위를 데시벨로 변환한 데시벨 영상을 생성하는 방법이다. 데시벨 영상은 강도영상과 마찬가지로 많은 스펙클 노이즈를 가지고 있으나 값의 범위가 줄어들어, 더 안정적인 학습을 할 수 있다. 세 번째는 본 연구에서 제안하는 위성레이더 전처리방법으로써, 강도차분과 거칠기영상을 생성하는 방법이다. 두 영상은 중간값 필터링을 이용해 스펙클 노이즈를 줄이고, 값의 분포 대역을 좁힘으로써 빠른 학습이 가능하다.&amp;#xD; 각 전처리된 위성레이더 영상을 이용해 딥러닝 학습을 하기 위해 본 연구에서는 M2Det 객체탐지 모델을 사용했다. 객체탐지 모델을 학습시킨 뒤 테스트 영상을 이용해 선박탐지를 수행했으며, 테스트 결과는 정밀도(Precision), 재현율(Recall)을 이용해 나타냈으며, 두 지수를 하나의 값으로 표현하기 위해 AP(Average Precision)와 F1 점수(F1-score)를 이용해 나타냈다. 각 영상의 정밀도, 재현율, AP, F1 점수는 강도 영상 93.18%, 91.11%, 89.78%, 92.13%, 데시벨 영상 94.16%, 94.16%, 92.34%, 94.16%, 강도차분과 거칠기 영상 97.40%, 94.94%, 95.55%, 96.15%로 계산되었다. 강도 영상을 이용한 경우 미탐지와 오탐지 선박이 많았으며, 전처리된 영상을 이용한 경우 강도 영상에 비해 미탐지와 오탐지 선박이 줄어든 것을 확인할 수 있었다. 데시벨 영상과 강도차분, 거칠기 영상의 결과를 비교했을 때, 두 영상의 오탐지율은 유사했다. 하지만 강도차분, 거칠기 영상을 이용했을 때 강도 영상에 비해 미탐지 선박의 비율이 4% 줄어든 것을 확인할 수 있었다. 이 결과를 통해 위성레이더 영상을 전처리함으로써 딥러닝 학습을 돕고 선박탐지 결과를 향상시킬 수 있다는 것을 알 수 있다. 본 연구결과는 향후 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구의 발전에 이바지할 수 있을 것으로 기대된다."
        },
        {
          "rank": 19,
          "score": 0.7090909481048584,
          "doc_id": "NPAP13842123",
          "title": "레이더 영상 기반 딥러닝을 이용한 물체 인식",
          "abstract": "본 연구에서는 컴퓨터 비전 기반의 딥러닝 객체 인식 기술을 이용하여 속초해수욕장에서 수집한 레이더 이미지에서 선박, 섬 및 부유체에 대해 탐지(Detection), 인식(Recognition)하는 연구를 수행하였다. 2021년 8월에 수집한 레이더 영상을 이용하여 본 연구를 수행하였으며, 움직이는 물표와 섬 등을 구분하였다. 일부 환경적인 제약에 따라 에러 발생이 있었지만, 향후 현재까지 수집한 레이더 영상을 추가하여 정확도를 높일 예정이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP13842123&target=NART&cn=NPAP13842123",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "레이더 영상 기반 딥러닝을 이용한 물체 인식 레이더 영상 기반 딥러닝을 이용한 물체 인식 레이더 영상 기반 딥러닝을 이용한 물체 인식 본 연구에서는 컴퓨터 비전 기반의 딥러닝 객체 인식 기술을 이용하여 속초해수욕장에서 수집한 레이더 이미지에서 선박, 섬 및 부유체에 대해 탐지(Detection), 인식(Recognition)하는 연구를 수행하였다. 2021년 8월에 수집한 레이더 영상을 이용하여 본 연구를 수행하였으며, 움직이는 물표와 섬 등을 구분하였다. 일부 환경적인 제약에 따라 에러 발생이 있었지만, 향후 현재까지 수집한 레이더 영상을 추가하여 정확도를 높일 예정이다."
        },
        {
          "rank": 20,
          "score": 0.7015848755836487,
          "doc_id": "DIKO0017114224",
          "title": "딥러닝을 활용한 초음파 영상 개선",
          "abstract": "의료용 초음파 이미지(Clinical Ultrasonic Image) 기법은 인체 내부의 대한 영상을 비침습적, 안전적, 실시간적 있는 도구로, 의료 분야에서 사용되는 대표적인 진단 의료 영상 중 하나이다. 초고속 초음파(Ultra-fast Ultrasound)는 다수의 초음파 송수신을 통하여 상대적으로 고품질의 초음파 이미지를 얻을 수 있다. 그러나, 초음파 빔의 다양성, 복원 이미지의 해상도, 관심 영역(Region of Interest)의 크기 등은 실시간성과 절충 관계(Trade-off)에 있기에 초당 프레임 수(FPS)를 방어하기에 하드웨어적으로 어려움이 있다. 본 연구에서는 딥러닝(Deep Learning) 모델을 활용하여 단일 평면파(Single Plane-wave)의 저품질의 초음파 이미지를 고품질 다중 평면파(Multi-angle Plane-wave)의 고품질 초음파 이미지로 강화하는 것을 목표로 한다. U-Net 구조로 이루어진 딥러닝 모델은 다양한 크기의 합성곱 필터를 이용하여 복잡한 이미지의 세부 정보의 특징을 효과적으로 추출할 수 있다. 제안된 딥러닝 모델은 피크 대 잡음 비율(PSNR), 신호 대 잡음 비율(SNR), 스페클 신호 대 잡음 비율(SSNR) 등의 성능 지표를 통해 효과적인 잡음 감소 및 신호 보존을 보였으며, 상관계수(Correlation)를 통하여 강화된 이미지와 실제 이미지 간의 높은 유사성 및 정확성을 보였다. 향후 연구로는 본 작업에 영향을 줄 수 있는 세부 요인들을 조사하고, 모델 구조를 세밀하게 조정 및 최적화하여 강화되는 이미지의 품질을 더욱 향상시키고, 보다 다양한 부위에 대한 실험을 통해 일반화 성능을 확장할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017114224&target=NART&cn=DIKO0017114224",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 활용한 초음파 영상 개선 딥러닝을 활용한 초음파 영상 개선 딥러닝을 활용한 초음파 영상 개선 의료용 초음파 이미지(Clinical Ultrasonic Image) 기법은 인체 내부의 대한 영상을 비침습적, 안전적, 실시간적 있는 도구로, 의료 분야에서 사용되는 대표적인 진단 의료 영상 중 하나이다. 초고속 초음파(Ultra-fast Ultrasound)는 다수의 초음파 송수신을 통하여 상대적으로 고품질의 초음파 이미지를 얻을 수 있다. 그러나, 초음파 빔의 다양성, 복원 이미지의 해상도, 관심 영역(Region of Interest)의 크기 등은 실시간성과 절충 관계(Trade-off)에 있기에 초당 프레임 수(FPS)를 방어하기에 하드웨어적으로 어려움이 있다. 본 연구에서는 딥러닝(Deep Learning) 모델을 활용하여 단일 평면파(Single Plane-wave)의 저품질의 초음파 이미지를 고품질 다중 평면파(Multi-angle Plane-wave)의 고품질 초음파 이미지로 강화하는 것을 목표로 한다. U-Net 구조로 이루어진 딥러닝 모델은 다양한 크기의 합성곱 필터를 이용하여 복잡한 이미지의 세부 정보의 특징을 효과적으로 추출할 수 있다. 제안된 딥러닝 모델은 피크 대 잡음 비율(PSNR), 신호 대 잡음 비율(SNR), 스페클 신호 대 잡음 비율(SSNR) 등의 성능 지표를 통해 효과적인 잡음 감소 및 신호 보존을 보였으며, 상관계수(Correlation)를 통하여 강화된 이미지와 실제 이미지 간의 높은 유사성 및 정확성을 보였다. 향후 연구로는 본 작업에 영향을 줄 수 있는 세부 요인들을 조사하고, 모델 구조를 세밀하게 조정 및 최적화하여 강화되는 이미지의 품질을 더욱 향상시키고, 보다 다양한 부위에 대한 실험을 통해 일반화 성능을 확장할 수 있다."
        },
        {
          "rank": 21,
          "score": 0.6955301761627197,
          "doc_id": "NART119629224",
          "title": "65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging",
          "abstract": "<P>High dynamic range (HDR) techniques have received significant attention in generating realistic, high&#x2010;quality images and videos and improving visual quality in new display systems. We have witnessed remarkable advances in HDR reconstruction using deep learning technologies in recent years. This review examines recent developments in HDR reconstruction using a deep learning approach, which takes a single low dynamic range (LDR) image as an input and aims to restore an HDR image featuring higher color gamut and a higher detail retention than the LDR image. We aim to provide a comprehensive survey in this field. Since there are numerous HDR algorithms, it is necessary to evaluate and organize theirperformance, therefore, we evaluate them using two objective evaluation metrics.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART119629224&target=NART&cn=NART119629224",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging 65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging 65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging <P>High dynamic range (HDR) techniques have received significant attention in generating realistic, high&#x2010;quality images and videos and improving visual quality in new display systems. We have witnessed remarkable advances in HDR reconstruction using deep learning technologies in recent years. This review examines recent developments in HDR reconstruction using a deep learning approach, which takes a single low dynamic range (LDR) image as an input and aims to restore an HDR image featuring higher color gamut and a higher detail retention than the LDR image. We aim to provide a comprehensive survey in this field. Since there are numerous HDR algorithms, it is necessary to evaluate and organize theirperformance, therefore, we evaluate them using two objective evaluation metrics.</P>"
        },
        {
          "rank": 22,
          "score": 0.6807849407196045,
          "doc_id": "DIKO0017187917",
          "title": "레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구",
          "abstract": "자율주행의 안정적인 운행을 보장하기 위해서는 도로 상황에 대한 깊이 있는 이해가 필수적이다. 이에 본 논문에서는 단일 딥러닝(deep learning, DL) 네트워크 구조를 활용해 차량, 사이클리스트, 보행자 등 도로에서 자주 마주하는 객체를 분류하고 동시에 이들의 이동 방향을 추정하는 방법을 제안한다. 먼저, 4차원 이미징 레이더를 이용해 대상의 거리, 속도, 방위각, 고도각과 같은 정보를 획득한다. 이후, 검출 결과를 포인트 클라우드 데이터로 변환하여 3차원 공간 좌표계로 표현한다. 다음으로, 포인트 클라우드 데이터를 XY 평면에 정사영하여 대상의 분류와 이동 방향 추정을 수행한다. XY 평면에서 밀도 기반의 클러스터링(density-based spatial clustering) 기법을 적용해 검출 결과에서 잡음을 제거하고 객체를 클러스터링하여, 이를 이미지 데이터로 변환하는 전처리 과정을 거친다. 그런 후, 이미지 데이터를 이용해 객체 분류와 이동 방향 예측을 동시에 수행할 수 있는 다중 출력 DL 네트워크를 학습시킨다. 제안된 방법의 성능 평가 결과, 객체 분류 정확도는 96.10%로 나타났고, 이동 방향 예측의 평균 제곱근 오차(root mean square error, RMSE)는 차량, 사이클리스트, 보행자에 대해 각각 5.54°, 3.89°, 15.35°로 측정되었으며, 실행시간은 0.1초로 측정되어 효율성을 입증하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017187917&target=NART&cn=DIKO0017187917",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 자율주행의 안정적인 운행을 보장하기 위해서는 도로 상황에 대한 깊이 있는 이해가 필수적이다. 이에 본 논문에서는 단일 딥러닝(deep learning, DL) 네트워크 구조를 활용해 차량, 사이클리스트, 보행자 등 도로에서 자주 마주하는 객체를 분류하고 동시에 이들의 이동 방향을 추정하는 방법을 제안한다. 먼저, 4차원 이미징 레이더를 이용해 대상의 거리, 속도, 방위각, 고도각과 같은 정보를 획득한다. 이후, 검출 결과를 포인트 클라우드 데이터로 변환하여 3차원 공간 좌표계로 표현한다. 다음으로, 포인트 클라우드 데이터를 XY 평면에 정사영하여 대상의 분류와 이동 방향 추정을 수행한다. XY 평면에서 밀도 기반의 클러스터링(density-based spatial clustering) 기법을 적용해 검출 결과에서 잡음을 제거하고 객체를 클러스터링하여, 이를 이미지 데이터로 변환하는 전처리 과정을 거친다. 그런 후, 이미지 데이터를 이용해 객체 분류와 이동 방향 예측을 동시에 수행할 수 있는 다중 출력 DL 네트워크를 학습시킨다. 제안된 방법의 성능 평가 결과, 객체 분류 정확도는 96.10%로 나타났고, 이동 방향 예측의 평균 제곱근 오차(root mean square error, RMSE)는 차량, 사이클리스트, 보행자에 대해 각각 5.54°, 3.89°, 15.35°로 측정되었으며, 실행시간은 0.1초로 측정되어 효율성을 입증하였다."
        },
        {
          "rank": 23,
          "score": 0.6783566474914551,
          "doc_id": "NART121556950",
          "title": "Integrating deep learning and traditional image enhancement techniques for underwater image enhancement",
          "abstract": "<P><B>Abstract</B><P>Underwater images usually suffer from colour distortion, blur, and low contrast, which hinder the subsequent processing of underwater information. To address these problems, this paper proposes a novel approach for single underwater images enhancement by integrating data&#x2010;driven deep learning and hand&#x2010;crafted image enhancement techniques. First, a statistical analysis is made on the average deviation of each channel of input underwater images to that of its corresponding ground truths, and it is found that both the red channel and the green channel of an underwater image contribute to its colour distortion. Concretely, the red channel of an underwater image is usually seriously attenuated, and the green channel is usually over strengthened. Motivated by such an observation, an attention mechanism guided residual module for underwater image colour correction is proposed, where the colour of the red channel of the underwater image and that of the green channel is compensated in a different way, respectively. Coupled with an attention mechanism, the residual module can adaptively extract and integrate the most discriminative features for colour correction. For scene contrast enhancement and scene deblurring, the traditional image enhancement techniques such as CLAHE (contrast limited adaptive histogram equalization) and Gamma correction are coupled with a multi&#x2010;scale convolutional neural network (MSCNN), where CLAHE and Gamma correction are used as complement to deal with the complex and changeable underwater imaging environment. Experiments on synthetic and real underwater images demonstrate that the proposed method performs favourably against the state&#x2010;of&#x2010;the&#x2010;art underwater image enhancement methods.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART121556950&target=NART&cn=NART121556950",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Integrating deep learning and traditional image enhancement techniques for underwater image enhancement Integrating deep learning and traditional image enhancement techniques for underwater image enhancement Integrating deep learning and traditional image enhancement techniques for underwater image enhancement <P><B>Abstract</B><P>Underwater images usually suffer from colour distortion, blur, and low contrast, which hinder the subsequent processing of underwater information. To address these problems, this paper proposes a novel approach for single underwater images enhancement by integrating data&#x2010;driven deep learning and hand&#x2010;crafted image enhancement techniques. First, a statistical analysis is made on the average deviation of each channel of input underwater images to that of its corresponding ground truths, and it is found that both the red channel and the green channel of an underwater image contribute to its colour distortion. Concretely, the red channel of an underwater image is usually seriously attenuated, and the green channel is usually over strengthened. Motivated by such an observation, an attention mechanism guided residual module for underwater image colour correction is proposed, where the colour of the red channel of the underwater image and that of the green channel is compensated in a different way, respectively. Coupled with an attention mechanism, the residual module can adaptively extract and integrate the most discriminative features for colour correction. For scene contrast enhancement and scene deblurring, the traditional image enhancement techniques such as CLAHE (contrast limited adaptive histogram equalization) and Gamma correction are coupled with a multi&#x2010;scale convolutional neural network (MSCNN), where CLAHE and Gamma correction are used as complement to deal with the complex and changeable underwater imaging environment. Experiments on synthetic and real underwater images demonstrate that the proposed method performs favourably against the state&#x2010;of&#x2010;the&#x2010;art underwater image enhancement methods.</P></P>"
        },
        {
          "rank": 24,
          "score": 0.6783449649810791,
          "doc_id": "JAKO202319937622688",
          "title": "머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구",
          "abstract": "본 논문에서는 유리섬유 직물 강화 복합재 소재위에 Cross-Dipole 패턴이 배치된 정형적 Circuit Analog(CA) 전파 흡수 구조 설계를 위한 머신러닝 및 딥러닝 모델을 제시하였다. 제시된 모델은 Cross-Dipole 패턴의 형상에 따라서 Ku-band (12-18 GHz)에서의 전파흡수성능을 3차원 전자파 수치해석 없이 바로 계산할 수 있다. 이를 위하여 다양한 머신러닝 및 딥러닝 기술을 적용한 최적 학습 모델을 도출하고, 학습 모델이 계산한 결과를 3차원 전자파 수치해석결과로 얻은 전파흡수특성과 비교함으로써 각각의 모델 간의 성능의 비교우위를 평가하였다. 개발된 모델들은 대부분 수치해석결과와 유사한 계산결과를 보여주었지만, 그 중 Fully-Connected 모델이 가장 유사한 계산결과를 제공할 수 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202319937622688&target=NART&cn=JAKO202319937622688",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구 머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구 머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구 본 논문에서는 유리섬유 직물 강화 복합재 소재위에 Cross-Dipole 패턴이 배치된 정형적 Circuit Analog(CA) 전파 흡수 구조 설계를 위한 머신러닝 및 딥러닝 모델을 제시하였다. 제시된 모델은 Cross-Dipole 패턴의 형상에 따라서 Ku-band (12-18 GHz)에서의 전파흡수성능을 3차원 전자파 수치해석 없이 바로 계산할 수 있다. 이를 위하여 다양한 머신러닝 및 딥러닝 기술을 적용한 최적 학습 모델을 도출하고, 학습 모델이 계산한 결과를 3차원 전자파 수치해석결과로 얻은 전파흡수특성과 비교함으로써 각각의 모델 간의 성능의 비교우위를 평가하였다. 개발된 모델들은 대부분 수치해석결과와 유사한 계산결과를 보여주었지만, 그 중 Fully-Connected 모델이 가장 유사한 계산결과를 제공할 수 있음을 확인하였다."
        },
        {
          "rank": 25,
          "score": 0.6777914762496948,
          "doc_id": "DIKO0015551607",
          "title": "데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법",
          "abstract": "오늘날 딥 러닝(Deep Learning)이란 머신러닝의 세부적인 방법과 개념&amp;#xD; 및 기법들을 통칭한다. 딥 러닝은 크게는 컴퓨터 비전(Computer vision)으&amp;#xD; 로부터 시작하여 패턴 인식(Pattern recognition), 색상 및 픽셀 복원, 추청&amp;#xD; 과 진단 등 다양한 곳에 사용이 되고 있다. 그 중 대게 객체 및 사람을 인&amp;#xD; 식하는 단계 및 추적을 더불어 대상의 안면 인식을 할 수 있는 단계까지&amp;#xD; 발달했다. 기본적인 네트워크인 컨볼루션 뉴럴 네트워크(CNN :&amp;#xD; convolutional neural network)를 시작으로 순환신경망(RNN : Recurrent&amp;#xD; Neural Network), 볼츠만 머신(RBM : Restricted Boltzmann Machine), 생&amp;#xD; 성 대립 신경망(GAN : Generative Adversarial Network) 그리고 Google의&amp;#xD; 딥 마인드에서 개발한 관계형 네트워크(RL : Relation Networks)등이 존재&amp;#xD; 한다. 이와 같은 네트워크 모델들은 다양한 강점들을 가지고 있는데 그 중&amp;#xD; 데이터를 이용한 요인 추출(feature extraction)이나 학습을 통한 결과 추론&amp;#xD; 이라고 볼 수 있다. 위와 같은 요인들을 성공적으로 학습시키기 위해서는&amp;#xD; 적합한 환경에 맞는 데이터 세트인지 판단하고, 모델에 관한 특징들을 파악&amp;#xD; 하여 가장 적합한 형태의 모델을 구현하여 효과적으로 학습 할 수 있도록&amp;#xD; 진행한다. 하지만 위 과정 중에서 데이터 세트들은 손쉽게 만들어지지 않는&amp;#xD; 다. 그 이유는 여러 다양한 방법으로 디자인되고 환경에 맞게 제작이 되어&amp;#xD; 야하기 때문이다.&amp;#xD; 본 논문에서는 기존 데이터 세트들을 이용하여 여러 다양한 방법을 이&amp;#xD; 용하여 데이터를 증강(data augmentation)시키는 연구를 진행한다. 객체 인&amp;#xD; 식 및 판단을 목적으로 딥 러닝을 학습 시킬 경우에는 이미지의 데이터 정&amp;#xD; 보들을 통해 학습을 진행한다. 학습하는 데이터 정보는 관심이 있는 영역이&amp;#xD; 나 혹은 주요 지정된 객체의 정보를 학습하는 것을 목표로 한다. 이것을 달&amp;#xD; 성하기 위해 데이터 세트를 이용하여 유용한 정보를 추출하고 학습 후 객&amp;#xD; 체에 관한 인식을 할 수 있게 진행했다. 여기에서 데이터 세트들은 대부분&amp;#xD; ILSVRC (Image Large Scale Visual Recognition Challenges) 및 PASCAL&amp;#xD; VOC (Visual Object Classes) 같은 것으로 이루어져 있다. 하지만 이와 같&amp;#xD; 은 데이터 세트는 특수한 상황이나 제한된 상황에서 사용하기가 매우 어렵&amp;#xD; 다. 상황에 맞게 데이터 세트들을 제작을 해야 하는 경우 이는 매우 많은&amp;#xD; 시간이 걸린다. 또한 만들어진 데이터 세트들을 테스트해야 하는 시간 또한&amp;#xD; 오래 걸린다. 본 논문에서는 제안된 방법을 사용하여 이를 해결한다. 기본&amp;#xD; 적인 영상처리부터 시작하여 알고리즘 및 3D 환경에서까지의 방법을 설명&amp;#xD; 한다. 이 방법들을 통해 생성된 데이터들은 성능 검증을 위해 실시간 모델&amp;#xD; 인 YOLO ver2(You Only Look Once)를 사용한다. 그리고 이미지 생성 후&amp;#xD; 분류에 사용할 CNN과 VGGNet(Very Deep Convolutional Networks for&amp;#xD; Large-Scale Image Recognition)을 이용한다. 최종적으로 제시한 방법을&amp;#xD; 통해 데이터 세트의 수를 수백 배 이상 생성했으며, 객체 간의 정확도는 5&amp;#xD; ∼ 10% 이상 증가시켰다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015551607&target=NART&cn=DIKO0015551607",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법 데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법 데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법 오늘날 딥 러닝(Deep Learning)이란 머신러닝의 세부적인 방법과 개념&amp;#xD; 및 기법들을 통칭한다. 딥 러닝은 크게는 컴퓨터 비전(Computer vision)으&amp;#xD; 로부터 시작하여 패턴 인식(Pattern recognition), 색상 및 픽셀 복원, 추청&amp;#xD; 과 진단 등 다양한 곳에 사용이 되고 있다. 그 중 대게 객체 및 사람을 인&amp;#xD; 식하는 단계 및 추적을 더불어 대상의 안면 인식을 할 수 있는 단계까지&amp;#xD; 발달했다. 기본적인 네트워크인 컨볼루션 뉴럴 네트워크(CNN :&amp;#xD; convolutional neural network)를 시작으로 순환신경망(RNN : Recurrent&amp;#xD; Neural Network), 볼츠만 머신(RBM : Restricted Boltzmann Machine), 생&amp;#xD; 성 대립 신경망(GAN : Generative Adversarial Network) 그리고 Google의&amp;#xD; 딥 마인드에서 개발한 관계형 네트워크(RL : Relation Networks)등이 존재&amp;#xD; 한다. 이와 같은 네트워크 모델들은 다양한 강점들을 가지고 있는데 그 중&amp;#xD; 데이터를 이용한 요인 추출(feature extraction)이나 학습을 통한 결과 추론&amp;#xD; 이라고 볼 수 있다. 위와 같은 요인들을 성공적으로 학습시키기 위해서는&amp;#xD; 적합한 환경에 맞는 데이터 세트인지 판단하고, 모델에 관한 특징들을 파악&amp;#xD; 하여 가장 적합한 형태의 모델을 구현하여 효과적으로 학습 할 수 있도록&amp;#xD; 진행한다. 하지만 위 과정 중에서 데이터 세트들은 손쉽게 만들어지지 않는&amp;#xD; 다. 그 이유는 여러 다양한 방법으로 디자인되고 환경에 맞게 제작이 되어&amp;#xD; 야하기 때문이다.&amp;#xD; 본 논문에서는 기존 데이터 세트들을 이용하여 여러 다양한 방법을 이&amp;#xD; 용하여 데이터를 증강(data augmentation)시키는 연구를 진행한다. 객체 인&amp;#xD; 식 및 판단을 목적으로 딥 러닝을 학습 시킬 경우에는 이미지의 데이터 정&amp;#xD; 보들을 통해 학습을 진행한다. 학습하는 데이터 정보는 관심이 있는 영역이&amp;#xD; 나 혹은 주요 지정된 객체의 정보를 학습하는 것을 목표로 한다. 이것을 달&amp;#xD; 성하기 위해 데이터 세트를 이용하여 유용한 정보를 추출하고 학습 후 객&amp;#xD; 체에 관한 인식을 할 수 있게 진행했다. 여기에서 데이터 세트들은 대부분&amp;#xD; ILSVRC (Image Large Scale Visual Recognition Challenges) 및 PASCAL&amp;#xD; VOC (Visual Object Classes) 같은 것으로 이루어져 있다. 하지만 이와 같&amp;#xD; 은 데이터 세트는 특수한 상황이나 제한된 상황에서 사용하기가 매우 어렵&amp;#xD; 다. 상황에 맞게 데이터 세트들을 제작을 해야 하는 경우 이는 매우 많은&amp;#xD; 시간이 걸린다. 또한 만들어진 데이터 세트들을 테스트해야 하는 시간 또한&amp;#xD; 오래 걸린다. 본 논문에서는 제안된 방법을 사용하여 이를 해결한다. 기본&amp;#xD; 적인 영상처리부터 시작하여 알고리즘 및 3D 환경에서까지의 방법을 설명&amp;#xD; 한다. 이 방법들을 통해 생성된 데이터들은 성능 검증을 위해 실시간 모델&amp;#xD; 인 YOLO ver2(You Only Look Once)를 사용한다. 그리고 이미지 생성 후&amp;#xD; 분류에 사용할 CNN과 VGGNet(Very Deep Convolutional Networks for&amp;#xD; Large-Scale Image Recognition)을 이용한다. 최종적으로 제시한 방법을&amp;#xD; 통해 데이터 세트의 수를 수백 배 이상 생성했으며, 객체 간의 정확도는 5&amp;#xD; ∼ 10% 이상 증가시켰다."
        },
        {
          "rank": 26,
          "score": 0.6753923892974854,
          "doc_id": "ART002342492",
          "title": "Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning",
          "abstract": "Heavy rainfall has frequently caused serious flooding and landslides, increasing traffic delays in most parts of the world. Consequently, the people in areas battered by heavy rainfall face many hardships. Thus, the negative effects of torrential rainfall always remind researchers to keep seeking the ways to prevent such damage. Therefore, we designed a system for short-term prediction of localized heavy downpours by using radar images coupled with a machine learning method. Here, we introduce a new approach, named dual k-nearest neighbor (dual-kNN), for shortterm rainfall prediction by upgrading the ordinary classification routines of classical k-nearest neighbors (k-NN). dual-kNN is able to maintain highly robust classification of various K values with an advanced simple dual consideration, where observation of a targeted object can be found not only in the specified region but also in other related regions. We conducted experimentations using 2011, 2013, and 2014 data sets collected from the WITH small-dish aviation radar installed on the rooftop of Information Engineering, University of the Ryukyus. Then, we compared the prediction accuracy of our new approach with classical k-NN. It was experimentally confirmed with test cases and simulations that the performance of dual-kNN is more effective than classical k- NN.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002342492&target=NART&cn=ART002342492",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Heavy rainfall has frequently caused serious flooding and landslides, increasing traffic delays in most parts of the world. Consequently, the people in areas battered by heavy rainfall face many hardships. Thus, the negative effects of torrential rainfall always remind researchers to keep seeking the ways to prevent such damage. Therefore, we designed a system for short-term prediction of localized heavy downpours by using radar images coupled with a machine learning method. Here, we introduce a new approach, named dual k-nearest neighbor (dual-kNN), for shortterm rainfall prediction by upgrading the ordinary classification routines of classical k-nearest neighbors (k-NN). dual-kNN is able to maintain highly robust classification of various K values with an advanced simple dual consideration, where observation of a targeted object can be found not only in the specified region but also in other related regions. We conducted experimentations using 2011, 2013, and 2014 data sets collected from the WITH small-dish aviation radar installed on the rooftop of Information Engineering, University of the Ryukyus. Then, we compared the prediction accuracy of our new approach with classical k-NN. It was experimentally confirmed with test cases and simulations that the performance of dual-kNN is more effective than classical k- NN."
        },
        {
          "rank": 27,
          "score": 0.6747578978538513,
          "doc_id": "NART119879737",
          "title": "Image Enhancement Method Based on Deep Learning",
          "abstract": "<P>Image enhancement and reconstruction are the basic processing steps of many real vision systems. Their purpose is to improve the visual quality of images and provide reliable information for subsequent visual decision-making. In this paper, convolution neural network, residual neural network, and generative countermeasure network are studied. A rain fog image enhancement generative countermeasure network model structure including a scalable auxiliary generation network is proposed. The objective loss function is defined, and the periodic consistency loss and periodic perceptual consistency loss analysis are introduced. The core problem of image layering is discussed, and a layering solution framework with a deep expansion structure is proposed. This method realizes multitasking through adaptive feature learning, which has a good theoretical guarantee. This paper can not only bring a pleasant visual experience to viewers but also help to improve the performance of computer vision applications. Through image enhancement technology, the quality of low illumination image can be effectively improved, so that the image has better definition, richer texture details, and lower image noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART119879737&target=NART&cn=NART119879737",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Image Enhancement Method Based on Deep Learning Image Enhancement Method Based on Deep Learning Image Enhancement Method Based on Deep Learning <P>Image enhancement and reconstruction are the basic processing steps of many real vision systems. Their purpose is to improve the visual quality of images and provide reliable information for subsequent visual decision-making. In this paper, convolution neural network, residual neural network, and generative countermeasure network are studied. A rain fog image enhancement generative countermeasure network model structure including a scalable auxiliary generation network is proposed. The objective loss function is defined, and the periodic consistency loss and periodic perceptual consistency loss analysis are introduced. The core problem of image layering is discussed, and a layering solution framework with a deep expansion structure is proposed. This method realizes multitasking through adaptive feature learning, which has a good theoretical guarantee. This paper can not only bring a pleasant visual experience to viewers but also help to improve the performance of computer vision applications. Through image enhancement technology, the quality of low illumination image can be effectively improved, so that the image has better definition, richer texture details, and lower image noise.</P>"
        },
        {
          "rank": 28,
          "score": 0.6719093918800354,
          "doc_id": "JAKO202407064802797",
          "title": "딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가",
          "abstract": "급격한 기후 변화로 인한 어획량 감소와 양식 기술의 발전으로 양식 생산물 수요가 전세계적으로 계속해서 증가하고 있다. 그러나 이에 따른 무분별한 시설물 확장이 연안 생태계와 어족 자원 가격 책정에 악영향을 미치기 때문에, 주기적인 연안 환경 모니터링을 통한 양식시설물 관리가 필수적이다. 본 연구에서는 Sentinel-2 광학 영상과 다양한 딥러닝 기반 탐지 기법을 활용하여 경상남도의 패류 양식시설물 탐지 정확도를 분석하였다. DeepLabv3+, ResUNet++ 그리고 Attention U-Net 모델을 적용하였으며, 실험 결과 Attention U-Net 모델이 F1 score 0.8708, Intersection over Union 0.7708로 가장 우수한 탐지 성능을 보였다. 연구에서 제시한 탐지 방법론은 조류 및 부유 물질에 영향을 받는 양식시설물을 주기적으로 관측할 수 있고, 다양한 양식 품종에 적용할 수 있어 넓은 지역으로의 확장 가능성이 높다. 따라서 본 연구 방법을 통해 도출된 양식 시설물 정보는 향후 해양 공간 활용에 관한 정책 결정에 유용하게 활용할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202407064802797&target=NART&cn=JAKO202407064802797",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 급격한 기후 변화로 인한 어획량 감소와 양식 기술의 발전으로 양식 생산물 수요가 전세계적으로 계속해서 증가하고 있다. 그러나 이에 따른 무분별한 시설물 확장이 연안 생태계와 어족 자원 가격 책정에 악영향을 미치기 때문에, 주기적인 연안 환경 모니터링을 통한 양식시설물 관리가 필수적이다. 본 연구에서는 Sentinel-2 광학 영상과 다양한 딥러닝 기반 탐지 기법을 활용하여 경상남도의 패류 양식시설물 탐지 정확도를 분석하였다. DeepLabv3+, ResUNet++ 그리고 Attention U-Net 모델을 적용하였으며, 실험 결과 Attention U-Net 모델이 F1 score 0.8708, Intersection over Union 0.7708로 가장 우수한 탐지 성능을 보였다. 연구에서 제시한 탐지 방법론은 조류 및 부유 물질에 영향을 받는 양식시설물을 주기적으로 관측할 수 있고, 다양한 양식 품종에 적용할 수 있어 넓은 지역으로의 확장 가능성이 높다. 따라서 본 연구 방법을 통해 도출된 양식 시설물 정보는 향후 해양 공간 활용에 관한 정책 결정에 유용하게 활용할 수 있을 것으로 기대된다."
        },
        {
          "rank": 29,
          "score": 0.6712521314620972,
          "doc_id": "JAKO202221359246132",
          "title": "불균일 안개 영상 합성을 이용한 딥러닝 기반 안개 영상 깊이 추정",
          "abstract": "영상의 깊이 추정은 다양한 영상 분석의 기반이 되는 기술이다. 딥러닝 모델을 활용한 분석 방법이 대두되면서, 영상의 깊이 추정 분야 또한 딥러닝을 활용하는 연구가 활발하게 이루어지고 있다. 현재 대부분의 딥러닝 영상 깊이 추정 모델들은 깨끗하고 이상적인 환경에서 학습되고 있다. 하지만 연무, 안개가 낀 열악한 환경에서도 깊이 추정 기술이 잘 동작할 수 있으려면 이러한 환경의 데이터를 포함하여야 한다. 하지만 열악한 환경의 영상을 충분히 확보하는 것이 어려운 실정이며, 불균일한 안개 데이터를 얻는 것은 특히 어려운 문제이다. 이를 해결하기 위해, 본 연구에서는 불균일 안개 영상 합성 방법과 이를 활용한 단안 기반의 깊이 추정 딥러닝 모델의 학습을 제안한다. 안개가 주로 실외에서 발생하는 것을 고려하여, 실외 위주의 데이터 세트를 구축한다. 그리고 실험을 통해 제안된 방법으로 학습된 모델이 합성 데이터와 실제 데이터에서 깊이를 잘 추정하는 것을 보인다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202221359246132&target=NART&cn=JAKO202221359246132",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "불균일 안개 영상 합성을 이용한 딥러닝 기반 안개 영상 깊이 추정 불균일 안개 영상 합성을 이용한 딥러닝 기반 안개 영상 깊이 추정 불균일 안개 영상 합성을 이용한 딥러닝 기반 안개 영상 깊이 추정 영상의 깊이 추정은 다양한 영상 분석의 기반이 되는 기술이다. 딥러닝 모델을 활용한 분석 방법이 대두되면서, 영상의 깊이 추정 분야 또한 딥러닝을 활용하는 연구가 활발하게 이루어지고 있다. 현재 대부분의 딥러닝 영상 깊이 추정 모델들은 깨끗하고 이상적인 환경에서 학습되고 있다. 하지만 연무, 안개가 낀 열악한 환경에서도 깊이 추정 기술이 잘 동작할 수 있으려면 이러한 환경의 데이터를 포함하여야 한다. 하지만 열악한 환경의 영상을 충분히 확보하는 것이 어려운 실정이며, 불균일한 안개 데이터를 얻는 것은 특히 어려운 문제이다. 이를 해결하기 위해, 본 연구에서는 불균일 안개 영상 합성 방법과 이를 활용한 단안 기반의 깊이 추정 딥러닝 모델의 학습을 제안한다. 안개가 주로 실외에서 발생하는 것을 고려하여, 실외 위주의 데이터 세트를 구축한다. 그리고 실험을 통해 제안된 방법으로 학습된 모델이 합성 데이터와 실제 데이터에서 깊이를 잘 추정하는 것을 보인다."
        },
        {
          "rank": 30,
          "score": 0.6700722575187683,
          "doc_id": "DIKO0016929635",
          "title": "딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식",
          "abstract": "전자기전에서 레이더 파형의 변조 방식은 다른 레이더 파형 제원과 함께 적대적인 레이더를 분석하고 대응책을 마련하는 데 활용될 수 있어 사전정보 없이 레이더 파형의 변조 방식을 자동 인식하는 기술은 매우 중요한 기술이다. 특히, 미래의 전자기전에서 복합 변조 방식이 적용된 레이더 파형에 선제적으로 대응하기 위해서는 다양한 단일 및 복합 변조 방식을 인식할 수 있는 기술을 개발하는 것이 필수적이다.&amp;#xD; 본 논문에서는 31종의 단일 및 복합 변조 방식의 레이더 파형을 자동으로 인식하는 시퀀스 데이터 기반의 레이더 파형 변조 인식 기법을 제안한다. 제안하는 기법은 수신 레이더 파형을 바탕으로 시퀀스 데이터를 구성하고, 이를 딥러닝 네트워크에 입력으로 사용하여 변조 방식을 자동으로 인식한다. 이때 시퀀스 데이터로 레이더 파형 원본, 레이더 파형의 이산 푸리에 변환(Discrete Fourier transform, DFT), 레이더 파형의 자기 상관 함수 각각의 실수부 및 허수부를 이용해 구성한 데이터를 고려하며, 딥러닝 네트워크로는 CNN(Convolutional neural network), CLDNN(Convolutional long short-term deep neural network), ResNet(Residual network)-18, 34, 50을 고려한다. 컴퓨터 모의실험을 통해 시퀀스 데이터와 딥러닝 네트워크에 따른 제안하는 시퀀스 데이터 기반 레이더 파형 자동 변조 인식 기법의 인식 성능을 비교하여 레이더 파형의 DFT로 구성한 시퀀스 데이터를 딥러닝 네트워크의 입력으로 사용할 때 가장 효과적인 변조 인식이 가능함을 보인다. 또한 레이더 파형의 DFT로 구성한 시퀀스 데이터를 이용할 때, 딥러닝 네트워크로 ResNet을 사용하는 경우가 CNN, CLDNN을 사용하는 경우보다 우수한 성능을 보임을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016929635&target=NART&cn=DIKO0016929635",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 전자기전에서 레이더 파형의 변조 방식은 다른 레이더 파형 제원과 함께 적대적인 레이더를 분석하고 대응책을 마련하는 데 활용될 수 있어 사전정보 없이 레이더 파형의 변조 방식을 자동 인식하는 기술은 매우 중요한 기술이다. 특히, 미래의 전자기전에서 복합 변조 방식이 적용된 레이더 파형에 선제적으로 대응하기 위해서는 다양한 단일 및 복합 변조 방식을 인식할 수 있는 기술을 개발하는 것이 필수적이다.&amp;#xD; 본 논문에서는 31종의 단일 및 복합 변조 방식의 레이더 파형을 자동으로 인식하는 시퀀스 데이터 기반의 레이더 파형 변조 인식 기법을 제안한다. 제안하는 기법은 수신 레이더 파형을 바탕으로 시퀀스 데이터를 구성하고, 이를 딥러닝 네트워크에 입력으로 사용하여 변조 방식을 자동으로 인식한다. 이때 시퀀스 데이터로 레이더 파형 원본, 레이더 파형의 이산 푸리에 변환(Discrete Fourier transform, DFT), 레이더 파형의 자기 상관 함수 각각의 실수부 및 허수부를 이용해 구성한 데이터를 고려하며, 딥러닝 네트워크로는 CNN(Convolutional neural network), CLDNN(Convolutional long short-term deep neural network), ResNet(Residual network)-18, 34, 50을 고려한다. 컴퓨터 모의실험을 통해 시퀀스 데이터와 딥러닝 네트워크에 따른 제안하는 시퀀스 데이터 기반 레이더 파형 자동 변조 인식 기법의 인식 성능을 비교하여 레이더 파형의 DFT로 구성한 시퀀스 데이터를 딥러닝 네트워크의 입력으로 사용할 때 가장 효과적인 변조 인식이 가능함을 보인다. 또한 레이더 파형의 DFT로 구성한 시퀀스 데이터를 이용할 때, 딥러닝 네트워크로 ResNet을 사용하는 경우가 CNN, CLDNN을 사용하는 경우보다 우수한 성능을 보임을 확인한다."
        },
        {
          "rank": 31,
          "score": 0.6700677275657654,
          "doc_id": "JAKO201962652079504",
          "title": "심층 강화학습 기술 동향",
          "abstract": "Recent trends in deep reinforcement learning (DRL) have revealed the considerable improvements to DRL algorithms in terms of performance, learning stability, and computational efficiency. DRL also enables the scenarios that it covers (e.g., partial observability; cooperation, competition, coexistence, and communications among multiple agents; multi-task; decentralized intelligence) to be vastly expanded. These features have cultivated multi-agent reinforcement learning research. DRL is also expanding its applications from robotics to natural language processing and computer vision into a wide array of fields such as finance, healthcare, chemistry, and even art. In this report, we briefly summarize various DRL techniques and research directions.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201962652079504&target=NART&cn=JAKO201962652079504",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층 강화학습 기술 동향 심층 강화학습 기술 동향 심층 강화학습 기술 동향 Recent trends in deep reinforcement learning (DRL) have revealed the considerable improvements to DRL algorithms in terms of performance, learning stability, and computational efficiency. DRL also enables the scenarios that it covers (e.g., partial observability; cooperation, competition, coexistence, and communications among multiple agents; multi-task; decentralized intelligence) to be vastly expanded. These features have cultivated multi-agent reinforcement learning research. DRL is also expanding its applications from robotics to natural language processing and computer vision into a wide array of fields such as finance, healthcare, chemistry, and even art. In this report, we briefly summarize various DRL techniques and research directions."
        },
        {
          "rank": 32,
          "score": 0.6693500280380249,
          "doc_id": "JAKO201914260902234",
          "title": "딥러닝을 이용한 차로이탈 경고 시스템",
          "abstract": "최근 인공지능 기술이 급격히 발전하면서 첨단 운전자 지원 시스템 분야에 딥러닝 기술을 접목하여 기존의 기술보다 뛰어난 성능을 보여주기 위한 여러 연구들이 진행 되고 있다. 이러한 동향에 맞춰 본 논문 또한 첨단 운전자 지원 시스템의 핵심 요소 중 하나인 차로이탈 경고시스템에 딥러닝 기술을 접목한 방법을 제안한다. 제안하는 방법과 기존의 차선검출 기반의 경고시스템과의 비교 실험을 통해 그 성능을 평가 하였다. 고속도로 주행영상과 시내 주행영상을 이용한 두 가지의 서로 다른 환경에서 모두 제안하는 방법이 정확도 및 정밀도 부분에서 더 높은 수치를 보여주었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201914260902234&target=NART&cn=JAKO201914260902234",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 이용한 차로이탈 경고 시스템 딥러닝을 이용한 차로이탈 경고 시스템 딥러닝을 이용한 차로이탈 경고 시스템 최근 인공지능 기술이 급격히 발전하면서 첨단 운전자 지원 시스템 분야에 딥러닝 기술을 접목하여 기존의 기술보다 뛰어난 성능을 보여주기 위한 여러 연구들이 진행 되고 있다. 이러한 동향에 맞춰 본 논문 또한 첨단 운전자 지원 시스템의 핵심 요소 중 하나인 차로이탈 경고시스템에 딥러닝 기술을 접목한 방법을 제안한다. 제안하는 방법과 기존의 차선검출 기반의 경고시스템과의 비교 실험을 통해 그 성능을 평가 하였다. 고속도로 주행영상과 시내 주행영상을 이용한 두 가지의 서로 다른 환경에서 모두 제안하는 방법이 정확도 및 정밀도 부분에서 더 높은 수치를 보여주었다."
        },
        {
          "rank": 33,
          "score": 0.66887366771698,
          "doc_id": "JAKO199911921528980",
          "title": "다층회귀예측신경망의 음성인식성능에 관한 연구",
          "abstract": "4층구조의 다층퍼셉트론을 변형하여 3 종류의 다층회귀예측신경망을 구성하고, 예측차수, 두 은닉층의 뉴런개수, 연결세기의 초기치 및 전달함수 변화에 따른 각 망의 음성인식성능을 실험을 통해 각각 비교 분석한다. 실험결과에 의하면, 다층회귀신경망이 다층퍼셉트론에 비해 음성인식성능이 우수하다. 그리고 구조적으로는 상위은닉층의 출력을 하위은닉층으로 회귀할 때 인식성능이 가장 우수하며, 각 망 공히 상, 하위은닉층의 뉴런 10 혹은 15개, 예측차수 3 혹은 4차일 때 인식률이 양호하다. 학습시 연결세기의 초기치를 -0.5에서 0.5사이로 설정하고, 하위은닉층에서 단극성 시그모이드 전달함수를 사용할 때 인식성능이 더욱 향상된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199911921528980&target=NART&cn=JAKO199911921528980",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀예측신경망의 음성인식성능에 관한 연구 다층회귀예측신경망의 음성인식성능에 관한 연구 다층회귀예측신경망의 음성인식성능에 관한 연구 4층구조의 다층퍼셉트론을 변형하여 3 종류의 다층회귀예측신경망을 구성하고, 예측차수, 두 은닉층의 뉴런개수, 연결세기의 초기치 및 전달함수 변화에 따른 각 망의 음성인식성능을 실험을 통해 각각 비교 분석한다. 실험결과에 의하면, 다층회귀신경망이 다층퍼셉트론에 비해 음성인식성능이 우수하다. 그리고 구조적으로는 상위은닉층의 출력을 하위은닉층으로 회귀할 때 인식성능이 가장 우수하며, 각 망 공히 상, 하위은닉층의 뉴런 10 혹은 15개, 예측차수 3 혹은 4차일 때 인식률이 양호하다. 학습시 연결세기의 초기치를 -0.5에서 0.5사이로 설정하고, 하위은닉층에서 단극성 시그모이드 전달함수를 사용할 때 인식성능이 더욱 향상된다."
        },
        {
          "rank": 34,
          "score": 0.6681583523750305,
          "doc_id": "JAKO201953457807295",
          "title": "경량 딥러닝 기술 동향",
          "abstract": "Considerable accuracy improvements in deep learning have recently been achieved in many applications that require large amounts of computation and expensive memory. However, recent advanced techniques for compacting and accelerating the deep learning model have been developed for deployment in lightweight devices with constrained resources. Lightweight deep learning techniques can be categorized into two schemes: lightweight deep learning algorithms (model simplification and efficient convolutional filters) in nature and transferring models into compact/small ones (model compression and knowledge distillation). In this report, we briefly summarize various lightweight deep learning techniques and possible research directions.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201953457807295&target=NART&cn=JAKO201953457807295",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "경량 딥러닝 기술 동향 경량 딥러닝 기술 동향 경량 딥러닝 기술 동향 Considerable accuracy improvements in deep learning have recently been achieved in many applications that require large amounts of computation and expensive memory. However, recent advanced techniques for compacting and accelerating the deep learning model have been developed for deployment in lightweight devices with constrained resources. Lightweight deep learning techniques can be categorized into two schemes: lightweight deep learning algorithms (model simplification and efficient convolutional filters) in nature and transferring models into compact/small ones (model compression and knowledge distillation). In this report, we briefly summarize various lightweight deep learning techniques and possible research directions."
        },
        {
          "rank": 35,
          "score": 0.6681249141693115,
          "doc_id": "NART108808939",
          "title": "Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출",
          "abstract": "This paper presents a novel remote infrared pedestrian detection method for night use by means of local projection-CNN. Conventional sliding window methods (HOG/ACF) or region proposal-based deep learning approaches (faster R-CNN, SSD, YOLO) either fail to detect small objects or generate many false positives. Multi-modal region proposal schemes (multi-scale contrast filters with local projection+ACF) are used to improve remote pedestrian detection. AlexNet-based CNN feature extraction and SVM classification can reduce false positives further. This paper's experimental evaluations indicate that the proposed method can improve remote IR pedestrian detection by 16%.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART108808939&target=NART&cn=NART108808939",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 This paper presents a novel remote infrared pedestrian detection method for night use by means of local projection-CNN. Conventional sliding window methods (HOG/ACF) or region proposal-based deep learning approaches (faster R-CNN, SSD, YOLO) either fail to detect small objects or generate many false positives. Multi-modal region proposal schemes (multi-scale contrast filters with local projection+ACF) are used to improve remote pedestrian detection. AlexNet-based CNN feature extraction and SVM classification can reduce false positives further. This paper's experimental evaluations indicate that the proposed method can improve remote IR pedestrian detection by 16%."
        },
        {
          "rank": 36,
          "score": 0.666458249092102,
          "doc_id": "ART002968156",
          "title": "Artificial Intelligence and Deep Learning in Musculoskeletal Magnetic Resonance Imaging",
          "abstract": "The application of artificial intelligence (AI) and deep learning (DL) in radiology is rapidly evolving. AI in healthcare has benefits for image recognition, classification, and radiological workflows from a clinical perspective. Additionally, clinical triage AI can be applied to triage systems. This review aims to introduce the concept of DL and discuss its applications in the interpretation of magnetic resonance (MR) images and the DL-based reconstruction of accelerated MR images, with an emphasis on musculoskeletal radiology. The most recent developments and future directions are also discussed briefly.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002968156&target=NART&cn=ART002968156",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Artificial Intelligence and Deep Learning in Musculoskeletal Magnetic Resonance Imaging Artificial Intelligence and Deep Learning in Musculoskeletal Magnetic Resonance Imaging Artificial Intelligence and Deep Learning in Musculoskeletal Magnetic Resonance Imaging The application of artificial intelligence (AI) and deep learning (DL) in radiology is rapidly evolving. AI in healthcare has benefits for image recognition, classification, and radiological workflows from a clinical perspective. Additionally, clinical triage AI can be applied to triage systems. This review aims to introduce the concept of DL and discuss its applications in the interpretation of magnetic resonance (MR) images and the DL-based reconstruction of accelerated MR images, with an emphasis on musculoskeletal radiology. The most recent developments and future directions are also discussed briefly."
        },
        {
          "rank": 37,
          "score": 0.6647491455078125,
          "doc_id": "JAKO202116954704821",
          "title": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측",
          "abstract": "본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116954704821&target=NART&cn=JAKO202116954704821",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다."
        },
        {
          "rank": 38,
          "score": 0.6632071733474731,
          "doc_id": "JAKO202126048601456",
          "title": "유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구",
          "abstract": "딥 러닝을 활용한 컴퓨터 비전 연구는 여전히 대규모의 학습 데이터와 컴퓨팅 파워가 필수적이며, 최적의 네트워크 구조를 도출하기 위해 많은 시행착오가 수반된다. 본 연구에서는 네트워크 최적화나 데이터를 보강하는 것과 무관하게 데이터 자체의 특성만을 고려한 CR(Confusion Rate)기반의 유사 이미지 분류 성능 향상 기법을 제안한다. 제안 방법은 유사한 이미지 데이터를 정확히 분류하기 위해 CR을 산출하고 이를 손실 함수의 가중치에 반영함으로서 딥 러닝 모델의 성능을 향상시키는 기법을 제안한다. 제안 방법은 네트워크 최적화 결과와 독립적으로 이미지 분류 성능의 향상을 가져올 수 있으며, 클래스 간의 유사성을 고려해 유사도가 높은 이미지 식별에 적합하다. 제안 방법의 평가결과 HanDB에서는 0.22%, Animal-10N에서는 3.38%의 성능향상을 보였다. 제안한 방법은 다양한 Noisy Labeled 데이터를 활용한 인공지능 연구에 기반이 될 것을 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202126048601456&target=NART&cn=JAKO202126048601456",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 딥 러닝을 활용한 컴퓨터 비전 연구는 여전히 대규모의 학습 데이터와 컴퓨팅 파워가 필수적이며, 최적의 네트워크 구조를 도출하기 위해 많은 시행착오가 수반된다. 본 연구에서는 네트워크 최적화나 데이터를 보강하는 것과 무관하게 데이터 자체의 특성만을 고려한 CR(Confusion Rate)기반의 유사 이미지 분류 성능 향상 기법을 제안한다. 제안 방법은 유사한 이미지 데이터를 정확히 분류하기 위해 CR을 산출하고 이를 손실 함수의 가중치에 반영함으로서 딥 러닝 모델의 성능을 향상시키는 기법을 제안한다. 제안 방법은 네트워크 최적화 결과와 독립적으로 이미지 분류 성능의 향상을 가져올 수 있으며, 클래스 간의 유사성을 고려해 유사도가 높은 이미지 식별에 적합하다. 제안 방법의 평가결과 HanDB에서는 0.22%, Animal-10N에서는 3.38%의 성능향상을 보였다. 제안한 방법은 다양한 Noisy Labeled 데이터를 활용한 인공지능 연구에 기반이 될 것을 기대한다."
        },
        {
          "rank": 39,
          "score": 0.661513090133667,
          "doc_id": "ART003167534",
          "title": "Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior",
          "abstract": "Thick pinhole imaging system is widely used for diagnosing intense pulsed radiation sources. However, owing to the trade-off among spatial resolution, field of view (FOV) and signal-to-noise ratio (SNR), the imaging system normally falls short in achieving high-precision spatial diagnosis. In this paper, we propose an unsupervised deep learning method for single image super-resolution (SISR) of the thick pinhole imaging system. The point spread function (PSF) of the imaging system is obtained by analytical calculation and Monte Carlo simulation methods, and the mathematical model of the imaging system is established using a linear equation. To solve the ill-posed inverse problem, we adopt randomly initialized deep convolutional neural networks (DCNNs) as an image prior without pre-training, which is named deep image prior (DIP). The results demonstrate that, by utilizing the SISR technique to increase the number of pixels in reconstructed images, the proposed DIP algorithm can mitigate the spatial resolution degradation caused by an insufficient spatial sampling frequency of the camera. Compared with various classical algorithms, the proposed DIP algorithm exhibits superior capabilities in recovering highfrequency signals and suppressing ringing artifacts. Furthermore, the convergence and robustness of the proposed DIP algorithm under different random seeds and SNR conditions are also verified.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART003167534&target=NART&cn=ART003167534",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Thick pinhole imaging system is widely used for diagnosing intense pulsed radiation sources. However, owing to the trade-off among spatial resolution, field of view (FOV) and signal-to-noise ratio (SNR), the imaging system normally falls short in achieving high-precision spatial diagnosis. In this paper, we propose an unsupervised deep learning method for single image super-resolution (SISR) of the thick pinhole imaging system. The point spread function (PSF) of the imaging system is obtained by analytical calculation and Monte Carlo simulation methods, and the mathematical model of the imaging system is established using a linear equation. To solve the ill-posed inverse problem, we adopt randomly initialized deep convolutional neural networks (DCNNs) as an image prior without pre-training, which is named deep image prior (DIP). The results demonstrate that, by utilizing the SISR technique to increase the number of pixels in reconstructed images, the proposed DIP algorithm can mitigate the spatial resolution degradation caused by an insufficient spatial sampling frequency of the camera. Compared with various classical algorithms, the proposed DIP algorithm exhibits superior capabilities in recovering highfrequency signals and suppressing ringing artifacts. Furthermore, the convergence and robustness of the proposed DIP algorithm under different random seeds and SNR conditions are also verified."
        },
        {
          "rank": 40,
          "score": 0.6610746383666992,
          "doc_id": "JAKO202007552827199",
          "title": "심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측",
          "abstract": "본 연구에서는 강우예측을 위해 U-Net과 SegNet에 기반한 합성곱 신경망 네트워크 구조에 장기간의 국내 기상레이더 자료를 활용하여 심층학습기반의 강우예측을 수행하였다. 또한, 기존 외삽기반의 강우예측 기법인 이류모델의 결과와 비교 평가하였다. 심층신경망의 학습 및 검정을 위해 2010부터 2016년 동안의 기상청 관악산과 광덕산 레이더의 원자료를 수집, 1 km 공간해상도를 갖는 480 &#215; 480의 픽셀의 회색조 영상으로 변환하여 HDF5 형태의 데이터를 구축하였다. 구축된 데이터로 30분 전부터 현재까지 10분 간격의 연속된 레이더 영상 4개를 이용하여 10분 후의 강수량을 예측하도록 심층신경망 모델을 학습하였으며, 학습된 심층신경망 모델로 60분의 선행예측을 수행하기 위해 예측값을 반복 사용하는 재귀적 방식을 적용하였다. 심층신경망 예측모델의 성능 평가를 위해 2017년에 발생한 24개의 호우사례에 대해 선행 60분까지 강우예측을 수행하였다. 임계강우강도 0.1, 1, 5 mm/hr에서 평균절대오차와 임계성공지수를 산정하여 예측성능을 평가한 결과, 강우강도 임계 값 0.1, 1 mm/hr의 경우 MAE는 60분 선행예측까지, CSI는 선행예측 50분까지 참조 예측모델인 이류모델이 보다 우수한 성능을 보였다. 특히, 5 mm/hr 이하의 약한 강우에 대해서는 심층신경망 예측모델이 이류모델보다 대체적으로 좋은 성능을 보였지만, 5 mm/hr의 임계 값에 대한 평가결과 심층신경망 예측모델은 고강도의 뚜렷한 강수 특징을 예측하는 데 한계가 있었다. 심층신경망 예측모델은 예측시간이 길어질수록 공간 평활화되는 경향이 뚜렷해지며, 이로 인해 강우 예측의 정확도가 저하되었다. 이류모델은 뚜렷한 강수 특성을 보존하기 때문에 강한 강도 (>5 mm/hr)에 대해 심층신경망 예측모델을 능가하지만, 강우 위치가 잘못 이동하는 경향이 있다. 본 연구결과는 이후 심층신경망을 이용한 레이더 강우 예측기술의 개발과 개선에 도움이 될 수 있을 것으로 판단된다. 또한, 본 연구에서 구축한 대용량 기상레이더 자료는 향후 후속연구에 활용될 수 있도록 개방형 저장소를 통해 제공될 예정이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202007552827199&target=NART&cn=JAKO202007552827199",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 본 연구에서는 강우예측을 위해 U-Net과 SegNet에 기반한 합성곱 신경망 네트워크 구조에 장기간의 국내 기상레이더 자료를 활용하여 심층학습기반의 강우예측을 수행하였다. 또한, 기존 외삽기반의 강우예측 기법인 이류모델의 결과와 비교 평가하였다. 심층신경망의 학습 및 검정을 위해 2010부터 2016년 동안의 기상청 관악산과 광덕산 레이더의 원자료를 수집, 1 km 공간해상도를 갖는 480 &#215; 480의 픽셀의 회색조 영상으로 변환하여 HDF5 형태의 데이터를 구축하였다. 구축된 데이터로 30분 전부터 현재까지 10분 간격의 연속된 레이더 영상 4개를 이용하여 10분 후의 강수량을 예측하도록 심층신경망 모델을 학습하였으며, 학습된 심층신경망 모델로 60분의 선행예측을 수행하기 위해 예측값을 반복 사용하는 재귀적 방식을 적용하였다. 심층신경망 예측모델의 성능 평가를 위해 2017년에 발생한 24개의 호우사례에 대해 선행 60분까지 강우예측을 수행하였다. 임계강우강도 0.1, 1, 5 mm/hr에서 평균절대오차와 임계성공지수를 산정하여 예측성능을 평가한 결과, 강우강도 임계 값 0.1, 1 mm/hr의 경우 MAE는 60분 선행예측까지, CSI는 선행예측 50분까지 참조 예측모델인 이류모델이 보다 우수한 성능을 보였다. 특히, 5 mm/hr 이하의 약한 강우에 대해서는 심층신경망 예측모델이 이류모델보다 대체적으로 좋은 성능을 보였지만, 5 mm/hr의 임계 값에 대한 평가결과 심층신경망 예측모델은 고강도의 뚜렷한 강수 특징을 예측하는 데 한계가 있었다. 심층신경망 예측모델은 예측시간이 길어질수록 공간 평활화되는 경향이 뚜렷해지며, 이로 인해 강우 예측의 정확도가 저하되었다. 이류모델은 뚜렷한 강수 특성을 보존하기 때문에 강한 강도 (>5 mm/hr)에 대해 심층신경망 예측모델을 능가하지만, 강우 위치가 잘못 이동하는 경향이 있다. 본 연구결과는 이후 심층신경망을 이용한 레이더 강우 예측기술의 개발과 개선에 도움이 될 수 있을 것으로 판단된다. 또한, 본 연구에서 구축한 대용량 기상레이더 자료는 향후 후속연구에 활용될 수 있도록 개방형 저장소를 통해 제공될 예정이다."
        },
        {
          "rank": 41,
          "score": 0.6609762907028198,
          "doc_id": "JAKO202300957609703",
          "title": "딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합",
          "abstract": "위성 시계열 데이터가 증가함에 따라 원격탐사 자료의 활용도가 높아지고 있다. 시계열 자료를 통한 분석에 있어 영상 간의 상대적인 위치 정확도는 결과에 큰 영향을 미치기 때문에 이를 보정하기 위한 영상 정합 과정은 필수적으로 선행되어야 한다. 최근에는 기존 알고리즘의 성능을 상회하는 딥러닝 기반 영상 정합 연구의 사례가 증가하고 있다. 딥러닝 기반 정합 모델을 학습하기 위해서는 수 많은 영상 쌍이 필요하다. 또한, 기존 딥러닝 모델의 데이터 간의 상관도 map을 제작하고, 이에 추가적인 연산을 적용하여 정합점을 추출는데 이는 비효율적이다. 이러한 문제를 해결하기 위해 본 연구에서는 영상 정합 모델 학습을 위한 데이터 증강 기법을 구축하여 데이터셋을 제작하였고, 이를 오프셋(offset) 양 자체를 예측하는 정합 모델인 OffsetNet에 적용하여 KOMSAT-2, -3, -3A 영상 정합을 수행하였다. 모델 학습 결과, OffsetNet은 평가 데이터에 대해 높은 정확도로 오프셋 양을 예측하였고, 이를 통해 주영상과 부영상을 효과적으로 정합하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202300957609703&target=NART&cn=JAKO202300957609703",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 위성 시계열 데이터가 증가함에 따라 원격탐사 자료의 활용도가 높아지고 있다. 시계열 자료를 통한 분석에 있어 영상 간의 상대적인 위치 정확도는 결과에 큰 영향을 미치기 때문에 이를 보정하기 위한 영상 정합 과정은 필수적으로 선행되어야 한다. 최근에는 기존 알고리즘의 성능을 상회하는 딥러닝 기반 영상 정합 연구의 사례가 증가하고 있다. 딥러닝 기반 정합 모델을 학습하기 위해서는 수 많은 영상 쌍이 필요하다. 또한, 기존 딥러닝 모델의 데이터 간의 상관도 map을 제작하고, 이에 추가적인 연산을 적용하여 정합점을 추출는데 이는 비효율적이다. 이러한 문제를 해결하기 위해 본 연구에서는 영상 정합 모델 학습을 위한 데이터 증강 기법을 구축하여 데이터셋을 제작하였고, 이를 오프셋(offset) 양 자체를 예측하는 정합 모델인 OffsetNet에 적용하여 KOMSAT-2, -3, -3A 영상 정합을 수행하였다. 모델 학습 결과, OffsetNet은 평가 데이터에 대해 높은 정확도로 오프셋 양을 예측하였고, 이를 통해 주영상과 부영상을 효과적으로 정합하였다."
        },
        {
          "rank": 42,
          "score": 0.6577869653701782,
          "doc_id": "JAKO202109835990951",
          "title": "분해 심층 학습을 이용한 저조도 영상 개선 방식",
          "abstract": "본 논문에서는 저조도 영상을 개선하기 위한 영상 분해 기반 심층 학습 방법 및 분해 채널 특성에 따른 손실함수를 제안한다. 기존 기법들의 문제점인 색신호 왜곡 및 할로 현상을 제거하기 위해, 입력 영상의 휘도 채널을 반사 성분과 조도 성분으로 분해하고, 반사 성분, 조도 성분 및 색차 신호를 신호 특성에 적합한 심층학습 과정을 적용하는 분해 기반 다중 구조 심층 학습 방법을 제안한다. 더불어, 분해 채널들의 특성에 따른 혼합 놈 기반의 손실함수를 정의하여 복원 영상의 안정성을 증대하고 열화 현상을 제거하기 위한 기법에 대해 기술한다. 실험 결과를 통해 제안한 방법이 다양한 저조도 영상을 효과적으로 개선하였음을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202109835990951&target=NART&cn=JAKO202109835990951",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "분해 심층 학습을 이용한 저조도 영상 개선 방식 분해 심층 학습을 이용한 저조도 영상 개선 방식 분해 심층 학습을 이용한 저조도 영상 개선 방식 본 논문에서는 저조도 영상을 개선하기 위한 영상 분해 기반 심층 학습 방법 및 분해 채널 특성에 따른 손실함수를 제안한다. 기존 기법들의 문제점인 색신호 왜곡 및 할로 현상을 제거하기 위해, 입력 영상의 휘도 채널을 반사 성분과 조도 성분으로 분해하고, 반사 성분, 조도 성분 및 색차 신호를 신호 특성에 적합한 심층학습 과정을 적용하는 분해 기반 다중 구조 심층 학습 방법을 제안한다. 더불어, 분해 채널들의 특성에 따른 혼합 놈 기반의 손실함수를 정의하여 복원 영상의 안정성을 증대하고 열화 현상을 제거하기 위한 기법에 대해 기술한다. 실험 결과를 통해 제안한 방법이 다양한 저조도 영상을 효과적으로 개선하였음을 확인할 수 있었다."
        },
        {
          "rank": 43,
          "score": 0.6569702625274658,
          "doc_id": "JAKO202106153187643",
          "title": "이질적 이미지의 딥러닝 분석을 위한 적대적 학습기반 이미지 보정 방법론",
          "abstract": "빅데이터 시대의 도래는 데이터에서 스스로 규칙을 배우는 딥러닝의 비약적인 발전을 가능하게 하였으며, 특히 CNN 알고리즘이 거둔 성과는 모델의 구조를 넘어 소스 데이터 자체를 조정하는 수준에 이르렀다. 하지만 기존의 이미지 처리 방법은 이미지 데이터 자체를 다룰 뿐, 해당 이미지가 생성된 이질적 환경을 충분히 고려하지 않았다. 이질적 환경에서 촬영된 이미지는 동일한 정보임에도 촬영 환경에 따라 각 이미지의 특징(Feature)이 상이하게 표현될 수 있다. 이는 각 이미지가 갖는 상이한 환경 정보뿐 아니라 이미지 고유의 정보조차 서로 상이한 특징으로 표현되며, 이로 인해 이들 이미지 정보는 서로 잡음(Noise)으로 작용해 모델의 분석 성능을 저해할 수 있음을 의미한다. 따라서 본 논문은 이질적 환경에서 생성된 이미지 데이터들을 동시에 사용하는 앤드-투-앤드(End-To-End) 구조의 적대적 학습(Adversarial Learning) 기반의 이미지 색 항상성 모델 성능 향상 방안을 제안한다. 구체적으로 제안 방법론은 이미지가 촬영된 환경인 도메인을 예측하는 '도메인 분류기'와 조명 값을 예측하는 '조명 예측기'의 상호 작용으로 동작하며, 도메인 분류의 성능을 떨어뜨리는 방향의 학습을 통해 도메인 특성을 제거한다. 제안 방법론의 성능을 평가하기 위해 이질적 환경에서 촬영된 이미지 데이터 셋 7,022장에 대한 색 항상성 실험을 수행한 결과, 제안 방법론이 기존 방법론에 비해 Angular Error 측면에서 우수한 성능을 나타냄을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202106153187643&target=NART&cn=JAKO202106153187643",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "이질적 이미지의 딥러닝 분석을 위한 적대적 학습기반 이미지 보정 방법론 이질적 이미지의 딥러닝 분석을 위한 적대적 학습기반 이미지 보정 방법론 이질적 이미지의 딥러닝 분석을 위한 적대적 학습기반 이미지 보정 방법론 빅데이터 시대의 도래는 데이터에서 스스로 규칙을 배우는 딥러닝의 비약적인 발전을 가능하게 하였으며, 특히 CNN 알고리즘이 거둔 성과는 모델의 구조를 넘어 소스 데이터 자체를 조정하는 수준에 이르렀다. 하지만 기존의 이미지 처리 방법은 이미지 데이터 자체를 다룰 뿐, 해당 이미지가 생성된 이질적 환경을 충분히 고려하지 않았다. 이질적 환경에서 촬영된 이미지는 동일한 정보임에도 촬영 환경에 따라 각 이미지의 특징(Feature)이 상이하게 표현될 수 있다. 이는 각 이미지가 갖는 상이한 환경 정보뿐 아니라 이미지 고유의 정보조차 서로 상이한 특징으로 표현되며, 이로 인해 이들 이미지 정보는 서로 잡음(Noise)으로 작용해 모델의 분석 성능을 저해할 수 있음을 의미한다. 따라서 본 논문은 이질적 환경에서 생성된 이미지 데이터들을 동시에 사용하는 앤드-투-앤드(End-To-End) 구조의 적대적 학습(Adversarial Learning) 기반의 이미지 색 항상성 모델 성능 향상 방안을 제안한다. 구체적으로 제안 방법론은 이미지가 촬영된 환경인 도메인을 예측하는 '도메인 분류기'와 조명 값을 예측하는 '조명 예측기'의 상호 작용으로 동작하며, 도메인 분류의 성능을 떨어뜨리는 방향의 학습을 통해 도메인 특성을 제거한다. 제안 방법론의 성능을 평가하기 위해 이질적 환경에서 촬영된 이미지 데이터 셋 7,022장에 대한 색 항상성 실험을 수행한 결과, 제안 방법론이 기존 방법론에 비해 Angular Error 측면에서 우수한 성능을 나타냄을 확인하였다."
        },
        {
          "rank": 44,
          "score": 0.6565110683441162,
          "doc_id": "NART136112293",
          "title": "Enhancement of Image Quality in Low-Field Knee MR Imaging Using Deep Learning",
          "abstract": "<P>Purpose:&nbsp;The purpose of this study is to investigate the potential of deep learning (DL) techniques to enhance the image quality of low-field knee MR images, with the ultimate goal of approximating the standards of&nbsp;high-field knee MR imaging.</P><P>Methods: We analyzed knee MR images collected from 45 patients with knee disorders and six normal subjects using a 3T MR scanner&nbsp;and those collected from 25 patients with knee disorders using a 0.4T MR scanner. Two DL models were developed: a fat-suppression contrast-generation model and a super-resolution model. These DL models were trained using 3T knee MR imaging data and applied to 0.4T knee MR imaging data. Visual assessments of anatomical structures and image noise and abnormality detection with diagnostic confidence levels on the original 0.4T MR images and those after&nbsp;DL enhancement were conducted by two board-certified radiologists. Statistical analyses were performed using McNemar&rsquo;s test and the Wilcoxon signed-rank test.</P><P>Results:&nbsp;DL-enhanced MR images significantly improved the depiction of anatomical structures and reduced image noise compared to the original MR images. The number of abnormal findings detected and the diagnostic confidence levels were higher in the DL-enhanced MR images, indicating the potential for more accurate diagnoses.</P><P>Conclusion: DL techniques effectively enhance the image quality of low-field knee MR images by leveraging 3T MR imaging data. This enhancement significantly improves image quality and diagnostic confidence levels, making low-field MR images much more reliable for detecting abnormalities. This advancement offers a useful alternative for clinical settings, especially in resource-limited environments, without compromising diagnostic accuracy.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART136112293&target=NART&cn=NART136112293",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Enhancement of Image Quality in Low-Field Knee MR Imaging Using Deep Learning Enhancement of Image Quality in Low-Field Knee MR Imaging Using Deep Learning Enhancement of Image Quality in Low-Field Knee MR Imaging Using Deep Learning <P>Purpose:&nbsp;The purpose of this study is to investigate the potential of deep learning (DL) techniques to enhance the image quality of low-field knee MR images, with the ultimate goal of approximating the standards of&nbsp;high-field knee MR imaging.</P><P>Methods: We analyzed knee MR images collected from 45 patients with knee disorders and six normal subjects using a 3T MR scanner&nbsp;and those collected from 25 patients with knee disorders using a 0.4T MR scanner. Two DL models were developed: a fat-suppression contrast-generation model and a super-resolution model. These DL models were trained using 3T knee MR imaging data and applied to 0.4T knee MR imaging data. Visual assessments of anatomical structures and image noise and abnormality detection with diagnostic confidence levels on the original 0.4T MR images and those after&nbsp;DL enhancement were conducted by two board-certified radiologists. Statistical analyses were performed using McNemar&rsquo;s test and the Wilcoxon signed-rank test.</P><P>Results:&nbsp;DL-enhanced MR images significantly improved the depiction of anatomical structures and reduced image noise compared to the original MR images. The number of abnormal findings detected and the diagnostic confidence levels were higher in the DL-enhanced MR images, indicating the potential for more accurate diagnoses.</P><P>Conclusion: DL techniques effectively enhance the image quality of low-field knee MR images by leveraging 3T MR imaging data. This enhancement significantly improves image quality and diagnostic confidence levels, making low-field MR images much more reliable for detecting abnormalities. This advancement offers a useful alternative for clinical settings, especially in resource-limited environments, without compromising diagnostic accuracy.</P>"
        },
        {
          "rank": 45,
          "score": 0.6564591526985168,
          "doc_id": "NART98464294",
          "title": "Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging",
          "abstract": "<P><B>Abstract</B></P>  <P>Artificial intelligence (AI) in medical imaging is a potentially disruptive technology. An understanding of the principles and application of radiomics, artificial neural networks, machine learning, and deep learning is an essential foundation to weave design solutions that accommodate ethical and regulatory requirements, and to craft AI-based algorithms that enhance outcomes, quality, and efficiency. Moreover, a more holistic perspective of applications, opportunities, and challenges from a programmatic perspective contributes to ethical and sustainable implementation of AI solutions.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART98464294&target=NART&cn=NART98464294",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging <P><B>Abstract</B></P>  <P>Artificial intelligence (AI) in medical imaging is a potentially disruptive technology. An understanding of the principles and application of radiomics, artificial neural networks, machine learning, and deep learning is an essential foundation to weave design solutions that accommodate ethical and regulatory requirements, and to craft AI-based algorithms that enhance outcomes, quality, and efficiency. Moreover, a more holistic perspective of applications, opportunities, and challenges from a programmatic perspective contributes to ethical and sustainable implementation of AI solutions.</P>"
        },
        {
          "rank": 46,
          "score": 0.653093695640564,
          "doc_id": "ART002885478",
          "title": "Detection of fake news using deep learning CNN–RNN based methods",
          "abstract": "Fake news is inaccurate information that is intentionally disseminated for a specific purpose. If allowed to spread, fake news can harm the political and social spheres, so several studies are conducted to detect fake news. This study uses a deep learning method with several architectures such as CNN, Bidirectional LSTM, and ResNet, combined with pre-trained word embedding, trained using four different datasets. Each data goes through a data augmentation process using the back-translation method to reduce data imbalances between classes. The results showed that the Bidirectional LSTM architecture outperformed CNN and ResNet on all tested datasets.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002885478&target=NART&cn=ART002885478",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Detection of fake news using deep learning CNN–RNN based methods Detection of fake news using deep learning CNN–RNN based methods Detection of fake news using deep learning CNN–RNN based methods Fake news is inaccurate information that is intentionally disseminated for a specific purpose. If allowed to spread, fake news can harm the political and social spheres, so several studies are conducted to detect fake news. This study uses a deep learning method with several architectures such as CNN, Bidirectional LSTM, and ResNet, combined with pre-trained word embedding, trained using four different datasets. Each data goes through a data augmentation process using the back-translation method to reduce data imbalances between classes. The results showed that the Bidirectional LSTM architecture outperformed CNN and ResNet on all tested datasets."
        },
        {
          "rank": 47,
          "score": 0.6521953344345093,
          "doc_id": "JAKO200428635215914",
          "title": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구",
          "abstract": "본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200428635215914&target=NART&cn=JAKO200428635215914",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다."
        },
        {
          "rank": 48,
          "score": 0.6518813967704773,
          "doc_id": "JAKO202210351407855",
          "title": "심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적",
          "abstract": "Nowadays, the complexity of object tracking models among hardware applications has become a more in-demand duty to complete in various indeterminable environment tracking situations with multifunctional algorithm skills. In this paper, we propose a virtual city environment using AirSim (Aerial Informatics and Robotics Simulation - AirSim, CityEnvironment) and use the DQN (Deep Q-Learning) model of deep reinforcement learning model in the virtual environment. The proposed object tracking DQN network observes the environment using a deep reinforcement learning model that receives continuous images taken by a virtual environment simulation system as input to control the operation of a virtual drone. The deep reinforcement learning model is pre-trained using various existing continuous image sets. Since the existing various continuous image sets are image data of real environments and objects, it is implemented in 3D to track virtual environments and moving objects in them.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202210351407855&target=NART&cn=JAKO202210351407855",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 Nowadays, the complexity of object tracking models among hardware applications has become a more in-demand duty to complete in various indeterminable environment tracking situations with multifunctional algorithm skills. In this paper, we propose a virtual city environment using AirSim (Aerial Informatics and Robotics Simulation - AirSim, CityEnvironment) and use the DQN (Deep Q-Learning) model of deep reinforcement learning model in the virtual environment. The proposed object tracking DQN network observes the environment using a deep reinforcement learning model that receives continuous images taken by a virtual environment simulation system as input to control the operation of a virtual drone. The deep reinforcement learning model is pre-trained using various existing continuous image sets. Since the existing various continuous image sets are image data of real environments and objects, it is implemented in 3D to track virtual environments and moving objects in them."
        },
        {
          "rank": 49,
          "score": 0.65028977394104,
          "doc_id": "JAKO201926358474403",
          "title": "GPR 영상에서 딥러닝 기반 CNN을 이용한 배관 위치 추정 연구",
          "abstract": "최근에 지하공동이나 배관의 위치 파악 등의 필요에 의해 금속을 포함하여 다양한 재질의 지하 물체를 탐지하는 일이 중요해지고 있다. 이러한 이유로 지하 탐지 분야에서 GPR(Ground Penetrating Radar) 기술이 주목을 받고 있다. GPR은 지하에 묻혀 있는 물체의 위치를 찾기 위하여 레이더파를 조사하고 물체로부터 반사되는 반사파를 영상으로 표현한다. 그런데 레이더 신호는 지하에서 여러가지 물체에서 반사되어 나오는 특징이 물체마다 유사한 경우가 많기 때문에 GPR 영상을 해석하는 것은 쉽지 않다. 따라서 본 논문에서는 이러한 문제를 해결하기 위해서 영상 인식 분야에서 최근에 많이 활용되고 있는 딥러닝 기반의 CNN(Convolutional Neural Network)모델을 이용하여 임계값에 따른 GPR 영상에서의 배관 위치를 추정하고 그 실험 결과 임계값이 7 혹은 8 일 때 가장 확실하게 배관의 위치를 찾음을 증명하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201926358474403&target=NART&cn=JAKO201926358474403",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "GPR 영상에서 딥러닝 기반 CNN을 이용한 배관 위치 추정 연구 GPR 영상에서 딥러닝 기반 CNN을 이용한 배관 위치 추정 연구 GPR 영상에서 딥러닝 기반 CNN을 이용한 배관 위치 추정 연구 최근에 지하공동이나 배관의 위치 파악 등의 필요에 의해 금속을 포함하여 다양한 재질의 지하 물체를 탐지하는 일이 중요해지고 있다. 이러한 이유로 지하 탐지 분야에서 GPR(Ground Penetrating Radar) 기술이 주목을 받고 있다. GPR은 지하에 묻혀 있는 물체의 위치를 찾기 위하여 레이더파를 조사하고 물체로부터 반사되는 반사파를 영상으로 표현한다. 그런데 레이더 신호는 지하에서 여러가지 물체에서 반사되어 나오는 특징이 물체마다 유사한 경우가 많기 때문에 GPR 영상을 해석하는 것은 쉽지 않다. 따라서 본 논문에서는 이러한 문제를 해결하기 위해서 영상 인식 분야에서 최근에 많이 활용되고 있는 딥러닝 기반의 CNN(Convolutional Neural Network)모델을 이용하여 임계값에 따른 GPR 영상에서의 배관 위치를 추정하고 그 실험 결과 임계값이 7 혹은 8 일 때 가장 확실하게 배관의 위치를 찾음을 증명하였다."
        },
        {
          "rank": 50,
          "score": 0.6484675407409668,
          "doc_id": "JAKO202011263332681",
          "title": "심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용",
          "abstract": "가우스 혼합 모델-은닉 마코프 모델(Gaussian Mixture Model-Hidden Markov Model, GMM-HMM)을 이용하는 전통적인 음성인식 시스템에서는, 극점 필터링 기반의 켑스트럼 특징 정규화 방식이 잡음 환경에서 짧은 발화의 인식 성능을 향상시키는데 효과적이었다. 본 논문에서는 심층신경망(Deep Neural Network, DNN)을 이용하는 최신의 음성인식 시스템에서도 이 방식의 유용성이 있는지 검토한다. AURORA 2 DB에 대한 실험 결과, 특히 훈련 및 테스트 환경 사이의 불일치가 클 때에, 극점 필터링 기반의 켑스트럼 평균 분산 정규화 방식이 극점 필터링을 사용하지 않는 방식에 비해 매우 짧은 발화의 인식 성능을 개선시킴을 보여 준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202011263332681&target=NART&cn=JAKO202011263332681",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 심층신경망을 이용한 짧은 발화 음성인식에서 극점 필터링 기반의 특징 정규화 적용 가우스 혼합 모델-은닉 마코프 모델(Gaussian Mixture Model-Hidden Markov Model, GMM-HMM)을 이용하는 전통적인 음성인식 시스템에서는, 극점 필터링 기반의 켑스트럼 특징 정규화 방식이 잡음 환경에서 짧은 발화의 인식 성능을 향상시키는데 효과적이었다. 본 논문에서는 심층신경망(Deep Neural Network, DNN)을 이용하는 최신의 음성인식 시스템에서도 이 방식의 유용성이 있는지 검토한다. AURORA 2 DB에 대한 실험 결과, 특히 훈련 및 테스트 환경 사이의 불일치가 클 때에, 극점 필터링 기반의 켑스트럼 평균 분산 정규화 방식이 극점 필터링을 사용하지 않는 방식에 비해 매우 짧은 발화의 인식 성능을 개선시킴을 보여 준다."
        }
      ]
    },
    {
      "query": "What are the main challenges discussed regarding deep learning in radar imaging?",
      "query_meta": {
        "type": "single_hop",
        "index": 1
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.7784861326217651,
          "doc_id": "NPAP12546494",
          "title": "Deep learning for radar",
          "abstract": "<P>Motivated by the recent advances in deep learning, we lay out a vision of how deep learning techniques can be used in radar. Specifically, our discussion focuses on the use of deep learning to advance the state-of-the-art in radar imaging. While deep learning can be directly applied to automatic target recognition (ATR), the relevance of these techniques in other radar problems is not obvious. We argue that deep learning can play a central role in advancing the state-of-the-art in a wide range of radar imaging problems, discuss the challenges associated with applying these methods, and the potential advancements that are expected. We lay out an approach to design a network architecture based on the specific structure of the synthetic aperture radar (SAR) imaging problem that augments learning with traditional SAR modelling. This framework allows for capture of the non-linearity of the SAR forward model. Furthermore, we demonstrate how this process can be used to learn and compensate for trajectory based phase error for the autofocus problem.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12546494&target=NART&cn=NPAP12546494",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning for radar Deep learning for radar Deep learning for radar <P>Motivated by the recent advances in deep learning, we lay out a vision of how deep learning techniques can be used in radar. Specifically, our discussion focuses on the use of deep learning to advance the state-of-the-art in radar imaging. While deep learning can be directly applied to automatic target recognition (ATR), the relevance of these techniques in other radar problems is not obvious. We argue that deep learning can play a central role in advancing the state-of-the-art in a wide range of radar imaging problems, discuss the challenges associated with applying these methods, and the potential advancements that are expected. We lay out an approach to design a network architecture based on the specific structure of the synthetic aperture radar (SAR) imaging problem that augments learning with traditional SAR modelling. This framework allows for capture of the non-linearity of the SAR forward model. Furthermore, we demonstrate how this process can be used to learn and compensate for trajectory based phase error for the autofocus problem.</P>"
        },
        {
          "rank": 2,
          "score": 0.7632582187652588,
          "doc_id": "NART116403822",
          "title": "Deep-Learning for Radar: A Survey",
          "abstract": "<P>A comprehensive and well-structured review on the application of deep learning (DL) based algorithms, such as convolutional neural networks (CNN) and long-short term memory (LSTM), in radar signal processing is given. The following DL application areas are covered: i) radar waveform and antenna array design; ii) passive or low probability of interception (LPI) radar waveform recognition; iii) automatic target recognition (ATR) based on high range resolution profiles (HRRPs), Doppler signatures, and synthetic aperture radar (SAR) images; and iv) radar jamming/clutter recognition and suppression. Although DL is unanimously praised as the ultimate solution to many bottleneck problems in most of existing works on similar topics, both the positive and the negative sides of stories about DL are checked in this work. Specifically, two limiting factors of the real-life performance of deep neural networks (DNNs), limited training samples and adversarial examples, are thoroughly examined. By investigating the relationship between the DL-based algorithms proposed in various papers and linking them together to form a full picture, this work serves as a valuable source for researchers who are seeking potential research opportunities in this promising research field.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART116403822&target=NART&cn=NART116403822",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep-Learning for Radar: A Survey Deep-Learning for Radar: A Survey Deep-Learning for Radar: A Survey <P>A comprehensive and well-structured review on the application of deep learning (DL) based algorithms, such as convolutional neural networks (CNN) and long-short term memory (LSTM), in radar signal processing is given. The following DL application areas are covered: i) radar waveform and antenna array design; ii) passive or low probability of interception (LPI) radar waveform recognition; iii) automatic target recognition (ATR) based on high range resolution profiles (HRRPs), Doppler signatures, and synthetic aperture radar (SAR) images; and iv) radar jamming/clutter recognition and suppression. Although DL is unanimously praised as the ultimate solution to many bottleneck problems in most of existing works on similar topics, both the positive and the negative sides of stories about DL are checked in this work. Specifically, two limiting factors of the real-life performance of deep neural networks (DNNs), limited training samples and adversarial examples, are thoroughly examined. By investigating the relationship between the DL-based algorithms proposed in various papers and linking them together to form a full picture, this work serves as a valuable source for researchers who are seeking potential research opportunities in this promising research field.</P>"
        },
        {
          "rank": 3,
          "score": 0.7456988096237183,
          "doc_id": "NART121030945",
          "title": "MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning",
          "abstract": "<P>Transmitting orthogonal waveforms are the basis for giving full play to the advantages of MIMO radar imaging technology, but the commonly used waveforms with the same frequency cannot meet the orthogonality requirement, resulting in serious coupling noise in traditional imaging methods and affecting the imaging effect. In order to effectively suppress the mutual coupling interference caused by non-orthogonal waveforms, a new non-orthogonal waveform MIMO radar imaging method based on deep learning is proposed in this paper: with the powerful nonlinear fitting ability of deep learning, the mapping relationship between the non-orthogonal waveform MIMO radar echo and ideal target image is automatically learned by constructing a deep imaging network and training on a large number of simulated training data. The learned imaging network can effectively suppress the coupling interference between non-ideal orthogonal waveforms and improve the imaging quality of MIMO radar. Finally, the effectiveness of the proposed method is verified by experiments with point scattering model data and electromagnetic scattering calculation data.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART121030945&target=NART&cn=NART121030945",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning <P>Transmitting orthogonal waveforms are the basis for giving full play to the advantages of MIMO radar imaging technology, but the commonly used waveforms with the same frequency cannot meet the orthogonality requirement, resulting in serious coupling noise in traditional imaging methods and affecting the imaging effect. In order to effectively suppress the mutual coupling interference caused by non-orthogonal waveforms, a new non-orthogonal waveform MIMO radar imaging method based on deep learning is proposed in this paper: with the powerful nonlinear fitting ability of deep learning, the mapping relationship between the non-orthogonal waveform MIMO radar echo and ideal target image is automatically learned by constructing a deep imaging network and training on a large number of simulated training data. The learned imaging network can effectively suppress the coupling interference between non-ideal orthogonal waveforms and improve the imaging quality of MIMO radar. Finally, the effectiveness of the proposed method is verified by experiments with point scattering model data and electromagnetic scattering calculation data.</P>"
        },
        {
          "rank": 4,
          "score": 0.7354378700256348,
          "doc_id": "NART127041948",
          "title": "Deep Learning Techniques in Radar Emitter Identification",
          "abstract": "<P>In the field of electronic warfare (EW), one of the crucial roles of electronic intelligence is the identification of radar signals. In an operational environment, it is very essential to identify radar emitters whether friend or foe so that appropriate radar countermeasures can be taken against them. With the electromagnetic environment becoming increasingly complex and the diversity of signal features, radar emitter identification with high recognition accuracy has become a significantly challenging task. Traditional radar identification methods have shown some limitations in this complex electromagnetic scenario. Several radar classification and identification methods based on artificial neural networks have emerged with the emergence of artificial neural networks, notably deep learning approaches. Machine learning and deep learning algorithms are now frequently utilized to extract various types of information from radar signals more accurately and robustly. This paper illustrates the use of Deep Neural Networks (DNN) in radar applications for emitter classification and identification. Since deep learning approaches are capable of accurately classifying complicated patterns in radar signals, they have demonstrated significant promise for identifying radar emitters. By offering a thorough literature analysis of deep learning-based methodologies, the study intends to assist researchers and practitioners in better understanding the application of deep learning techniques to challenges related to the classification and identification of radar emitters. The study demonstrates that DNN can be used successfully in applications for radar classification and identification.&amp;#xD; &amp;#xD; </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART127041948&target=NART&cn=NART127041948",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning Techniques in Radar Emitter Identification Deep Learning Techniques in Radar Emitter Identification Deep Learning Techniques in Radar Emitter Identification <P>In the field of electronic warfare (EW), one of the crucial roles of electronic intelligence is the identification of radar signals. In an operational environment, it is very essential to identify radar emitters whether friend or foe so that appropriate radar countermeasures can be taken against them. With the electromagnetic environment becoming increasingly complex and the diversity of signal features, radar emitter identification with high recognition accuracy has become a significantly challenging task. Traditional radar identification methods have shown some limitations in this complex electromagnetic scenario. Several radar classification and identification methods based on artificial neural networks have emerged with the emergence of artificial neural networks, notably deep learning approaches. Machine learning and deep learning algorithms are now frequently utilized to extract various types of information from radar signals more accurately and robustly. This paper illustrates the use of Deep Neural Networks (DNN) in radar applications for emitter classification and identification. Since deep learning approaches are capable of accurately classifying complicated patterns in radar signals, they have demonstrated significant promise for identifying radar emitters. By offering a thorough literature analysis of deep learning-based methodologies, the study intends to assist researchers and practitioners in better understanding the application of deep learning techniques to challenges related to the classification and identification of radar emitters. The study demonstrates that DNN can be used successfully in applications for radar classification and identification.&amp;#xD; &amp;#xD; </P>"
        },
        {
          "rank": 5,
          "score": 0.7284632921218872,
          "doc_id": "JAKO201923233204235",
          "title": "딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구",
          "abstract": "Classification of radar signals in the field of electronic warfare is a problem of discriminating threat types by analyzing enemy threat radar signals such as aircraft, radar, and missile received through electronic warfare equipment. Recent radar systems have adopted a variety of modulation schemes that are different from those used in conventional systems, and are often difficult to analyze using existing algorithms. Also, it is necessary to design a robust algorithm for the signal received in the real environment due to the environmental influence and the measurement error due to the characteristics of the hardware. In this paper, we propose a radar signal classification method which are not affected by radar signal modulation methods and noise generation by using deep learning techniques.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201923233204235&target=NART&cn=JAKO201923233204235",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 Classification of radar signals in the field of electronic warfare is a problem of discriminating threat types by analyzing enemy threat radar signals such as aircraft, radar, and missile received through electronic warfare equipment. Recent radar systems have adopted a variety of modulation schemes that are different from those used in conventional systems, and are often difficult to analyze using existing algorithms. Also, it is necessary to design a robust algorithm for the signal received in the real environment due to the environmental influence and the measurement error due to the characteristics of the hardware. In this paper, we propose a radar signal classification method which are not affected by radar signal modulation methods and noise generation by using deep learning techniques."
        },
        {
          "rank": 6,
          "score": 0.7165532112121582,
          "doc_id": "NART106334316",
          "title": "Deep learning for waveform estimation and imaging in passive radar",
          "abstract": "<P>The authors consider a bistatic configuration with a stationary transmitter transmitting unknown waveforms of opportunity and a single moving receiver and present a deep learning (DL) framework for passive synthetic aperture radar (SAR) imaging. They approach DL from an optimisation based perspective and formulate image reconstruction as a machine learning task. By unfolding the iterations of a proximal gradient descent algorithm, they construct a deep recurrent neural network (RNN) that is parameterised by the transmitted waveforms. They cascade the RNN structure with a decoder stage to form a recurrent auto&#x2010;encoder architecture. They then use backpropagation to learn transmitted waveforms by training the network in an unsupervised manner using SAR measurements. The highly non&#x2010;convex problem of backpropagation is guided to a feasible solution over the parameter space by initialising the network with the known components of the SAR forward model. Moreover, prior information regarding the waveform structure is incorporated during initialisation and backpropagation. They demonstrate the effectiveness of the DL&#x2010;based approach through numerical simulations that show focused, high contrast imagery using a single receiver antenna at realistic signal&#x2010;to&#x2010;noise&#x2010;ratio levels.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106334316&target=NART&cn=NART106334316",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning for waveform estimation and imaging in passive radar Deep learning for waveform estimation and imaging in passive radar Deep learning for waveform estimation and imaging in passive radar <P>The authors consider a bistatic configuration with a stationary transmitter transmitting unknown waveforms of opportunity and a single moving receiver and present a deep learning (DL) framework for passive synthetic aperture radar (SAR) imaging. They approach DL from an optimisation based perspective and formulate image reconstruction as a machine learning task. By unfolding the iterations of a proximal gradient descent algorithm, they construct a deep recurrent neural network (RNN) that is parameterised by the transmitted waveforms. They cascade the RNN structure with a decoder stage to form a recurrent auto&#x2010;encoder architecture. They then use backpropagation to learn transmitted waveforms by training the network in an unsupervised manner using SAR measurements. The highly non&#x2010;convex problem of backpropagation is guided to a feasible solution over the parameter space by initialising the network with the known components of the SAR forward model. Moreover, prior information regarding the waveform structure is incorporated during initialisation and backpropagation. They demonstrate the effectiveness of the DL&#x2010;based approach through numerical simulations that show focused, high contrast imagery using a single receiver antenna at realistic signal&#x2010;to&#x2010;noise&#x2010;ratio levels.</P>"
        },
        {
          "rank": 7,
          "score": 0.7051048874855042,
          "doc_id": "DIKO0016954237",
          "title": "밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구",
          "abstract": "본 논문에서는 합성개구레이더 (Synthetic Aperture Radar, SAR) 시스템의 데이터를 획득하는 과정에서 발생할 수 있는 위상 오차를 보상하기 위해, 딥러닝 기반 요동 보상 방법으로 Unsupervised Image-to-image Translation (UNIT) 네트워크를 제안한다. 일반적으로 SAR 시스템을 이용한 데이터 취득 과정에서 레이더가 부착된 플랫폼의 비이상적인 경로나 불안정한 자세로 인해 위상 오차가 포함된 데이터를 얻는 문제가 발생할 수 있다. 이러한 위상 오차는 주변 환경 인식 및 표적 탐지 성능을 감소시키며, 군사 목적의 감시, 정찰을 위한 SAR 시스템과 자율주행 분야에서 지능형 차량의 경로 계획 및 사고 위협 회피를 위해 주변 환경 표현이 필수적이다. 따라서, 본 연구에서는 딥러닝 기반 요동 보상 방법을 제안하고, 밀리미터파 레이더 센서를 이용한 실험을 통해 제안된 방법의 성능을 검증한다. 제안된 방법은 Peak Signal-to-Noise Ratio (PSNR)와 Structural Similarity Index Measure (SSIM) 측면에서 기존의 요동 보상 기법들과 성능 평가 및 비교가 수행된다. 실제 측정 데이터를 기반으로 성능을 비교한 결과, 제안된 UNIT 네트워크는 기존 요동 보상 기법들 대비 PSNR은 평균 10.17%, SSIM은 9.4% 향상되는 것을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016954237&target=NART&cn=DIKO0016954237",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 본 논문에서는 합성개구레이더 (Synthetic Aperture Radar, SAR) 시스템의 데이터를 획득하는 과정에서 발생할 수 있는 위상 오차를 보상하기 위해, 딥러닝 기반 요동 보상 방법으로 Unsupervised Image-to-image Translation (UNIT) 네트워크를 제안한다. 일반적으로 SAR 시스템을 이용한 데이터 취득 과정에서 레이더가 부착된 플랫폼의 비이상적인 경로나 불안정한 자세로 인해 위상 오차가 포함된 데이터를 얻는 문제가 발생할 수 있다. 이러한 위상 오차는 주변 환경 인식 및 표적 탐지 성능을 감소시키며, 군사 목적의 감시, 정찰을 위한 SAR 시스템과 자율주행 분야에서 지능형 차량의 경로 계획 및 사고 위협 회피를 위해 주변 환경 표현이 필수적이다. 따라서, 본 연구에서는 딥러닝 기반 요동 보상 방법을 제안하고, 밀리미터파 레이더 센서를 이용한 실험을 통해 제안된 방법의 성능을 검증한다. 제안된 방법은 Peak Signal-to-Noise Ratio (PSNR)와 Structural Similarity Index Measure (SSIM) 측면에서 기존의 요동 보상 기법들과 성능 평가 및 비교가 수행된다. 실제 측정 데이터를 기반으로 성능을 비교한 결과, 제안된 UNIT 네트워크는 기존 요동 보상 기법들 대비 PSNR은 평균 10.17%, SSIM은 9.4% 향상되는 것을 확인하였다."
        },
        {
          "rank": 8,
          "score": 0.7010417580604553,
          "doc_id": "NART123168643",
          "title": "Synthetic Aperture Radar (SAR) Meets Deep Learning",
          "abstract": "<P>Synthetic aperture radar (SAR) is an important active microwave imaging sensor [...]</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART123168643&target=NART&cn=NART123168643",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Synthetic Aperture Radar (SAR) Meets Deep Learning Synthetic Aperture Radar (SAR) Meets Deep Learning Synthetic Aperture Radar (SAR) Meets Deep Learning <P>Synthetic aperture radar (SAR) is an important active microwave imaging sensor [...]</P>"
        },
        {
          "rank": 9,
          "score": 0.6992959380149841,
          "doc_id": "NART125907540",
          "title": "Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review",
          "abstract": "<P>Radar automatic target recognition (RATR) technology is fundamental but complicated system engineering that combines sensor, target, environment, and signal processing technology, etc. It plays a significant role in improving the level and capabilities of military and civilian automation. Although RATR has been successfully applied in some aspects, the complete theoretical system has not been established. At present, deep learning algorithms have received a lot of attention and have emerged as potential and feasible solutions in RATR. This paper mainly reviews related articles published between 2010 and 2022, which corresponds to the period when deep learning methods were introduced into RATR research. In this paper, the current research status of radar target characteristics is summarized, including motion, micro-motion, one-dimensional, and two-dimensional characteristics, etc. This paper reviews the progress of deep learning methods in the feature extraction and recognition of radar target characteristics in recent years, including space, air, ground, sea-surface targets, etc. Due to more and more attention and research results published in the past few years, it is hoped that this review can provide potential guidance for future research and application of deep learning in fields related to RATR.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART125907540&target=NART&cn=NART125907540",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review <P>Radar automatic target recognition (RATR) technology is fundamental but complicated system engineering that combines sensor, target, environment, and signal processing technology, etc. It plays a significant role in improving the level and capabilities of military and civilian automation. Although RATR has been successfully applied in some aspects, the complete theoretical system has not been established. At present, deep learning algorithms have received a lot of attention and have emerged as potential and feasible solutions in RATR. This paper mainly reviews related articles published between 2010 and 2022, which corresponds to the period when deep learning methods were introduced into RATR research. In this paper, the current research status of radar target characteristics is summarized, including motion, micro-motion, one-dimensional, and two-dimensional characteristics, etc. This paper reviews the progress of deep learning methods in the feature extraction and recognition of radar target characteristics in recent years, including space, air, ground, sea-surface targets, etc. Due to more and more attention and research results published in the past few years, it is hoped that this review can provide potential guidance for future research and application of deep learning in fields related to RATR.</P>"
        },
        {
          "rank": 10,
          "score": 0.695995569229126,
          "doc_id": "DIKO0015644673",
          "title": "M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지",
          "abstract": "해상 교통량의 증가로 인해 해상 선박관리의 필요성이 늘어남에 따라 선박을 탐지하기 위한 연구들이 꾸준히 수행되어왔다. 특히 위성레이더 영상은 시간과 기후에 영향을 받지 않고 촬영할 수 있다는 장점으로 인해 선박탐지를 위한 많은 연구에서 활용되어왔다. 최근에는 딥러닝 기법의 발전으로 인해 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구들이 꾸준히 수행되고 있다. 그런데 위성레이더 영상은 값의 분포범위가 매우 넓고, 많은 스펙클 노이즈가 존재한다. 이러한 요소들은 딥러닝 모델의 학습에 부정적인 영향을 끼칠 수 있으므로 전처리를 통해 해당 요소들을 저감해줄 필요가 있다. 본 연구에서는 전처리된 위성레이더 영상으로부터 딥러닝 선박탐지를 수행하고, 영상의 전처리가 딥러닝 선박탐지에 미치는 요소를 비교분석 하고자 한다.&amp;#xD; 본 연구를 위해 TerraSAR-X와 COSMO-SkyMed 위성레이더 영상을 이용했다. 영상을 딥러닝 학습에 이용하기 전에 먼저 총 세 가지 다른 방법으로 전처리를 수행했다. 첫 번째는 위성레이더 영상에서 강도 값만을 추출한 강도 영상을 생성하는 방법이다. 강도 영상은 값의 범위가 매우 넓을 뿐만 아니라 많은 스펙클 노이즈를 가지고 있다. 두 번째는 강도영상에서 값의 단위를 데시벨로 변환한 데시벨 영상을 생성하는 방법이다. 데시벨 영상은 강도영상과 마찬가지로 많은 스펙클 노이즈를 가지고 있으나 값의 범위가 줄어들어, 더 안정적인 학습을 할 수 있다. 세 번째는 본 연구에서 제안하는 위성레이더 전처리방법으로써, 강도차분과 거칠기영상을 생성하는 방법이다. 두 영상은 중간값 필터링을 이용해 스펙클 노이즈를 줄이고, 값의 분포 대역을 좁힘으로써 빠른 학습이 가능하다.&amp;#xD; 각 전처리된 위성레이더 영상을 이용해 딥러닝 학습을 하기 위해 본 연구에서는 M2Det 객체탐지 모델을 사용했다. 객체탐지 모델을 학습시킨 뒤 테스트 영상을 이용해 선박탐지를 수행했으며, 테스트 결과는 정밀도(Precision), 재현율(Recall)을 이용해 나타냈으며, 두 지수를 하나의 값으로 표현하기 위해 AP(Average Precision)와 F1 점수(F1-score)를 이용해 나타냈다. 각 영상의 정밀도, 재현율, AP, F1 점수는 강도 영상 93.18%, 91.11%, 89.78%, 92.13%, 데시벨 영상 94.16%, 94.16%, 92.34%, 94.16%, 강도차분과 거칠기 영상 97.40%, 94.94%, 95.55%, 96.15%로 계산되었다. 강도 영상을 이용한 경우 미탐지와 오탐지 선박이 많았으며, 전처리된 영상을 이용한 경우 강도 영상에 비해 미탐지와 오탐지 선박이 줄어든 것을 확인할 수 있었다. 데시벨 영상과 강도차분, 거칠기 영상의 결과를 비교했을 때, 두 영상의 오탐지율은 유사했다. 하지만 강도차분, 거칠기 영상을 이용했을 때 강도 영상에 비해 미탐지 선박의 비율이 4% 줄어든 것을 확인할 수 있었다. 이 결과를 통해 위성레이더 영상을 전처리함으로써 딥러닝 학습을 돕고 선박탐지 결과를 향상시킬 수 있다는 것을 알 수 있다. 본 연구결과는 향후 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구의 발전에 이바지할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015644673&target=NART&cn=DIKO0015644673",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 해상 교통량의 증가로 인해 해상 선박관리의 필요성이 늘어남에 따라 선박을 탐지하기 위한 연구들이 꾸준히 수행되어왔다. 특히 위성레이더 영상은 시간과 기후에 영향을 받지 않고 촬영할 수 있다는 장점으로 인해 선박탐지를 위한 많은 연구에서 활용되어왔다. 최근에는 딥러닝 기법의 발전으로 인해 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구들이 꾸준히 수행되고 있다. 그런데 위성레이더 영상은 값의 분포범위가 매우 넓고, 많은 스펙클 노이즈가 존재한다. 이러한 요소들은 딥러닝 모델의 학습에 부정적인 영향을 끼칠 수 있으므로 전처리를 통해 해당 요소들을 저감해줄 필요가 있다. 본 연구에서는 전처리된 위성레이더 영상으로부터 딥러닝 선박탐지를 수행하고, 영상의 전처리가 딥러닝 선박탐지에 미치는 요소를 비교분석 하고자 한다.&amp;#xD; 본 연구를 위해 TerraSAR-X와 COSMO-SkyMed 위성레이더 영상을 이용했다. 영상을 딥러닝 학습에 이용하기 전에 먼저 총 세 가지 다른 방법으로 전처리를 수행했다. 첫 번째는 위성레이더 영상에서 강도 값만을 추출한 강도 영상을 생성하는 방법이다. 강도 영상은 값의 범위가 매우 넓을 뿐만 아니라 많은 스펙클 노이즈를 가지고 있다. 두 번째는 강도영상에서 값의 단위를 데시벨로 변환한 데시벨 영상을 생성하는 방법이다. 데시벨 영상은 강도영상과 마찬가지로 많은 스펙클 노이즈를 가지고 있으나 값의 범위가 줄어들어, 더 안정적인 학습을 할 수 있다. 세 번째는 본 연구에서 제안하는 위성레이더 전처리방법으로써, 강도차분과 거칠기영상을 생성하는 방법이다. 두 영상은 중간값 필터링을 이용해 스펙클 노이즈를 줄이고, 값의 분포 대역을 좁힘으로써 빠른 학습이 가능하다.&amp;#xD; 각 전처리된 위성레이더 영상을 이용해 딥러닝 학습을 하기 위해 본 연구에서는 M2Det 객체탐지 모델을 사용했다. 객체탐지 모델을 학습시킨 뒤 테스트 영상을 이용해 선박탐지를 수행했으며, 테스트 결과는 정밀도(Precision), 재현율(Recall)을 이용해 나타냈으며, 두 지수를 하나의 값으로 표현하기 위해 AP(Average Precision)와 F1 점수(F1-score)를 이용해 나타냈다. 각 영상의 정밀도, 재현율, AP, F1 점수는 강도 영상 93.18%, 91.11%, 89.78%, 92.13%, 데시벨 영상 94.16%, 94.16%, 92.34%, 94.16%, 강도차분과 거칠기 영상 97.40%, 94.94%, 95.55%, 96.15%로 계산되었다. 강도 영상을 이용한 경우 미탐지와 오탐지 선박이 많았으며, 전처리된 영상을 이용한 경우 강도 영상에 비해 미탐지와 오탐지 선박이 줄어든 것을 확인할 수 있었다. 데시벨 영상과 강도차분, 거칠기 영상의 결과를 비교했을 때, 두 영상의 오탐지율은 유사했다. 하지만 강도차분, 거칠기 영상을 이용했을 때 강도 영상에 비해 미탐지 선박의 비율이 4% 줄어든 것을 확인할 수 있었다. 이 결과를 통해 위성레이더 영상을 전처리함으로써 딥러닝 학습을 돕고 선박탐지 결과를 향상시킬 수 있다는 것을 알 수 있다. 본 연구결과는 향후 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구의 발전에 이바지할 수 있을 것으로 기대된다."
        },
        {
          "rank": 11,
          "score": 0.6959017515182495,
          "doc_id": "NART117063882",
          "title": "Advancing Radar Nowcasting Through Deep Transfer Learning",
          "abstract": "<P>Deep learning is emerging as a powerful tool in scientific applications, such as radar-based convective storm nowcasting. However, it is still a challenge to extend the application of a well-trained deep learning nowcasting model, which demands to incorporate the learned knowledge at a certain location to other locations characterized by different precipitation features. This article designs a transfer learning framework to tackle this problem. A convolutional neural network (CNN)-based nowcasting method is utilized as the benchmark, based on which two transfer learning models are constructed through fine-tune and maximum mean discrepancy (MMD) minimization. The base CNN model is trained using radar data in the source study domain near Beijing, China, whereas the transferred models are applied to the target domain near Guangzhou, China, with only a small amount of data in the target area. The influence of a varying number of target data samples on the nowcasting performance is quantified. The experimental results demonstrate that the deep transfer learning models can improve the nowcasting skills.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART117063882&target=NART&cn=NART117063882",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Advancing Radar Nowcasting Through Deep Transfer Learning Advancing Radar Nowcasting Through Deep Transfer Learning Advancing Radar Nowcasting Through Deep Transfer Learning <P>Deep learning is emerging as a powerful tool in scientific applications, such as radar-based convective storm nowcasting. However, it is still a challenge to extend the application of a well-trained deep learning nowcasting model, which demands to incorporate the learned knowledge at a certain location to other locations characterized by different precipitation features. This article designs a transfer learning framework to tackle this problem. A convolutional neural network (CNN)-based nowcasting method is utilized as the benchmark, based on which two transfer learning models are constructed through fine-tune and maximum mean discrepancy (MMD) minimization. The base CNN model is trained using radar data in the source study domain near Beijing, China, whereas the transferred models are applied to the target domain near Guangzhou, China, with only a small amount of data in the target area. The influence of a varying number of target data samples on the nowcasting performance is quantified. The experimental results demonstrate that the deep transfer learning models can improve the nowcasting skills.</P>"
        },
        {
          "rank": 12,
          "score": 0.6953844428062439,
          "doc_id": "NART84975182",
          "title": "Deep Learning for Passive Synthetic Aperture Radar",
          "abstract": "<P>We introduce a deep learning (DL) framework for inverse problems in imaging, and demonstrate the advantages and applicability of this approach in passive synthetic aperture radar (SAR) image reconstruction. We interpret image reconstruction as a machine learning task and utilize deep networks as forward and inverse solvers for imaging. Specifically, we design a recurrent neural network (RNN) architecture as an inverse solver based on the iterations of proximal gradient descent optimization methods. We further adapt the RNN architecture to image reconstruction problems by transforming the network into a recurrent auto-encoder, thereby allowing for unsupervised training. Our DL based inverse solver is particularly suitable for a class of image formation problems in which the forward model is only partially known. The ability to learn forward models and hyper parameters combined with unsupervised training approach establish our recurrent auto-encoder suitable for real world applications. We demonstrate the performance of our method in passive SAR image reconstruction. In this regime a source of opportunity, with unknown location and transmitted waveform, is used to illuminate a scene of interest. We investigate recurrent auto-encoder architecture based on the <TEX>$\\ell _1$</TEX> and <TEX>$\\ell _0$</TEX> constrained least-squares problem. We present a projected stochastic gradient descent based training scheme which incorporates constraints of the unknown model parameters. We demonstrate through extensive numerical simulations that our DL based approach out performs conventional sparse coding methods in terms of computation and reconstructed image quality, specifically, when no information about the transmitter is available.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART84975182&target=NART&cn=NART84975182",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning for Passive Synthetic Aperture Radar Deep Learning for Passive Synthetic Aperture Radar Deep Learning for Passive Synthetic Aperture Radar <P>We introduce a deep learning (DL) framework for inverse problems in imaging, and demonstrate the advantages and applicability of this approach in passive synthetic aperture radar (SAR) image reconstruction. We interpret image reconstruction as a machine learning task and utilize deep networks as forward and inverse solvers for imaging. Specifically, we design a recurrent neural network (RNN) architecture as an inverse solver based on the iterations of proximal gradient descent optimization methods. We further adapt the RNN architecture to image reconstruction problems by transforming the network into a recurrent auto-encoder, thereby allowing for unsupervised training. Our DL based inverse solver is particularly suitable for a class of image formation problems in which the forward model is only partially known. The ability to learn forward models and hyper parameters combined with unsupervised training approach establish our recurrent auto-encoder suitable for real world applications. We demonstrate the performance of our method in passive SAR image reconstruction. In this regime a source of opportunity, with unknown location and transmitted waveform, is used to illuminate a scene of interest. We investigate recurrent auto-encoder architecture based on the <TEX>$\\ell _1$</TEX> and <TEX>$\\ell _0$</TEX> constrained least-squares problem. We present a projected stochastic gradient descent based training scheme which incorporates constraints of the unknown model parameters. We demonstrate through extensive numerical simulations that our DL based approach out performs conventional sparse coding methods in terms of computation and reconstructed image quality, specifically, when no information about the transmitter is available.</P>"
        },
        {
          "rank": 13,
          "score": 0.6864945888519287,
          "doc_id": "NART106334309",
          "title": "Cognitive radar antenna selection via deep learning",
          "abstract": "<P>Direction&#x2010;of&#x2010;arrival (DoA) estimation of targets improves with the number of elements employed by a phased array radar antenna. Since larger arrays have high associated cost, area and computational load, there is a recent interest in thinning the antenna arrays without loss of far&#x2010;field DoA accuracy. In this context, a cognitive radar may deploy a full array and then select an optimal subarray to transmit and receive the signals in response to changes in the target environment. Prior works have used optimisation and greedy search methods to pick the best subarrays cognitively. In this study, deep learning is leveraged to address the antenna selection problem. Specifically, they construct a convolutional neural network (CNN) as a multi&#x2010;class classification framework, where each class designates a different subarray. The proposed network determines a new array every time data is received by the radar, thereby making antenna selection a cognitive operation. Their numerical experiments show that the proposed CNN structure provides 22% better classification performance than a support vector machine and the resulting subarrays yield 72% more accurate DoA estimates than random array selections.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106334309&target=NART&cn=NART106334309",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Cognitive radar antenna selection via deep learning Cognitive radar antenna selection via deep learning Cognitive radar antenna selection via deep learning <P>Direction&#x2010;of&#x2010;arrival (DoA) estimation of targets improves with the number of elements employed by a phased array radar antenna. Since larger arrays have high associated cost, area and computational load, there is a recent interest in thinning the antenna arrays without loss of far&#x2010;field DoA accuracy. In this context, a cognitive radar may deploy a full array and then select an optimal subarray to transmit and receive the signals in response to changes in the target environment. Prior works have used optimisation and greedy search methods to pick the best subarrays cognitively. In this study, deep learning is leveraged to address the antenna selection problem. Specifically, they construct a convolutional neural network (CNN) as a multi&#x2010;class classification framework, where each class designates a different subarray. The proposed network determines a new array every time data is received by the radar, thereby making antenna selection a cognitive operation. Their numerical experiments show that the proposed CNN structure provides 22% better classification performance than a support vector machine and the resulting subarrays yield 72% more accurate DoA estimates than random array selections.</P>"
        },
        {
          "rank": 14,
          "score": 0.6826446056365967,
          "doc_id": "NART110796699",
          "title": "Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network",
          "abstract": "<P>As compared with traditional ISAR imaging methods, the compressive sensing (CS)&#x2010;based imaging methods can obtain high&#x2010;quality images using much less under&#x2010;sampled data. However, the availability or appropriateness of the sparse representation of the target scene and the relatively low computational efficiency of image reconstruction algorithms limit the performance and application of the CS&#x2010;based ISAR imaging methods. In recent years, the deep learning technology has been applied in many fields and achieved outstanding performance in image classification, image reconstruction etc. DL implements the tasks using the deep neural network (DNN), which composes multiple hidden layers and non&#x2010;linear activation layer. In this study, a novel ISAR imaging method that uses a complex&#x2010;value deep neural network (CV&#x2010;DNN) to perform the image formation using under&#x2010;sampled data is proposed. The CV&#x2010;DNN architecture can extract and exploit the sparse feature of the target image extremely well by multilayer non&#x2010;linear processing. The experimental results show that the proposed CV&#x2010;DNN&#x2010;based ISAR imaging method can provide better shape reconstruction of target with less data than state&#x2010;of&#x2010;the&#x2010;art CS reconstruction algorithms and improve the imaging efficiency obviously.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART110796699&target=NART&cn=NART110796699",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network <P>As compared with traditional ISAR imaging methods, the compressive sensing (CS)&#x2010;based imaging methods can obtain high&#x2010;quality images using much less under&#x2010;sampled data. However, the availability or appropriateness of the sparse representation of the target scene and the relatively low computational efficiency of image reconstruction algorithms limit the performance and application of the CS&#x2010;based ISAR imaging methods. In recent years, the deep learning technology has been applied in many fields and achieved outstanding performance in image classification, image reconstruction etc. DL implements the tasks using the deep neural network (DNN), which composes multiple hidden layers and non&#x2010;linear activation layer. In this study, a novel ISAR imaging method that uses a complex&#x2010;value deep neural network (CV&#x2010;DNN) to perform the image formation using under&#x2010;sampled data is proposed. The CV&#x2010;DNN architecture can extract and exploit the sparse feature of the target image extremely well by multilayer non&#x2010;linear processing. The experimental results show that the proposed CV&#x2010;DNN&#x2010;based ISAR imaging method can provide better shape reconstruction of target with less data than state&#x2010;of&#x2010;the&#x2010;art CS reconstruction algorithms and improve the imaging efficiency obviously.</P>"
        },
        {
          "rank": 15,
          "score": 0.682091236114502,
          "doc_id": "NART124851615",
          "title": "Radar Spectrum Image Classification Based on Deep Learning",
          "abstract": "<P>With the continuous development and progress of science and technology, the increasingly complex electromagnetic environment and the research and development of new radar systems have led to the emergence of various radar signals. Traditional methods of radar emitter identification cannot meet the needs of current practical applications. For the purpose of classification and recognition of radar emitter signals, this paper proposes an improved EfficientNetv2-s classification method based on deep learning for more precise classification and recognition of radar radiation source signals. Using 16 different types of radar signal parameters from the signal parameter setting table, the proposed method generates random data sets consisting of spectrum images with varying amplitude. The proposed method replaces two-dimensional convolution in EfficientNetV2 with one-dimensional convolution. Additionally, the channel attention mechanism of the EfficientNetv2-s is optimized and modified to obtain attention weights without dimensional reduction, resulting in superior accuracy. Compared with other deep-learning image-classification methods, the test results of this method have better classification accuracy on the test set: the top1 accuracy reaches 98.12%, which is 0.17~3.12% higher than other methods. Furthermore, the proposed method has lower complexity compared to most methods.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART124851615&target=NART&cn=NART124851615",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Radar Spectrum Image Classification Based on Deep Learning Radar Spectrum Image Classification Based on Deep Learning Radar Spectrum Image Classification Based on Deep Learning <P>With the continuous development and progress of science and technology, the increasingly complex electromagnetic environment and the research and development of new radar systems have led to the emergence of various radar signals. Traditional methods of radar emitter identification cannot meet the needs of current practical applications. For the purpose of classification and recognition of radar emitter signals, this paper proposes an improved EfficientNetv2-s classification method based on deep learning for more precise classification and recognition of radar radiation source signals. Using 16 different types of radar signal parameters from the signal parameter setting table, the proposed method generates random data sets consisting of spectrum images with varying amplitude. The proposed method replaces two-dimensional convolution in EfficientNetV2 with one-dimensional convolution. Additionally, the channel attention mechanism of the EfficientNetv2-s is optimized and modified to obtain attention weights without dimensional reduction, resulting in superior accuracy. Compared with other deep-learning image-classification methods, the test results of this method have better classification accuracy on the test set: the top1 accuracy reaches 98.12%, which is 0.17~3.12% higher than other methods. Furthermore, the proposed method has lower complexity compared to most methods.</P>"
        },
        {
          "rank": 16,
          "score": 0.6774669885635376,
          "doc_id": "NPAP13842123",
          "title": "레이더 영상 기반 딥러닝을 이용한 물체 인식",
          "abstract": "본 연구에서는 컴퓨터 비전 기반의 딥러닝 객체 인식 기술을 이용하여 속초해수욕장에서 수집한 레이더 이미지에서 선박, 섬 및 부유체에 대해 탐지(Detection), 인식(Recognition)하는 연구를 수행하였다. 2021년 8월에 수집한 레이더 영상을 이용하여 본 연구를 수행하였으며, 움직이는 물표와 섬 등을 구분하였다. 일부 환경적인 제약에 따라 에러 발생이 있었지만, 향후 현재까지 수집한 레이더 영상을 추가하여 정확도를 높일 예정이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP13842123&target=NART&cn=NPAP13842123",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "레이더 영상 기반 딥러닝을 이용한 물체 인식 레이더 영상 기반 딥러닝을 이용한 물체 인식 레이더 영상 기반 딥러닝을 이용한 물체 인식 본 연구에서는 컴퓨터 비전 기반의 딥러닝 객체 인식 기술을 이용하여 속초해수욕장에서 수집한 레이더 이미지에서 선박, 섬 및 부유체에 대해 탐지(Detection), 인식(Recognition)하는 연구를 수행하였다. 2021년 8월에 수집한 레이더 영상을 이용하여 본 연구를 수행하였으며, 움직이는 물표와 섬 등을 구분하였다. 일부 환경적인 제약에 따라 에러 발생이 있었지만, 향후 현재까지 수집한 레이더 영상을 추가하여 정확도를 높일 예정이다."
        },
        {
          "rank": 17,
          "score": 0.6575784683227539,
          "doc_id": "NART66897872",
          "title": "Image enhancement in forward imaging radar using modified apodisation technique",
          "abstract": "<P>Forward imaging radar using an UWB signal has been developed by many researchers. The image quality in this radar is not satisfactory because of the limitation of aperture length. Proposed is an image enhancement method using a modified apodisation technique in forward imaging radar. An experiment is carried out to validate the proposed method. The azimuth resolution and peak-to-sidelobe ratio are improved by the proposed method by up to about 11.7% and 10 dB, respectively.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART66897872&target=NART&cn=NART66897872",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Image enhancement in forward imaging radar using modified apodisation technique Image enhancement in forward imaging radar using modified apodisation technique Image enhancement in forward imaging radar using modified apodisation technique <P>Forward imaging radar using an UWB signal has been developed by many researchers. The image quality in this radar is not satisfactory because of the limitation of aperture length. Proposed is an image enhancement method using a modified apodisation technique in forward imaging radar. An experiment is carried out to validate the proposed method. The azimuth resolution and peak-to-sidelobe ratio are improved by the proposed method by up to about 11.7% and 10 dB, respectively.</P>"
        },
        {
          "rank": 18,
          "score": 0.6476176977157593,
          "doc_id": "JAKO202116954704821",
          "title": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측",
          "abstract": "본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116954704821&target=NART&cn=JAKO202116954704821",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다."
        },
        {
          "rank": 19,
          "score": 0.6474111080169678,
          "doc_id": "JAKO202201253148351",
          "title": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰",
          "abstract": "위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202201253148351&target=NART&cn=JAKO202201253148351",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다."
        },
        {
          "rank": 20,
          "score": 0.6467728614807129,
          "doc_id": "JAKO202009863559871",
          "title": "A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis",
          "abstract": "The image analysis is an important and predominant task for classifying the different parts of the image. The analysis of complex image analysis like histopathological define a crucial factor in oncology due to its ability to help pathologists for interpretation of images and therefore various feature extraction techniques have been evolved from time to time for such analysis. Although deep reinforcement learning is a new and emerging technique but very less effort has been made to compare the deep learning and deep reinforcement learning for image analysis. The paper highlights how both techniques differ in feature extraction from complex images and discusses the potential pros and cons. The use of Convolution Neural Network (CNN) in image segmentation, detection and diagnosis of tumour, feature extraction is important but there are several challenges that need to be overcome before Deep Learning can be applied to digital pathology. The one being is the availability of sufficient training examples for medical image datasets, feature extraction from whole area of the image, ground truth localized annotations, adversarial effects of input representations and extremely large size of the digital pathological slides (in gigabytes).Even though formulating Histopathological Image Analysis (HIA) as Multi Instance Learning (MIL) problem is a remarkable step where histopathological image is divided into high resolution patches to make predictions for the patch and then combining them for overall slide predictions but it suffers from loss of contextual and spatial information. In such cases the deep reinforcement learning techniques can be used to learn feature from the limited data without losing contextual and spatial information.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202009863559871&target=NART&cn=JAKO202009863559871",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis The image analysis is an important and predominant task for classifying the different parts of the image. The analysis of complex image analysis like histopathological define a crucial factor in oncology due to its ability to help pathologists for interpretation of images and therefore various feature extraction techniques have been evolved from time to time for such analysis. Although deep reinforcement learning is a new and emerging technique but very less effort has been made to compare the deep learning and deep reinforcement learning for image analysis. The paper highlights how both techniques differ in feature extraction from complex images and discusses the potential pros and cons. The use of Convolution Neural Network (CNN) in image segmentation, detection and diagnosis of tumour, feature extraction is important but there are several challenges that need to be overcome before Deep Learning can be applied to digital pathology. The one being is the availability of sufficient training examples for medical image datasets, feature extraction from whole area of the image, ground truth localized annotations, adversarial effects of input representations and extremely large size of the digital pathological slides (in gigabytes).Even though formulating Histopathological Image Analysis (HIA) as Multi Instance Learning (MIL) problem is a remarkable step where histopathological image is divided into high resolution patches to make predictions for the patch and then combining them for overall slide predictions but it suffers from loss of contextual and spatial information. In such cases the deep reinforcement learning techniques can be used to learn feature from the limited data without losing contextual and spatial information."
        },
        {
          "rank": 21,
          "score": 0.6467728614807129,
          "doc_id": "ART002574280",
          "title": "A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis",
          "abstract": "The image analysis is an important and predominant task for classifying the different parts of the image. The analysis of complex image analysis like histopathological define a crucial factor in oncology due to its ability to help pathologists for interpretation of images and therefore various feature extraction techniques have been evolved from time to time for such analysis. Although deep reinforcement learning is a new and emerging technique but very less effort has been made to compare the deep learning and deep reinforcement learning for image analysis. The paper highlights how both techniques differ in feature extraction from complex images and discusses the potential pros and cons. The use of Convolution Neural Network (CNN) in image segmentation, detection and diagnosis of tumour, feature extraction is important but there are several challenges that need to be overcome before Deep Learning can be applied to digital pathology. The one being is the availability of sufficient training examples for medical image datasets, feature extraction from whole area of the image, ground truth localized annotations, adversarial effects of input representations and extremely large size of the digital pathological slides (in gigabytes).Even though formulating Histopathological Image Analysis (HIA) as Multi Instance Learning (MIL) problem is a remarkable step where histopathological image is divided into high resolution patches to make predictions for the patch and then combining them for overall slide predictions but it suffers from loss of contextual and spatial information. In such cases the deep reinforcement learning techniques can be used to learn feature from the limited data without losing contextual and spatial information.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002574280&target=NART&cn=ART002574280",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis A Comparison of Deep Reinforcement Learning and Deep learning for Complex Image Analysis The image analysis is an important and predominant task for classifying the different parts of the image. The analysis of complex image analysis like histopathological define a crucial factor in oncology due to its ability to help pathologists for interpretation of images and therefore various feature extraction techniques have been evolved from time to time for such analysis. Although deep reinforcement learning is a new and emerging technique but very less effort has been made to compare the deep learning and deep reinforcement learning for image analysis. The paper highlights how both techniques differ in feature extraction from complex images and discusses the potential pros and cons. The use of Convolution Neural Network (CNN) in image segmentation, detection and diagnosis of tumour, feature extraction is important but there are several challenges that need to be overcome before Deep Learning can be applied to digital pathology. The one being is the availability of sufficient training examples for medical image datasets, feature extraction from whole area of the image, ground truth localized annotations, adversarial effects of input representations and extremely large size of the digital pathological slides (in gigabytes).Even though formulating Histopathological Image Analysis (HIA) as Multi Instance Learning (MIL) problem is a remarkable step where histopathological image is divided into high resolution patches to make predictions for the patch and then combining them for overall slide predictions but it suffers from loss of contextual and spatial information. In such cases the deep reinforcement learning techniques can be used to learn feature from the limited data without losing contextual and spatial information."
        },
        {
          "rank": 22,
          "score": 0.6413567066192627,
          "doc_id": "JAKO202226461575702",
          "title": "의료영상 분야를 위한 설명가능한 인공지능 기술 리뷰",
          "abstract": "Artificial intelligence (AI) has been studied in various fields of medical imaging. Currently, top-notch deep learning (DL) techniques have led to high diagnostic accuracy and fast computation. However, they are rarely used in real clinical practices because of a lack of reliability concerning their results. Most DL models can achieve high performance by extracting features from large volumes of data. However, increasing model complexity and nonlinearity turn such models into black boxes that are seldom accessible, interpretable, and transparent. As a result, scientific interest in the field of explainable artificial intelligence (XAI) is gradually emerging. This study aims to review diverse XAI approaches currently exploited in medical imaging. We identify the concepts of the methods, introduce studies applying them to imaging modalities such as computational tomography (CT), magnetic resonance imaging (MRI), and endoscopy, and lastly discuss limitations and challenges faced by XAI for future studies.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202226461575702&target=NART&cn=JAKO202226461575702",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "의료영상 분야를 위한 설명가능한 인공지능 기술 리뷰 의료영상 분야를 위한 설명가능한 인공지능 기술 리뷰 의료영상 분야를 위한 설명가능한 인공지능 기술 리뷰 Artificial intelligence (AI) has been studied in various fields of medical imaging. Currently, top-notch deep learning (DL) techniques have led to high diagnostic accuracy and fast computation. However, they are rarely used in real clinical practices because of a lack of reliability concerning their results. Most DL models can achieve high performance by extracting features from large volumes of data. However, increasing model complexity and nonlinearity turn such models into black boxes that are seldom accessible, interpretable, and transparent. As a result, scientific interest in the field of explainable artificial intelligence (XAI) is gradually emerging. This study aims to review diverse XAI approaches currently exploited in medical imaging. We identify the concepts of the methods, introduce studies applying them to imaging modalities such as computational tomography (CT), magnetic resonance imaging (MRI), and endoscopy, and lastly discuss limitations and challenges faced by XAI for future studies."
        },
        {
          "rank": 23,
          "score": 0.6402601003646851,
          "doc_id": "ART002342492",
          "title": "Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning",
          "abstract": "Heavy rainfall has frequently caused serious flooding and landslides, increasing traffic delays in most parts of the world. Consequently, the people in areas battered by heavy rainfall face many hardships. Thus, the negative effects of torrential rainfall always remind researchers to keep seeking the ways to prevent such damage. Therefore, we designed a system for short-term prediction of localized heavy downpours by using radar images coupled with a machine learning method. Here, we introduce a new approach, named dual k-nearest neighbor (dual-kNN), for shortterm rainfall prediction by upgrading the ordinary classification routines of classical k-nearest neighbors (k-NN). dual-kNN is able to maintain highly robust classification of various K values with an advanced simple dual consideration, where observation of a targeted object can be found not only in the specified region but also in other related regions. We conducted experimentations using 2011, 2013, and 2014 data sets collected from the WITH small-dish aviation radar installed on the rooftop of Information Engineering, University of the Ryukyus. Then, we compared the prediction accuracy of our new approach with classical k-NN. It was experimentally confirmed with test cases and simulations that the performance of dual-kNN is more effective than classical k- NN.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002342492&target=NART&cn=ART002342492",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Heavy rainfall has frequently caused serious flooding and landslides, increasing traffic delays in most parts of the world. Consequently, the people in areas battered by heavy rainfall face many hardships. Thus, the negative effects of torrential rainfall always remind researchers to keep seeking the ways to prevent such damage. Therefore, we designed a system for short-term prediction of localized heavy downpours by using radar images coupled with a machine learning method. Here, we introduce a new approach, named dual k-nearest neighbor (dual-kNN), for shortterm rainfall prediction by upgrading the ordinary classification routines of classical k-nearest neighbors (k-NN). dual-kNN is able to maintain highly robust classification of various K values with an advanced simple dual consideration, where observation of a targeted object can be found not only in the specified region but also in other related regions. We conducted experimentations using 2011, 2013, and 2014 data sets collected from the WITH small-dish aviation radar installed on the rooftop of Information Engineering, University of the Ryukyus. Then, we compared the prediction accuracy of our new approach with classical k-NN. It was experimentally confirmed with test cases and simulations that the performance of dual-kNN is more effective than classical k- NN."
        },
        {
          "rank": 24,
          "score": 0.6387320756912231,
          "doc_id": "ART002483857",
          "title": "Deep Learning in MR Image Processing",
          "abstract": "Recently, deep learning methods have shown great potential in various tasks that involve handling large amounts of digital data. In the field of MR imaging research, deep learning methods are also rapidly being applied in a wide range of areas to complement or replace traditional model-based methods. Deep learning methods have shown remarkable improvements in several MR image processing areas such as image reconstruction, image quality improvement, parameter mapping, image contrast conversion, and image segmentation. With the current rapid development of deep learning technologies, the importance of the role of deep learning in MR imaging research appears to be growing. In this article, we introduce the basic concepts of deep learning and review recent studies on various MR image processing applications.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002483857&target=NART&cn=ART002483857",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning in MR Image Processing Deep Learning in MR Image Processing Deep Learning in MR Image Processing Recently, deep learning methods have shown great potential in various tasks that involve handling large amounts of digital data. In the field of MR imaging research, deep learning methods are also rapidly being applied in a wide range of areas to complement or replace traditional model-based methods. Deep learning methods have shown remarkable improvements in several MR image processing areas such as image reconstruction, image quality improvement, parameter mapping, image contrast conversion, and image segmentation. With the current rapid development of deep learning technologies, the importance of the role of deep learning in MR imaging research appears to be growing. In this article, we introduce the basic concepts of deep learning and review recent studies on various MR image processing applications."
        },
        {
          "rank": 25,
          "score": 0.6363089084625244,
          "doc_id": "DIKO0017187917",
          "title": "레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구",
          "abstract": "자율주행의 안정적인 운행을 보장하기 위해서는 도로 상황에 대한 깊이 있는 이해가 필수적이다. 이에 본 논문에서는 단일 딥러닝(deep learning, DL) 네트워크 구조를 활용해 차량, 사이클리스트, 보행자 등 도로에서 자주 마주하는 객체를 분류하고 동시에 이들의 이동 방향을 추정하는 방법을 제안한다. 먼저, 4차원 이미징 레이더를 이용해 대상의 거리, 속도, 방위각, 고도각과 같은 정보를 획득한다. 이후, 검출 결과를 포인트 클라우드 데이터로 변환하여 3차원 공간 좌표계로 표현한다. 다음으로, 포인트 클라우드 데이터를 XY 평면에 정사영하여 대상의 분류와 이동 방향 추정을 수행한다. XY 평면에서 밀도 기반의 클러스터링(density-based spatial clustering) 기법을 적용해 검출 결과에서 잡음을 제거하고 객체를 클러스터링하여, 이를 이미지 데이터로 변환하는 전처리 과정을 거친다. 그런 후, 이미지 데이터를 이용해 객체 분류와 이동 방향 예측을 동시에 수행할 수 있는 다중 출력 DL 네트워크를 학습시킨다. 제안된 방법의 성능 평가 결과, 객체 분류 정확도는 96.10%로 나타났고, 이동 방향 예측의 평균 제곱근 오차(root mean square error, RMSE)는 차량, 사이클리스트, 보행자에 대해 각각 5.54°, 3.89°, 15.35°로 측정되었으며, 실행시간은 0.1초로 측정되어 효율성을 입증하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017187917&target=NART&cn=DIKO0017187917",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 자율주행의 안정적인 운행을 보장하기 위해서는 도로 상황에 대한 깊이 있는 이해가 필수적이다. 이에 본 논문에서는 단일 딥러닝(deep learning, DL) 네트워크 구조를 활용해 차량, 사이클리스트, 보행자 등 도로에서 자주 마주하는 객체를 분류하고 동시에 이들의 이동 방향을 추정하는 방법을 제안한다. 먼저, 4차원 이미징 레이더를 이용해 대상의 거리, 속도, 방위각, 고도각과 같은 정보를 획득한다. 이후, 검출 결과를 포인트 클라우드 데이터로 변환하여 3차원 공간 좌표계로 표현한다. 다음으로, 포인트 클라우드 데이터를 XY 평면에 정사영하여 대상의 분류와 이동 방향 추정을 수행한다. XY 평면에서 밀도 기반의 클러스터링(density-based spatial clustering) 기법을 적용해 검출 결과에서 잡음을 제거하고 객체를 클러스터링하여, 이를 이미지 데이터로 변환하는 전처리 과정을 거친다. 그런 후, 이미지 데이터를 이용해 객체 분류와 이동 방향 예측을 동시에 수행할 수 있는 다중 출력 DL 네트워크를 학습시킨다. 제안된 방법의 성능 평가 결과, 객체 분류 정확도는 96.10%로 나타났고, 이동 방향 예측의 평균 제곱근 오차(root mean square error, RMSE)는 차량, 사이클리스트, 보행자에 대해 각각 5.54°, 3.89°, 15.35°로 측정되었으며, 실행시간은 0.1초로 측정되어 효율성을 입증하였다."
        },
        {
          "rank": 26,
          "score": 0.633406400680542,
          "doc_id": "JAKO202221359246132",
          "title": "불균일 안개 영상 합성을 이용한 딥러닝 기반 안개 영상 깊이 추정",
          "abstract": "영상의 깊이 추정은 다양한 영상 분석의 기반이 되는 기술이다. 딥러닝 모델을 활용한 분석 방법이 대두되면서, 영상의 깊이 추정 분야 또한 딥러닝을 활용하는 연구가 활발하게 이루어지고 있다. 현재 대부분의 딥러닝 영상 깊이 추정 모델들은 깨끗하고 이상적인 환경에서 학습되고 있다. 하지만 연무, 안개가 낀 열악한 환경에서도 깊이 추정 기술이 잘 동작할 수 있으려면 이러한 환경의 데이터를 포함하여야 한다. 하지만 열악한 환경의 영상을 충분히 확보하는 것이 어려운 실정이며, 불균일한 안개 데이터를 얻는 것은 특히 어려운 문제이다. 이를 해결하기 위해, 본 연구에서는 불균일 안개 영상 합성 방법과 이를 활용한 단안 기반의 깊이 추정 딥러닝 모델의 학습을 제안한다. 안개가 주로 실외에서 발생하는 것을 고려하여, 실외 위주의 데이터 세트를 구축한다. 그리고 실험을 통해 제안된 방법으로 학습된 모델이 합성 데이터와 실제 데이터에서 깊이를 잘 추정하는 것을 보인다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202221359246132&target=NART&cn=JAKO202221359246132",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "불균일 안개 영상 합성을 이용한 딥러닝 기반 안개 영상 깊이 추정 불균일 안개 영상 합성을 이용한 딥러닝 기반 안개 영상 깊이 추정 불균일 안개 영상 합성을 이용한 딥러닝 기반 안개 영상 깊이 추정 영상의 깊이 추정은 다양한 영상 분석의 기반이 되는 기술이다. 딥러닝 모델을 활용한 분석 방법이 대두되면서, 영상의 깊이 추정 분야 또한 딥러닝을 활용하는 연구가 활발하게 이루어지고 있다. 현재 대부분의 딥러닝 영상 깊이 추정 모델들은 깨끗하고 이상적인 환경에서 학습되고 있다. 하지만 연무, 안개가 낀 열악한 환경에서도 깊이 추정 기술이 잘 동작할 수 있으려면 이러한 환경의 데이터를 포함하여야 한다. 하지만 열악한 환경의 영상을 충분히 확보하는 것이 어려운 실정이며, 불균일한 안개 데이터를 얻는 것은 특히 어려운 문제이다. 이를 해결하기 위해, 본 연구에서는 불균일 안개 영상 합성 방법과 이를 활용한 단안 기반의 깊이 추정 딥러닝 모델의 학습을 제안한다. 안개가 주로 실외에서 발생하는 것을 고려하여, 실외 위주의 데이터 세트를 구축한다. 그리고 실험을 통해 제안된 방법으로 학습된 모델이 합성 데이터와 실제 데이터에서 깊이를 잘 추정하는 것을 보인다."
        },
        {
          "rank": 27,
          "score": 0.6296734809875488,
          "doc_id": "NART119629224",
          "title": "65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging",
          "abstract": "<P>High dynamic range (HDR) techniques have received significant attention in generating realistic, high&#x2010;quality images and videos and improving visual quality in new display systems. We have witnessed remarkable advances in HDR reconstruction using deep learning technologies in recent years. This review examines recent developments in HDR reconstruction using a deep learning approach, which takes a single low dynamic range (LDR) image as an input and aims to restore an HDR image featuring higher color gamut and a higher detail retention than the LDR image. We aim to provide a comprehensive survey in this field. Since there are numerous HDR algorithms, it is necessary to evaluate and organize theirperformance, therefore, we evaluate them using two objective evaluation metrics.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART119629224&target=NART&cn=NART119629224",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging 65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging 65&#x2010;3: <i>Invited Paper:</i> Deep Learning&#x2010;Based Image Enhancement for HDR Imaging <P>High dynamic range (HDR) techniques have received significant attention in generating realistic, high&#x2010;quality images and videos and improving visual quality in new display systems. We have witnessed remarkable advances in HDR reconstruction using deep learning technologies in recent years. This review examines recent developments in HDR reconstruction using a deep learning approach, which takes a single low dynamic range (LDR) image as an input and aims to restore an HDR image featuring higher color gamut and a higher detail retention than the LDR image. We aim to provide a comprehensive survey in this field. Since there are numerous HDR algorithms, it is necessary to evaluate and organize theirperformance, therefore, we evaluate them using two objective evaluation metrics.</P>"
        },
        {
          "rank": 28,
          "score": 0.627436637878418,
          "doc_id": "JAKO202007552827199",
          "title": "심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측",
          "abstract": "본 연구에서는 강우예측을 위해 U-Net과 SegNet에 기반한 합성곱 신경망 네트워크 구조에 장기간의 국내 기상레이더 자료를 활용하여 심층학습기반의 강우예측을 수행하였다. 또한, 기존 외삽기반의 강우예측 기법인 이류모델의 결과와 비교 평가하였다. 심층신경망의 학습 및 검정을 위해 2010부터 2016년 동안의 기상청 관악산과 광덕산 레이더의 원자료를 수집, 1 km 공간해상도를 갖는 480 &#215; 480의 픽셀의 회색조 영상으로 변환하여 HDF5 형태의 데이터를 구축하였다. 구축된 데이터로 30분 전부터 현재까지 10분 간격의 연속된 레이더 영상 4개를 이용하여 10분 후의 강수량을 예측하도록 심층신경망 모델을 학습하였으며, 학습된 심층신경망 모델로 60분의 선행예측을 수행하기 위해 예측값을 반복 사용하는 재귀적 방식을 적용하였다. 심층신경망 예측모델의 성능 평가를 위해 2017년에 발생한 24개의 호우사례에 대해 선행 60분까지 강우예측을 수행하였다. 임계강우강도 0.1, 1, 5 mm/hr에서 평균절대오차와 임계성공지수를 산정하여 예측성능을 평가한 결과, 강우강도 임계 값 0.1, 1 mm/hr의 경우 MAE는 60분 선행예측까지, CSI는 선행예측 50분까지 참조 예측모델인 이류모델이 보다 우수한 성능을 보였다. 특히, 5 mm/hr 이하의 약한 강우에 대해서는 심층신경망 예측모델이 이류모델보다 대체적으로 좋은 성능을 보였지만, 5 mm/hr의 임계 값에 대한 평가결과 심층신경망 예측모델은 고강도의 뚜렷한 강수 특징을 예측하는 데 한계가 있었다. 심층신경망 예측모델은 예측시간이 길어질수록 공간 평활화되는 경향이 뚜렷해지며, 이로 인해 강우 예측의 정확도가 저하되었다. 이류모델은 뚜렷한 강수 특성을 보존하기 때문에 강한 강도 (>5 mm/hr)에 대해 심층신경망 예측모델을 능가하지만, 강우 위치가 잘못 이동하는 경향이 있다. 본 연구결과는 이후 심층신경망을 이용한 레이더 강우 예측기술의 개발과 개선에 도움이 될 수 있을 것으로 판단된다. 또한, 본 연구에서 구축한 대용량 기상레이더 자료는 향후 후속연구에 활용될 수 있도록 개방형 저장소를 통해 제공될 예정이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202007552827199&target=NART&cn=JAKO202007552827199",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 본 연구에서는 강우예측을 위해 U-Net과 SegNet에 기반한 합성곱 신경망 네트워크 구조에 장기간의 국내 기상레이더 자료를 활용하여 심층학습기반의 강우예측을 수행하였다. 또한, 기존 외삽기반의 강우예측 기법인 이류모델의 결과와 비교 평가하였다. 심층신경망의 학습 및 검정을 위해 2010부터 2016년 동안의 기상청 관악산과 광덕산 레이더의 원자료를 수집, 1 km 공간해상도를 갖는 480 &#215; 480의 픽셀의 회색조 영상으로 변환하여 HDF5 형태의 데이터를 구축하였다. 구축된 데이터로 30분 전부터 현재까지 10분 간격의 연속된 레이더 영상 4개를 이용하여 10분 후의 강수량을 예측하도록 심층신경망 모델을 학습하였으며, 학습된 심층신경망 모델로 60분의 선행예측을 수행하기 위해 예측값을 반복 사용하는 재귀적 방식을 적용하였다. 심층신경망 예측모델의 성능 평가를 위해 2017년에 발생한 24개의 호우사례에 대해 선행 60분까지 강우예측을 수행하였다. 임계강우강도 0.1, 1, 5 mm/hr에서 평균절대오차와 임계성공지수를 산정하여 예측성능을 평가한 결과, 강우강도 임계 값 0.1, 1 mm/hr의 경우 MAE는 60분 선행예측까지, CSI는 선행예측 50분까지 참조 예측모델인 이류모델이 보다 우수한 성능을 보였다. 특히, 5 mm/hr 이하의 약한 강우에 대해서는 심층신경망 예측모델이 이류모델보다 대체적으로 좋은 성능을 보였지만, 5 mm/hr의 임계 값에 대한 평가결과 심층신경망 예측모델은 고강도의 뚜렷한 강수 특징을 예측하는 데 한계가 있었다. 심층신경망 예측모델은 예측시간이 길어질수록 공간 평활화되는 경향이 뚜렷해지며, 이로 인해 강우 예측의 정확도가 저하되었다. 이류모델은 뚜렷한 강수 특성을 보존하기 때문에 강한 강도 (>5 mm/hr)에 대해 심층신경망 예측모델을 능가하지만, 강우 위치가 잘못 이동하는 경향이 있다. 본 연구결과는 이후 심층신경망을 이용한 레이더 강우 예측기술의 개발과 개선에 도움이 될 수 있을 것으로 판단된다. 또한, 본 연구에서 구축한 대용량 기상레이더 자료는 향후 후속연구에 활용될 수 있도록 개방형 저장소를 통해 제공될 예정이다."
        },
        {
          "rank": 29,
          "score": 0.6268537044525146,
          "doc_id": "JAKO202407064802797",
          "title": "딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가",
          "abstract": "급격한 기후 변화로 인한 어획량 감소와 양식 기술의 발전으로 양식 생산물 수요가 전세계적으로 계속해서 증가하고 있다. 그러나 이에 따른 무분별한 시설물 확장이 연안 생태계와 어족 자원 가격 책정에 악영향을 미치기 때문에, 주기적인 연안 환경 모니터링을 통한 양식시설물 관리가 필수적이다. 본 연구에서는 Sentinel-2 광학 영상과 다양한 딥러닝 기반 탐지 기법을 활용하여 경상남도의 패류 양식시설물 탐지 정확도를 분석하였다. DeepLabv3+, ResUNet++ 그리고 Attention U-Net 모델을 적용하였으며, 실험 결과 Attention U-Net 모델이 F1 score 0.8708, Intersection over Union 0.7708로 가장 우수한 탐지 성능을 보였다. 연구에서 제시한 탐지 방법론은 조류 및 부유 물질에 영향을 받는 양식시설물을 주기적으로 관측할 수 있고, 다양한 양식 품종에 적용할 수 있어 넓은 지역으로의 확장 가능성이 높다. 따라서 본 연구 방법을 통해 도출된 양식 시설물 정보는 향후 해양 공간 활용에 관한 정책 결정에 유용하게 활용할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202407064802797&target=NART&cn=JAKO202407064802797",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 급격한 기후 변화로 인한 어획량 감소와 양식 기술의 발전으로 양식 생산물 수요가 전세계적으로 계속해서 증가하고 있다. 그러나 이에 따른 무분별한 시설물 확장이 연안 생태계와 어족 자원 가격 책정에 악영향을 미치기 때문에, 주기적인 연안 환경 모니터링을 통한 양식시설물 관리가 필수적이다. 본 연구에서는 Sentinel-2 광학 영상과 다양한 딥러닝 기반 탐지 기법을 활용하여 경상남도의 패류 양식시설물 탐지 정확도를 분석하였다. DeepLabv3+, ResUNet++ 그리고 Attention U-Net 모델을 적용하였으며, 실험 결과 Attention U-Net 모델이 F1 score 0.8708, Intersection over Union 0.7708로 가장 우수한 탐지 성능을 보였다. 연구에서 제시한 탐지 방법론은 조류 및 부유 물질에 영향을 받는 양식시설물을 주기적으로 관측할 수 있고, 다양한 양식 품종에 적용할 수 있어 넓은 지역으로의 확장 가능성이 높다. 따라서 본 연구 방법을 통해 도출된 양식 시설물 정보는 향후 해양 공간 활용에 관한 정책 결정에 유용하게 활용할 수 있을 것으로 기대된다."
        },
        {
          "rank": 30,
          "score": 0.62552809715271,
          "doc_id": "JAKO202320150299733",
          "title": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가",
          "abstract": "본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202320150299733&target=NART&cn=JAKO202320150299733",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다."
        },
        {
          "rank": 31,
          "score": 0.6240168213844299,
          "doc_id": "JAKO202300957609703",
          "title": "딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합",
          "abstract": "위성 시계열 데이터가 증가함에 따라 원격탐사 자료의 활용도가 높아지고 있다. 시계열 자료를 통한 분석에 있어 영상 간의 상대적인 위치 정확도는 결과에 큰 영향을 미치기 때문에 이를 보정하기 위한 영상 정합 과정은 필수적으로 선행되어야 한다. 최근에는 기존 알고리즘의 성능을 상회하는 딥러닝 기반 영상 정합 연구의 사례가 증가하고 있다. 딥러닝 기반 정합 모델을 학습하기 위해서는 수 많은 영상 쌍이 필요하다. 또한, 기존 딥러닝 모델의 데이터 간의 상관도 map을 제작하고, 이에 추가적인 연산을 적용하여 정합점을 추출는데 이는 비효율적이다. 이러한 문제를 해결하기 위해 본 연구에서는 영상 정합 모델 학습을 위한 데이터 증강 기법을 구축하여 데이터셋을 제작하였고, 이를 오프셋(offset) 양 자체를 예측하는 정합 모델인 OffsetNet에 적용하여 KOMSAT-2, -3, -3A 영상 정합을 수행하였다. 모델 학습 결과, OffsetNet은 평가 데이터에 대해 높은 정확도로 오프셋 양을 예측하였고, 이를 통해 주영상과 부영상을 효과적으로 정합하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202300957609703&target=NART&cn=JAKO202300957609703",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 위성 시계열 데이터가 증가함에 따라 원격탐사 자료의 활용도가 높아지고 있다. 시계열 자료를 통한 분석에 있어 영상 간의 상대적인 위치 정확도는 결과에 큰 영향을 미치기 때문에 이를 보정하기 위한 영상 정합 과정은 필수적으로 선행되어야 한다. 최근에는 기존 알고리즘의 성능을 상회하는 딥러닝 기반 영상 정합 연구의 사례가 증가하고 있다. 딥러닝 기반 정합 모델을 학습하기 위해서는 수 많은 영상 쌍이 필요하다. 또한, 기존 딥러닝 모델의 데이터 간의 상관도 map을 제작하고, 이에 추가적인 연산을 적용하여 정합점을 추출는데 이는 비효율적이다. 이러한 문제를 해결하기 위해 본 연구에서는 영상 정합 모델 학습을 위한 데이터 증강 기법을 구축하여 데이터셋을 제작하였고, 이를 오프셋(offset) 양 자체를 예측하는 정합 모델인 OffsetNet에 적용하여 KOMSAT-2, -3, -3A 영상 정합을 수행하였다. 모델 학습 결과, OffsetNet은 평가 데이터에 대해 높은 정확도로 오프셋 양을 예측하였고, 이를 통해 주영상과 부영상을 효과적으로 정합하였다."
        },
        {
          "rank": 32,
          "score": 0.6229976415634155,
          "doc_id": "ART003167534",
          "title": "Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior",
          "abstract": "Thick pinhole imaging system is widely used for diagnosing intense pulsed radiation sources. However, owing to the trade-off among spatial resolution, field of view (FOV) and signal-to-noise ratio (SNR), the imaging system normally falls short in achieving high-precision spatial diagnosis. In this paper, we propose an unsupervised deep learning method for single image super-resolution (SISR) of the thick pinhole imaging system. The point spread function (PSF) of the imaging system is obtained by analytical calculation and Monte Carlo simulation methods, and the mathematical model of the imaging system is established using a linear equation. To solve the ill-posed inverse problem, we adopt randomly initialized deep convolutional neural networks (DCNNs) as an image prior without pre-training, which is named deep image prior (DIP). The results demonstrate that, by utilizing the SISR technique to increase the number of pixels in reconstructed images, the proposed DIP algorithm can mitigate the spatial resolution degradation caused by an insufficient spatial sampling frequency of the camera. Compared with various classical algorithms, the proposed DIP algorithm exhibits superior capabilities in recovering highfrequency signals and suppressing ringing artifacts. Furthermore, the convergence and robustness of the proposed DIP algorithm under different random seeds and SNR conditions are also verified.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART003167534&target=NART&cn=ART003167534",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Thick pinhole imaging system is widely used for diagnosing intense pulsed radiation sources. However, owing to the trade-off among spatial resolution, field of view (FOV) and signal-to-noise ratio (SNR), the imaging system normally falls short in achieving high-precision spatial diagnosis. In this paper, we propose an unsupervised deep learning method for single image super-resolution (SISR) of the thick pinhole imaging system. The point spread function (PSF) of the imaging system is obtained by analytical calculation and Monte Carlo simulation methods, and the mathematical model of the imaging system is established using a linear equation. To solve the ill-posed inverse problem, we adopt randomly initialized deep convolutional neural networks (DCNNs) as an image prior without pre-training, which is named deep image prior (DIP). The results demonstrate that, by utilizing the SISR technique to increase the number of pixels in reconstructed images, the proposed DIP algorithm can mitigate the spatial resolution degradation caused by an insufficient spatial sampling frequency of the camera. Compared with various classical algorithms, the proposed DIP algorithm exhibits superior capabilities in recovering highfrequency signals and suppressing ringing artifacts. Furthermore, the convergence and robustness of the proposed DIP algorithm under different random seeds and SNR conditions are also verified."
        },
        {
          "rank": 33,
          "score": 0.6225110292434692,
          "doc_id": "NART111939444",
          "title": "Review of deep learning for photoacoustic imaging",
          "abstract": "<P>Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging.</P><P>The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART111939444&target=NART&cn=NART111939444",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Review of deep learning for photoacoustic imaging Review of deep learning for photoacoustic imaging Review of deep learning for photoacoustic imaging <P>Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging.</P><P>The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.</P>"
        },
        {
          "rank": 34,
          "score": 0.6207361221313477,
          "doc_id": "NART98464294",
          "title": "Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging",
          "abstract": "<P><B>Abstract</B></P>  <P>Artificial intelligence (AI) in medical imaging is a potentially disruptive technology. An understanding of the principles and application of radiomics, artificial neural networks, machine learning, and deep learning is an essential foundation to weave design solutions that accommodate ethical and regulatory requirements, and to craft AI-based algorithms that enhance outcomes, quality, and efficiency. Moreover, a more holistic perspective of applications, opportunities, and challenges from a programmatic perspective contributes to ethical and sustainable implementation of AI solutions.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART98464294&target=NART&cn=NART98464294",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging Machine Learning and Deep Learning in Medical Imaging: Intelligent Imaging <P><B>Abstract</B></P>  <P>Artificial intelligence (AI) in medical imaging is a potentially disruptive technology. An understanding of the principles and application of radiomics, artificial neural networks, machine learning, and deep learning is an essential foundation to weave design solutions that accommodate ethical and regulatory requirements, and to craft AI-based algorithms that enhance outcomes, quality, and efficiency. Moreover, a more holistic perspective of applications, opportunities, and challenges from a programmatic perspective contributes to ethical and sustainable implementation of AI solutions.</P>"
        },
        {
          "rank": 35,
          "score": 0.6206772327423096,
          "doc_id": "DIKO0015063257",
          "title": "Visual object tracking using deep reinforcement learning",
          "abstract": "Visual object tracking task plays an important role in computer vision research area, which is widely applied on public surveillance, robot navigation and driverless car and so on.&amp;#xD; In this dissertation, two deep reinforcement learning (DRL) based approaches are presented for visual tracking tasks: single object tracking (SOT) and multiple object tracking (MOT). SOT task is essentially to connect two neighboring targets which are co-located in two adjacent video frames and then make all these pairs into one complete trajectory. MOT task is to find the correct relationship of each target in between two adjacent frames, whereby combining object detection and target association becomes necessary. A good MOT algorithm should be able to produce complete trajectory of each target accurately at every frame of video sequence.&amp;#xD; This dissertation proposes an effective SOT approach by means of generating a sequence of actions to transfer previous bounding box towards updating it to current target location. The action sequence is produced by two intelligent agents which are trained via the dueling deep Q-learning (Dueling DQN) algorithm which is composed of movement agent and scaling agent. Movement agent generates horizontal or vertical movement actions while scaling agent performs the actions which can change size of the bounding box. Furthermore, the proposed method enlarges field-of-view with a Siamese network structure which makes judicial adjustment on fast moving targets. Moreover, in order to tackle the low training efficiency and unstable problem of traditional Dueling DQN structure, the action tasks are distributed into movement actions and scaling actions. The proposed distributed action achieves dimensionality reduction which speeds up and stabilizes the training process. The proposed method is tested on two popular standard datasets and compared with state-of-art trackers. The experiment results show that the proposed approach achieves outstanding results in accuracy, speed and robustness.&amp;#xD; For MOT task, rather than introducing yet another MOT tracker, this dissertation proposes to focus on increasing the tracking accuracy with DRL techniques. Due to the unreliable object detection results and complex tracking scenes, recent MOT trackers suffer from low tracking accuracy and poor success rate which can be represented in three types of errors: oversized, partial and false bounding box. The proposed method focuses mainly on oversized and partial errors. In order to correct these errors and improve the tracking accuracy, an intelligent agent is used to generate a sequence of action to transition the incorrect bounding box to its intended right location. The transition model is accomplished by training it with deep Q-learning (DQN) algorithm. After comparing with several state-of-the-art correctors for MOT task, the results indicate that the proposed method achieves better performance in tracking accuracy on existing MOT trackers than other correctors.&amp;#xD; Both of the proposed methods have been proved for addressing and solving the SOT task and the imprecise bounding box problem of MOT task with DRL algorithms. For SOT task, the proposed tracker achieves 0.901 precision and 0.676 success rate on OTB50 benchmark, 0.903 precision and 0.673 success rate on OTB100 benchmark, which makes it completive among many different state-of-the-art trackers. In the case of MOT task, the proposed method is shown to improve tracking accuracy for state-of-the-art MOT trackers from 2% to 7.3%, while having no negative influence on target ID. This helps MOT trackers avoid being influenced by bad object detection results and complex background.&amp;#xD;",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015063257&target=NART&cn=DIKO0015063257",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Visual object tracking using deep reinforcement learning Visual object tracking using deep reinforcement learning Visual object tracking using deep reinforcement learning Visual object tracking task plays an important role in computer vision research area, which is widely applied on public surveillance, robot navigation and driverless car and so on.&amp;#xD; In this dissertation, two deep reinforcement learning (DRL) based approaches are presented for visual tracking tasks: single object tracking (SOT) and multiple object tracking (MOT). SOT task is essentially to connect two neighboring targets which are co-located in two adjacent video frames and then make all these pairs into one complete trajectory. MOT task is to find the correct relationship of each target in between two adjacent frames, whereby combining object detection and target association becomes necessary. A good MOT algorithm should be able to produce complete trajectory of each target accurately at every frame of video sequence.&amp;#xD; This dissertation proposes an effective SOT approach by means of generating a sequence of actions to transfer previous bounding box towards updating it to current target location. The action sequence is produced by two intelligent agents which are trained via the dueling deep Q-learning (Dueling DQN) algorithm which is composed of movement agent and scaling agent. Movement agent generates horizontal or vertical movement actions while scaling agent performs the actions which can change size of the bounding box. Furthermore, the proposed method enlarges field-of-view with a Siamese network structure which makes judicial adjustment on fast moving targets. Moreover, in order to tackle the low training efficiency and unstable problem of traditional Dueling DQN structure, the action tasks are distributed into movement actions and scaling actions. The proposed distributed action achieves dimensionality reduction which speeds up and stabilizes the training process. The proposed method is tested on two popular standard datasets and compared with state-of-art trackers. The experiment results show that the proposed approach achieves outstanding results in accuracy, speed and robustness.&amp;#xD; For MOT task, rather than introducing yet another MOT tracker, this dissertation proposes to focus on increasing the tracking accuracy with DRL techniques. Due to the unreliable object detection results and complex tracking scenes, recent MOT trackers suffer from low tracking accuracy and poor success rate which can be represented in three types of errors: oversized, partial and false bounding box. The proposed method focuses mainly on oversized and partial errors. In order to correct these errors and improve the tracking accuracy, an intelligent agent is used to generate a sequence of action to transition the incorrect bounding box to its intended right location. The transition model is accomplished by training it with deep Q-learning (DQN) algorithm. After comparing with several state-of-the-art correctors for MOT task, the results indicate that the proposed method achieves better performance in tracking accuracy on existing MOT trackers than other correctors.&amp;#xD; Both of the proposed methods have been proved for addressing and solving the SOT task and the imprecise bounding box problem of MOT task with DRL algorithms. For SOT task, the proposed tracker achieves 0.901 precision and 0.676 success rate on OTB50 benchmark, 0.903 precision and 0.673 success rate on OTB100 benchmark, which makes it completive among many different state-of-the-art trackers. In the case of MOT task, the proposed method is shown to improve tracking accuracy for state-of-the-art MOT trackers from 2% to 7.3%, while having no negative influence on target ID. This helps MOT trackers avoid being influenced by bad object detection results and complex background.&amp;#xD;"
        },
        {
          "rank": 36,
          "score": 0.6197823882102966,
          "doc_id": "NART121556950",
          "title": "Integrating deep learning and traditional image enhancement techniques for underwater image enhancement",
          "abstract": "<P><B>Abstract</B><P>Underwater images usually suffer from colour distortion, blur, and low contrast, which hinder the subsequent processing of underwater information. To address these problems, this paper proposes a novel approach for single underwater images enhancement by integrating data&#x2010;driven deep learning and hand&#x2010;crafted image enhancement techniques. First, a statistical analysis is made on the average deviation of each channel of input underwater images to that of its corresponding ground truths, and it is found that both the red channel and the green channel of an underwater image contribute to its colour distortion. Concretely, the red channel of an underwater image is usually seriously attenuated, and the green channel is usually over strengthened. Motivated by such an observation, an attention mechanism guided residual module for underwater image colour correction is proposed, where the colour of the red channel of the underwater image and that of the green channel is compensated in a different way, respectively. Coupled with an attention mechanism, the residual module can adaptively extract and integrate the most discriminative features for colour correction. For scene contrast enhancement and scene deblurring, the traditional image enhancement techniques such as CLAHE (contrast limited adaptive histogram equalization) and Gamma correction are coupled with a multi&#x2010;scale convolutional neural network (MSCNN), where CLAHE and Gamma correction are used as complement to deal with the complex and changeable underwater imaging environment. Experiments on synthetic and real underwater images demonstrate that the proposed method performs favourably against the state&#x2010;of&#x2010;the&#x2010;art underwater image enhancement methods.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART121556950&target=NART&cn=NART121556950",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Integrating deep learning and traditional image enhancement techniques for underwater image enhancement Integrating deep learning and traditional image enhancement techniques for underwater image enhancement Integrating deep learning and traditional image enhancement techniques for underwater image enhancement <P><B>Abstract</B><P>Underwater images usually suffer from colour distortion, blur, and low contrast, which hinder the subsequent processing of underwater information. To address these problems, this paper proposes a novel approach for single underwater images enhancement by integrating data&#x2010;driven deep learning and hand&#x2010;crafted image enhancement techniques. First, a statistical analysis is made on the average deviation of each channel of input underwater images to that of its corresponding ground truths, and it is found that both the red channel and the green channel of an underwater image contribute to its colour distortion. Concretely, the red channel of an underwater image is usually seriously attenuated, and the green channel is usually over strengthened. Motivated by such an observation, an attention mechanism guided residual module for underwater image colour correction is proposed, where the colour of the red channel of the underwater image and that of the green channel is compensated in a different way, respectively. Coupled with an attention mechanism, the residual module can adaptively extract and integrate the most discriminative features for colour correction. For scene contrast enhancement and scene deblurring, the traditional image enhancement techniques such as CLAHE (contrast limited adaptive histogram equalization) and Gamma correction are coupled with a multi&#x2010;scale convolutional neural network (MSCNN), where CLAHE and Gamma correction are used as complement to deal with the complex and changeable underwater imaging environment. Experiments on synthetic and real underwater images demonstrate that the proposed method performs favourably against the state&#x2010;of&#x2010;the&#x2010;art underwater image enhancement methods.</P></P>"
        },
        {
          "rank": 37,
          "score": 0.6189330220222473,
          "doc_id": "NART108808939",
          "title": "Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출",
          "abstract": "This paper presents a novel remote infrared pedestrian detection method for night use by means of local projection-CNN. Conventional sliding window methods (HOG/ACF) or region proposal-based deep learning approaches (faster R-CNN, SSD, YOLO) either fail to detect small objects or generate many false positives. Multi-modal region proposal schemes (multi-scale contrast filters with local projection+ACF) are used to improve remote pedestrian detection. AlexNet-based CNN feature extraction and SVM classification can reduce false positives further. This paper's experimental evaluations indicate that the proposed method can improve remote IR pedestrian detection by 16%.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART108808939&target=NART&cn=NART108808939",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 This paper presents a novel remote infrared pedestrian detection method for night use by means of local projection-CNN. Conventional sliding window methods (HOG/ACF) or region proposal-based deep learning approaches (faster R-CNN, SSD, YOLO) either fail to detect small objects or generate many false positives. Multi-modal region proposal schemes (multi-scale contrast filters with local projection+ACF) are used to improve remote pedestrian detection. AlexNet-based CNN feature extraction and SVM classification can reduce false positives further. This paper's experimental evaluations indicate that the proposed method can improve remote IR pedestrian detection by 16%."
        },
        {
          "rank": 38,
          "score": 0.6174650192260742,
          "doc_id": "DIKO0017114224",
          "title": "딥러닝을 활용한 초음파 영상 개선",
          "abstract": "의료용 초음파 이미지(Clinical Ultrasonic Image) 기법은 인체 내부의 대한 영상을 비침습적, 안전적, 실시간적 있는 도구로, 의료 분야에서 사용되는 대표적인 진단 의료 영상 중 하나이다. 초고속 초음파(Ultra-fast Ultrasound)는 다수의 초음파 송수신을 통하여 상대적으로 고품질의 초음파 이미지를 얻을 수 있다. 그러나, 초음파 빔의 다양성, 복원 이미지의 해상도, 관심 영역(Region of Interest)의 크기 등은 실시간성과 절충 관계(Trade-off)에 있기에 초당 프레임 수(FPS)를 방어하기에 하드웨어적으로 어려움이 있다. 본 연구에서는 딥러닝(Deep Learning) 모델을 활용하여 단일 평면파(Single Plane-wave)의 저품질의 초음파 이미지를 고품질 다중 평면파(Multi-angle Plane-wave)의 고품질 초음파 이미지로 강화하는 것을 목표로 한다. U-Net 구조로 이루어진 딥러닝 모델은 다양한 크기의 합성곱 필터를 이용하여 복잡한 이미지의 세부 정보의 특징을 효과적으로 추출할 수 있다. 제안된 딥러닝 모델은 피크 대 잡음 비율(PSNR), 신호 대 잡음 비율(SNR), 스페클 신호 대 잡음 비율(SSNR) 등의 성능 지표를 통해 효과적인 잡음 감소 및 신호 보존을 보였으며, 상관계수(Correlation)를 통하여 강화된 이미지와 실제 이미지 간의 높은 유사성 및 정확성을 보였다. 향후 연구로는 본 작업에 영향을 줄 수 있는 세부 요인들을 조사하고, 모델 구조를 세밀하게 조정 및 최적화하여 강화되는 이미지의 품질을 더욱 향상시키고, 보다 다양한 부위에 대한 실험을 통해 일반화 성능을 확장할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017114224&target=NART&cn=DIKO0017114224",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 활용한 초음파 영상 개선 딥러닝을 활용한 초음파 영상 개선 딥러닝을 활용한 초음파 영상 개선 의료용 초음파 이미지(Clinical Ultrasonic Image) 기법은 인체 내부의 대한 영상을 비침습적, 안전적, 실시간적 있는 도구로, 의료 분야에서 사용되는 대표적인 진단 의료 영상 중 하나이다. 초고속 초음파(Ultra-fast Ultrasound)는 다수의 초음파 송수신을 통하여 상대적으로 고품질의 초음파 이미지를 얻을 수 있다. 그러나, 초음파 빔의 다양성, 복원 이미지의 해상도, 관심 영역(Region of Interest)의 크기 등은 실시간성과 절충 관계(Trade-off)에 있기에 초당 프레임 수(FPS)를 방어하기에 하드웨어적으로 어려움이 있다. 본 연구에서는 딥러닝(Deep Learning) 모델을 활용하여 단일 평면파(Single Plane-wave)의 저품질의 초음파 이미지를 고품질 다중 평면파(Multi-angle Plane-wave)의 고품질 초음파 이미지로 강화하는 것을 목표로 한다. U-Net 구조로 이루어진 딥러닝 모델은 다양한 크기의 합성곱 필터를 이용하여 복잡한 이미지의 세부 정보의 특징을 효과적으로 추출할 수 있다. 제안된 딥러닝 모델은 피크 대 잡음 비율(PSNR), 신호 대 잡음 비율(SNR), 스페클 신호 대 잡음 비율(SSNR) 등의 성능 지표를 통해 효과적인 잡음 감소 및 신호 보존을 보였으며, 상관계수(Correlation)를 통하여 강화된 이미지와 실제 이미지 간의 높은 유사성 및 정확성을 보였다. 향후 연구로는 본 작업에 영향을 줄 수 있는 세부 요인들을 조사하고, 모델 구조를 세밀하게 조정 및 최적화하여 강화되는 이미지의 품질을 더욱 향상시키고, 보다 다양한 부위에 대한 실험을 통해 일반화 성능을 확장할 수 있다."
        },
        {
          "rank": 39,
          "score": 0.6173251271247864,
          "doc_id": "DIKO0013710110",
          "title": "딥 러닝을 이용한 DC 모터 제어",
          "abstract": "딥 러닝(deep learning)은 최근에 많이 알려지게 된 심층 인공신경망 알고리즘이다. 일반적인 인공신경망보다 은닉층의 개수와 뉴런의 개수를 확장시키고, 학습이 효율적으로 될 수 있게 알고리즘을 개선한 것이 가장 큰 특징이다. 이러한 특징을 활용하여 기존의 인공신경망으로 풀지 못했던 크고 복잡한 문제들을 해결할 수 있게 되었다. 음성인식, 손 글씨 인식, 얼굴 인식 등 복잡한 패턴인식과 분류에 관련된 다양한 분야에 대한 적용 연구가 활발히 진행되고 있다. 하지만 이러한 장점에도 불구하고, 아직까지 딥 러닝이 제어문제를 해결하기 위해 적용된 사례는 찾아보기 어렵다. 본 논문에서는 간단한 사례를 통해 딥 러닝의 제어문제에 대한 적용 가능성을 확인해 본다. 딥 러닝 알고리즘 중에서 가장 잘 알려진, 깊은 믿음 네트워크(deep belief network) 알고리즘을 사용하여 산업현장에서 가장 많이 사용되고 있는 PID 제어기를 모방하는 딥 러닝 제어기를 설계한다. DC 모터를 제어하는 시스템에서 PID 제어기에 들어오는 입력과 PID 제어기에서 나오는 출력값을 학습 데이터로 사용하여 딥 러닝으로 학습하는 방법을 사용한다. 시뮬레이션을 통해 제안한 딥 러닝 제어기와 PID 제어기를 비교하여 딥 러닝 알고리즘의 성능을 검증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0013710110&target=NART&cn=DIKO0013710110",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝을 이용한 DC 모터 제어 딥 러닝을 이용한 DC 모터 제어 딥 러닝을 이용한 DC 모터 제어 딥 러닝(deep learning)은 최근에 많이 알려지게 된 심층 인공신경망 알고리즘이다. 일반적인 인공신경망보다 은닉층의 개수와 뉴런의 개수를 확장시키고, 학습이 효율적으로 될 수 있게 알고리즘을 개선한 것이 가장 큰 특징이다. 이러한 특징을 활용하여 기존의 인공신경망으로 풀지 못했던 크고 복잡한 문제들을 해결할 수 있게 되었다. 음성인식, 손 글씨 인식, 얼굴 인식 등 복잡한 패턴인식과 분류에 관련된 다양한 분야에 대한 적용 연구가 활발히 진행되고 있다. 하지만 이러한 장점에도 불구하고, 아직까지 딥 러닝이 제어문제를 해결하기 위해 적용된 사례는 찾아보기 어렵다. 본 논문에서는 간단한 사례를 통해 딥 러닝의 제어문제에 대한 적용 가능성을 확인해 본다. 딥 러닝 알고리즘 중에서 가장 잘 알려진, 깊은 믿음 네트워크(deep belief network) 알고리즘을 사용하여 산업현장에서 가장 많이 사용되고 있는 PID 제어기를 모방하는 딥 러닝 제어기를 설계한다. DC 모터를 제어하는 시스템에서 PID 제어기에 들어오는 입력과 PID 제어기에서 나오는 출력값을 학습 데이터로 사용하여 딥 러닝으로 학습하는 방법을 사용한다. 시뮬레이션을 통해 제안한 딥 러닝 제어기와 PID 제어기를 비교하여 딥 러닝 알고리즘의 성능을 검증한다."
        },
        {
          "rank": 40,
          "score": 0.6148977279663086,
          "doc_id": "NART123616052",
          "title": "Deep Learning for Image Enhancement and Correction in Magnetic Resonance Imaging&#x2014;State-of-the-Art and Challenges",
          "abstract": "<P>Magnetic resonance imaging (MRI) provides excellent soft-tissue contrast for clinical diagnoses and research which underpin many recent breakthroughs in medicine and biology. The post-processing of reconstructed MR images is often automated for incorporation into MRI scanners by the manufacturers and increasingly plays a critical role in the final image quality for clinical reporting and interpretation. For image enhancement and correction, the post-processing steps include noise reduction, image artefact correction, and image resolution improvements. With the recent success of deep learning in many research fields, there is great potential to apply deep learning for MR image enhancement, and recent publications have demonstrated promising results. Motivated by the rapidly growing literature in this area, in this review paper, we provide a comprehensive overview of deep learning-based methods for post-processing MR images to enhance image quality and correct image artefacts. We aim to provide researchers in MRI or other research fields, including computer vision and image processing, a literature survey of deep learning approaches for MR image enhancement. We discuss the current limitations of the application of artificial intelligence in MRI and highlight possible directions for future developments. In the era of deep learning, we highlight the importance of a critical appraisal of the explanatory information provided and the generalizability of deep learning algorithms in medical imaging. </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART123616052&target=NART&cn=NART123616052",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning for Image Enhancement and Correction in Magnetic Resonance Imaging&#x2014;State-of-the-Art and Challenges Deep Learning for Image Enhancement and Correction in Magnetic Resonance Imaging&#x2014;State-of-the-Art and Challenges Deep Learning for Image Enhancement and Correction in Magnetic Resonance Imaging&#x2014;State-of-the-Art and Challenges <P>Magnetic resonance imaging (MRI) provides excellent soft-tissue contrast for clinical diagnoses and research which underpin many recent breakthroughs in medicine and biology. The post-processing of reconstructed MR images is often automated for incorporation into MRI scanners by the manufacturers and increasingly plays a critical role in the final image quality for clinical reporting and interpretation. For image enhancement and correction, the post-processing steps include noise reduction, image artefact correction, and image resolution improvements. With the recent success of deep learning in many research fields, there is great potential to apply deep learning for MR image enhancement, and recent publications have demonstrated promising results. Motivated by the rapidly growing literature in this area, in this review paper, we provide a comprehensive overview of deep learning-based methods for post-processing MR images to enhance image quality and correct image artefacts. We aim to provide researchers in MRI or other research fields, including computer vision and image processing, a literature survey of deep learning approaches for MR image enhancement. We discuss the current limitations of the application of artificial intelligence in MRI and highlight possible directions for future developments. In the era of deep learning, we highlight the importance of a critical appraisal of the explanatory information provided and the generalizability of deep learning algorithms in medical imaging. </P>"
        },
        {
          "rank": 41,
          "score": 0.614361047744751,
          "doc_id": "ART002968156",
          "title": "Artificial Intelligence and Deep Learning in Musculoskeletal Magnetic Resonance Imaging",
          "abstract": "The application of artificial intelligence (AI) and deep learning (DL) in radiology is rapidly evolving. AI in healthcare has benefits for image recognition, classification, and radiological workflows from a clinical perspective. Additionally, clinical triage AI can be applied to triage systems. This review aims to introduce the concept of DL and discuss its applications in the interpretation of magnetic resonance (MR) images and the DL-based reconstruction of accelerated MR images, with an emphasis on musculoskeletal radiology. The most recent developments and future directions are also discussed briefly.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002968156&target=NART&cn=ART002968156",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Artificial Intelligence and Deep Learning in Musculoskeletal Magnetic Resonance Imaging Artificial Intelligence and Deep Learning in Musculoskeletal Magnetic Resonance Imaging Artificial Intelligence and Deep Learning in Musculoskeletal Magnetic Resonance Imaging The application of artificial intelligence (AI) and deep learning (DL) in radiology is rapidly evolving. AI in healthcare has benefits for image recognition, classification, and radiological workflows from a clinical perspective. Additionally, clinical triage AI can be applied to triage systems. This review aims to introduce the concept of DL and discuss its applications in the interpretation of magnetic resonance (MR) images and the DL-based reconstruction of accelerated MR images, with an emphasis on musculoskeletal radiology. The most recent developments and future directions are also discussed briefly."
        },
        {
          "rank": 42,
          "score": 0.6138876676559448,
          "doc_id": "NART136112293",
          "title": "Enhancement of Image Quality in Low-Field Knee MR Imaging Using Deep Learning",
          "abstract": "<P>Purpose:&nbsp;The purpose of this study is to investigate the potential of deep learning (DL) techniques to enhance the image quality of low-field knee MR images, with the ultimate goal of approximating the standards of&nbsp;high-field knee MR imaging.</P><P>Methods: We analyzed knee MR images collected from 45 patients with knee disorders and six normal subjects using a 3T MR scanner&nbsp;and those collected from 25 patients with knee disorders using a 0.4T MR scanner. Two DL models were developed: a fat-suppression contrast-generation model and a super-resolution model. These DL models were trained using 3T knee MR imaging data and applied to 0.4T knee MR imaging data. Visual assessments of anatomical structures and image noise and abnormality detection with diagnostic confidence levels on the original 0.4T MR images and those after&nbsp;DL enhancement were conducted by two board-certified radiologists. Statistical analyses were performed using McNemar&rsquo;s test and the Wilcoxon signed-rank test.</P><P>Results:&nbsp;DL-enhanced MR images significantly improved the depiction of anatomical structures and reduced image noise compared to the original MR images. The number of abnormal findings detected and the diagnostic confidence levels were higher in the DL-enhanced MR images, indicating the potential for more accurate diagnoses.</P><P>Conclusion: DL techniques effectively enhance the image quality of low-field knee MR images by leveraging 3T MR imaging data. This enhancement significantly improves image quality and diagnostic confidence levels, making low-field MR images much more reliable for detecting abnormalities. This advancement offers a useful alternative for clinical settings, especially in resource-limited environments, without compromising diagnostic accuracy.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART136112293&target=NART&cn=NART136112293",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Enhancement of Image Quality in Low-Field Knee MR Imaging Using Deep Learning Enhancement of Image Quality in Low-Field Knee MR Imaging Using Deep Learning Enhancement of Image Quality in Low-Field Knee MR Imaging Using Deep Learning <P>Purpose:&nbsp;The purpose of this study is to investigate the potential of deep learning (DL) techniques to enhance the image quality of low-field knee MR images, with the ultimate goal of approximating the standards of&nbsp;high-field knee MR imaging.</P><P>Methods: We analyzed knee MR images collected from 45 patients with knee disorders and six normal subjects using a 3T MR scanner&nbsp;and those collected from 25 patients with knee disorders using a 0.4T MR scanner. Two DL models were developed: a fat-suppression contrast-generation model and a super-resolution model. These DL models were trained using 3T knee MR imaging data and applied to 0.4T knee MR imaging data. Visual assessments of anatomical structures and image noise and abnormality detection with diagnostic confidence levels on the original 0.4T MR images and those after&nbsp;DL enhancement were conducted by two board-certified radiologists. Statistical analyses were performed using McNemar&rsquo;s test and the Wilcoxon signed-rank test.</P><P>Results:&nbsp;DL-enhanced MR images significantly improved the depiction of anatomical structures and reduced image noise compared to the original MR images. The number of abnormal findings detected and the diagnostic confidence levels were higher in the DL-enhanced MR images, indicating the potential for more accurate diagnoses.</P><P>Conclusion: DL techniques effectively enhance the image quality of low-field knee MR images by leveraging 3T MR imaging data. This enhancement significantly improves image quality and diagnostic confidence levels, making low-field MR images much more reliable for detecting abnormalities. This advancement offers a useful alternative for clinical settings, especially in resource-limited environments, without compromising diagnostic accuracy.</P>"
        },
        {
          "rank": 43,
          "score": 0.6121186017990112,
          "doc_id": "DIKO0016929635",
          "title": "딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식",
          "abstract": "전자기전에서 레이더 파형의 변조 방식은 다른 레이더 파형 제원과 함께 적대적인 레이더를 분석하고 대응책을 마련하는 데 활용될 수 있어 사전정보 없이 레이더 파형의 변조 방식을 자동 인식하는 기술은 매우 중요한 기술이다. 특히, 미래의 전자기전에서 복합 변조 방식이 적용된 레이더 파형에 선제적으로 대응하기 위해서는 다양한 단일 및 복합 변조 방식을 인식할 수 있는 기술을 개발하는 것이 필수적이다.&amp;#xD; 본 논문에서는 31종의 단일 및 복합 변조 방식의 레이더 파형을 자동으로 인식하는 시퀀스 데이터 기반의 레이더 파형 변조 인식 기법을 제안한다. 제안하는 기법은 수신 레이더 파형을 바탕으로 시퀀스 데이터를 구성하고, 이를 딥러닝 네트워크에 입력으로 사용하여 변조 방식을 자동으로 인식한다. 이때 시퀀스 데이터로 레이더 파형 원본, 레이더 파형의 이산 푸리에 변환(Discrete Fourier transform, DFT), 레이더 파형의 자기 상관 함수 각각의 실수부 및 허수부를 이용해 구성한 데이터를 고려하며, 딥러닝 네트워크로는 CNN(Convolutional neural network), CLDNN(Convolutional long short-term deep neural network), ResNet(Residual network)-18, 34, 50을 고려한다. 컴퓨터 모의실험을 통해 시퀀스 데이터와 딥러닝 네트워크에 따른 제안하는 시퀀스 데이터 기반 레이더 파형 자동 변조 인식 기법의 인식 성능을 비교하여 레이더 파형의 DFT로 구성한 시퀀스 데이터를 딥러닝 네트워크의 입력으로 사용할 때 가장 효과적인 변조 인식이 가능함을 보인다. 또한 레이더 파형의 DFT로 구성한 시퀀스 데이터를 이용할 때, 딥러닝 네트워크로 ResNet을 사용하는 경우가 CNN, CLDNN을 사용하는 경우보다 우수한 성능을 보임을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016929635&target=NART&cn=DIKO0016929635",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 전자기전에서 레이더 파형의 변조 방식은 다른 레이더 파형 제원과 함께 적대적인 레이더를 분석하고 대응책을 마련하는 데 활용될 수 있어 사전정보 없이 레이더 파형의 변조 방식을 자동 인식하는 기술은 매우 중요한 기술이다. 특히, 미래의 전자기전에서 복합 변조 방식이 적용된 레이더 파형에 선제적으로 대응하기 위해서는 다양한 단일 및 복합 변조 방식을 인식할 수 있는 기술을 개발하는 것이 필수적이다.&amp;#xD; 본 논문에서는 31종의 단일 및 복합 변조 방식의 레이더 파형을 자동으로 인식하는 시퀀스 데이터 기반의 레이더 파형 변조 인식 기법을 제안한다. 제안하는 기법은 수신 레이더 파형을 바탕으로 시퀀스 데이터를 구성하고, 이를 딥러닝 네트워크에 입력으로 사용하여 변조 방식을 자동으로 인식한다. 이때 시퀀스 데이터로 레이더 파형 원본, 레이더 파형의 이산 푸리에 변환(Discrete Fourier transform, DFT), 레이더 파형의 자기 상관 함수 각각의 실수부 및 허수부를 이용해 구성한 데이터를 고려하며, 딥러닝 네트워크로는 CNN(Convolutional neural network), CLDNN(Convolutional long short-term deep neural network), ResNet(Residual network)-18, 34, 50을 고려한다. 컴퓨터 모의실험을 통해 시퀀스 데이터와 딥러닝 네트워크에 따른 제안하는 시퀀스 데이터 기반 레이더 파형 자동 변조 인식 기법의 인식 성능을 비교하여 레이더 파형의 DFT로 구성한 시퀀스 데이터를 딥러닝 네트워크의 입력으로 사용할 때 가장 효과적인 변조 인식이 가능함을 보인다. 또한 레이더 파형의 DFT로 구성한 시퀀스 데이터를 이용할 때, 딥러닝 네트워크로 ResNet을 사용하는 경우가 CNN, CLDNN을 사용하는 경우보다 우수한 성능을 보임을 확인한다."
        },
        {
          "rank": 44,
          "score": 0.6112473607063293,
          "doc_id": "ART002337392",
          "title": "Deep Learning in Nuclear Medicine and Molecular Imaging: Current Perspectives and Future Directions",
          "abstract": "Recent advances in deep learning have impacted various scientific and industrial fields. Due to the rapid application of deep learning in biomedical data, molecular imaging has also started to adopt this technique. In this regard, it is expected that deep learning will potentially affect the roles of molecular imaging experts as well as clinical decision making. This review firstly offers a basic overview of deep learning particularly for image data analysis to give knowledge to nuclear medicine physicians and researchers. Because of the unique characteristics and distinctive aims of various types of molecular imaging, deep learning applications can be different from other fields. In this context, the review deals with current perspectives of deep learning in molecular imaging particularly in terms of development of biomarkers. Finally, future challenges of deep learning application for molecular imaging and future roles of experts in molecular imaging will be discussed.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002337392&target=NART&cn=ART002337392",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning in Nuclear Medicine and Molecular Imaging: Current Perspectives and Future Directions Deep Learning in Nuclear Medicine and Molecular Imaging: Current Perspectives and Future Directions Deep Learning in Nuclear Medicine and Molecular Imaging: Current Perspectives and Future Directions Recent advances in deep learning have impacted various scientific and industrial fields. Due to the rapid application of deep learning in biomedical data, molecular imaging has also started to adopt this technique. In this regard, it is expected that deep learning will potentially affect the roles of molecular imaging experts as well as clinical decision making. This review firstly offers a basic overview of deep learning particularly for image data analysis to give knowledge to nuclear medicine physicians and researchers. Because of the unique characteristics and distinctive aims of various types of molecular imaging, deep learning applications can be different from other fields. In this context, the review deals with current perspectives of deep learning in molecular imaging particularly in terms of development of biomarkers. Finally, future challenges of deep learning application for molecular imaging and future roles of experts in molecular imaging will be discussed."
        },
        {
          "rank": 45,
          "score": 0.6098142862319946,
          "doc_id": "NART84800827",
          "title": "Smart in-car camera system using mobile cloud computing framework for deep learning",
          "abstract": "<P><B>Abstract</B></P>  <P>Deep learning is becoming a popular technology in various applications, such as image recognition, gaming, information retrieval, for intelligent data processing. However, huge amount of data and complex computations prevent deep learning from being practical on mobile devices. In this paper, we designed a smart in-car camera system that utilizes mobile cloud computing framework for deep learning. The smart in-car camera can detect objects in recorded videos during driving, and can decide which part of videos needs to be stored in cloud platforms to save local storage space. The system puts the training process and model repository in cloud platforms, and the recognition process and data gathering in mobile devices. The mobile side is implemented in NVIDIA Jetson TK1, and the communication is carried out via Git protocol to ensure the success of data transmission in unstable network environments. Experimental results show that detection rate can achieve up to four frame-per-second with Faster R-CNN, and the system can work well even when the network connection is unstable. We also compared the performance of system with and without GPU, and found that GPU still plays a critical role in the recognition side for deep learning.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART84800827&target=NART&cn=NART84800827",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Smart in-car camera system using mobile cloud computing framework for deep learning Smart in-car camera system using mobile cloud computing framework for deep learning Smart in-car camera system using mobile cloud computing framework for deep learning <P><B>Abstract</B></P>  <P>Deep learning is becoming a popular technology in various applications, such as image recognition, gaming, information retrieval, for intelligent data processing. However, huge amount of data and complex computations prevent deep learning from being practical on mobile devices. In this paper, we designed a smart in-car camera system that utilizes mobile cloud computing framework for deep learning. The smart in-car camera can detect objects in recorded videos during driving, and can decide which part of videos needs to be stored in cloud platforms to save local storage space. The system puts the training process and model repository in cloud platforms, and the recognition process and data gathering in mobile devices. The mobile side is implemented in NVIDIA Jetson TK1, and the communication is carried out via Git protocol to ensure the success of data transmission in unstable network environments. Experimental results show that detection rate can achieve up to four frame-per-second with Faster R-CNN, and the system can work well even when the network connection is unstable. We also compared the performance of system with and without GPU, and found that GPU still plays a critical role in the recognition side for deep learning.</P>"
        },
        {
          "rank": 46,
          "score": 0.6050858497619629,
          "doc_id": "JAKO202009759219313",
          "title": "콘크리트 균열 탐지를 위한 딥 러닝 기반 CNN 모델 비교",
          "abstract": "The purpose of this study is to compare the models of Deep Learning-based Convolution Neural Network(CNN) for concrete crack detection. The comparison models are AlexNet, GoogLeNet, VGG16, VGG19, ResNet-18, ResNet-50, ResNet-101, and SqueezeNet which won ImageNet Large Scale Visual Recognition Challenge(ILSVRC). To train, validate and test these models, we constructed 3000 training data and 12000 validation data with 256&#x00D7;256 pixel resolution consisting of cracked and non-cracked images, and constructed 5 test data with 4160&#x00D7;3120 pixel resolution consisting of concrete images with crack. In order to increase the efficiency of the training, transfer learning was performed by taking the weight from the pre-trained network supported by MATLAB. From the trained network, the validation data is classified into crack image and non-crack image, yielding True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN), and 6 performance indicators, False Negative Rate (FNR), False Positive Rate (FPR), Error Rate, Recall, Precision, Accuracy were calculated. The test image was scanned twice with a sliding window of 256&#x00D7;256 pixel resolution to classify the cracks, resulting in a crack map. From the comparison of the performance indicators and the crack map, it was concluded that VGG16 and VGG19 were the most suitable for detecting concrete cracks.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202009759219313&target=NART&cn=JAKO202009759219313",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "콘크리트 균열 탐지를 위한 딥 러닝 기반 CNN 모델 비교 콘크리트 균열 탐지를 위한 딥 러닝 기반 CNN 모델 비교 콘크리트 균열 탐지를 위한 딥 러닝 기반 CNN 모델 비교 The purpose of this study is to compare the models of Deep Learning-based Convolution Neural Network(CNN) for concrete crack detection. The comparison models are AlexNet, GoogLeNet, VGG16, VGG19, ResNet-18, ResNet-50, ResNet-101, and SqueezeNet which won ImageNet Large Scale Visual Recognition Challenge(ILSVRC). To train, validate and test these models, we constructed 3000 training data and 12000 validation data with 256&#x00D7;256 pixel resolution consisting of cracked and non-cracked images, and constructed 5 test data with 4160&#x00D7;3120 pixel resolution consisting of concrete images with crack. In order to increase the efficiency of the training, transfer learning was performed by taking the weight from the pre-trained network supported by MATLAB. From the trained network, the validation data is classified into crack image and non-crack image, yielding True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN), and 6 performance indicators, False Negative Rate (FNR), False Positive Rate (FPR), Error Rate, Recall, Precision, Accuracy were calculated. The test image was scanned twice with a sliding window of 256&#x00D7;256 pixel resolution to classify the cracks, resulting in a crack map. From the comparison of the performance indicators and the crack map, it was concluded that VGG16 and VGG19 were the most suitable for detecting concrete cracks."
        },
        {
          "rank": 47,
          "score": 0.6049118041992188,
          "doc_id": "NART80772700",
          "title": "Noisy training for deep neural networks in speech recognition",
          "abstract": "<P><B>Abstract</B><P>Deep neural networks (DNNs) have gained remarkable success in speech recognition, partially attributed to the flexibility of DNN models in learning complex patterns of speech signals. This flexibility, however, may lead to serious over-fitting and hence miserable performance degradation in adverse acoustic conditions such as those with high ambient noises. We propose a noisy training approach to tackle this problem: by injecting moderate noises into the training data intentionally and randomly, more generalizable DNN models can be learned. This &lsquo;noise injection&rsquo; technique, although known to the neural computation community already, has not been studied with DNNs which involve a highly complex objective function. The experiments presented in this paper confirm that the noisy training approach works well for the DNN model and can provide substantial performance improvement for DNN-based speech recognition.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART80772700&target=NART&cn=NART80772700",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Noisy training for deep neural networks in speech recognition Noisy training for deep neural networks in speech recognition Noisy training for deep neural networks in speech recognition <P><B>Abstract</B><P>Deep neural networks (DNNs) have gained remarkable success in speech recognition, partially attributed to the flexibility of DNN models in learning complex patterns of speech signals. This flexibility, however, may lead to serious over-fitting and hence miserable performance degradation in adverse acoustic conditions such as those with high ambient noises. We propose a noisy training approach to tackle this problem: by injecting moderate noises into the training data intentionally and randomly, more generalizable DNN models can be learned. This &lsquo;noise injection&rsquo; technique, although known to the neural computation community already, has not been studied with DNNs which involve a highly complex objective function. The experiments presented in this paper confirm that the noisy training approach works well for the DNN model and can provide substantial performance improvement for DNN-based speech recognition.</P></P>"
        },
        {
          "rank": 48,
          "score": 0.6045964956283569,
          "doc_id": "JAKO201809863000185",
          "title": "영상기반의 화재 검출에 효과적인 CNN 심층학습의 커널 특성에 대한 연구",
          "abstract": "본 논문에서는 보안 감시 카메라 영상을 활용하여 화재 검출을 위한 효과적인 심층학습 방안을 제안한다. AlexNet 모델을 기준으로 효과적인 화재 검출을 위한 커널 크기와 커널 이동 간격의 변화에 따른 분류 성능을 비교 분석한다. 학습을 위한 데이터셋은 정상과 화재 2가지 클래스로 분류한다, 정상 영상에는 구름과 안개 낀 영상을 포함하고, 화재 영상에는 연기와 화염을 각각 포함한다. AlexNet 모델의 첫 번째 계층의 커널 크기와 이동 간격에 따른 분류 성능 분석 결과 커널의 크기는 크고, 이동 간격은 작을수록 화재 분류 성능이 우수한 것을 확인할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201809863000185&target=NART&cn=JAKO201809863000185",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "영상기반의 화재 검출에 효과적인 CNN 심층학습의 커널 특성에 대한 연구 영상기반의 화재 검출에 효과적인 CNN 심층학습의 커널 특성에 대한 연구 영상기반의 화재 검출에 효과적인 CNN 심층학습의 커널 특성에 대한 연구 본 논문에서는 보안 감시 카메라 영상을 활용하여 화재 검출을 위한 효과적인 심층학습 방안을 제안한다. AlexNet 모델을 기준으로 효과적인 화재 검출을 위한 커널 크기와 커널 이동 간격의 변화에 따른 분류 성능을 비교 분석한다. 학습을 위한 데이터셋은 정상과 화재 2가지 클래스로 분류한다, 정상 영상에는 구름과 안개 낀 영상을 포함하고, 화재 영상에는 연기와 화염을 각각 포함한다. AlexNet 모델의 첫 번째 계층의 커널 크기와 이동 간격에 따른 분류 성능 분석 결과 커널의 크기는 크고, 이동 간격은 작을수록 화재 분류 성능이 우수한 것을 확인할 수 있다."
        },
        {
          "rank": 49,
          "score": 0.6044920086860657,
          "doc_id": "ATN0037502209",
          "title": "CNN 기반 딥러닝을 이용한 베어링 고장 진단의 정확도 및 계산 복잡도 분석",
          "abstract": "Bearing faults account for a large portion of machine faults in real industrial sites. To minimize the damages caused by bearing faults, it is necessary to provide an accurate real-time bearing faults diagnosis system. In this work, we propose a new bearing fault diagnosis method using deep learning with various CNN models, where acoustic emission signals from working bearings are used to analyze the status of bearings. In the proposed method, the acoustic emission signals acquired from the bearings are converted into spectrogram images in the time-frequency domain, and then the status of bearings can be diagnosed through deep learning with various CNN models. Traditional signal processing based bearing fault diagnosis methods show the accuracy of about 80%, while our proposed deep learning based method provides very high fault diagnosis accuracy of about 100%. We also apply the proposed deep learning based fault diagnosis method to Raspberry Pi to test the availability for the working conditions in real industrial sites. We measured the multiply-accumulate complexity of the fault diagnosis process with various CNN models to find the CNN model suitable for real working conditions, and then the selected ShuffleNet CNN model was implanted and operated in Raspberry Pi to check the possibility of real-time bearing fault diagnosis.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037502209&target=NART&cn=ATN0037502209",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "CNN 기반 딥러닝을 이용한 베어링 고장 진단의 정확도 및 계산 복잡도 분석 CNN 기반 딥러닝을 이용한 베어링 고장 진단의 정확도 및 계산 복잡도 분석 CNN 기반 딥러닝을 이용한 베어링 고장 진단의 정확도 및 계산 복잡도 분석 Bearing faults account for a large portion of machine faults in real industrial sites. To minimize the damages caused by bearing faults, it is necessary to provide an accurate real-time bearing faults diagnosis system. In this work, we propose a new bearing fault diagnosis method using deep learning with various CNN models, where acoustic emission signals from working bearings are used to analyze the status of bearings. In the proposed method, the acoustic emission signals acquired from the bearings are converted into spectrogram images in the time-frequency domain, and then the status of bearings can be diagnosed through deep learning with various CNN models. Traditional signal processing based bearing fault diagnosis methods show the accuracy of about 80%, while our proposed deep learning based method provides very high fault diagnosis accuracy of about 100%. We also apply the proposed deep learning based fault diagnosis method to Raspberry Pi to test the availability for the working conditions in real industrial sites. We measured the multiply-accumulate complexity of the fault diagnosis process with various CNN models to find the CNN model suitable for real working conditions, and then the selected ShuffleNet CNN model was implanted and operated in Raspberry Pi to check the possibility of real-time bearing fault diagnosis."
        },
        {
          "rank": 50,
          "score": 0.603848397731781,
          "doc_id": "NART115326214",
          "title": "Deep learning-based single image face depth data enhancement",
          "abstract": "<P><B>Abstract</B></P>  <P>Face recognition can benefit from the utilization of depth data captured using low-cost cameras, in particular for presentation attack detection purposes. Depth video output from these capture devices can however contain defects such as holes or general depth inaccuracies. This work proposes a deep learning face depth enhancement method in this context of facial biometrics, which adds a security aspect to the topic. U-Net-like architectures are utilized, and the networks are compared against hand-crafted enhancer types, as well as a similar depth enhancer network from related work trained for an adjacent application scenario. All tested enhancer types exclusively use depth data as input, which differs from methods that enhance depth based on additional input data such as visible light color images. Synthetic face depth ground truth images and degraded forms thereof are created with help of PRNet, to train multiple deep learning enhancer models with different network sizes and training configurations. Evaluations are carried out on the synthetic data, on Kinect v1 images from the KinectFaceDB, and on in-house RealSense D435 images. These evaluations include an assessment of the falsification for occluded face depth input, which is relevant to biometric security. The proposed deep learning enhancers yield noticeably better results than the tested preexisting enhancers, without overly falsifying depth data when non-face input is provided, and are shown to reduce the error of a simple landmark-based PAD method.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Pure depth image enhancement using deep learning is effective for facial biometrics. </LI> <LI>  Synthesis of realistic low detail face depth enhancer training data is viable. </LI> <LI>  Comparisons with more general enhancers favor the face-specific model. </LI> <LI>  Depth is not overly falsified for non-face input during enhancement. </LI> <LI>  Face depth enhancement can be used to aid real-time presentation attack detection. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART115326214&target=NART&cn=NART115326214",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning-based single image face depth data enhancement Deep learning-based single image face depth data enhancement Deep learning-based single image face depth data enhancement <P><B>Abstract</B></P>  <P>Face recognition can benefit from the utilization of depth data captured using low-cost cameras, in particular for presentation attack detection purposes. Depth video output from these capture devices can however contain defects such as holes or general depth inaccuracies. This work proposes a deep learning face depth enhancement method in this context of facial biometrics, which adds a security aspect to the topic. U-Net-like architectures are utilized, and the networks are compared against hand-crafted enhancer types, as well as a similar depth enhancer network from related work trained for an adjacent application scenario. All tested enhancer types exclusively use depth data as input, which differs from methods that enhance depth based on additional input data such as visible light color images. Synthetic face depth ground truth images and degraded forms thereof are created with help of PRNet, to train multiple deep learning enhancer models with different network sizes and training configurations. Evaluations are carried out on the synthetic data, on Kinect v1 images from the KinectFaceDB, and on in-house RealSense D435 images. These evaluations include an assessment of the falsification for occluded face depth input, which is relevant to biometric security. The proposed deep learning enhancers yield noticeably better results than the tested preexisting enhancers, without overly falsifying depth data when non-face input is provided, and are shown to reduce the error of a simple landmark-based PAD method.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Pure depth image enhancement using deep learning is effective for facial biometrics. </LI> <LI>  Synthesis of realistic low detail face depth enhancer training data is viable. </LI> <LI>  Comparisons with more general enhancers favor the face-specific model. </LI> <LI>  Depth is not overly falsified for non-face input during enhancement. </LI> <LI>  Face depth enhancement can be used to aid real-time presentation attack detection. </LI> </UL> </P>"
        }
      ]
    },
    {
      "query": "What is the proposed network architecture discussed for deep learning in radar imaging?",
      "query_meta": {
        "type": "single_hop",
        "index": 2
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8236844539642334,
          "doc_id": "NPAP12546494",
          "title": "Deep learning for radar",
          "abstract": "<P>Motivated by the recent advances in deep learning, we lay out a vision of how deep learning techniques can be used in radar. Specifically, our discussion focuses on the use of deep learning to advance the state-of-the-art in radar imaging. While deep learning can be directly applied to automatic target recognition (ATR), the relevance of these techniques in other radar problems is not obvious. We argue that deep learning can play a central role in advancing the state-of-the-art in a wide range of radar imaging problems, discuss the challenges associated with applying these methods, and the potential advancements that are expected. We lay out an approach to design a network architecture based on the specific structure of the synthetic aperture radar (SAR) imaging problem that augments learning with traditional SAR modelling. This framework allows for capture of the non-linearity of the SAR forward model. Furthermore, we demonstrate how this process can be used to learn and compensate for trajectory based phase error for the autofocus problem.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP12546494&target=NART&cn=NPAP12546494",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning for radar Deep learning for radar Deep learning for radar <P>Motivated by the recent advances in deep learning, we lay out a vision of how deep learning techniques can be used in radar. Specifically, our discussion focuses on the use of deep learning to advance the state-of-the-art in radar imaging. While deep learning can be directly applied to automatic target recognition (ATR), the relevance of these techniques in other radar problems is not obvious. We argue that deep learning can play a central role in advancing the state-of-the-art in a wide range of radar imaging problems, discuss the challenges associated with applying these methods, and the potential advancements that are expected. We lay out an approach to design a network architecture based on the specific structure of the synthetic aperture radar (SAR) imaging problem that augments learning with traditional SAR modelling. This framework allows for capture of the non-linearity of the SAR forward model. Furthermore, we demonstrate how this process can be used to learn and compensate for trajectory based phase error for the autofocus problem.</P>"
        },
        {
          "rank": 2,
          "score": 0.7721927165985107,
          "doc_id": "NART121030945",
          "title": "MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning",
          "abstract": "<P>Transmitting orthogonal waveforms are the basis for giving full play to the advantages of MIMO radar imaging technology, but the commonly used waveforms with the same frequency cannot meet the orthogonality requirement, resulting in serious coupling noise in traditional imaging methods and affecting the imaging effect. In order to effectively suppress the mutual coupling interference caused by non-orthogonal waveforms, a new non-orthogonal waveform MIMO radar imaging method based on deep learning is proposed in this paper: with the powerful nonlinear fitting ability of deep learning, the mapping relationship between the non-orthogonal waveform MIMO radar echo and ideal target image is automatically learned by constructing a deep imaging network and training on a large number of simulated training data. The learned imaging network can effectively suppress the coupling interference between non-ideal orthogonal waveforms and improve the imaging quality of MIMO radar. Finally, the effectiveness of the proposed method is verified by experiments with point scattering model data and electromagnetic scattering calculation data.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART121030945&target=NART&cn=NART121030945",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning MIMO Radar Imaging Method with Non-Orthogonal Waveforms Based on Deep Learning <P>Transmitting orthogonal waveforms are the basis for giving full play to the advantages of MIMO radar imaging technology, but the commonly used waveforms with the same frequency cannot meet the orthogonality requirement, resulting in serious coupling noise in traditional imaging methods and affecting the imaging effect. In order to effectively suppress the mutual coupling interference caused by non-orthogonal waveforms, a new non-orthogonal waveform MIMO radar imaging method based on deep learning is proposed in this paper: with the powerful nonlinear fitting ability of deep learning, the mapping relationship between the non-orthogonal waveform MIMO radar echo and ideal target image is automatically learned by constructing a deep imaging network and training on a large number of simulated training data. The learned imaging network can effectively suppress the coupling interference between non-ideal orthogonal waveforms and improve the imaging quality of MIMO radar. Finally, the effectiveness of the proposed method is verified by experiments with point scattering model data and electromagnetic scattering calculation data.</P>"
        },
        {
          "rank": 3,
          "score": 0.7692354917526245,
          "doc_id": "NART106334316",
          "title": "Deep learning for waveform estimation and imaging in passive radar",
          "abstract": "<P>The authors consider a bistatic configuration with a stationary transmitter transmitting unknown waveforms of opportunity and a single moving receiver and present a deep learning (DL) framework for passive synthetic aperture radar (SAR) imaging. They approach DL from an optimisation based perspective and formulate image reconstruction as a machine learning task. By unfolding the iterations of a proximal gradient descent algorithm, they construct a deep recurrent neural network (RNN) that is parameterised by the transmitted waveforms. They cascade the RNN structure with a decoder stage to form a recurrent auto&#x2010;encoder architecture. They then use backpropagation to learn transmitted waveforms by training the network in an unsupervised manner using SAR measurements. The highly non&#x2010;convex problem of backpropagation is guided to a feasible solution over the parameter space by initialising the network with the known components of the SAR forward model. Moreover, prior information regarding the waveform structure is incorporated during initialisation and backpropagation. They demonstrate the effectiveness of the DL&#x2010;based approach through numerical simulations that show focused, high contrast imagery using a single receiver antenna at realistic signal&#x2010;to&#x2010;noise&#x2010;ratio levels.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106334316&target=NART&cn=NART106334316",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning for waveform estimation and imaging in passive radar Deep learning for waveform estimation and imaging in passive radar Deep learning for waveform estimation and imaging in passive radar <P>The authors consider a bistatic configuration with a stationary transmitter transmitting unknown waveforms of opportunity and a single moving receiver and present a deep learning (DL) framework for passive synthetic aperture radar (SAR) imaging. They approach DL from an optimisation based perspective and formulate image reconstruction as a machine learning task. By unfolding the iterations of a proximal gradient descent algorithm, they construct a deep recurrent neural network (RNN) that is parameterised by the transmitted waveforms. They cascade the RNN structure with a decoder stage to form a recurrent auto&#x2010;encoder architecture. They then use backpropagation to learn transmitted waveforms by training the network in an unsupervised manner using SAR measurements. The highly non&#x2010;convex problem of backpropagation is guided to a feasible solution over the parameter space by initialising the network with the known components of the SAR forward model. Moreover, prior information regarding the waveform structure is incorporated during initialisation and backpropagation. They demonstrate the effectiveness of the DL&#x2010;based approach through numerical simulations that show focused, high contrast imagery using a single receiver antenna at realistic signal&#x2010;to&#x2010;noise&#x2010;ratio levels.</P>"
        },
        {
          "rank": 4,
          "score": 0.7609075307846069,
          "doc_id": "NART110796699",
          "title": "Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network",
          "abstract": "<P>As compared with traditional ISAR imaging methods, the compressive sensing (CS)&#x2010;based imaging methods can obtain high&#x2010;quality images using much less under&#x2010;sampled data. However, the availability or appropriateness of the sparse representation of the target scene and the relatively low computational efficiency of image reconstruction algorithms limit the performance and application of the CS&#x2010;based ISAR imaging methods. In recent years, the deep learning technology has been applied in many fields and achieved outstanding performance in image classification, image reconstruction etc. DL implements the tasks using the deep neural network (DNN), which composes multiple hidden layers and non&#x2010;linear activation layer. In this study, a novel ISAR imaging method that uses a complex&#x2010;value deep neural network (CV&#x2010;DNN) to perform the image formation using under&#x2010;sampled data is proposed. The CV&#x2010;DNN architecture can extract and exploit the sparse feature of the target image extremely well by multilayer non&#x2010;linear processing. The experimental results show that the proposed CV&#x2010;DNN&#x2010;based ISAR imaging method can provide better shape reconstruction of target with less data than state&#x2010;of&#x2010;the&#x2010;art CS reconstruction algorithms and improve the imaging efficiency obviously.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART110796699&target=NART&cn=NART110796699",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network Inverse synthetic aperture radar imaging using complex&#x2010;value deep neural network <P>As compared with traditional ISAR imaging methods, the compressive sensing (CS)&#x2010;based imaging methods can obtain high&#x2010;quality images using much less under&#x2010;sampled data. However, the availability or appropriateness of the sparse representation of the target scene and the relatively low computational efficiency of image reconstruction algorithms limit the performance and application of the CS&#x2010;based ISAR imaging methods. In recent years, the deep learning technology has been applied in many fields and achieved outstanding performance in image classification, image reconstruction etc. DL implements the tasks using the deep neural network (DNN), which composes multiple hidden layers and non&#x2010;linear activation layer. In this study, a novel ISAR imaging method that uses a complex&#x2010;value deep neural network (CV&#x2010;DNN) to perform the image formation using under&#x2010;sampled data is proposed. The CV&#x2010;DNN architecture can extract and exploit the sparse feature of the target image extremely well by multilayer non&#x2010;linear processing. The experimental results show that the proposed CV&#x2010;DNN&#x2010;based ISAR imaging method can provide better shape reconstruction of target with less data than state&#x2010;of&#x2010;the&#x2010;art CS reconstruction algorithms and improve the imaging efficiency obviously.</P>"
        },
        {
          "rank": 5,
          "score": 0.7596166133880615,
          "doc_id": "NART116403822",
          "title": "Deep-Learning for Radar: A Survey",
          "abstract": "<P>A comprehensive and well-structured review on the application of deep learning (DL) based algorithms, such as convolutional neural networks (CNN) and long-short term memory (LSTM), in radar signal processing is given. The following DL application areas are covered: i) radar waveform and antenna array design; ii) passive or low probability of interception (LPI) radar waveform recognition; iii) automatic target recognition (ATR) based on high range resolution profiles (HRRPs), Doppler signatures, and synthetic aperture radar (SAR) images; and iv) radar jamming/clutter recognition and suppression. Although DL is unanimously praised as the ultimate solution to many bottleneck problems in most of existing works on similar topics, both the positive and the negative sides of stories about DL are checked in this work. Specifically, two limiting factors of the real-life performance of deep neural networks (DNNs), limited training samples and adversarial examples, are thoroughly examined. By investigating the relationship between the DL-based algorithms proposed in various papers and linking them together to form a full picture, this work serves as a valuable source for researchers who are seeking potential research opportunities in this promising research field.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART116403822&target=NART&cn=NART116403822",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep-Learning for Radar: A Survey Deep-Learning for Radar: A Survey Deep-Learning for Radar: A Survey <P>A comprehensive and well-structured review on the application of deep learning (DL) based algorithms, such as convolutional neural networks (CNN) and long-short term memory (LSTM), in radar signal processing is given. The following DL application areas are covered: i) radar waveform and antenna array design; ii) passive or low probability of interception (LPI) radar waveform recognition; iii) automatic target recognition (ATR) based on high range resolution profiles (HRRPs), Doppler signatures, and synthetic aperture radar (SAR) images; and iv) radar jamming/clutter recognition and suppression. Although DL is unanimously praised as the ultimate solution to many bottleneck problems in most of existing works on similar topics, both the positive and the negative sides of stories about DL are checked in this work. Specifically, two limiting factors of the real-life performance of deep neural networks (DNNs), limited training samples and adversarial examples, are thoroughly examined. By investigating the relationship between the DL-based algorithms proposed in various papers and linking them together to form a full picture, this work serves as a valuable source for researchers who are seeking potential research opportunities in this promising research field.</P>"
        },
        {
          "rank": 6,
          "score": 0.7589601278305054,
          "doc_id": "NART106334309",
          "title": "Cognitive radar antenna selection via deep learning",
          "abstract": "<P>Direction&#x2010;of&#x2010;arrival (DoA) estimation of targets improves with the number of elements employed by a phased array radar antenna. Since larger arrays have high associated cost, area and computational load, there is a recent interest in thinning the antenna arrays without loss of far&#x2010;field DoA accuracy. In this context, a cognitive radar may deploy a full array and then select an optimal subarray to transmit and receive the signals in response to changes in the target environment. Prior works have used optimisation and greedy search methods to pick the best subarrays cognitively. In this study, deep learning is leveraged to address the antenna selection problem. Specifically, they construct a convolutional neural network (CNN) as a multi&#x2010;class classification framework, where each class designates a different subarray. The proposed network determines a new array every time data is received by the radar, thereby making antenna selection a cognitive operation. Their numerical experiments show that the proposed CNN structure provides 22% better classification performance than a support vector machine and the resulting subarrays yield 72% more accurate DoA estimates than random array selections.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106334309&target=NART&cn=NART106334309",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Cognitive radar antenna selection via deep learning Cognitive radar antenna selection via deep learning Cognitive radar antenna selection via deep learning <P>Direction&#x2010;of&#x2010;arrival (DoA) estimation of targets improves with the number of elements employed by a phased array radar antenna. Since larger arrays have high associated cost, area and computational load, there is a recent interest in thinning the antenna arrays without loss of far&#x2010;field DoA accuracy. In this context, a cognitive radar may deploy a full array and then select an optimal subarray to transmit and receive the signals in response to changes in the target environment. Prior works have used optimisation and greedy search methods to pick the best subarrays cognitively. In this study, deep learning is leveraged to address the antenna selection problem. Specifically, they construct a convolutional neural network (CNN) as a multi&#x2010;class classification framework, where each class designates a different subarray. The proposed network determines a new array every time data is received by the radar, thereby making antenna selection a cognitive operation. Their numerical experiments show that the proposed CNN structure provides 22% better classification performance than a support vector machine and the resulting subarrays yield 72% more accurate DoA estimates than random array selections.</P>"
        },
        {
          "rank": 7,
          "score": 0.7504903674125671,
          "doc_id": "DIKO0016954237",
          "title": "밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구",
          "abstract": "본 논문에서는 합성개구레이더 (Synthetic Aperture Radar, SAR) 시스템의 데이터를 획득하는 과정에서 발생할 수 있는 위상 오차를 보상하기 위해, 딥러닝 기반 요동 보상 방법으로 Unsupervised Image-to-image Translation (UNIT) 네트워크를 제안한다. 일반적으로 SAR 시스템을 이용한 데이터 취득 과정에서 레이더가 부착된 플랫폼의 비이상적인 경로나 불안정한 자세로 인해 위상 오차가 포함된 데이터를 얻는 문제가 발생할 수 있다. 이러한 위상 오차는 주변 환경 인식 및 표적 탐지 성능을 감소시키며, 군사 목적의 감시, 정찰을 위한 SAR 시스템과 자율주행 분야에서 지능형 차량의 경로 계획 및 사고 위협 회피를 위해 주변 환경 표현이 필수적이다. 따라서, 본 연구에서는 딥러닝 기반 요동 보상 방법을 제안하고, 밀리미터파 레이더 센서를 이용한 실험을 통해 제안된 방법의 성능을 검증한다. 제안된 방법은 Peak Signal-to-Noise Ratio (PSNR)와 Structural Similarity Index Measure (SSIM) 측면에서 기존의 요동 보상 기법들과 성능 평가 및 비교가 수행된다. 실제 측정 데이터를 기반으로 성능을 비교한 결과, 제안된 UNIT 네트워크는 기존 요동 보상 기법들 대비 PSNR은 평균 10.17%, SSIM은 9.4% 향상되는 것을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016954237&target=NART&cn=DIKO0016954237",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 밀리미터파 레이더를 이용한 영상 형성 및 딥러닝 기반 요동 보상 기법 연구 본 논문에서는 합성개구레이더 (Synthetic Aperture Radar, SAR) 시스템의 데이터를 획득하는 과정에서 발생할 수 있는 위상 오차를 보상하기 위해, 딥러닝 기반 요동 보상 방법으로 Unsupervised Image-to-image Translation (UNIT) 네트워크를 제안한다. 일반적으로 SAR 시스템을 이용한 데이터 취득 과정에서 레이더가 부착된 플랫폼의 비이상적인 경로나 불안정한 자세로 인해 위상 오차가 포함된 데이터를 얻는 문제가 발생할 수 있다. 이러한 위상 오차는 주변 환경 인식 및 표적 탐지 성능을 감소시키며, 군사 목적의 감시, 정찰을 위한 SAR 시스템과 자율주행 분야에서 지능형 차량의 경로 계획 및 사고 위협 회피를 위해 주변 환경 표현이 필수적이다. 따라서, 본 연구에서는 딥러닝 기반 요동 보상 방법을 제안하고, 밀리미터파 레이더 센서를 이용한 실험을 통해 제안된 방법의 성능을 검증한다. 제안된 방법은 Peak Signal-to-Noise Ratio (PSNR)와 Structural Similarity Index Measure (SSIM) 측면에서 기존의 요동 보상 기법들과 성능 평가 및 비교가 수행된다. 실제 측정 데이터를 기반으로 성능을 비교한 결과, 제안된 UNIT 네트워크는 기존 요동 보상 기법들 대비 PSNR은 평균 10.17%, SSIM은 9.4% 향상되는 것을 확인하였다."
        },
        {
          "rank": 8,
          "score": 0.7388116121292114,
          "doc_id": "NART124851615",
          "title": "Radar Spectrum Image Classification Based on Deep Learning",
          "abstract": "<P>With the continuous development and progress of science and technology, the increasingly complex electromagnetic environment and the research and development of new radar systems have led to the emergence of various radar signals. Traditional methods of radar emitter identification cannot meet the needs of current practical applications. For the purpose of classification and recognition of radar emitter signals, this paper proposes an improved EfficientNetv2-s classification method based on deep learning for more precise classification and recognition of radar radiation source signals. Using 16 different types of radar signal parameters from the signal parameter setting table, the proposed method generates random data sets consisting of spectrum images with varying amplitude. The proposed method replaces two-dimensional convolution in EfficientNetV2 with one-dimensional convolution. Additionally, the channel attention mechanism of the EfficientNetv2-s is optimized and modified to obtain attention weights without dimensional reduction, resulting in superior accuracy. Compared with other deep-learning image-classification methods, the test results of this method have better classification accuracy on the test set: the top1 accuracy reaches 98.12%, which is 0.17~3.12% higher than other methods. Furthermore, the proposed method has lower complexity compared to most methods.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART124851615&target=NART&cn=NART124851615",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Radar Spectrum Image Classification Based on Deep Learning Radar Spectrum Image Classification Based on Deep Learning Radar Spectrum Image Classification Based on Deep Learning <P>With the continuous development and progress of science and technology, the increasingly complex electromagnetic environment and the research and development of new radar systems have led to the emergence of various radar signals. Traditional methods of radar emitter identification cannot meet the needs of current practical applications. For the purpose of classification and recognition of radar emitter signals, this paper proposes an improved EfficientNetv2-s classification method based on deep learning for more precise classification and recognition of radar radiation source signals. Using 16 different types of radar signal parameters from the signal parameter setting table, the proposed method generates random data sets consisting of spectrum images with varying amplitude. The proposed method replaces two-dimensional convolution in EfficientNetV2 with one-dimensional convolution. Additionally, the channel attention mechanism of the EfficientNetv2-s is optimized and modified to obtain attention weights without dimensional reduction, resulting in superior accuracy. Compared with other deep-learning image-classification methods, the test results of this method have better classification accuracy on the test set: the top1 accuracy reaches 98.12%, which is 0.17~3.12% higher than other methods. Furthermore, the proposed method has lower complexity compared to most methods.</P>"
        },
        {
          "rank": 9,
          "score": 0.7343988418579102,
          "doc_id": "JAKO201923233204235",
          "title": "딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구",
          "abstract": "Classification of radar signals in the field of electronic warfare is a problem of discriminating threat types by analyzing enemy threat radar signals such as aircraft, radar, and missile received through electronic warfare equipment. Recent radar systems have adopted a variety of modulation schemes that are different from those used in conventional systems, and are often difficult to analyze using existing algorithms. Also, it is necessary to design a robust algorithm for the signal received in the real environment due to the environmental influence and the measurement error due to the characteristics of the hardware. In this paper, we propose a radar signal classification method which are not affected by radar signal modulation methods and noise generation by using deep learning techniques.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201923233204235&target=NART&cn=JAKO201923233204235",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 딥 러닝 기법을 이용한 레이더 신호 분류 모델 연구 Classification of radar signals in the field of electronic warfare is a problem of discriminating threat types by analyzing enemy threat radar signals such as aircraft, radar, and missile received through electronic warfare equipment. Recent radar systems have adopted a variety of modulation schemes that are different from those used in conventional systems, and are often difficult to analyze using existing algorithms. Also, it is necessary to design a robust algorithm for the signal received in the real environment due to the environmental influence and the measurement error due to the characteristics of the hardware. In this paper, we propose a radar signal classification method which are not affected by radar signal modulation methods and noise generation by using deep learning techniques."
        },
        {
          "rank": 10,
          "score": 0.7311897277832031,
          "doc_id": "DIKO0017187917",
          "title": "레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구",
          "abstract": "자율주행의 안정적인 운행을 보장하기 위해서는 도로 상황에 대한 깊이 있는 이해가 필수적이다. 이에 본 논문에서는 단일 딥러닝(deep learning, DL) 네트워크 구조를 활용해 차량, 사이클리스트, 보행자 등 도로에서 자주 마주하는 객체를 분류하고 동시에 이들의 이동 방향을 추정하는 방법을 제안한다. 먼저, 4차원 이미징 레이더를 이용해 대상의 거리, 속도, 방위각, 고도각과 같은 정보를 획득한다. 이후, 검출 결과를 포인트 클라우드 데이터로 변환하여 3차원 공간 좌표계로 표현한다. 다음으로, 포인트 클라우드 데이터를 XY 평면에 정사영하여 대상의 분류와 이동 방향 추정을 수행한다. XY 평면에서 밀도 기반의 클러스터링(density-based spatial clustering) 기법을 적용해 검출 결과에서 잡음을 제거하고 객체를 클러스터링하여, 이를 이미지 데이터로 변환하는 전처리 과정을 거친다. 그런 후, 이미지 데이터를 이용해 객체 분류와 이동 방향 예측을 동시에 수행할 수 있는 다중 출력 DL 네트워크를 학습시킨다. 제안된 방법의 성능 평가 결과, 객체 분류 정확도는 96.10%로 나타났고, 이동 방향 예측의 평균 제곱근 오차(root mean square error, RMSE)는 차량, 사이클리스트, 보행자에 대해 각각 5.54°, 3.89°, 15.35°로 측정되었으며, 실행시간은 0.1초로 측정되어 효율성을 입증하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017187917&target=NART&cn=DIKO0017187917",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 레이더 시스템에서 동시적 표적 분류와 이동 방향 추정을 위한 딥러닝 네트워크 연구 자율주행의 안정적인 운행을 보장하기 위해서는 도로 상황에 대한 깊이 있는 이해가 필수적이다. 이에 본 논문에서는 단일 딥러닝(deep learning, DL) 네트워크 구조를 활용해 차량, 사이클리스트, 보행자 등 도로에서 자주 마주하는 객체를 분류하고 동시에 이들의 이동 방향을 추정하는 방법을 제안한다. 먼저, 4차원 이미징 레이더를 이용해 대상의 거리, 속도, 방위각, 고도각과 같은 정보를 획득한다. 이후, 검출 결과를 포인트 클라우드 데이터로 변환하여 3차원 공간 좌표계로 표현한다. 다음으로, 포인트 클라우드 데이터를 XY 평면에 정사영하여 대상의 분류와 이동 방향 추정을 수행한다. XY 평면에서 밀도 기반의 클러스터링(density-based spatial clustering) 기법을 적용해 검출 결과에서 잡음을 제거하고 객체를 클러스터링하여, 이를 이미지 데이터로 변환하는 전처리 과정을 거친다. 그런 후, 이미지 데이터를 이용해 객체 분류와 이동 방향 예측을 동시에 수행할 수 있는 다중 출력 DL 네트워크를 학습시킨다. 제안된 방법의 성능 평가 결과, 객체 분류 정확도는 96.10%로 나타났고, 이동 방향 예측의 평균 제곱근 오차(root mean square error, RMSE)는 차량, 사이클리스트, 보행자에 대해 각각 5.54°, 3.89°, 15.35°로 측정되었으며, 실행시간은 0.1초로 측정되어 효율성을 입증하였다."
        },
        {
          "rank": 11,
          "score": 0.7303420305252075,
          "doc_id": "NART123168643",
          "title": "Synthetic Aperture Radar (SAR) Meets Deep Learning",
          "abstract": "<P>Synthetic aperture radar (SAR) is an important active microwave imaging sensor [...]</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART123168643&target=NART&cn=NART123168643",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Synthetic Aperture Radar (SAR) Meets Deep Learning Synthetic Aperture Radar (SAR) Meets Deep Learning Synthetic Aperture Radar (SAR) Meets Deep Learning <P>Synthetic aperture radar (SAR) is an important active microwave imaging sensor [...]</P>"
        },
        {
          "rank": 12,
          "score": 0.7296417951583862,
          "doc_id": "NART127041948",
          "title": "Deep Learning Techniques in Radar Emitter Identification",
          "abstract": "<P>In the field of electronic warfare (EW), one of the crucial roles of electronic intelligence is the identification of radar signals. In an operational environment, it is very essential to identify radar emitters whether friend or foe so that appropriate radar countermeasures can be taken against them. With the electromagnetic environment becoming increasingly complex and the diversity of signal features, radar emitter identification with high recognition accuracy has become a significantly challenging task. Traditional radar identification methods have shown some limitations in this complex electromagnetic scenario. Several radar classification and identification methods based on artificial neural networks have emerged with the emergence of artificial neural networks, notably deep learning approaches. Machine learning and deep learning algorithms are now frequently utilized to extract various types of information from radar signals more accurately and robustly. This paper illustrates the use of Deep Neural Networks (DNN) in radar applications for emitter classification and identification. Since deep learning approaches are capable of accurately classifying complicated patterns in radar signals, they have demonstrated significant promise for identifying radar emitters. By offering a thorough literature analysis of deep learning-based methodologies, the study intends to assist researchers and practitioners in better understanding the application of deep learning techniques to challenges related to the classification and identification of radar emitters. The study demonstrates that DNN can be used successfully in applications for radar classification and identification.&amp;#xD; &amp;#xD; </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART127041948&target=NART&cn=NART127041948",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning Techniques in Radar Emitter Identification Deep Learning Techniques in Radar Emitter Identification Deep Learning Techniques in Radar Emitter Identification <P>In the field of electronic warfare (EW), one of the crucial roles of electronic intelligence is the identification of radar signals. In an operational environment, it is very essential to identify radar emitters whether friend or foe so that appropriate radar countermeasures can be taken against them. With the electromagnetic environment becoming increasingly complex and the diversity of signal features, radar emitter identification with high recognition accuracy has become a significantly challenging task. Traditional radar identification methods have shown some limitations in this complex electromagnetic scenario. Several radar classification and identification methods based on artificial neural networks have emerged with the emergence of artificial neural networks, notably deep learning approaches. Machine learning and deep learning algorithms are now frequently utilized to extract various types of information from radar signals more accurately and robustly. This paper illustrates the use of Deep Neural Networks (DNN) in radar applications for emitter classification and identification. Since deep learning approaches are capable of accurately classifying complicated patterns in radar signals, they have demonstrated significant promise for identifying radar emitters. By offering a thorough literature analysis of deep learning-based methodologies, the study intends to assist researchers and practitioners in better understanding the application of deep learning techniques to challenges related to the classification and identification of radar emitters. The study demonstrates that DNN can be used successfully in applications for radar classification and identification.&amp;#xD; &amp;#xD; </P>"
        },
        {
          "rank": 13,
          "score": 0.7270398736000061,
          "doc_id": "NART84975182",
          "title": "Deep Learning for Passive Synthetic Aperture Radar",
          "abstract": "<P>We introduce a deep learning (DL) framework for inverse problems in imaging, and demonstrate the advantages and applicability of this approach in passive synthetic aperture radar (SAR) image reconstruction. We interpret image reconstruction as a machine learning task and utilize deep networks as forward and inverse solvers for imaging. Specifically, we design a recurrent neural network (RNN) architecture as an inverse solver based on the iterations of proximal gradient descent optimization methods. We further adapt the RNN architecture to image reconstruction problems by transforming the network into a recurrent auto-encoder, thereby allowing for unsupervised training. Our DL based inverse solver is particularly suitable for a class of image formation problems in which the forward model is only partially known. The ability to learn forward models and hyper parameters combined with unsupervised training approach establish our recurrent auto-encoder suitable for real world applications. We demonstrate the performance of our method in passive SAR image reconstruction. In this regime a source of opportunity, with unknown location and transmitted waveform, is used to illuminate a scene of interest. We investigate recurrent auto-encoder architecture based on the <TEX>$\\ell _1$</TEX> and <TEX>$\\ell _0$</TEX> constrained least-squares problem. We present a projected stochastic gradient descent based training scheme which incorporates constraints of the unknown model parameters. We demonstrate through extensive numerical simulations that our DL based approach out performs conventional sparse coding methods in terms of computation and reconstructed image quality, specifically, when no information about the transmitter is available.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART84975182&target=NART&cn=NART84975182",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Learning for Passive Synthetic Aperture Radar Deep Learning for Passive Synthetic Aperture Radar Deep Learning for Passive Synthetic Aperture Radar <P>We introduce a deep learning (DL) framework for inverse problems in imaging, and demonstrate the advantages and applicability of this approach in passive synthetic aperture radar (SAR) image reconstruction. We interpret image reconstruction as a machine learning task and utilize deep networks as forward and inverse solvers for imaging. Specifically, we design a recurrent neural network (RNN) architecture as an inverse solver based on the iterations of proximal gradient descent optimization methods. We further adapt the RNN architecture to image reconstruction problems by transforming the network into a recurrent auto-encoder, thereby allowing for unsupervised training. Our DL based inverse solver is particularly suitable for a class of image formation problems in which the forward model is only partially known. The ability to learn forward models and hyper parameters combined with unsupervised training approach establish our recurrent auto-encoder suitable for real world applications. We demonstrate the performance of our method in passive SAR image reconstruction. In this regime a source of opportunity, with unknown location and transmitted waveform, is used to illuminate a scene of interest. We investigate recurrent auto-encoder architecture based on the <TEX>$\\ell _1$</TEX> and <TEX>$\\ell _0$</TEX> constrained least-squares problem. We present a projected stochastic gradient descent based training scheme which incorporates constraints of the unknown model parameters. We demonstrate through extensive numerical simulations that our DL based approach out performs conventional sparse coding methods in terms of computation and reconstructed image quality, specifically, when no information about the transmitter is available.</P>"
        },
        {
          "rank": 14,
          "score": 0.71532142162323,
          "doc_id": "NART117063882",
          "title": "Advancing Radar Nowcasting Through Deep Transfer Learning",
          "abstract": "<P>Deep learning is emerging as a powerful tool in scientific applications, such as radar-based convective storm nowcasting. However, it is still a challenge to extend the application of a well-trained deep learning nowcasting model, which demands to incorporate the learned knowledge at a certain location to other locations characterized by different precipitation features. This article designs a transfer learning framework to tackle this problem. A convolutional neural network (CNN)-based nowcasting method is utilized as the benchmark, based on which two transfer learning models are constructed through fine-tune and maximum mean discrepancy (MMD) minimization. The base CNN model is trained using radar data in the source study domain near Beijing, China, whereas the transferred models are applied to the target domain near Guangzhou, China, with only a small amount of data in the target area. The influence of a varying number of target data samples on the nowcasting performance is quantified. The experimental results demonstrate that the deep transfer learning models can improve the nowcasting skills.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART117063882&target=NART&cn=NART117063882",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Advancing Radar Nowcasting Through Deep Transfer Learning Advancing Radar Nowcasting Through Deep Transfer Learning Advancing Radar Nowcasting Through Deep Transfer Learning <P>Deep learning is emerging as a powerful tool in scientific applications, such as radar-based convective storm nowcasting. However, it is still a challenge to extend the application of a well-trained deep learning nowcasting model, which demands to incorporate the learned knowledge at a certain location to other locations characterized by different precipitation features. This article designs a transfer learning framework to tackle this problem. A convolutional neural network (CNN)-based nowcasting method is utilized as the benchmark, based on which two transfer learning models are constructed through fine-tune and maximum mean discrepancy (MMD) minimization. The base CNN model is trained using radar data in the source study domain near Beijing, China, whereas the transferred models are applied to the target domain near Guangzhou, China, with only a small amount of data in the target area. The influence of a varying number of target data samples on the nowcasting performance is quantified. The experimental results demonstrate that the deep transfer learning models can improve the nowcasting skills.</P>"
        },
        {
          "rank": 15,
          "score": 0.7067503929138184,
          "doc_id": "NART125907540",
          "title": "Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review",
          "abstract": "<P>Radar automatic target recognition (RATR) technology is fundamental but complicated system engineering that combines sensor, target, environment, and signal processing technology, etc. It plays a significant role in improving the level and capabilities of military and civilian automation. Although RATR has been successfully applied in some aspects, the complete theoretical system has not been established. At present, deep learning algorithms have received a lot of attention and have emerged as potential and feasible solutions in RATR. This paper mainly reviews related articles published between 2010 and 2022, which corresponds to the period when deep learning methods were introduced into RATR research. In this paper, the current research status of radar target characteristics is summarized, including motion, micro-motion, one-dimensional, and two-dimensional characteristics, etc. This paper reviews the progress of deep learning methods in the feature extraction and recognition of radar target characteristics in recent years, including space, air, ground, sea-surface targets, etc. Due to more and more attention and research results published in the past few years, it is hoped that this review can provide potential guidance for future research and application of deep learning in fields related to RATR.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART125907540&target=NART&cn=NART125907540",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review Radar Target Characterization and Deep Learning in Radar Automatic Target Recognition: A Review <P>Radar automatic target recognition (RATR) technology is fundamental but complicated system engineering that combines sensor, target, environment, and signal processing technology, etc. It plays a significant role in improving the level and capabilities of military and civilian automation. Although RATR has been successfully applied in some aspects, the complete theoretical system has not been established. At present, deep learning algorithms have received a lot of attention and have emerged as potential and feasible solutions in RATR. This paper mainly reviews related articles published between 2010 and 2022, which corresponds to the period when deep learning methods were introduced into RATR research. In this paper, the current research status of radar target characteristics is summarized, including motion, micro-motion, one-dimensional, and two-dimensional characteristics, etc. This paper reviews the progress of deep learning methods in the feature extraction and recognition of radar target characteristics in recent years, including space, air, ground, sea-surface targets, etc. Due to more and more attention and research results published in the past few years, it is hoped that this review can provide potential guidance for future research and application of deep learning in fields related to RATR.</P>"
        },
        {
          "rank": 16,
          "score": 0.7015281915664673,
          "doc_id": "JAKO202201253148351",
          "title": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰",
          "abstract": "위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202201253148351&target=NART&cn=JAKO202201253148351",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다."
        },
        {
          "rank": 17,
          "score": 0.6940212845802307,
          "doc_id": "JAKO202319937622688",
          "title": "머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구",
          "abstract": "본 논문에서는 유리섬유 직물 강화 복합재 소재위에 Cross-Dipole 패턴이 배치된 정형적 Circuit Analog(CA) 전파 흡수 구조 설계를 위한 머신러닝 및 딥러닝 모델을 제시하였다. 제시된 모델은 Cross-Dipole 패턴의 형상에 따라서 Ku-band (12-18 GHz)에서의 전파흡수성능을 3차원 전자파 수치해석 없이 바로 계산할 수 있다. 이를 위하여 다양한 머신러닝 및 딥러닝 기술을 적용한 최적 학습 모델을 도출하고, 학습 모델이 계산한 결과를 3차원 전자파 수치해석결과로 얻은 전파흡수특성과 비교함으로써 각각의 모델 간의 성능의 비교우위를 평가하였다. 개발된 모델들은 대부분 수치해석결과와 유사한 계산결과를 보여주었지만, 그 중 Fully-Connected 모델이 가장 유사한 계산결과를 제공할 수 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202319937622688&target=NART&cn=JAKO202319937622688",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구 머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구 머신러닝 및 딥러닝 기법을 활용한 유리섬유 직물 강화 복합재 적층판형 Circuit Analog 전파 흡수구조 설계에 대한 연구 본 논문에서는 유리섬유 직물 강화 복합재 소재위에 Cross-Dipole 패턴이 배치된 정형적 Circuit Analog(CA) 전파 흡수 구조 설계를 위한 머신러닝 및 딥러닝 모델을 제시하였다. 제시된 모델은 Cross-Dipole 패턴의 형상에 따라서 Ku-band (12-18 GHz)에서의 전파흡수성능을 3차원 전자파 수치해석 없이 바로 계산할 수 있다. 이를 위하여 다양한 머신러닝 및 딥러닝 기술을 적용한 최적 학습 모델을 도출하고, 학습 모델이 계산한 결과를 3차원 전자파 수치해석결과로 얻은 전파흡수특성과 비교함으로써 각각의 모델 간의 성능의 비교우위를 평가하였다. 개발된 모델들은 대부분 수치해석결과와 유사한 계산결과를 보여주었지만, 그 중 Fully-Connected 모델이 가장 유사한 계산결과를 제공할 수 있음을 확인하였다."
        },
        {
          "rank": 18,
          "score": 0.6868108510971069,
          "doc_id": "ART002342492",
          "title": "Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning",
          "abstract": "Heavy rainfall has frequently caused serious flooding and landslides, increasing traffic delays in most parts of the world. Consequently, the people in areas battered by heavy rainfall face many hardships. Thus, the negative effects of torrential rainfall always remind researchers to keep seeking the ways to prevent such damage. Therefore, we designed a system for short-term prediction of localized heavy downpours by using radar images coupled with a machine learning method. Here, we introduce a new approach, named dual k-nearest neighbor (dual-kNN), for shortterm rainfall prediction by upgrading the ordinary classification routines of classical k-nearest neighbors (k-NN). dual-kNN is able to maintain highly robust classification of various K values with an advanced simple dual consideration, where observation of a targeted object can be found not only in the specified region but also in other related regions. We conducted experimentations using 2011, 2013, and 2014 data sets collected from the WITH small-dish aviation radar installed on the rooftop of Information Engineering, University of the Ryukyus. Then, we compared the prediction accuracy of our new approach with classical k-NN. It was experimentally confirmed with test cases and simulations that the performance of dual-kNN is more effective than classical k- NN.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002342492&target=NART&cn=ART002342492",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Short-term Prediction of Localized Heavy Rain from Radar Imaging and Machine Learning Heavy rainfall has frequently caused serious flooding and landslides, increasing traffic delays in most parts of the world. Consequently, the people in areas battered by heavy rainfall face many hardships. Thus, the negative effects of torrential rainfall always remind researchers to keep seeking the ways to prevent such damage. Therefore, we designed a system for short-term prediction of localized heavy downpours by using radar images coupled with a machine learning method. Here, we introduce a new approach, named dual k-nearest neighbor (dual-kNN), for shortterm rainfall prediction by upgrading the ordinary classification routines of classical k-nearest neighbors (k-NN). dual-kNN is able to maintain highly robust classification of various K values with an advanced simple dual consideration, where observation of a targeted object can be found not only in the specified region but also in other related regions. We conducted experimentations using 2011, 2013, and 2014 data sets collected from the WITH small-dish aviation radar installed on the rooftop of Information Engineering, University of the Ryukyus. Then, we compared the prediction accuracy of our new approach with classical k-NN. It was experimentally confirmed with test cases and simulations that the performance of dual-kNN is more effective than classical k- NN."
        },
        {
          "rank": 19,
          "score": 0.6832684278488159,
          "doc_id": "JAKO202007552827199",
          "title": "심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측",
          "abstract": "본 연구에서는 강우예측을 위해 U-Net과 SegNet에 기반한 합성곱 신경망 네트워크 구조에 장기간의 국내 기상레이더 자료를 활용하여 심층학습기반의 강우예측을 수행하였다. 또한, 기존 외삽기반의 강우예측 기법인 이류모델의 결과와 비교 평가하였다. 심층신경망의 학습 및 검정을 위해 2010부터 2016년 동안의 기상청 관악산과 광덕산 레이더의 원자료를 수집, 1 km 공간해상도를 갖는 480 &#215; 480의 픽셀의 회색조 영상으로 변환하여 HDF5 형태의 데이터를 구축하였다. 구축된 데이터로 30분 전부터 현재까지 10분 간격의 연속된 레이더 영상 4개를 이용하여 10분 후의 강수량을 예측하도록 심층신경망 모델을 학습하였으며, 학습된 심층신경망 모델로 60분의 선행예측을 수행하기 위해 예측값을 반복 사용하는 재귀적 방식을 적용하였다. 심층신경망 예측모델의 성능 평가를 위해 2017년에 발생한 24개의 호우사례에 대해 선행 60분까지 강우예측을 수행하였다. 임계강우강도 0.1, 1, 5 mm/hr에서 평균절대오차와 임계성공지수를 산정하여 예측성능을 평가한 결과, 강우강도 임계 값 0.1, 1 mm/hr의 경우 MAE는 60분 선행예측까지, CSI는 선행예측 50분까지 참조 예측모델인 이류모델이 보다 우수한 성능을 보였다. 특히, 5 mm/hr 이하의 약한 강우에 대해서는 심층신경망 예측모델이 이류모델보다 대체적으로 좋은 성능을 보였지만, 5 mm/hr의 임계 값에 대한 평가결과 심층신경망 예측모델은 고강도의 뚜렷한 강수 특징을 예측하는 데 한계가 있었다. 심층신경망 예측모델은 예측시간이 길어질수록 공간 평활화되는 경향이 뚜렷해지며, 이로 인해 강우 예측의 정확도가 저하되었다. 이류모델은 뚜렷한 강수 특성을 보존하기 때문에 강한 강도 (>5 mm/hr)에 대해 심층신경망 예측모델을 능가하지만, 강우 위치가 잘못 이동하는 경향이 있다. 본 연구결과는 이후 심층신경망을 이용한 레이더 강우 예측기술의 개발과 개선에 도움이 될 수 있을 것으로 판단된다. 또한, 본 연구에서 구축한 대용량 기상레이더 자료는 향후 후속연구에 활용될 수 있도록 개방형 저장소를 통해 제공될 예정이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202007552827199&target=NART&cn=JAKO202007552827199",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 심층신경망을 이용한 레이더 영상 학습 기반 초단시간 강우예측 본 연구에서는 강우예측을 위해 U-Net과 SegNet에 기반한 합성곱 신경망 네트워크 구조에 장기간의 국내 기상레이더 자료를 활용하여 심층학습기반의 강우예측을 수행하였다. 또한, 기존 외삽기반의 강우예측 기법인 이류모델의 결과와 비교 평가하였다. 심층신경망의 학습 및 검정을 위해 2010부터 2016년 동안의 기상청 관악산과 광덕산 레이더의 원자료를 수집, 1 km 공간해상도를 갖는 480 &#215; 480의 픽셀의 회색조 영상으로 변환하여 HDF5 형태의 데이터를 구축하였다. 구축된 데이터로 30분 전부터 현재까지 10분 간격의 연속된 레이더 영상 4개를 이용하여 10분 후의 강수량을 예측하도록 심층신경망 모델을 학습하였으며, 학습된 심층신경망 모델로 60분의 선행예측을 수행하기 위해 예측값을 반복 사용하는 재귀적 방식을 적용하였다. 심층신경망 예측모델의 성능 평가를 위해 2017년에 발생한 24개의 호우사례에 대해 선행 60분까지 강우예측을 수행하였다. 임계강우강도 0.1, 1, 5 mm/hr에서 평균절대오차와 임계성공지수를 산정하여 예측성능을 평가한 결과, 강우강도 임계 값 0.1, 1 mm/hr의 경우 MAE는 60분 선행예측까지, CSI는 선행예측 50분까지 참조 예측모델인 이류모델이 보다 우수한 성능을 보였다. 특히, 5 mm/hr 이하의 약한 강우에 대해서는 심층신경망 예측모델이 이류모델보다 대체적으로 좋은 성능을 보였지만, 5 mm/hr의 임계 값에 대한 평가결과 심층신경망 예측모델은 고강도의 뚜렷한 강수 특징을 예측하는 데 한계가 있었다. 심층신경망 예측모델은 예측시간이 길어질수록 공간 평활화되는 경향이 뚜렷해지며, 이로 인해 강우 예측의 정확도가 저하되었다. 이류모델은 뚜렷한 강수 특성을 보존하기 때문에 강한 강도 (>5 mm/hr)에 대해 심층신경망 예측모델을 능가하지만, 강우 위치가 잘못 이동하는 경향이 있다. 본 연구결과는 이후 심층신경망을 이용한 레이더 강우 예측기술의 개발과 개선에 도움이 될 수 있을 것으로 판단된다. 또한, 본 연구에서 구축한 대용량 기상레이더 자료는 향후 후속연구에 활용될 수 있도록 개방형 저장소를 통해 제공될 예정이다."
        },
        {
          "rank": 20,
          "score": 0.6821686029434204,
          "doc_id": "JAKO202116954704821",
          "title": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측",
          "abstract": "본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116954704821&target=NART&cn=JAKO202116954704821",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다."
        },
        {
          "rank": 21,
          "score": 0.6776905059814453,
          "doc_id": "NART108808939",
          "title": "Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출",
          "abstract": "This paper presents a novel remote infrared pedestrian detection method for night use by means of local projection-CNN. Conventional sliding window methods (HOG/ACF) or region proposal-based deep learning approaches (faster R-CNN, SSD, YOLO) either fail to detect small objects or generate many false positives. Multi-modal region proposal schemes (multi-scale contrast filters with local projection+ACF) are used to improve remote pedestrian detection. AlexNet-based CNN feature extraction and SVM classification can reduce false positives further. This paper's experimental evaluations indicate that the proposed method can improve remote IR pedestrian detection by 16%.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART108808939&target=NART&cn=NART108808939",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 Multi-Modal 영역제안 및 CNN-SVM 기반 야간 원거리 원적외선 보행자 검출 This paper presents a novel remote infrared pedestrian detection method for night use by means of local projection-CNN. Conventional sliding window methods (HOG/ACF) or region proposal-based deep learning approaches (faster R-CNN, SSD, YOLO) either fail to detect small objects or generate many false positives. Multi-modal region proposal schemes (multi-scale contrast filters with local projection+ACF) are used to improve remote pedestrian detection. AlexNet-based CNN feature extraction and SVM classification can reduce false positives further. This paper's experimental evaluations indicate that the proposed method can improve remote IR pedestrian detection by 16%."
        },
        {
          "rank": 22,
          "score": 0.6770976781845093,
          "doc_id": "JAKO202231363544671",
          "title": "입자 군집 최적화(PSO) 알고리즘 기반 다층 레이더 흡수 구조체 설계",
          "abstract": "본 논문에서는 입자 군집 최적화 (Particle Swarm Optimization: PSO) 알고리즘을 이용하여 다층 레이더 흡수 구조체를 설계하고, 다층 레이더 흡수 구조체의 특성을 분석하였다. 다층 레이더 흡수 구조체 설계에 PSO를 적용함으로써 빠르고 정확하게 설계 값을 도출할 수 있음을 보였으며, 특히 경사 입사에 대한 경우에 대해서도 최적의 다층 레이더 흡수 구조체를 설계할 수 있음을 보였다. 또한, 다양한 설계 파라미터의 조합에서도 성능 요구 조건에 부합하는 최적의 값이 결정될 수 있음을 보였다. 각 단계별로 필요한 방정식 및 모든 변수에 대한 자세한 설명을 포함해서 포괄적인 순서도를 통해 제시하였고 본 논문의 결과로부터 다층 레이더 흡수 구조체를 설계하기 위한 복잡하고 많은 계산을 생략할 수 있으며, 다양한 복합 재료를 활용한 다층 레이다 흡수 구조체 설계 및 개발에 활용할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202231363544671&target=NART&cn=JAKO202231363544671",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "입자 군집 최적화(PSO) 알고리즘 기반 다층 레이더 흡수 구조체 설계 입자 군집 최적화(PSO) 알고리즘 기반 다층 레이더 흡수 구조체 설계 입자 군집 최적화(PSO) 알고리즘 기반 다층 레이더 흡수 구조체 설계 본 논문에서는 입자 군집 최적화 (Particle Swarm Optimization: PSO) 알고리즘을 이용하여 다층 레이더 흡수 구조체를 설계하고, 다층 레이더 흡수 구조체의 특성을 분석하였다. 다층 레이더 흡수 구조체 설계에 PSO를 적용함으로써 빠르고 정확하게 설계 값을 도출할 수 있음을 보였으며, 특히 경사 입사에 대한 경우에 대해서도 최적의 다층 레이더 흡수 구조체를 설계할 수 있음을 보였다. 또한, 다양한 설계 파라미터의 조합에서도 성능 요구 조건에 부합하는 최적의 값이 결정될 수 있음을 보였다. 각 단계별로 필요한 방정식 및 모든 변수에 대한 자세한 설명을 포함해서 포괄적인 순서도를 통해 제시하였고 본 논문의 결과로부터 다층 레이더 흡수 구조체를 설계하기 위한 복잡하고 많은 계산을 생략할 수 있으며, 다양한 복합 재료를 활용한 다층 레이다 흡수 구조체 설계 및 개발에 활용할 수 있다."
        },
        {
          "rank": 23,
          "score": 0.6765679717063904,
          "doc_id": "JAKO200011920771446",
          "title": "건설적 선택학습 신경망을 이용한 앙상블 머신의 구축",
          "abstract": "본 논문에서는 효과적인 앙상블 머신의 구축을 위한 새로운 방안을 제시한다. 효과적인 앙상블의 구축을 위해서는 앙상블 멤버들간의 상관관계가 아주 낮아야 하며 또한 각 앙상블 멤버들은 전체 문제를 어느 정도는 정확하게 학습하면서도 서로들간의 불일치 하는 부분이 존재해야 한다는 것이 여러 논문들에 발표되었다. 본 논문에서는 주어진 문제의 다양한 면을 학습한 다수의 앙상블 후보 네트웍을 생성하기 위하여 건설적 학습 알고리즘과 능동 학습 알고리즘을 결합한 형태의 신경망 학습 알고리즘을 이용한다. 이 신경망의 학습은 최소 은닉 노드에서 최대 은닉노드까지 점진적으로 은닉노드를 늘려나감과 동시에 후보 데이타 집합에서 학습에 사용할 훈련 데이타를 점진적으로 선택해 나가면서 이루어진다. 은닉 노드의 증가시점에서 앙상블의 후부 네트웍이 생성된다. 이러한 한 차례의 학습 진행을 한 chain이라 정의한다. 다수의 chain을 통하여 다양한 형태의 네트웍 크기와 다양한 형태의 데이타 분포를 학습한 후보 내트웍들이 생성된다. 이렇게 생성된 후보 네트웍들은 확률적 비례 선택법에 의해 선택된 후 generalized ensemble method (GEM)에 의해 결합되어 최종적인 앙상블 성능을 보여준다. 제안된 알고리즘은 한개의 인공 데이타와 한 개의 실세계 데이타에 적용되었다. 실험을 통하여 제안된 알고리즘에 의해 구성된 앙상블의 최대 일반화 성능은 다른 알고리즘에 의한 그것보다 우수함을 알 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200011920771446&target=NART&cn=JAKO200011920771446",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "건설적 선택학습 신경망을 이용한 앙상블 머신의 구축 건설적 선택학습 신경망을 이용한 앙상블 머신의 구축 건설적 선택학습 신경망을 이용한 앙상블 머신의 구축 본 논문에서는 효과적인 앙상블 머신의 구축을 위한 새로운 방안을 제시한다. 효과적인 앙상블의 구축을 위해서는 앙상블 멤버들간의 상관관계가 아주 낮아야 하며 또한 각 앙상블 멤버들은 전체 문제를 어느 정도는 정확하게 학습하면서도 서로들간의 불일치 하는 부분이 존재해야 한다는 것이 여러 논문들에 발표되었다. 본 논문에서는 주어진 문제의 다양한 면을 학습한 다수의 앙상블 후보 네트웍을 생성하기 위하여 건설적 학습 알고리즘과 능동 학습 알고리즘을 결합한 형태의 신경망 학습 알고리즘을 이용한다. 이 신경망의 학습은 최소 은닉 노드에서 최대 은닉노드까지 점진적으로 은닉노드를 늘려나감과 동시에 후보 데이타 집합에서 학습에 사용할 훈련 데이타를 점진적으로 선택해 나가면서 이루어진다. 은닉 노드의 증가시점에서 앙상블의 후부 네트웍이 생성된다. 이러한 한 차례의 학습 진행을 한 chain이라 정의한다. 다수의 chain을 통하여 다양한 형태의 네트웍 크기와 다양한 형태의 데이타 분포를 학습한 후보 내트웍들이 생성된다. 이렇게 생성된 후보 네트웍들은 확률적 비례 선택법에 의해 선택된 후 generalized ensemble method (GEM)에 의해 결합되어 최종적인 앙상블 성능을 보여준다. 제안된 알고리즘은 한개의 인공 데이타와 한 개의 실세계 데이타에 적용되었다. 실험을 통하여 제안된 알고리즘에 의해 구성된 앙상블의 최대 일반화 성능은 다른 알고리즘에 의한 그것보다 우수함을 알 수 있다."
        },
        {
          "rank": 24,
          "score": 0.6753087043762207,
          "doc_id": "ART002356179",
          "title": "Intelligent intrusion detection systems using artificial neural networks",
          "abstract": "This paper presents a novel approach to detection of malicious network traffic using artificial neural networks suitable for use in deep packet inspection based intrusion detection systems. Experimental results using a range of typical benign network traffic data (images, dynamic link library files, and a selection of other miscellaneous files such as logs, music files, and word processing documents) and malicious shell code files sourced from the online exploit and vulnerability repository exploitdb [1], have shown that the proposed artificial neural network architecture is able to distinguish between benign and malicious network traffic accurately.The proposed artificial neural network architecture obtains an average accuracy of 98%, an average area under the receiver operator characteristic curve of 0.98, and an average false positive rate of less than 2% in repeated 10-fold cross-validation. This shows that the proposed classification technique is robust, accurate, and precise. The novel approach to malicious network traffic detection proposed in this paper has the potential to significantly enhance the utility of intrusion detection systems applied to both conventional network traffic analysis and network traffic analysis for cyber–physical systems such as smart-grids.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002356179&target=NART&cn=ART002356179",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Intelligent intrusion detection systems using artificial neural networks Intelligent intrusion detection systems using artificial neural networks Intelligent intrusion detection systems using artificial neural networks This paper presents a novel approach to detection of malicious network traffic using artificial neural networks suitable for use in deep packet inspection based intrusion detection systems. Experimental results using a range of typical benign network traffic data (images, dynamic link library files, and a selection of other miscellaneous files such as logs, music files, and word processing documents) and malicious shell code files sourced from the online exploit and vulnerability repository exploitdb [1], have shown that the proposed artificial neural network architecture is able to distinguish between benign and malicious network traffic accurately.The proposed artificial neural network architecture obtains an average accuracy of 98%, an average area under the receiver operator characteristic curve of 0.98, and an average false positive rate of less than 2% in repeated 10-fold cross-validation. This shows that the proposed classification technique is robust, accurate, and precise. The novel approach to malicious network traffic detection proposed in this paper has the potential to significantly enhance the utility of intrusion detection systems applied to both conventional network traffic analysis and network traffic analysis for cyber–physical systems such as smart-grids."
        },
        {
          "rank": 25,
          "score": 0.673241376876831,
          "doc_id": "JAKO199911921528980",
          "title": "다층회귀예측신경망의 음성인식성능에 관한 연구",
          "abstract": "4층구조의 다층퍼셉트론을 변형하여 3 종류의 다층회귀예측신경망을 구성하고, 예측차수, 두 은닉층의 뉴런개수, 연결세기의 초기치 및 전달함수 변화에 따른 각 망의 음성인식성능을 실험을 통해 각각 비교 분석한다. 실험결과에 의하면, 다층회귀신경망이 다층퍼셉트론에 비해 음성인식성능이 우수하다. 그리고 구조적으로는 상위은닉층의 출력을 하위은닉층으로 회귀할 때 인식성능이 가장 우수하며, 각 망 공히 상, 하위은닉층의 뉴런 10 혹은 15개, 예측차수 3 혹은 4차일 때 인식률이 양호하다. 학습시 연결세기의 초기치를 -0.5에서 0.5사이로 설정하고, 하위은닉층에서 단극성 시그모이드 전달함수를 사용할 때 인식성능이 더욱 향상된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199911921528980&target=NART&cn=JAKO199911921528980",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀예측신경망의 음성인식성능에 관한 연구 다층회귀예측신경망의 음성인식성능에 관한 연구 다층회귀예측신경망의 음성인식성능에 관한 연구 4층구조의 다층퍼셉트론을 변형하여 3 종류의 다층회귀예측신경망을 구성하고, 예측차수, 두 은닉층의 뉴런개수, 연결세기의 초기치 및 전달함수 변화에 따른 각 망의 음성인식성능을 실험을 통해 각각 비교 분석한다. 실험결과에 의하면, 다층회귀신경망이 다층퍼셉트론에 비해 음성인식성능이 우수하다. 그리고 구조적으로는 상위은닉층의 출력을 하위은닉층으로 회귀할 때 인식성능이 가장 우수하며, 각 망 공히 상, 하위은닉층의 뉴런 10 혹은 15개, 예측차수 3 혹은 4차일 때 인식률이 양호하다. 학습시 연결세기의 초기치를 -0.5에서 0.5사이로 설정하고, 하위은닉층에서 단극성 시그모이드 전달함수를 사용할 때 인식성능이 더욱 향상된다."
        },
        {
          "rank": 26,
          "score": 0.6728464365005493,
          "doc_id": "JAKO202311540154298",
          "title": "그래프 합성곱-신경망 구조 탐색 : 그래프 합성곱 신경망을 이용한 신경망 구조 탐색",
          "abstract": "본 논문은 그래프 합성곱 신경망을 이용한 신경망 구조 탐색 모델 설계를 제안한다. 딥 러닝은 블랙박스로 학습이 진행되는 특성으로 인해 설계한 모델이 최적화된 성능을 가지는 구조인지 검증하지 못하는 문제점이 존재한다. 신경망 구조 탐색 모델은 모델을 생성하는 순환 신경망과 생성된 네트워크인 합성곱 신경망으로 구성되어있다. 통상의 신경망 구조 탐색 모델은 순환신경망 계열을 사용하지만 우리는 본 논문에서 순환신경망 대신 그래프 합성곱 신경망을 사용하여 합성곱 신경망 모델을 생성하는 GC-NAS를 제안한다. 제안하는 GC-NAS는 Layer Extraction Block을 이용하여 Depth를 탐색하며 Hyper Parameter Prediction Block을 이용하여 Depth 정보를 기반으로 한 spatial, temporal 정보(hyper parameter)를 병렬적으로 탐색합니다. 따라서 Depth 정보를 반영하기 때문에 탐색 영역이 더 넓으며 Depth 정보와 병렬적 탐색을 진행함으로 모델의 탐색 영역의 목적성이 분명하기 때문에 GC-NAS대비 이론적 구조에 있어서 우위에 있다고 판단된다. GC-NAS는 그래프 합성곱 신경망 블록 및 그래프 생성 알고리즘을 통하여 기존 신경망 구조 탐색 모델에서 순환 신경망이 가지는 고차원 시간 축의 문제와 공간적 탐색의 범위 문제를 해결할 것으로 기대한다. 또한 우리는 본 논문이 제안하는 GC-NAS를 통하여 신경망 구조 탐색에 그래프 합성곱 신경망을 적용하는 연구가 활발히 이루어질 수 있는 계기가 될 수 있기를 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202311540154298&target=NART&cn=JAKO202311540154298",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "그래프 합성곱-신경망 구조 탐색 : 그래프 합성곱 신경망을 이용한 신경망 구조 탐색 그래프 합성곱-신경망 구조 탐색 : 그래프 합성곱 신경망을 이용한 신경망 구조 탐색 그래프 합성곱-신경망 구조 탐색 : 그래프 합성곱 신경망을 이용한 신경망 구조 탐색 본 논문은 그래프 합성곱 신경망을 이용한 신경망 구조 탐색 모델 설계를 제안한다. 딥 러닝은 블랙박스로 학습이 진행되는 특성으로 인해 설계한 모델이 최적화된 성능을 가지는 구조인지 검증하지 못하는 문제점이 존재한다. 신경망 구조 탐색 모델은 모델을 생성하는 순환 신경망과 생성된 네트워크인 합성곱 신경망으로 구성되어있다. 통상의 신경망 구조 탐색 모델은 순환신경망 계열을 사용하지만 우리는 본 논문에서 순환신경망 대신 그래프 합성곱 신경망을 사용하여 합성곱 신경망 모델을 생성하는 GC-NAS를 제안한다. 제안하는 GC-NAS는 Layer Extraction Block을 이용하여 Depth를 탐색하며 Hyper Parameter Prediction Block을 이용하여 Depth 정보를 기반으로 한 spatial, temporal 정보(hyper parameter)를 병렬적으로 탐색합니다. 따라서 Depth 정보를 반영하기 때문에 탐색 영역이 더 넓으며 Depth 정보와 병렬적 탐색을 진행함으로 모델의 탐색 영역의 목적성이 분명하기 때문에 GC-NAS대비 이론적 구조에 있어서 우위에 있다고 판단된다. GC-NAS는 그래프 합성곱 신경망 블록 및 그래프 생성 알고리즘을 통하여 기존 신경망 구조 탐색 모델에서 순환 신경망이 가지는 고차원 시간 축의 문제와 공간적 탐색의 범위 문제를 해결할 것으로 기대한다. 또한 우리는 본 논문이 제안하는 GC-NAS를 통하여 신경망 구조 탐색에 그래프 합성곱 신경망을 적용하는 연구가 활발히 이루어질 수 있는 계기가 될 수 있기를 기대한다."
        },
        {
          "rank": 27,
          "score": 0.6719558238983154,
          "doc_id": "JAKO202300957609703",
          "title": "딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합",
          "abstract": "위성 시계열 데이터가 증가함에 따라 원격탐사 자료의 활용도가 높아지고 있다. 시계열 자료를 통한 분석에 있어 영상 간의 상대적인 위치 정확도는 결과에 큰 영향을 미치기 때문에 이를 보정하기 위한 영상 정합 과정은 필수적으로 선행되어야 한다. 최근에는 기존 알고리즘의 성능을 상회하는 딥러닝 기반 영상 정합 연구의 사례가 증가하고 있다. 딥러닝 기반 정합 모델을 학습하기 위해서는 수 많은 영상 쌍이 필요하다. 또한, 기존 딥러닝 모델의 데이터 간의 상관도 map을 제작하고, 이에 추가적인 연산을 적용하여 정합점을 추출는데 이는 비효율적이다. 이러한 문제를 해결하기 위해 본 연구에서는 영상 정합 모델 학습을 위한 데이터 증강 기법을 구축하여 데이터셋을 제작하였고, 이를 오프셋(offset) 양 자체를 예측하는 정합 모델인 OffsetNet에 적용하여 KOMSAT-2, -3, -3A 영상 정합을 수행하였다. 모델 학습 결과, OffsetNet은 평가 데이터에 대해 높은 정확도로 오프셋 양을 예측하였고, 이를 통해 주영상과 부영상을 효과적으로 정합하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202300957609703&target=NART&cn=JAKO202300957609703",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 딥러닝 기반 OffsetNet 모델을 통한 KOMPSAT 광학 영상 정합 위성 시계열 데이터가 증가함에 따라 원격탐사 자료의 활용도가 높아지고 있다. 시계열 자료를 통한 분석에 있어 영상 간의 상대적인 위치 정확도는 결과에 큰 영향을 미치기 때문에 이를 보정하기 위한 영상 정합 과정은 필수적으로 선행되어야 한다. 최근에는 기존 알고리즘의 성능을 상회하는 딥러닝 기반 영상 정합 연구의 사례가 증가하고 있다. 딥러닝 기반 정합 모델을 학습하기 위해서는 수 많은 영상 쌍이 필요하다. 또한, 기존 딥러닝 모델의 데이터 간의 상관도 map을 제작하고, 이에 추가적인 연산을 적용하여 정합점을 추출는데 이는 비효율적이다. 이러한 문제를 해결하기 위해 본 연구에서는 영상 정합 모델 학습을 위한 데이터 증강 기법을 구축하여 데이터셋을 제작하였고, 이를 오프셋(offset) 양 자체를 예측하는 정합 모델인 OffsetNet에 적용하여 KOMSAT-2, -3, -3A 영상 정합을 수행하였다. 모델 학습 결과, OffsetNet은 평가 데이터에 대해 높은 정확도로 오프셋 양을 예측하였고, 이를 통해 주영상과 부영상을 효과적으로 정합하였다."
        },
        {
          "rank": 28,
          "score": 0.6708123683929443,
          "doc_id": "JAKO202126048601456",
          "title": "유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구",
          "abstract": "딥 러닝을 활용한 컴퓨터 비전 연구는 여전히 대규모의 학습 데이터와 컴퓨팅 파워가 필수적이며, 최적의 네트워크 구조를 도출하기 위해 많은 시행착오가 수반된다. 본 연구에서는 네트워크 최적화나 데이터를 보강하는 것과 무관하게 데이터 자체의 특성만을 고려한 CR(Confusion Rate)기반의 유사 이미지 분류 성능 향상 기법을 제안한다. 제안 방법은 유사한 이미지 데이터를 정확히 분류하기 위해 CR을 산출하고 이를 손실 함수의 가중치에 반영함으로서 딥 러닝 모델의 성능을 향상시키는 기법을 제안한다. 제안 방법은 네트워크 최적화 결과와 독립적으로 이미지 분류 성능의 향상을 가져올 수 있으며, 클래스 간의 유사성을 고려해 유사도가 높은 이미지 식별에 적합하다. 제안 방법의 평가결과 HanDB에서는 0.22%, Animal-10N에서는 3.38%의 성능향상을 보였다. 제안한 방법은 다양한 Noisy Labeled 데이터를 활용한 인공지능 연구에 기반이 될 것을 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202126048601456&target=NART&cn=JAKO202126048601456",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 유사 이미지 분류를 위한 딥 러닝 성능 향상 기법 연구 딥 러닝을 활용한 컴퓨터 비전 연구는 여전히 대규모의 학습 데이터와 컴퓨팅 파워가 필수적이며, 최적의 네트워크 구조를 도출하기 위해 많은 시행착오가 수반된다. 본 연구에서는 네트워크 최적화나 데이터를 보강하는 것과 무관하게 데이터 자체의 특성만을 고려한 CR(Confusion Rate)기반의 유사 이미지 분류 성능 향상 기법을 제안한다. 제안 방법은 유사한 이미지 데이터를 정확히 분류하기 위해 CR을 산출하고 이를 손실 함수의 가중치에 반영함으로서 딥 러닝 모델의 성능을 향상시키는 기법을 제안한다. 제안 방법은 네트워크 최적화 결과와 독립적으로 이미지 분류 성능의 향상을 가져올 수 있으며, 클래스 간의 유사성을 고려해 유사도가 높은 이미지 식별에 적합하다. 제안 방법의 평가결과 HanDB에서는 0.22%, Animal-10N에서는 3.38%의 성능향상을 보였다. 제안한 방법은 다양한 Noisy Labeled 데이터를 활용한 인공지능 연구에 기반이 될 것을 기대한다."
        },
        {
          "rank": 29,
          "score": 0.6657402515411377,
          "doc_id": "ART002885478",
          "title": "Detection of fake news using deep learning CNN–RNN based methods",
          "abstract": "Fake news is inaccurate information that is intentionally disseminated for a specific purpose. If allowed to spread, fake news can harm the political and social spheres, so several studies are conducted to detect fake news. This study uses a deep learning method with several architectures such as CNN, Bidirectional LSTM, and ResNet, combined with pre-trained word embedding, trained using four different datasets. Each data goes through a data augmentation process using the back-translation method to reduce data imbalances between classes. The results showed that the Bidirectional LSTM architecture outperformed CNN and ResNet on all tested datasets.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002885478&target=NART&cn=ART002885478",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Detection of fake news using deep learning CNN–RNN based methods Detection of fake news using deep learning CNN–RNN based methods Detection of fake news using deep learning CNN–RNN based methods Fake news is inaccurate information that is intentionally disseminated for a specific purpose. If allowed to spread, fake news can harm the political and social spheres, so several studies are conducted to detect fake news. This study uses a deep learning method with several architectures such as CNN, Bidirectional LSTM, and ResNet, combined with pre-trained word embedding, trained using four different datasets. Each data goes through a data augmentation process using the back-translation method to reduce data imbalances between classes. The results showed that the Bidirectional LSTM architecture outperformed CNN and ResNet on all tested datasets."
        },
        {
          "rank": 30,
          "score": 0.6632645726203918,
          "doc_id": "JAKO202320150299733",
          "title": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가",
          "abstract": "본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202320150299733&target=NART&cn=JAKO202320150299733",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다."
        },
        {
          "rank": 31,
          "score": 0.661675214767456,
          "doc_id": "DIKO0016929635",
          "title": "딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식",
          "abstract": "전자기전에서 레이더 파형의 변조 방식은 다른 레이더 파형 제원과 함께 적대적인 레이더를 분석하고 대응책을 마련하는 데 활용될 수 있어 사전정보 없이 레이더 파형의 변조 방식을 자동 인식하는 기술은 매우 중요한 기술이다. 특히, 미래의 전자기전에서 복합 변조 방식이 적용된 레이더 파형에 선제적으로 대응하기 위해서는 다양한 단일 및 복합 변조 방식을 인식할 수 있는 기술을 개발하는 것이 필수적이다.&amp;#xD; 본 논문에서는 31종의 단일 및 복합 변조 방식의 레이더 파형을 자동으로 인식하는 시퀀스 데이터 기반의 레이더 파형 변조 인식 기법을 제안한다. 제안하는 기법은 수신 레이더 파형을 바탕으로 시퀀스 데이터를 구성하고, 이를 딥러닝 네트워크에 입력으로 사용하여 변조 방식을 자동으로 인식한다. 이때 시퀀스 데이터로 레이더 파형 원본, 레이더 파형의 이산 푸리에 변환(Discrete Fourier transform, DFT), 레이더 파형의 자기 상관 함수 각각의 실수부 및 허수부를 이용해 구성한 데이터를 고려하며, 딥러닝 네트워크로는 CNN(Convolutional neural network), CLDNN(Convolutional long short-term deep neural network), ResNet(Residual network)-18, 34, 50을 고려한다. 컴퓨터 모의실험을 통해 시퀀스 데이터와 딥러닝 네트워크에 따른 제안하는 시퀀스 데이터 기반 레이더 파형 자동 변조 인식 기법의 인식 성능을 비교하여 레이더 파형의 DFT로 구성한 시퀀스 데이터를 딥러닝 네트워크의 입력으로 사용할 때 가장 효과적인 변조 인식이 가능함을 보인다. 또한 레이더 파형의 DFT로 구성한 시퀀스 데이터를 이용할 때, 딥러닝 네트워크로 ResNet을 사용하는 경우가 CNN, CLDNN을 사용하는 경우보다 우수한 성능을 보임을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016929635&target=NART&cn=DIKO0016929635",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 딥러닝을 이용한 시퀀스 데이터 기반의 레이더 파형 자동 변조 인식 전자기전에서 레이더 파형의 변조 방식은 다른 레이더 파형 제원과 함께 적대적인 레이더를 분석하고 대응책을 마련하는 데 활용될 수 있어 사전정보 없이 레이더 파형의 변조 방식을 자동 인식하는 기술은 매우 중요한 기술이다. 특히, 미래의 전자기전에서 복합 변조 방식이 적용된 레이더 파형에 선제적으로 대응하기 위해서는 다양한 단일 및 복합 변조 방식을 인식할 수 있는 기술을 개발하는 것이 필수적이다.&amp;#xD; 본 논문에서는 31종의 단일 및 복합 변조 방식의 레이더 파형을 자동으로 인식하는 시퀀스 데이터 기반의 레이더 파형 변조 인식 기법을 제안한다. 제안하는 기법은 수신 레이더 파형을 바탕으로 시퀀스 데이터를 구성하고, 이를 딥러닝 네트워크에 입력으로 사용하여 변조 방식을 자동으로 인식한다. 이때 시퀀스 데이터로 레이더 파형 원본, 레이더 파형의 이산 푸리에 변환(Discrete Fourier transform, DFT), 레이더 파형의 자기 상관 함수 각각의 실수부 및 허수부를 이용해 구성한 데이터를 고려하며, 딥러닝 네트워크로는 CNN(Convolutional neural network), CLDNN(Convolutional long short-term deep neural network), ResNet(Residual network)-18, 34, 50을 고려한다. 컴퓨터 모의실험을 통해 시퀀스 데이터와 딥러닝 네트워크에 따른 제안하는 시퀀스 데이터 기반 레이더 파형 자동 변조 인식 기법의 인식 성능을 비교하여 레이더 파형의 DFT로 구성한 시퀀스 데이터를 딥러닝 네트워크의 입력으로 사용할 때 가장 효과적인 변조 인식이 가능함을 보인다. 또한 레이더 파형의 DFT로 구성한 시퀀스 데이터를 이용할 때, 딥러닝 네트워크로 ResNet을 사용하는 경우가 CNN, CLDNN을 사용하는 경우보다 우수한 성능을 보임을 확인한다."
        },
        {
          "rank": 32,
          "score": 0.6613011360168457,
          "doc_id": "NART66897872",
          "title": "Image enhancement in forward imaging radar using modified apodisation technique",
          "abstract": "<P>Forward imaging radar using an UWB signal has been developed by many researchers. The image quality in this radar is not satisfactory because of the limitation of aperture length. Proposed is an image enhancement method using a modified apodisation technique in forward imaging radar. An experiment is carried out to validate the proposed method. The azimuth resolution and peak-to-sidelobe ratio are improved by the proposed method by up to about 11.7% and 10 dB, respectively.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART66897872&target=NART&cn=NART66897872",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Image enhancement in forward imaging radar using modified apodisation technique Image enhancement in forward imaging radar using modified apodisation technique Image enhancement in forward imaging radar using modified apodisation technique <P>Forward imaging radar using an UWB signal has been developed by many researchers. The image quality in this radar is not satisfactory because of the limitation of aperture length. Proposed is an image enhancement method using a modified apodisation technique in forward imaging radar. An experiment is carried out to validate the proposed method. The azimuth resolution and peak-to-sidelobe ratio are improved by the proposed method by up to about 11.7% and 10 dB, respectively.</P>"
        },
        {
          "rank": 33,
          "score": 0.661274790763855,
          "doc_id": "JAKO202220154416716",
          "title": "밀리미터파 대역 딥러닝 기반 다중빔 전송링크 성능 예측기법",
          "abstract": "차세대 와이파이 표준기술인 IEEE 802.11ay는 밀리미터파 대역에서 AP (Access Point)가 다수의 STA (Station)로 동시에 데이터를 전송하도록 MU-MIMO (Multiple User Multiple Input Multiple Output) 통신을 지원한다. 이를 위해, 주기적으로 MU-MIMO 빔포밍 훈련을 수행해야 하고, 효율적인 빔포밍 훈련을 위해서는 AP가 다수의 안테나로 다수의 빔을 동시에 전송할 때, 각 STA에서 측정되는 신호 세기를 정확히 예측하는 것이 중요하다. 본 논문에서는 딥러닝 기반 다중 빔 전송링크 성능 예측기법을 제안한다. 제안한 예측기법은 특정 실내 또는 실외 환경에서 미리 학습된 딥러닝 모델을 이용하여 다수의 빔이 동시에 전송될 때 STA에서 측정되는 신호 세기 예측의 정확성을 높인다. 이때, 딥러닝의 입력으로 개별 빔이 전송될 때 STA에서 측정되는 신호 세기 정보를 이용하고, 개별 빔의 신호 세기 정보를 얻는 과정은 이미 기존의 빔포밍 훈련에 포함되어 있으므로 정보 수집을 위해 추가적인 비용을 발생하지 않는다. 성능평가를 위해 NIST (National Institute of Standards and Technology)에 의해 개발된 Q-D 채널구현 (Quasi-Deterministic Channel Realization) 오픈소스 소프트웨어를 활용하였고 실측 데이터 기반으로 밀리미터파 채널을 구현하였다. 실험결과에서는 제안한 예측기법이 다른 비교기법보다 향상된 예측성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202220154416716&target=NART&cn=JAKO202220154416716",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "밀리미터파 대역 딥러닝 기반 다중빔 전송링크 성능 예측기법 밀리미터파 대역 딥러닝 기반 다중빔 전송링크 성능 예측기법 밀리미터파 대역 딥러닝 기반 다중빔 전송링크 성능 예측기법 차세대 와이파이 표준기술인 IEEE 802.11ay는 밀리미터파 대역에서 AP (Access Point)가 다수의 STA (Station)로 동시에 데이터를 전송하도록 MU-MIMO (Multiple User Multiple Input Multiple Output) 통신을 지원한다. 이를 위해, 주기적으로 MU-MIMO 빔포밍 훈련을 수행해야 하고, 효율적인 빔포밍 훈련을 위해서는 AP가 다수의 안테나로 다수의 빔을 동시에 전송할 때, 각 STA에서 측정되는 신호 세기를 정확히 예측하는 것이 중요하다. 본 논문에서는 딥러닝 기반 다중 빔 전송링크 성능 예측기법을 제안한다. 제안한 예측기법은 특정 실내 또는 실외 환경에서 미리 학습된 딥러닝 모델을 이용하여 다수의 빔이 동시에 전송될 때 STA에서 측정되는 신호 세기 예측의 정확성을 높인다. 이때, 딥러닝의 입력으로 개별 빔이 전송될 때 STA에서 측정되는 신호 세기 정보를 이용하고, 개별 빔의 신호 세기 정보를 얻는 과정은 이미 기존의 빔포밍 훈련에 포함되어 있으므로 정보 수집을 위해 추가적인 비용을 발생하지 않는다. 성능평가를 위해 NIST (National Institute of Standards and Technology)에 의해 개발된 Q-D 채널구현 (Quasi-Deterministic Channel Realization) 오픈소스 소프트웨어를 활용하였고 실측 데이터 기반으로 밀리미터파 채널을 구현하였다. 실험결과에서는 제안한 예측기법이 다른 비교기법보다 향상된 예측성능을 보였다."
        },
        {
          "rank": 34,
          "score": 0.6610277891159058,
          "doc_id": "NART118990104",
          "title": "Hierarchical Image Object Search Based on Deep Reinforcement Learning",
          "abstract": "<P><B>Abstract</B></P><P>Object detection technology occupies a pivotal position in the field of modern computer vision research, its purpose is to accurately locate the object human beings are looking for in the image and classify the object. With the development of deep learning technology, convolutional neural networks are widely used because of their outstanding performance in feature extraction, which greatly improves the speed and accuracy of object detection. In recent years, reinforcement learning technology has emerged in the field of artificial intelligence, showing excellent decision-making ability to deal with problems. In order to combine the perception ability of deep learning technology with the decision-making ability of reinforcement learning technology, this paper incorporate reinforcement learning into the convolutional neural network, and propose a hierarchical deep reinforcement learning object detection model.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART118990104&target=NART&cn=NART118990104",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hierarchical Image Object Search Based on Deep Reinforcement Learning Hierarchical Image Object Search Based on Deep Reinforcement Learning Hierarchical Image Object Search Based on Deep Reinforcement Learning <P><B>Abstract</B></P><P>Object detection technology occupies a pivotal position in the field of modern computer vision research, its purpose is to accurately locate the object human beings are looking for in the image and classify the object. With the development of deep learning technology, convolutional neural networks are widely used because of their outstanding performance in feature extraction, which greatly improves the speed and accuracy of object detection. In recent years, reinforcement learning technology has emerged in the field of artificial intelligence, showing excellent decision-making ability to deal with problems. In order to combine the perception ability of deep learning technology with the decision-making ability of reinforcement learning technology, this paper incorporate reinforcement learning into the convolutional neural network, and propose a hierarchical deep reinforcement learning object detection model.</P>"
        },
        {
          "rank": 35,
          "score": 0.6609971523284912,
          "doc_id": "JAKO202407064802797",
          "title": "딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가",
          "abstract": "급격한 기후 변화로 인한 어획량 감소와 양식 기술의 발전으로 양식 생산물 수요가 전세계적으로 계속해서 증가하고 있다. 그러나 이에 따른 무분별한 시설물 확장이 연안 생태계와 어족 자원 가격 책정에 악영향을 미치기 때문에, 주기적인 연안 환경 모니터링을 통한 양식시설물 관리가 필수적이다. 본 연구에서는 Sentinel-2 광학 영상과 다양한 딥러닝 기반 탐지 기법을 활용하여 경상남도의 패류 양식시설물 탐지 정확도를 분석하였다. DeepLabv3+, ResUNet++ 그리고 Attention U-Net 모델을 적용하였으며, 실험 결과 Attention U-Net 모델이 F1 score 0.8708, Intersection over Union 0.7708로 가장 우수한 탐지 성능을 보였다. 연구에서 제시한 탐지 방법론은 조류 및 부유 물질에 영향을 받는 양식시설물을 주기적으로 관측할 수 있고, 다양한 양식 품종에 적용할 수 있어 넓은 지역으로의 확장 가능성이 높다. 따라서 본 연구 방법을 통해 도출된 양식 시설물 정보는 향후 해양 공간 활용에 관한 정책 결정에 유용하게 활용할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202407064802797&target=NART&cn=JAKO202407064802797",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 급격한 기후 변화로 인한 어획량 감소와 양식 기술의 발전으로 양식 생산물 수요가 전세계적으로 계속해서 증가하고 있다. 그러나 이에 따른 무분별한 시설물 확장이 연안 생태계와 어족 자원 가격 책정에 악영향을 미치기 때문에, 주기적인 연안 환경 모니터링을 통한 양식시설물 관리가 필수적이다. 본 연구에서는 Sentinel-2 광학 영상과 다양한 딥러닝 기반 탐지 기법을 활용하여 경상남도의 패류 양식시설물 탐지 정확도를 분석하였다. DeepLabv3+, ResUNet++ 그리고 Attention U-Net 모델을 적용하였으며, 실험 결과 Attention U-Net 모델이 F1 score 0.8708, Intersection over Union 0.7708로 가장 우수한 탐지 성능을 보였다. 연구에서 제시한 탐지 방법론은 조류 및 부유 물질에 영향을 받는 양식시설물을 주기적으로 관측할 수 있고, 다양한 양식 품종에 적용할 수 있어 넓은 지역으로의 확장 가능성이 높다. 따라서 본 연구 방법을 통해 도출된 양식 시설물 정보는 향후 해양 공간 활용에 관한 정책 결정에 유용하게 활용할 수 있을 것으로 기대된다."
        },
        {
          "rank": 36,
          "score": 0.6600806713104248,
          "doc_id": "DIKO0015551607",
          "title": "데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법",
          "abstract": "오늘날 딥 러닝(Deep Learning)이란 머신러닝의 세부적인 방법과 개념&amp;#xD; 및 기법들을 통칭한다. 딥 러닝은 크게는 컴퓨터 비전(Computer vision)으&amp;#xD; 로부터 시작하여 패턴 인식(Pattern recognition), 색상 및 픽셀 복원, 추청&amp;#xD; 과 진단 등 다양한 곳에 사용이 되고 있다. 그 중 대게 객체 및 사람을 인&amp;#xD; 식하는 단계 및 추적을 더불어 대상의 안면 인식을 할 수 있는 단계까지&amp;#xD; 발달했다. 기본적인 네트워크인 컨볼루션 뉴럴 네트워크(CNN :&amp;#xD; convolutional neural network)를 시작으로 순환신경망(RNN : Recurrent&amp;#xD; Neural Network), 볼츠만 머신(RBM : Restricted Boltzmann Machine), 생&amp;#xD; 성 대립 신경망(GAN : Generative Adversarial Network) 그리고 Google의&amp;#xD; 딥 마인드에서 개발한 관계형 네트워크(RL : Relation Networks)등이 존재&amp;#xD; 한다. 이와 같은 네트워크 모델들은 다양한 강점들을 가지고 있는데 그 중&amp;#xD; 데이터를 이용한 요인 추출(feature extraction)이나 학습을 통한 결과 추론&amp;#xD; 이라고 볼 수 있다. 위와 같은 요인들을 성공적으로 학습시키기 위해서는&amp;#xD; 적합한 환경에 맞는 데이터 세트인지 판단하고, 모델에 관한 특징들을 파악&amp;#xD; 하여 가장 적합한 형태의 모델을 구현하여 효과적으로 학습 할 수 있도록&amp;#xD; 진행한다. 하지만 위 과정 중에서 데이터 세트들은 손쉽게 만들어지지 않는&amp;#xD; 다. 그 이유는 여러 다양한 방법으로 디자인되고 환경에 맞게 제작이 되어&amp;#xD; 야하기 때문이다.&amp;#xD; 본 논문에서는 기존 데이터 세트들을 이용하여 여러 다양한 방법을 이&amp;#xD; 용하여 데이터를 증강(data augmentation)시키는 연구를 진행한다. 객체 인&amp;#xD; 식 및 판단을 목적으로 딥 러닝을 학습 시킬 경우에는 이미지의 데이터 정&amp;#xD; 보들을 통해 학습을 진행한다. 학습하는 데이터 정보는 관심이 있는 영역이&amp;#xD; 나 혹은 주요 지정된 객체의 정보를 학습하는 것을 목표로 한다. 이것을 달&amp;#xD; 성하기 위해 데이터 세트를 이용하여 유용한 정보를 추출하고 학습 후 객&amp;#xD; 체에 관한 인식을 할 수 있게 진행했다. 여기에서 데이터 세트들은 대부분&amp;#xD; ILSVRC (Image Large Scale Visual Recognition Challenges) 및 PASCAL&amp;#xD; VOC (Visual Object Classes) 같은 것으로 이루어져 있다. 하지만 이와 같&amp;#xD; 은 데이터 세트는 특수한 상황이나 제한된 상황에서 사용하기가 매우 어렵&amp;#xD; 다. 상황에 맞게 데이터 세트들을 제작을 해야 하는 경우 이는 매우 많은&amp;#xD; 시간이 걸린다. 또한 만들어진 데이터 세트들을 테스트해야 하는 시간 또한&amp;#xD; 오래 걸린다. 본 논문에서는 제안된 방법을 사용하여 이를 해결한다. 기본&amp;#xD; 적인 영상처리부터 시작하여 알고리즘 및 3D 환경에서까지의 방법을 설명&amp;#xD; 한다. 이 방법들을 통해 생성된 데이터들은 성능 검증을 위해 실시간 모델&amp;#xD; 인 YOLO ver2(You Only Look Once)를 사용한다. 그리고 이미지 생성 후&amp;#xD; 분류에 사용할 CNN과 VGGNet(Very Deep Convolutional Networks for&amp;#xD; Large-Scale Image Recognition)을 이용한다. 최종적으로 제시한 방법을&amp;#xD; 통해 데이터 세트의 수를 수백 배 이상 생성했으며, 객체 간의 정확도는 5&amp;#xD; ∼ 10% 이상 증가시켰다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015551607&target=NART&cn=DIKO0015551607",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법 데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법 데이터 증강을 통한 딥 러닝 네트워크 정확도 향상 방법 오늘날 딥 러닝(Deep Learning)이란 머신러닝의 세부적인 방법과 개념&amp;#xD; 및 기법들을 통칭한다. 딥 러닝은 크게는 컴퓨터 비전(Computer vision)으&amp;#xD; 로부터 시작하여 패턴 인식(Pattern recognition), 색상 및 픽셀 복원, 추청&amp;#xD; 과 진단 등 다양한 곳에 사용이 되고 있다. 그 중 대게 객체 및 사람을 인&amp;#xD; 식하는 단계 및 추적을 더불어 대상의 안면 인식을 할 수 있는 단계까지&amp;#xD; 발달했다. 기본적인 네트워크인 컨볼루션 뉴럴 네트워크(CNN :&amp;#xD; convolutional neural network)를 시작으로 순환신경망(RNN : Recurrent&amp;#xD; Neural Network), 볼츠만 머신(RBM : Restricted Boltzmann Machine), 생&amp;#xD; 성 대립 신경망(GAN : Generative Adversarial Network) 그리고 Google의&amp;#xD; 딥 마인드에서 개발한 관계형 네트워크(RL : Relation Networks)등이 존재&amp;#xD; 한다. 이와 같은 네트워크 모델들은 다양한 강점들을 가지고 있는데 그 중&amp;#xD; 데이터를 이용한 요인 추출(feature extraction)이나 학습을 통한 결과 추론&amp;#xD; 이라고 볼 수 있다. 위와 같은 요인들을 성공적으로 학습시키기 위해서는&amp;#xD; 적합한 환경에 맞는 데이터 세트인지 판단하고, 모델에 관한 특징들을 파악&amp;#xD; 하여 가장 적합한 형태의 모델을 구현하여 효과적으로 학습 할 수 있도록&amp;#xD; 진행한다. 하지만 위 과정 중에서 데이터 세트들은 손쉽게 만들어지지 않는&amp;#xD; 다. 그 이유는 여러 다양한 방법으로 디자인되고 환경에 맞게 제작이 되어&amp;#xD; 야하기 때문이다.&amp;#xD; 본 논문에서는 기존 데이터 세트들을 이용하여 여러 다양한 방법을 이&amp;#xD; 용하여 데이터를 증강(data augmentation)시키는 연구를 진행한다. 객체 인&amp;#xD; 식 및 판단을 목적으로 딥 러닝을 학습 시킬 경우에는 이미지의 데이터 정&amp;#xD; 보들을 통해 학습을 진행한다. 학습하는 데이터 정보는 관심이 있는 영역이&amp;#xD; 나 혹은 주요 지정된 객체의 정보를 학습하는 것을 목표로 한다. 이것을 달&amp;#xD; 성하기 위해 데이터 세트를 이용하여 유용한 정보를 추출하고 학습 후 객&amp;#xD; 체에 관한 인식을 할 수 있게 진행했다. 여기에서 데이터 세트들은 대부분&amp;#xD; ILSVRC (Image Large Scale Visual Recognition Challenges) 및 PASCAL&amp;#xD; VOC (Visual Object Classes) 같은 것으로 이루어져 있다. 하지만 이와 같&amp;#xD; 은 데이터 세트는 특수한 상황이나 제한된 상황에서 사용하기가 매우 어렵&amp;#xD; 다. 상황에 맞게 데이터 세트들을 제작을 해야 하는 경우 이는 매우 많은&amp;#xD; 시간이 걸린다. 또한 만들어진 데이터 세트들을 테스트해야 하는 시간 또한&amp;#xD; 오래 걸린다. 본 논문에서는 제안된 방법을 사용하여 이를 해결한다. 기본&amp;#xD; 적인 영상처리부터 시작하여 알고리즘 및 3D 환경에서까지의 방법을 설명&amp;#xD; 한다. 이 방법들을 통해 생성된 데이터들은 성능 검증을 위해 실시간 모델&amp;#xD; 인 YOLO ver2(You Only Look Once)를 사용한다. 그리고 이미지 생성 후&amp;#xD; 분류에 사용할 CNN과 VGGNet(Very Deep Convolutional Networks for&amp;#xD; Large-Scale Image Recognition)을 이용한다. 최종적으로 제시한 방법을&amp;#xD; 통해 데이터 세트의 수를 수백 배 이상 생성했으며, 객체 간의 정확도는 5&amp;#xD; ∼ 10% 이상 증가시켰다."
        },
        {
          "rank": 37,
          "score": 0.6585665345191956,
          "doc_id": "NART104701803",
          "title": "Improved Feature Learning: A Maximum-Average-Out Deep Neural Network for the Game Go",
          "abstract": "<P>Computer game-playing programs based on deep reinforcement learning have surpassed the performance of even the best human players. However, the huge analysis space of such neural networks and their numerous parameters require extensive computing power. Hence, in this study, we aimed to increase the network learning efficiency by modifying the neural network structure, which should reduce the number of learning iterations and the required computing power. A convolutional neural network with a maximum-average-out (MAO) unit structure based on piecewise function thinking is proposed, through which features can be effectively learned and the expression ability of hidden layer features can be enhanced. To verify the performance of the MAO structure, we compared it with the ResNet18 network by applying them both to the framework of AlphaGo Zero, which was developed for playing the game Go. The two network structures were trained from scratch using a low-cost server environment. MAO unit won eight out of ten games against the ResNet18 network. The superior performance of the MAO unit compared with the ResNet18 network is significant for the further development of game algorithms that require less computing power than those currently in use.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART104701803&target=NART&cn=NART104701803",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Improved Feature Learning: A Maximum-Average-Out Deep Neural Network for the Game Go Improved Feature Learning: A Maximum-Average-Out Deep Neural Network for the Game Go Improved Feature Learning: A Maximum-Average-Out Deep Neural Network for the Game Go <P>Computer game-playing programs based on deep reinforcement learning have surpassed the performance of even the best human players. However, the huge analysis space of such neural networks and their numerous parameters require extensive computing power. Hence, in this study, we aimed to increase the network learning efficiency by modifying the neural network structure, which should reduce the number of learning iterations and the required computing power. A convolutional neural network with a maximum-average-out (MAO) unit structure based on piecewise function thinking is proposed, through which features can be effectively learned and the expression ability of hidden layer features can be enhanced. To verify the performance of the MAO structure, we compared it with the ResNet18 network by applying them both to the framework of AlphaGo Zero, which was developed for playing the game Go. The two network structures were trained from scratch using a low-cost server environment. MAO unit won eight out of ten games against the ResNet18 network. The superior performance of the MAO unit compared with the ResNet18 network is significant for the further development of game algorithms that require less computing power than those currently in use.</P>"
        },
        {
          "rank": 38,
          "score": 0.6579152941703796,
          "doc_id": "ATN0040736710",
          "title": "Edge TPU에서의 실시간성 보장을 위한 실시간 DNN 프레임워크",
          "abstract": "As deep neural networks (DNNs) have been recently deployed in many safety-critical real-time embedded systems, it has become essential to meet timing requirements that DNN inference tasks must complete their execution within given deadlines. To this end, most previous studies focus on utilizing GPU or CPU in edge computing environments to support real-time DNN tasks. Meanwhile, although Edge TPU is regarded as a next-generation AI processor for edge computing, there is a lack of studies on real-time guarantee for Edge TPU. Thus, this paper presents a novel real-time DNN framework that can satisfy the timing requirements of multiple DNN inference tasks in edge TPU environments. This framework provides 1) a cache memory allocation technique that considers the deadline of each task and 2) a scheduling technique based on model segmentation to reduce priority inversion phenomena. We evaluated the performance of our framework by using Google Coral dev board, an embedded board equipped with Edge TPU, and several image classification models. The experiment result shows that our framework can provide higher schedulability by 37% than the existing Edge TPU system.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0040736710&target=NART&cn=ATN0040736710",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Edge TPU에서의 실시간성 보장을 위한 실시간 DNN 프레임워크 Edge TPU에서의 실시간성 보장을 위한 실시간 DNN 프레임워크 Edge TPU에서의 실시간성 보장을 위한 실시간 DNN 프레임워크 As deep neural networks (DNNs) have been recently deployed in many safety-critical real-time embedded systems, it has become essential to meet timing requirements that DNN inference tasks must complete their execution within given deadlines. To this end, most previous studies focus on utilizing GPU or CPU in edge computing environments to support real-time DNN tasks. Meanwhile, although Edge TPU is regarded as a next-generation AI processor for edge computing, there is a lack of studies on real-time guarantee for Edge TPU. Thus, this paper presents a novel real-time DNN framework that can satisfy the timing requirements of multiple DNN inference tasks in edge TPU environments. This framework provides 1) a cache memory allocation technique that considers the deadline of each task and 2) a scheduling technique based on model segmentation to reduce priority inversion phenomena. We evaluated the performance of our framework by using Google Coral dev board, an embedded board equipped with Edge TPU, and several image classification models. The experiment result shows that our framework can provide higher schedulability by 37% than the existing Edge TPU system."
        },
        {
          "rank": 39,
          "score": 0.6569546461105347,
          "doc_id": "NART119879737",
          "title": "Image Enhancement Method Based on Deep Learning",
          "abstract": "<P>Image enhancement and reconstruction are the basic processing steps of many real vision systems. Their purpose is to improve the visual quality of images and provide reliable information for subsequent visual decision-making. In this paper, convolution neural network, residual neural network, and generative countermeasure network are studied. A rain fog image enhancement generative countermeasure network model structure including a scalable auxiliary generation network is proposed. The objective loss function is defined, and the periodic consistency loss and periodic perceptual consistency loss analysis are introduced. The core problem of image layering is discussed, and a layering solution framework with a deep expansion structure is proposed. This method realizes multitasking through adaptive feature learning, which has a good theoretical guarantee. This paper can not only bring a pleasant visual experience to viewers but also help to improve the performance of computer vision applications. Through image enhancement technology, the quality of low illumination image can be effectively improved, so that the image has better definition, richer texture details, and lower image noise.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART119879737&target=NART&cn=NART119879737",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Image Enhancement Method Based on Deep Learning Image Enhancement Method Based on Deep Learning Image Enhancement Method Based on Deep Learning <P>Image enhancement and reconstruction are the basic processing steps of many real vision systems. Their purpose is to improve the visual quality of images and provide reliable information for subsequent visual decision-making. In this paper, convolution neural network, residual neural network, and generative countermeasure network are studied. A rain fog image enhancement generative countermeasure network model structure including a scalable auxiliary generation network is proposed. The objective loss function is defined, and the periodic consistency loss and periodic perceptual consistency loss analysis are introduced. The core problem of image layering is discussed, and a layering solution framework with a deep expansion structure is proposed. This method realizes multitasking through adaptive feature learning, which has a good theoretical guarantee. This paper can not only bring a pleasant visual experience to viewers but also help to improve the performance of computer vision applications. Through image enhancement technology, the quality of low illumination image can be effectively improved, so that the image has better definition, richer texture details, and lower image noise.</P>"
        },
        {
          "rank": 40,
          "score": 0.6566148400306702,
          "doc_id": "NPAP13485077",
          "title": "Deep Interpretable Learning for a Rapid Response System",
          "abstract": "In-hospital cardiac arrest is a significant problem for medical systems. Although the traditional early warning systems have been widely applied, they still contain many drawbacks, such as the high false warning rate and low sensitivity. This paper proposed a strategy that involves a deep learning approach based on a novel interpretable deep tabular data learning architecture, named TabNet, for the Rapid Response System. This study has been processed and validated on a dataset collected from two hospitals of Chonnam National University, Korea, in over 10 years. The learning metrics used for the experiment are the area under the receiver operating characteristic curve score (AUROC) and the area under the precision-recall curve score (AUPRC). The experiment on a large real-time dataset shows that our method improves compared to other machine learning-based approaches.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP13485077&target=NART&cn=NPAP13485077",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep Interpretable Learning for a Rapid Response System Deep Interpretable Learning for a Rapid Response System Deep Interpretable Learning for a Rapid Response System In-hospital cardiac arrest is a significant problem for medical systems. Although the traditional early warning systems have been widely applied, they still contain many drawbacks, such as the high false warning rate and low sensitivity. This paper proposed a strategy that involves a deep learning approach based on a novel interpretable deep tabular data learning architecture, named TabNet, for the Rapid Response System. This study has been processed and validated on a dataset collected from two hospitals of Chonnam National University, Korea, in over 10 years. The learning metrics used for the experiment are the area under the receiver operating characteristic curve score (AUROC) and the area under the precision-recall curve score (AUPRC). The experiment on a large real-time dataset shows that our method improves compared to other machine learning-based approaches."
        },
        {
          "rank": 41,
          "score": 0.6562065482139587,
          "doc_id": "JAKO202210351407855",
          "title": "심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적",
          "abstract": "Nowadays, the complexity of object tracking models among hardware applications has become a more in-demand duty to complete in various indeterminable environment tracking situations with multifunctional algorithm skills. In this paper, we propose a virtual city environment using AirSim (Aerial Informatics and Robotics Simulation - AirSim, CityEnvironment) and use the DQN (Deep Q-Learning) model of deep reinforcement learning model in the virtual environment. The proposed object tracking DQN network observes the environment using a deep reinforcement learning model that receives continuous images taken by a virtual environment simulation system as input to control the operation of a virtual drone. The deep reinforcement learning model is pre-trained using various existing continuous image sets. Since the existing various continuous image sets are image data of real environments and objects, it is implemented in 3D to track virtual environments and moving objects in them.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202210351407855&target=NART&cn=JAKO202210351407855",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 심층 강화학습을 이용한 디지털트윈 및 시각적 객체 추적 Nowadays, the complexity of object tracking models among hardware applications has become a more in-demand duty to complete in various indeterminable environment tracking situations with multifunctional algorithm skills. In this paper, we propose a virtual city environment using AirSim (Aerial Informatics and Robotics Simulation - AirSim, CityEnvironment) and use the DQN (Deep Q-Learning) model of deep reinforcement learning model in the virtual environment. The proposed object tracking DQN network observes the environment using a deep reinforcement learning model that receives continuous images taken by a virtual environment simulation system as input to control the operation of a virtual drone. The deep reinforcement learning model is pre-trained using various existing continuous image sets. Since the existing various continuous image sets are image data of real environments and objects, it is implemented in 3D to track virtual environments and moving objects in them."
        },
        {
          "rank": 42,
          "score": 0.6552718877792358,
          "doc_id": "NART108328574",
          "title": "A fully open-source framework for deep learning protein real-valued distances",
          "abstract": "<P>As deep learning algorithms drive the progress in protein structure prediction, a lot remains to be studied at this merging superhighway of deep learning and protein structure prediction. Recent findings show that inter-residue distance prediction, a more granular version of the well-known contact prediction problem, is a key to predicting accurate models. However, deep learning methods that predict these distances are still in the early stages of their development. To advance these methods and develop other novel methods, a need exists for a small and representative dataset packaged for faster development and testing. In this work, we introduce protein distance net (PDNET), a framework that consists of one such representative dataset along with the scripts for training and testing deep learning methods. The framework also includes all the scripts that were used to curate the dataset, and generate the input features and distance maps. Deep learning models can also be trained and tested in a web browser using free platforms such as Google Colab. We discuss how PDNET can be used to predict contacts, distance intervals, and real-valued distances.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART108328574&target=NART&cn=NART108328574",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A fully open-source framework for deep learning protein real-valued distances A fully open-source framework for deep learning protein real-valued distances A fully open-source framework for deep learning protein real-valued distances <P>As deep learning algorithms drive the progress in protein structure prediction, a lot remains to be studied at this merging superhighway of deep learning and protein structure prediction. Recent findings show that inter-residue distance prediction, a more granular version of the well-known contact prediction problem, is a key to predicting accurate models. However, deep learning methods that predict these distances are still in the early stages of their development. To advance these methods and develop other novel methods, a need exists for a small and representative dataset packaged for faster development and testing. In this work, we introduce protein distance net (PDNET), a framework that consists of one such representative dataset along with the scripts for training and testing deep learning methods. The framework also includes all the scripts that were used to curate the dataset, and generate the input features and distance maps. Deep learning models can also be trained and tested in a web browser using free platforms such as Google Colab. We discuss how PDNET can be used to predict contacts, distance intervals, and real-valued distances.</P>"
        },
        {
          "rank": 43,
          "score": 0.6535700559616089,
          "doc_id": "JAKO200428635215914",
          "title": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구",
          "abstract": "본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200428635215914&target=NART&cn=JAKO200428635215914",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 다층회귀신경예측 모델 및 HMM 를 이용한 임베디드 음성인식 시스템 개발에 관한 연구 본 논문은 주인식기로 흔히 사용되는 HMM 인식 알고리즘을 보완하기 위한 방법으로 회귀신경회로망(Recurrent neural networks : RNN)을 적용하였다. 이 회귀신경회로망 중에서 실 시간적으로 동작이 가능하게 한 방법인 다층회귀신경예측 모델 (Multi-layer Recurrent Neural Prediction Model : MRNPM)을 사용하여 학습 및 인식기로 구현하였으며, HMM과 MRNPM 을 이용하여 Hybrid형태의 주 인식기로 설계하였다. 설계된 음성 인식 알고리즘을 잘 구별되지 않는 한국어 숫자음(13개 단어)에 대해 화자 독립형으로 인식률 테스트 한 결과 기존의 HMM인식기 보다 5%정도의 인식률 향상이 나타났다. 이 결과를 이용하여 실제 DSP(TMS320C6711) 환경 내에서 최적(인식) 코드만을 추출하여 임베디드 음성 인식 시스템을 구현하였다. 마찬가지로 임베디드 시스템의 구현 결과도 기존 단독 HMM 인식시스템보다 향상된 인식시스템을 구현할 수 있게 되었다."
        },
        {
          "rank": 44,
          "score": 0.6527067422866821,
          "doc_id": "NART132433770",
          "title": "A Deep Learning-Based Car Accident Detection Framework Using Edge and Cloud Computing",
          "abstract": "<P> The evolving technological landscape has seen a pivotal shift with the advent of edge computing, transforming various sectors, particularly accident detection. Edge computing enhances road safety by enabling realtime data processing from onboard sensors, cameras, and connected devices, addressing limitations in traditional cloudbased systems. This paper introduces a deep learning-based accident detection framework within an edge-cloud setup. Utilizing a CNN model, accidents are detected at the edge node near the data source, ensuring low latency, reduced network usage, and faster execution times. The model achieves a remarkable 95.91% accuracy.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART132433770&target=NART&cn=NART132433770",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A Deep Learning-Based Car Accident Detection Framework Using Edge and Cloud Computing A Deep Learning-Based Car Accident Detection Framework Using Edge and Cloud Computing A Deep Learning-Based Car Accident Detection Framework Using Edge and Cloud Computing <P> The evolving technological landscape has seen a pivotal shift with the advent of edge computing, transforming various sectors, particularly accident detection. Edge computing enhances road safety by enabling realtime data processing from onboard sensors, cameras, and connected devices, addressing limitations in traditional cloudbased systems. This paper introduces a deep learning-based accident detection framework within an edge-cloud setup. Utilizing a CNN model, accidents are detected at the edge node near the data source, ensuring low latency, reduced network usage, and faster execution times. The model achieves a remarkable 95.91% accuracy.</P>"
        },
        {
          "rank": 45,
          "score": 0.6520153284072876,
          "doc_id": "DIKO0017114224",
          "title": "딥러닝을 활용한 초음파 영상 개선",
          "abstract": "의료용 초음파 이미지(Clinical Ultrasonic Image) 기법은 인체 내부의 대한 영상을 비침습적, 안전적, 실시간적 있는 도구로, 의료 분야에서 사용되는 대표적인 진단 의료 영상 중 하나이다. 초고속 초음파(Ultra-fast Ultrasound)는 다수의 초음파 송수신을 통하여 상대적으로 고품질의 초음파 이미지를 얻을 수 있다. 그러나, 초음파 빔의 다양성, 복원 이미지의 해상도, 관심 영역(Region of Interest)의 크기 등은 실시간성과 절충 관계(Trade-off)에 있기에 초당 프레임 수(FPS)를 방어하기에 하드웨어적으로 어려움이 있다. 본 연구에서는 딥러닝(Deep Learning) 모델을 활용하여 단일 평면파(Single Plane-wave)의 저품질의 초음파 이미지를 고품질 다중 평면파(Multi-angle Plane-wave)의 고품질 초음파 이미지로 강화하는 것을 목표로 한다. U-Net 구조로 이루어진 딥러닝 모델은 다양한 크기의 합성곱 필터를 이용하여 복잡한 이미지의 세부 정보의 특징을 효과적으로 추출할 수 있다. 제안된 딥러닝 모델은 피크 대 잡음 비율(PSNR), 신호 대 잡음 비율(SNR), 스페클 신호 대 잡음 비율(SSNR) 등의 성능 지표를 통해 효과적인 잡음 감소 및 신호 보존을 보였으며, 상관계수(Correlation)를 통하여 강화된 이미지와 실제 이미지 간의 높은 유사성 및 정확성을 보였다. 향후 연구로는 본 작업에 영향을 줄 수 있는 세부 요인들을 조사하고, 모델 구조를 세밀하게 조정 및 최적화하여 강화되는 이미지의 품질을 더욱 향상시키고, 보다 다양한 부위에 대한 실험을 통해 일반화 성능을 확장할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017114224&target=NART&cn=DIKO0017114224",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 활용한 초음파 영상 개선 딥러닝을 활용한 초음파 영상 개선 딥러닝을 활용한 초음파 영상 개선 의료용 초음파 이미지(Clinical Ultrasonic Image) 기법은 인체 내부의 대한 영상을 비침습적, 안전적, 실시간적 있는 도구로, 의료 분야에서 사용되는 대표적인 진단 의료 영상 중 하나이다. 초고속 초음파(Ultra-fast Ultrasound)는 다수의 초음파 송수신을 통하여 상대적으로 고품질의 초음파 이미지를 얻을 수 있다. 그러나, 초음파 빔의 다양성, 복원 이미지의 해상도, 관심 영역(Region of Interest)의 크기 등은 실시간성과 절충 관계(Trade-off)에 있기에 초당 프레임 수(FPS)를 방어하기에 하드웨어적으로 어려움이 있다. 본 연구에서는 딥러닝(Deep Learning) 모델을 활용하여 단일 평면파(Single Plane-wave)의 저품질의 초음파 이미지를 고품질 다중 평면파(Multi-angle Plane-wave)의 고품질 초음파 이미지로 강화하는 것을 목표로 한다. U-Net 구조로 이루어진 딥러닝 모델은 다양한 크기의 합성곱 필터를 이용하여 복잡한 이미지의 세부 정보의 특징을 효과적으로 추출할 수 있다. 제안된 딥러닝 모델은 피크 대 잡음 비율(PSNR), 신호 대 잡음 비율(SNR), 스페클 신호 대 잡음 비율(SSNR) 등의 성능 지표를 통해 효과적인 잡음 감소 및 신호 보존을 보였으며, 상관계수(Correlation)를 통하여 강화된 이미지와 실제 이미지 간의 높은 유사성 및 정확성을 보였다. 향후 연구로는 본 작업에 영향을 줄 수 있는 세부 요인들을 조사하고, 모델 구조를 세밀하게 조정 및 최적화하여 강화되는 이미지의 품질을 더욱 향상시키고, 보다 다양한 부위에 대한 실험을 통해 일반화 성능을 확장할 수 있다."
        },
        {
          "rank": 46,
          "score": 0.6506803035736084,
          "doc_id": "JAKO202223540366088",
          "title": "이미지 학습을 위한 딥러닝 프레임워크 비교분석",
          "abstract": "딥러닝 프레임워크는 현재에도 계속해서 발전되어 가고 있으며, 다양한 프레임워크들이 존재한다. 딥러닝의 대표적인 프레임워크는 TensorFlow, PyTorch, Keras 등이 있다. 딥러님 프레임워크는 이미지 학습을 통해 이미지 분류에서의 최적화 모델을 이용한다. 본 논문에서는 딥러닝 이미지 인식 분야에서 가장 많이 사용하고 있는 TensorFlow와 PyTorch 프레임워크를 활용하여 이미지 학습을 진행하였으며, 이 과정에서 도출한 결과를 비교 분석하여 최적화된 프레임워크을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202223540366088&target=NART&cn=JAKO202223540366088",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "이미지 학습을 위한 딥러닝 프레임워크 비교분석 이미지 학습을 위한 딥러닝 프레임워크 비교분석 이미지 학습을 위한 딥러닝 프레임워크 비교분석 딥러닝 프레임워크는 현재에도 계속해서 발전되어 가고 있으며, 다양한 프레임워크들이 존재한다. 딥러닝의 대표적인 프레임워크는 TensorFlow, PyTorch, Keras 등이 있다. 딥러님 프레임워크는 이미지 학습을 통해 이미지 분류에서의 최적화 모델을 이용한다. 본 논문에서는 딥러닝 이미지 인식 분야에서 가장 많이 사용하고 있는 TensorFlow와 PyTorch 프레임워크를 활용하여 이미지 학습을 진행하였으며, 이 과정에서 도출한 결과를 비교 분석하여 최적화된 프레임워크을 알 수 있었다."
        },
        {
          "rank": 47,
          "score": 0.6495199203491211,
          "doc_id": "ART002606556",
          "title": "Influence of random topology in artificial neural networks: A survey",
          "abstract": "Due to the fully-connected complex structure of Artificial Neural Networks (ANNs), systems based on ANN may consume much computational time, energy and space. Therefore, intense research has been recently centered on changing the topology and design of ANNs to obtain high performance. To explore the influence of network structure on ANNs complex systems topologies have been applied in these networks to have more efficient and less complex structures while they are more similar to biological systems at the same time. In this paper, the methodology and results of some recent papers are summarized and discussed in which the authors investigated the efficacy of random complex networks on the performance of Hopfield associative memory and multi-layer ANNs compared with ANNs with small-world, scale-free and regular structures.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002606556&target=NART&cn=ART002606556",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Influence of random topology in artificial neural networks: A survey Influence of random topology in artificial neural networks: A survey Influence of random topology in artificial neural networks: A survey Due to the fully-connected complex structure of Artificial Neural Networks (ANNs), systems based on ANN may consume much computational time, energy and space. Therefore, intense research has been recently centered on changing the topology and design of ANNs to obtain high performance. To explore the influence of network structure on ANNs complex systems topologies have been applied in these networks to have more efficient and less complex structures while they are more similar to biological systems at the same time. In this paper, the methodology and results of some recent papers are summarized and discussed in which the authors investigated the efficacy of random complex networks on the performance of Hopfield associative memory and multi-layer ANNs compared with ANNs with small-world, scale-free and regular structures."
        },
        {
          "rank": 48,
          "score": 0.6494013071060181,
          "doc_id": "ART003167534",
          "title": "Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior",
          "abstract": "Thick pinhole imaging system is widely used for diagnosing intense pulsed radiation sources. However, owing to the trade-off among spatial resolution, field of view (FOV) and signal-to-noise ratio (SNR), the imaging system normally falls short in achieving high-precision spatial diagnosis. In this paper, we propose an unsupervised deep learning method for single image super-resolution (SISR) of the thick pinhole imaging system. The point spread function (PSF) of the imaging system is obtained by analytical calculation and Monte Carlo simulation methods, and the mathematical model of the imaging system is established using a linear equation. To solve the ill-posed inverse problem, we adopt randomly initialized deep convolutional neural networks (DCNNs) as an image prior without pre-training, which is named deep image prior (DIP). The results demonstrate that, by utilizing the SISR technique to increase the number of pixels in reconstructed images, the proposed DIP algorithm can mitigate the spatial resolution degradation caused by an insufficient spatial sampling frequency of the camera. Compared with various classical algorithms, the proposed DIP algorithm exhibits superior capabilities in recovering highfrequency signals and suppressing ringing artifacts. Furthermore, the convergence and robustness of the proposed DIP algorithm under different random seeds and SNR conditions are also verified.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART003167534&target=NART&cn=ART003167534",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Unsupervised deep learning method for single image super-resolution of the thick pinhole imaging system using deep image prior Thick pinhole imaging system is widely used for diagnosing intense pulsed radiation sources. However, owing to the trade-off among spatial resolution, field of view (FOV) and signal-to-noise ratio (SNR), the imaging system normally falls short in achieving high-precision spatial diagnosis. In this paper, we propose an unsupervised deep learning method for single image super-resolution (SISR) of the thick pinhole imaging system. The point spread function (PSF) of the imaging system is obtained by analytical calculation and Monte Carlo simulation methods, and the mathematical model of the imaging system is established using a linear equation. To solve the ill-posed inverse problem, we adopt randomly initialized deep convolutional neural networks (DCNNs) as an image prior without pre-training, which is named deep image prior (DIP). The results demonstrate that, by utilizing the SISR technique to increase the number of pixels in reconstructed images, the proposed DIP algorithm can mitigate the spatial resolution degradation caused by an insufficient spatial sampling frequency of the camera. Compared with various classical algorithms, the proposed DIP algorithm exhibits superior capabilities in recovering highfrequency signals and suppressing ringing artifacts. Furthermore, the convergence and robustness of the proposed DIP algorithm under different random seeds and SNR conditions are also verified."
        },
        {
          "rank": 49,
          "score": 0.6486415863037109,
          "doc_id": "DIKO0015644673",
          "title": "M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지",
          "abstract": "해상 교통량의 증가로 인해 해상 선박관리의 필요성이 늘어남에 따라 선박을 탐지하기 위한 연구들이 꾸준히 수행되어왔다. 특히 위성레이더 영상은 시간과 기후에 영향을 받지 않고 촬영할 수 있다는 장점으로 인해 선박탐지를 위한 많은 연구에서 활용되어왔다. 최근에는 딥러닝 기법의 발전으로 인해 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구들이 꾸준히 수행되고 있다. 그런데 위성레이더 영상은 값의 분포범위가 매우 넓고, 많은 스펙클 노이즈가 존재한다. 이러한 요소들은 딥러닝 모델의 학습에 부정적인 영향을 끼칠 수 있으므로 전처리를 통해 해당 요소들을 저감해줄 필요가 있다. 본 연구에서는 전처리된 위성레이더 영상으로부터 딥러닝 선박탐지를 수행하고, 영상의 전처리가 딥러닝 선박탐지에 미치는 요소를 비교분석 하고자 한다.&amp;#xD; 본 연구를 위해 TerraSAR-X와 COSMO-SkyMed 위성레이더 영상을 이용했다. 영상을 딥러닝 학습에 이용하기 전에 먼저 총 세 가지 다른 방법으로 전처리를 수행했다. 첫 번째는 위성레이더 영상에서 강도 값만을 추출한 강도 영상을 생성하는 방법이다. 강도 영상은 값의 범위가 매우 넓을 뿐만 아니라 많은 스펙클 노이즈를 가지고 있다. 두 번째는 강도영상에서 값의 단위를 데시벨로 변환한 데시벨 영상을 생성하는 방법이다. 데시벨 영상은 강도영상과 마찬가지로 많은 스펙클 노이즈를 가지고 있으나 값의 범위가 줄어들어, 더 안정적인 학습을 할 수 있다. 세 번째는 본 연구에서 제안하는 위성레이더 전처리방법으로써, 강도차분과 거칠기영상을 생성하는 방법이다. 두 영상은 중간값 필터링을 이용해 스펙클 노이즈를 줄이고, 값의 분포 대역을 좁힘으로써 빠른 학습이 가능하다.&amp;#xD; 각 전처리된 위성레이더 영상을 이용해 딥러닝 학습을 하기 위해 본 연구에서는 M2Det 객체탐지 모델을 사용했다. 객체탐지 모델을 학습시킨 뒤 테스트 영상을 이용해 선박탐지를 수행했으며, 테스트 결과는 정밀도(Precision), 재현율(Recall)을 이용해 나타냈으며, 두 지수를 하나의 값으로 표현하기 위해 AP(Average Precision)와 F1 점수(F1-score)를 이용해 나타냈다. 각 영상의 정밀도, 재현율, AP, F1 점수는 강도 영상 93.18%, 91.11%, 89.78%, 92.13%, 데시벨 영상 94.16%, 94.16%, 92.34%, 94.16%, 강도차분과 거칠기 영상 97.40%, 94.94%, 95.55%, 96.15%로 계산되었다. 강도 영상을 이용한 경우 미탐지와 오탐지 선박이 많았으며, 전처리된 영상을 이용한 경우 강도 영상에 비해 미탐지와 오탐지 선박이 줄어든 것을 확인할 수 있었다. 데시벨 영상과 강도차분, 거칠기 영상의 결과를 비교했을 때, 두 영상의 오탐지율은 유사했다. 하지만 강도차분, 거칠기 영상을 이용했을 때 강도 영상에 비해 미탐지 선박의 비율이 4% 줄어든 것을 확인할 수 있었다. 이 결과를 통해 위성레이더 영상을 전처리함으로써 딥러닝 학습을 돕고 선박탐지 결과를 향상시킬 수 있다는 것을 알 수 있다. 본 연구결과는 향후 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구의 발전에 이바지할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015644673&target=NART&cn=DIKO0015644673",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 해상 교통량의 증가로 인해 해상 선박관리의 필요성이 늘어남에 따라 선박을 탐지하기 위한 연구들이 꾸준히 수행되어왔다. 특히 위성레이더 영상은 시간과 기후에 영향을 받지 않고 촬영할 수 있다는 장점으로 인해 선박탐지를 위한 많은 연구에서 활용되어왔다. 최근에는 딥러닝 기법의 발전으로 인해 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구들이 꾸준히 수행되고 있다. 그런데 위성레이더 영상은 값의 분포범위가 매우 넓고, 많은 스펙클 노이즈가 존재한다. 이러한 요소들은 딥러닝 모델의 학습에 부정적인 영향을 끼칠 수 있으므로 전처리를 통해 해당 요소들을 저감해줄 필요가 있다. 본 연구에서는 전처리된 위성레이더 영상으로부터 딥러닝 선박탐지를 수행하고, 영상의 전처리가 딥러닝 선박탐지에 미치는 요소를 비교분석 하고자 한다.&amp;#xD; 본 연구를 위해 TerraSAR-X와 COSMO-SkyMed 위성레이더 영상을 이용했다. 영상을 딥러닝 학습에 이용하기 전에 먼저 총 세 가지 다른 방법으로 전처리를 수행했다. 첫 번째는 위성레이더 영상에서 강도 값만을 추출한 강도 영상을 생성하는 방법이다. 강도 영상은 값의 범위가 매우 넓을 뿐만 아니라 많은 스펙클 노이즈를 가지고 있다. 두 번째는 강도영상에서 값의 단위를 데시벨로 변환한 데시벨 영상을 생성하는 방법이다. 데시벨 영상은 강도영상과 마찬가지로 많은 스펙클 노이즈를 가지고 있으나 값의 범위가 줄어들어, 더 안정적인 학습을 할 수 있다. 세 번째는 본 연구에서 제안하는 위성레이더 전처리방법으로써, 강도차분과 거칠기영상을 생성하는 방법이다. 두 영상은 중간값 필터링을 이용해 스펙클 노이즈를 줄이고, 값의 분포 대역을 좁힘으로써 빠른 학습이 가능하다.&amp;#xD; 각 전처리된 위성레이더 영상을 이용해 딥러닝 학습을 하기 위해 본 연구에서는 M2Det 객체탐지 모델을 사용했다. 객체탐지 모델을 학습시킨 뒤 테스트 영상을 이용해 선박탐지를 수행했으며, 테스트 결과는 정밀도(Precision), 재현율(Recall)을 이용해 나타냈으며, 두 지수를 하나의 값으로 표현하기 위해 AP(Average Precision)와 F1 점수(F1-score)를 이용해 나타냈다. 각 영상의 정밀도, 재현율, AP, F1 점수는 강도 영상 93.18%, 91.11%, 89.78%, 92.13%, 데시벨 영상 94.16%, 94.16%, 92.34%, 94.16%, 강도차분과 거칠기 영상 97.40%, 94.94%, 95.55%, 96.15%로 계산되었다. 강도 영상을 이용한 경우 미탐지와 오탐지 선박이 많았으며, 전처리된 영상을 이용한 경우 강도 영상에 비해 미탐지와 오탐지 선박이 줄어든 것을 확인할 수 있었다. 데시벨 영상과 강도차분, 거칠기 영상의 결과를 비교했을 때, 두 영상의 오탐지율은 유사했다. 하지만 강도차분, 거칠기 영상을 이용했을 때 강도 영상에 비해 미탐지 선박의 비율이 4% 줄어든 것을 확인할 수 있었다. 이 결과를 통해 위성레이더 영상을 전처리함으로써 딥러닝 학습을 돕고 선박탐지 결과를 향상시킬 수 있다는 것을 알 수 있다. 본 연구결과는 향후 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구의 발전에 이바지할 수 있을 것으로 기대된다."
        },
        {
          "rank": 50,
          "score": 0.6471964716911316,
          "doc_id": "NART111939444",
          "title": "Review of deep learning for photoacoustic imaging",
          "abstract": "<P>Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging.</P><P>The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART111939444&target=NART&cn=NART111939444",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Review of deep learning for photoacoustic imaging Review of deep learning for photoacoustic imaging Review of deep learning for photoacoustic imaging <P>Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging.</P><P>The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.</P>"
        }
      ]
    }
  ],
  "meta": {
    "model": "gemini-2.5-flash",
    "temperature": 0.2
  }
}