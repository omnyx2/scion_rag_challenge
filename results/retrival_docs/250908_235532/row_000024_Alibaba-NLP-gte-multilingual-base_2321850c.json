{
  "id": "row_000024",
  "model_name": "Alibaba-NLP/gte-multilingual-base",
  "timestamp_kst": "2025-09-08T23:55:35.506827+09:00",
  "trial_id": "2321850c",
  "queries": [
    {
      "query": "Can you outline the key deep learning and machine learning models evaluated for electricity demand prediction and their comparative performance in terms of MSE and MAPE?",
      "query_meta": {
        "type": "original"
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.7784646153450012,
          "doc_id": "ART002787934",
          "title": "Effective Electricity Demand Prediction via Deep Learning",
          "abstract": "Prediction of electricity demand in homes and buildings can be used to optimize an energy management system by decreasing energy wastage. A time-series prediction system is still a challenging problem in machine learning and deep learning. Our main idea is to compare three methods. For this work, we analyzed an electricity demand prediction system using the current state-of-the-art deep-learning methods with a machine-learning method: error correction with multi-layer perceptron (eMLP) structure, autoregressive integrated moving average (ARIMA) structure, and a proposed structure named CNN-LSTM. For this, we measured and collected electricity demand data in Germany for home appliances. We report the prediction accuracy in terms of the mean square error (MSE) and mean absolute percentage error (MAPE). The experimental result indicates that CNN-LSTM outperforms eMLP and ARIMA in accuracy.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002787934&target=NART&cn=ART002787934",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Effective Electricity Demand Prediction via Deep Learning Effective Electricity Demand Prediction via Deep Learning Effective Electricity Demand Prediction via Deep Learning Prediction of electricity demand in homes and buildings can be used to optimize an energy management system by decreasing energy wastage. A time-series prediction system is still a challenging problem in machine learning and deep learning. Our main idea is to compare three methods. For this work, we analyzed an electricity demand prediction system using the current state-of-the-art deep-learning methods with a machine-learning method: error correction with multi-layer perceptron (eMLP) structure, autoregressive integrated moving average (ARIMA) structure, and a proposed structure named CNN-LSTM. For this, we measured and collected electricity demand data in Germany for home appliances. We report the prediction accuracy in terms of the mean square error (MSE) and mean absolute percentage error (MAPE). The experimental result indicates that CNN-LSTM outperforms eMLP and ARIMA in accuracy."
        },
        {
          "rank": 2,
          "score": 0.766948401927948,
          "doc_id": "NART113695778",
          "title": "Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning",
          "abstract": "<P>Due to severe climate change impact on electricity consumption, as well as new trends in smart grids (such as the use of renewable resources and the advent of prosumers and energy commons), medium-term and long-term electricity load forecasting has become a crucial need. Such forecasts are necessary to support the plans and decisions related to the capacity evaluation of centralized and decentralized power generation systems, demand response strategies, and controlling the operation. To address this problem, the main objective of this study is to develop and compare precise district level models for predicting the electrical load demand based on machine learning techniques including support vector machine (SVM) and Random Forest (RF), and deep learning methods such as non-linear auto-regressive exogenous (NARX) neural network and recurrent neural networks (Long Short-Term Memory-LSTM). A dataset including nine years of historical load demand for Bruce County, Ontario, Canada, fused with the climatic information (temperature and wind speed) are used to train the models after completing the preprocessing and cleaning stages. The results show that by employing deep learning, the model could predict the load demand more accurately than SVM and RF, with an R-Squared of about 0.93-0.96 and Mean Absolute Percentage Error (MAPE) of about 4-10%. The model can be used not only by the municipalities as well as utility companies and power distributors in the management and expansion of electricity grids; but also by the households to make decisions on the adoption of home- and district-scale renewable energy technologies.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART113695778&target=NART&cn=NART113695778",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning <P>Due to severe climate change impact on electricity consumption, as well as new trends in smart grids (such as the use of renewable resources and the advent of prosumers and energy commons), medium-term and long-term electricity load forecasting has become a crucial need. Such forecasts are necessary to support the plans and decisions related to the capacity evaluation of centralized and decentralized power generation systems, demand response strategies, and controlling the operation. To address this problem, the main objective of this study is to develop and compare precise district level models for predicting the electrical load demand based on machine learning techniques including support vector machine (SVM) and Random Forest (RF), and deep learning methods such as non-linear auto-regressive exogenous (NARX) neural network and recurrent neural networks (Long Short-Term Memory-LSTM). A dataset including nine years of historical load demand for Bruce County, Ontario, Canada, fused with the climatic information (temperature and wind speed) are used to train the models after completing the preprocessing and cleaning stages. The results show that by employing deep learning, the model could predict the load demand more accurately than SVM and RF, with an R-Squared of about 0.93-0.96 and Mean Absolute Percentage Error (MAPE) of about 4-10%. The model can be used not only by the municipalities as well as utility companies and power distributors in the management and expansion of electricity grids; but also by the households to make decisions on the adoption of home- and district-scale renewable energy technologies.</P>"
        },
        {
          "rank": 3,
          "score": 0.7661370038986206,
          "doc_id": "JAKO201726163356540",
          "title": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델",
          "abstract": "본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201726163356540&target=NART&cn=JAKO201726163356540",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다."
        },
        {
          "rank": 4,
          "score": 0.7346385717391968,
          "doc_id": "NPAP13485205",
          "title": "머신러닝 및 딥러닝 모델의 스태킹 앙상블을 이용한 단기 전력수요 예측에 관한 연구",
          "abstract": "전력수요는 월, 요일 및 시간의 계절성(Seasonality)을 보이는 데이터이다. 각 계절성에 따라 특성이 다르기 때문에, 전력수요를 예측하기 위해서는 계절성의 특성을 고려한 다양한 모델을 선정하고, 병합하는 방법이 필요하다. 본 연구에서는 전력수요의 계절성을 고려한 다양한 예측모델을 병합하여 이용할 수 있도록 스태킹 앙상블 적용하고 실험결과를 기술한다. 또한, 162개 도시의 기상 데이터와 인구 데이터를 예측에 이용하는 방법, Regression 모델과 Time-series모델에 입력하는 특징(Feature)의 전처리 방법, 베이지안 최적화를 이용한 머신러닝 및 딥러닝 모델의 하이퍼파라메터 최적화 방법을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP13485205&target=NART&cn=NPAP13485205",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 및 딥러닝 모델의 스태킹 앙상블을 이용한 단기 전력수요 예측에 관한 연구 머신러닝 및 딥러닝 모델의 스태킹 앙상블을 이용한 단기 전력수요 예측에 관한 연구 머신러닝 및 딥러닝 모델의 스태킹 앙상블을 이용한 단기 전력수요 예측에 관한 연구 전력수요는 월, 요일 및 시간의 계절성(Seasonality)을 보이는 데이터이다. 각 계절성에 따라 특성이 다르기 때문에, 전력수요를 예측하기 위해서는 계절성의 특성을 고려한 다양한 모델을 선정하고, 병합하는 방법이 필요하다. 본 연구에서는 전력수요의 계절성을 고려한 다양한 예측모델을 병합하여 이용할 수 있도록 스태킹 앙상블 적용하고 실험결과를 기술한다. 또한, 162개 도시의 기상 데이터와 인구 데이터를 예측에 이용하는 방법, Regression 모델과 Time-series모델에 입력하는 특징(Feature)의 전처리 방법, 베이지안 최적화를 이용한 머신러닝 및 딥러닝 모델의 하이퍼파라메터 최적화 방법을 제시한다."
        },
        {
          "rank": 5,
          "score": 0.7331571578979492,
          "doc_id": "DIKO0017291669",
          "title": "다중 변수 융합을 통한 Hybrid Gated Fusion 기반 딥러닝 모델을 활용한 전력 수요 예측",
          "abstract": "최근 환경오염으로 인한 기후 이상 현상, 산업구조의 디지털 전환, 에너지 정책 및 인구 변화 등 복합적인 외부 요인으로 인해 전력수요는 과거보다 더 복잡하고 예측이 어려운 양상을 띄고 있다. &amp;#xD; 특히, 우리나라 전력 시장은 하루 전 수요예측 데이터를 기반으로 전력 공급이 이루어지기 때문에, 예측의 정확도가 낮을 경우 불필요한 전력 생산 또는 공급 부족같은 문제로 이어질 수 있다. 이는 발전 비용 및 출력 제어 비용 낭비, 급전 비용 상승으로 인한 전력 요금 인상, 정전 위험 등 많은 손실을 초래하므로 보다 정밀하고 신뢰성 높은 예측 모델이 필요하다. &amp;#xD; 이에 본 연구는 전력 수요 예측의 정확도 한계를 극복하고 전력 수요 패턴의 변동성에 능동적으로 대응하기 위해, 다양한 외생 변수를 통합하고 이를 효과적으로 학습할 수 있는 Hybrid 딥러닝 모델을 제안하고자 한다. &amp;#xD; 특히 시계열 데이터 흐름을 잘 반영하는 LSTM(Long Short-Term Memory)과 전역적 패턴 학습에 특화된 Transformer 의 장점을 동시에 활용하기 위해 두 모델의 구조를 통합한 Hybrid 모델을 구성하였으며, 여러가지 Fusion 기법을 적용하여 두 모델 간 정보를 효과적으로 조합하여 예측의 정확성과 안정성을 동시에 향상시켰다. &amp;#xD; 예측 모델은 전력 사용량, 캘린더 정보, 기온 민감도(CDD/HDD), 대중교통 이용량 등 6 개 외생 변수를 중심으로 설계된 5 가지 시나리오에 따라 학습되었으며, MAE, RMSE, MAPE를 기준으로 성능을 비교하였다.&amp;#xD; 단일 모델은 외생 변수가 없는 경우 높은 오차율을 보인 반면, Hybrid 모델은 모든 시나리오에서 우수한 예측 성능을 보였다. 특히 Gated Fusion 기반 Hybrid 모델은 최종 시나리오에서 MAPE 4.3%로 가장 낮은 오차를 기록하였다. 추가적으로 수행한 잔차 분석, 정규성 검정, 대응표본 t-검정 결과를 통해 해당 모델의 통계적 유의성과 예측 신뢰도를 뒷받침하였다. &amp;#xD; 결론적으로 본 연구는 전력 수요 예측에서 외생 변수 융합과 Hybrid 모델 구조가 실질적인 예측 성능 향상에 기여함을 입증하였으며, 향후 에너지 수급 계획 및 정책 수립 등에 실무적으로 적용 가능한 정교한 수요 예측 모델 개발의 기반을 제공하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017291669&target=NART&cn=DIKO0017291669",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다중 변수 융합을 통한 Hybrid Gated Fusion 기반 딥러닝 모델을 활용한 전력 수요 예측 다중 변수 융합을 통한 Hybrid Gated Fusion 기반 딥러닝 모델을 활용한 전력 수요 예측 다중 변수 융합을 통한 Hybrid Gated Fusion 기반 딥러닝 모델을 활용한 전력 수요 예측 최근 환경오염으로 인한 기후 이상 현상, 산업구조의 디지털 전환, 에너지 정책 및 인구 변화 등 복합적인 외부 요인으로 인해 전력수요는 과거보다 더 복잡하고 예측이 어려운 양상을 띄고 있다. &amp;#xD; 특히, 우리나라 전력 시장은 하루 전 수요예측 데이터를 기반으로 전력 공급이 이루어지기 때문에, 예측의 정확도가 낮을 경우 불필요한 전력 생산 또는 공급 부족같은 문제로 이어질 수 있다. 이는 발전 비용 및 출력 제어 비용 낭비, 급전 비용 상승으로 인한 전력 요금 인상, 정전 위험 등 많은 손실을 초래하므로 보다 정밀하고 신뢰성 높은 예측 모델이 필요하다. &amp;#xD; 이에 본 연구는 전력 수요 예측의 정확도 한계를 극복하고 전력 수요 패턴의 변동성에 능동적으로 대응하기 위해, 다양한 외생 변수를 통합하고 이를 효과적으로 학습할 수 있는 Hybrid 딥러닝 모델을 제안하고자 한다. &amp;#xD; 특히 시계열 데이터 흐름을 잘 반영하는 LSTM(Long Short-Term Memory)과 전역적 패턴 학습에 특화된 Transformer 의 장점을 동시에 활용하기 위해 두 모델의 구조를 통합한 Hybrid 모델을 구성하였으며, 여러가지 Fusion 기법을 적용하여 두 모델 간 정보를 효과적으로 조합하여 예측의 정확성과 안정성을 동시에 향상시켰다. &amp;#xD; 예측 모델은 전력 사용량, 캘린더 정보, 기온 민감도(CDD/HDD), 대중교통 이용량 등 6 개 외생 변수를 중심으로 설계된 5 가지 시나리오에 따라 학습되었으며, MAE, RMSE, MAPE를 기준으로 성능을 비교하였다.&amp;#xD; 단일 모델은 외생 변수가 없는 경우 높은 오차율을 보인 반면, Hybrid 모델은 모든 시나리오에서 우수한 예측 성능을 보였다. 특히 Gated Fusion 기반 Hybrid 모델은 최종 시나리오에서 MAPE 4.3%로 가장 낮은 오차를 기록하였다. 추가적으로 수행한 잔차 분석, 정규성 검정, 대응표본 t-검정 결과를 통해 해당 모델의 통계적 유의성과 예측 신뢰도를 뒷받침하였다. &amp;#xD; 결론적으로 본 연구는 전력 수요 예측에서 외생 변수 융합과 Hybrid 모델 구조가 실질적인 예측 성능 향상에 기여함을 입증하였으며, 향후 에너지 수급 계획 및 정책 수립 등에 실무적으로 적용 가능한 정교한 수요 예측 모델 개발의 기반을 제공하고자 한다."
        },
        {
          "rank": 6,
          "score": 0.7307977676391602,
          "doc_id": "DIKO0015771393",
          "title": "딥러닝 기술을 이용한 전력 수요 예측 방법",
          "abstract": "정확한 전력 수요 예측은 전력수급시스템의 안정을 위해 중요하다. 또한, 불필요한 비용 및 재난 안전사고를 최소화하기 위해 필수적이다. 그러나 전력 수요는 기후, 시간대, 공휴일 등의 영향을 받아 변동성이 있으며 비선형적인 특성이 있기에 예측에 어려움을 겪는다.&amp;#xD; 본 논문에서는 전력 수요 예측 과정에서 발생하는 불확실성을 최소화하기 위한 전력 수요 예측 모델을 제시한다. 국내 전력 공급업체 중 하나인 ㈜JB의 발전기 전력 데이터를 사용해 발전기 전력 수요 예측 모델을 구현하였으며, AMI(Advanced Metering Infrastructure) 데이터를 사용해 AMI 전력 수요 예측 모델을 구현하였다. &amp;#xD; 발전기 전력 수요 예측에는 전력 수요량에 영향을 줄 수 있는 기상 변수와 공휴일 변수 등을 사용한다. 그리고 LSTM에 Attention Mechanism을 추가한 알고리즘을 사용해 예측 모델을 구현한다. 실험을 통해 성능을 측정한 결과, 제안한 모델이 가장 낮은 평균 제곱근 오차와 절대 평균 백분율 오차를 가지며 우수한 성능을 보인다. 또한, 결과에 영향을 미치는 중요 변수를 확인함으로써 설명이 가능한 모델을 제안한다. &amp;#xD; AMI 전력 수요 예측은 전체 71세대의 전력 사용량을 HDBSCAN 클러스터링을 통해 분석한다. 그리고 클러스터별로 Bayesian Optimization 기법을 적용해 LSTM 알고리즘의 최적 하이퍼 파라미터를 선정한다. 선정한 하이퍼 파라미터를 적용한 클러스터별 예측 모델을 구현한다. 실험을 통해 성능을 측정한 결과, 제안한 모델이 기본 하이퍼 파라미터를 적용한 모델보다 낮은 평균 제곱근 오차를 가지며 우수한 성능을 보인다.&amp;#xD; 본 연구에서 제안하는 방법을 사용했을 때 더욱 정확한 전력 수요 예측을 기대할 수 있으며, 상황에 따른 전력 수요량 예측이 가능하므로 안정적인 전력의 공급, 전력 시스템의 효율적인 운영관리 및 안전 운행을 기대할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015771393&target=NART&cn=DIKO0015771393",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기술을 이용한 전력 수요 예측 방법 딥러닝 기술을 이용한 전력 수요 예측 방법 딥러닝 기술을 이용한 전력 수요 예측 방법 정확한 전력 수요 예측은 전력수급시스템의 안정을 위해 중요하다. 또한, 불필요한 비용 및 재난 안전사고를 최소화하기 위해 필수적이다. 그러나 전력 수요는 기후, 시간대, 공휴일 등의 영향을 받아 변동성이 있으며 비선형적인 특성이 있기에 예측에 어려움을 겪는다.&amp;#xD; 본 논문에서는 전력 수요 예측 과정에서 발생하는 불확실성을 최소화하기 위한 전력 수요 예측 모델을 제시한다. 국내 전력 공급업체 중 하나인 ㈜JB의 발전기 전력 데이터를 사용해 발전기 전력 수요 예측 모델을 구현하였으며, AMI(Advanced Metering Infrastructure) 데이터를 사용해 AMI 전력 수요 예측 모델을 구현하였다. &amp;#xD; 발전기 전력 수요 예측에는 전력 수요량에 영향을 줄 수 있는 기상 변수와 공휴일 변수 등을 사용한다. 그리고 LSTM에 Attention Mechanism을 추가한 알고리즘을 사용해 예측 모델을 구현한다. 실험을 통해 성능을 측정한 결과, 제안한 모델이 가장 낮은 평균 제곱근 오차와 절대 평균 백분율 오차를 가지며 우수한 성능을 보인다. 또한, 결과에 영향을 미치는 중요 변수를 확인함으로써 설명이 가능한 모델을 제안한다. &amp;#xD; AMI 전력 수요 예측은 전체 71세대의 전력 사용량을 HDBSCAN 클러스터링을 통해 분석한다. 그리고 클러스터별로 Bayesian Optimization 기법을 적용해 LSTM 알고리즘의 최적 하이퍼 파라미터를 선정한다. 선정한 하이퍼 파라미터를 적용한 클러스터별 예측 모델을 구현한다. 실험을 통해 성능을 측정한 결과, 제안한 모델이 기본 하이퍼 파라미터를 적용한 모델보다 낮은 평균 제곱근 오차를 가지며 우수한 성능을 보인다.&amp;#xD; 본 연구에서 제안하는 방법을 사용했을 때 더욱 정확한 전력 수요 예측을 기대할 수 있으며, 상황에 따른 전력 수요량 예측이 가능하므로 안정적인 전력의 공급, 전력 시스템의 효율적인 운영관리 및 안전 운행을 기대할 수 있다."
        },
        {
          "rank": 7,
          "score": 0.7254918813705444,
          "doc_id": "NART125164824",
          "title": "Electricity Consumption Prediction Using Machine Learning",
          "abstract": "<P>The use of electricity has a significant impact on the environment, energy distribution costs, and energy management since it directly impacts these costs. Long-standing techniques have inherent limits in terms of accuracy and scalability when it comes to predicting power usage. It is now feasible to properly anticipate power use using previous data thanks to improvements in machine learning techniques. In this paper, we provide a machine learning-based method for forecasting power use. In this study, we investigate a number of machine learning techniques, including linear regression, K Nearest Neighbours, XGBOOST, random forest, and artificial neural networks(ANN), to forecast power usage. Using historical electricity use data received from a power utility business, we trained and assessed these models. The data is a year&rsquo;s worth of hourly power use that has been pre-processed to address outliers and missing numbers. Various assessment measures, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination (R2), were used to assess the performance of the models [19]. The outcomes demonstrate that the suggested method may accurately forecast power use. The K Nearest Neighbours(KNN) model outperformed all others in terms of performance, with a 90.92% accuracy rate for predicting agricultural production</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART125164824&target=NART&cn=NART125164824",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Electricity Consumption Prediction Using Machine Learning Electricity Consumption Prediction Using Machine Learning Electricity Consumption Prediction Using Machine Learning <P>The use of electricity has a significant impact on the environment, energy distribution costs, and energy management since it directly impacts these costs. Long-standing techniques have inherent limits in terms of accuracy and scalability when it comes to predicting power usage. It is now feasible to properly anticipate power use using previous data thanks to improvements in machine learning techniques. In this paper, we provide a machine learning-based method for forecasting power use. In this study, we investigate a number of machine learning techniques, including linear regression, K Nearest Neighbours, XGBOOST, random forest, and artificial neural networks(ANN), to forecast power usage. Using historical electricity use data received from a power utility business, we trained and assessed these models. The data is a year&rsquo;s worth of hourly power use that has been pre-processed to address outliers and missing numbers. Various assessment measures, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination (R2), were used to assess the performance of the models [19]. The outcomes demonstrate that the suggested method may accurately forecast power use. The K Nearest Neighbours(KNN) model outperformed all others in terms of performance, with a 90.92% accuracy rate for predicting agricultural production</P>"
        },
        {
          "rank": 8,
          "score": 0.7219722270965576,
          "doc_id": "JAKO202222059037013",
          "title": "기온 데이터를 반영한 전력수요 예측 딥러닝 모델",
          "abstract": "최근 전력수요를 예측하기 위해 통계기반 시계열 분석 기법을 대체하기 위해 딥러닝 기법을 활용한 연구가 활발히 진행되고 있다. 딥러닝 기반 전력수요 예측 연구 결과를 분석한 결과, LSTM 기반 예측 모델의 성능이 우수한 것으로 규명되었으나 장기간의 지역 범위 전력수요 예측에 대해 LSTM 기반 모델의 성능이 충분하지 않음을 확인할 수 있다. 본 연구에서는 기온 데이터를 반영하여 24시간 이전에 전력수요를 예측하는 WaveNet 기반 딥러닝 모델을 개발하여, 실제 사용하고 있는 통계적 시계열 예측 기법의 정확도(MAPE 값 2%)보다 우수한 예측 성능을 달성하는 모델을 개발하고자 한다. 먼저 WaveNet의 핵심 구조인 팽창인과 1차원 합성곱 신경망 구조를 소개하고, 전력수요와 기온 데이터를 입력값으로 모델에 주입하기 위한 데이터 전처리 과정을 제시한다. 다음으로, 개선된 WaveNet 모델을 학습하고 검증하는 방법을 제시한다. 성능 비교 결과, WaveNet 기반 모델에 기온 데이터를 반영한 방법은 전체 검증데이터에 대해 MAPE 값 1.33%를 달성하였고, 동일한 구조의 모델에서 기온 데이터를 반영하지 않는 것(MAPE 값 2.31%)보다 우수한 전력수요 예측 결과를 나타내고 있음을 확인할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202222059037013&target=NART&cn=JAKO202222059037013",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "기온 데이터를 반영한 전력수요 예측 딥러닝 모델 기온 데이터를 반영한 전력수요 예측 딥러닝 모델 기온 데이터를 반영한 전력수요 예측 딥러닝 모델 최근 전력수요를 예측하기 위해 통계기반 시계열 분석 기법을 대체하기 위해 딥러닝 기법을 활용한 연구가 활발히 진행되고 있다. 딥러닝 기반 전력수요 예측 연구 결과를 분석한 결과, LSTM 기반 예측 모델의 성능이 우수한 것으로 규명되었으나 장기간의 지역 범위 전력수요 예측에 대해 LSTM 기반 모델의 성능이 충분하지 않음을 확인할 수 있다. 본 연구에서는 기온 데이터를 반영하여 24시간 이전에 전력수요를 예측하는 WaveNet 기반 딥러닝 모델을 개발하여, 실제 사용하고 있는 통계적 시계열 예측 기법의 정확도(MAPE 값 2%)보다 우수한 예측 성능을 달성하는 모델을 개발하고자 한다. 먼저 WaveNet의 핵심 구조인 팽창인과 1차원 합성곱 신경망 구조를 소개하고, 전력수요와 기온 데이터를 입력값으로 모델에 주입하기 위한 데이터 전처리 과정을 제시한다. 다음으로, 개선된 WaveNet 모델을 학습하고 검증하는 방법을 제시한다. 성능 비교 결과, WaveNet 기반 모델에 기온 데이터를 반영한 방법은 전체 검증데이터에 대해 MAPE 값 1.33%를 달성하였고, 동일한 구조의 모델에서 기온 데이터를 반영하지 않는 것(MAPE 값 2.31%)보다 우수한 전력수요 예측 결과를 나타내고 있음을 확인할 수 있다."
        },
        {
          "rank": 9,
          "score": 0.7199639081954956,
          "doc_id": "ATN0037496660",
          "title": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교",
          "abstract": "Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037496660&target=NART&cn=ATN0037496660",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model."
        },
        {
          "rank": 10,
          "score": 0.715814471244812,
          "doc_id": "ATN0044029065",
          "title": "TCN 딥러닝 모델을 이용한 최대전력 예측에 관한 연구",
          "abstract": "It is necessary to predict peak load accurately in order to supply electric power and operate the power system stably. Especially,it is more important to predict peak load accurately in winter and summer because peak load is higher than other seasons. If peakload is predicted to be higher than actual peak load, the start-up costs of power plants would increase. It causes economic loss to thecompany. On the other hand, if the peak load is predicted to be lower than the actual peak load, blackout may occur due to a lackof power plants capable of generating electricity. Economic losses and blackouts can be prevented by minimizing the prediction errorof the peak load. In this paper, the latest deep learning model such as TCN is used to minimize the prediction error of peak load. Evenif the same deep learning model is used, there is a difference in performance depending on the hyper-parameters. So, I propose methodsfor optimizing hyper-parameters of TCN for predicting the peak load. Data from 2006 to 2021 were input into the model and trained,and prediction error was tested using data in 2022. It was confirmed that the performance of the deep learning model optimized bythe methods proposed in this study is superior to other deep learning models.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0044029065&target=NART&cn=ATN0044029065",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "TCN 딥러닝 모델을 이용한 최대전력 예측에 관한 연구 TCN 딥러닝 모델을 이용한 최대전력 예측에 관한 연구 TCN 딥러닝 모델을 이용한 최대전력 예측에 관한 연구 It is necessary to predict peak load accurately in order to supply electric power and operate the power system stably. Especially,it is more important to predict peak load accurately in winter and summer because peak load is higher than other seasons. If peakload is predicted to be higher than actual peak load, the start-up costs of power plants would increase. It causes economic loss to thecompany. On the other hand, if the peak load is predicted to be lower than the actual peak load, blackout may occur due to a lackof power plants capable of generating electricity. Economic losses and blackouts can be prevented by minimizing the prediction errorof the peak load. In this paper, the latest deep learning model such as TCN is used to minimize the prediction error of peak load. Evenif the same deep learning model is used, there is a difference in performance depending on the hyper-parameters. So, I propose methodsfor optimizing hyper-parameters of TCN for predicting the peak load. Data from 2006 to 2021 were input into the model and trained,and prediction error was tested using data in 2022. It was confirmed that the performance of the deep learning model optimized bythe methods proposed in this study is superior to other deep learning models."
        },
        {
          "rank": 11,
          "score": 0.7119025588035583,
          "doc_id": "NART133895722",
          "title": "Enhancing Electricity Load Forecasting with Machine Learning and Deep Learning",
          "abstract": "<P>The electricity load forecasting handles the process of determining how much electricity will be available at a given time while maintaining the balance and stability of the power grid. The accuracy of electricity load forecasting plays an important role in ensuring safe operation and improving the reliability of power systems and is a key component in the operational planning and efficient market. For many years, a conventional method has been used by using historical data as input parameters. With swift progress and improvement in technology, which shows more potential due to its accuracy, different methods can be applied depending on the identified model. To enhance the forecast of load, this paper introduces and proposes a framework developed on graph database technology to archive large amounts of data, which collects measured data from electrical substations in Pristina, Kosovo. The data includes electrical and weather parameters collected over a four-year timeframe. The proposed framework is designed to handle short-term load forecasting. Machine learning Linear Regression and deep learning Long Short-Term Memory algorithms are applied to multiple datasets and mean absolute error and root mean square error are calculated. The results show the promising performance and effectiveness of the proposed model, with high accuracy in load forecasting.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART133895722&target=NART&cn=NART133895722",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Enhancing Electricity Load Forecasting with Machine Learning and Deep Learning Enhancing Electricity Load Forecasting with Machine Learning and Deep Learning Enhancing Electricity Load Forecasting with Machine Learning and Deep Learning <P>The electricity load forecasting handles the process of determining how much electricity will be available at a given time while maintaining the balance and stability of the power grid. The accuracy of electricity load forecasting plays an important role in ensuring safe operation and improving the reliability of power systems and is a key component in the operational planning and efficient market. For many years, a conventional method has been used by using historical data as input parameters. With swift progress and improvement in technology, which shows more potential due to its accuracy, different methods can be applied depending on the identified model. To enhance the forecast of load, this paper introduces and proposes a framework developed on graph database technology to archive large amounts of data, which collects measured data from electrical substations in Pristina, Kosovo. The data includes electrical and weather parameters collected over a four-year timeframe. The proposed framework is designed to handle short-term load forecasting. Machine learning Linear Regression and deep learning Long Short-Term Memory algorithms are applied to multiple datasets and mean absolute error and root mean square error are calculated. The results show the promising performance and effectiveness of the proposed model, with high accuracy in load forecasting.</P>"
        },
        {
          "rank": 12,
          "score": 0.7022566199302673,
          "doc_id": "NART99478101",
          "title": "Building thermal load prediction through shallow machine learning and deep learning",
          "abstract": "<P><B>Abstract</B></P>  <P>Building thermal load prediction informs the optimization of cooling plant and thermal energy storage. Physics-based prediction models of building thermal load are constrained by the model and input complexity. In this study, we developed 12 data-driven models (7 shallow learning, 2 deep learning, and 3 heuristic methods) to predict building thermal load and compared shallow machine learning and deep learning. The 12 prediction models were compared with the measured cooling demand. It was found XGBoost (Extreme Gradient Boost) and LSTM (Long Short Term Memory) provided the most accurate load prediction in the shallow and deep learning category, and both outperformed the best baseline model, which uses the previous day&rsquo;s data for prediction. Then, we discussed how the prediction horizon and input uncertainty would influence the load prediction accuracy. Major conclusions are twofold: first, LSTM performs well in short-term prediction (1 h ahead) but not in long term prediction (24 h ahead), because the sequential information becomes less relevant and accordingly not so useful when the prediction horizon is long. Second, the presence of weather forecast uncertainty deteriorates XGBoost&rsquo;s accuracy and favors LSTM, because the sequential information makes the model more robust to input uncertainty. Training the model with the uncertain rather than accurate weather data could enhance the model&rsquo;s robustness. Our findings have two implications for practice. First, LSTM is recommended for short-term load prediction given that weather forecast uncertainty is unavoidable. Second, XGBoost is recommended for long term prediction, and the model should be trained with the presence of input uncertainty.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Building load prediction informs chiller plant and thermal storage optimization. </LI> <LI>  We used and compared 9 machine learning algorithms and 3 heuristic prediction methods. </LI> <LI>  XGBoost and LSTM are found to be the best shallow and deep learning algorithm. </LI> <LI>  LSTM is better for short term prediction, while XGBoost for long term prediction. </LI> <LI>  It is better to train the model with uncertain rather than accurate weather data. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART99478101&target=NART&cn=NART99478101",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Building thermal load prediction through shallow machine learning and deep learning Building thermal load prediction through shallow machine learning and deep learning Building thermal load prediction through shallow machine learning and deep learning <P><B>Abstract</B></P>  <P>Building thermal load prediction informs the optimization of cooling plant and thermal energy storage. Physics-based prediction models of building thermal load are constrained by the model and input complexity. In this study, we developed 12 data-driven models (7 shallow learning, 2 deep learning, and 3 heuristic methods) to predict building thermal load and compared shallow machine learning and deep learning. The 12 prediction models were compared with the measured cooling demand. It was found XGBoost (Extreme Gradient Boost) and LSTM (Long Short Term Memory) provided the most accurate load prediction in the shallow and deep learning category, and both outperformed the best baseline model, which uses the previous day&rsquo;s data for prediction. Then, we discussed how the prediction horizon and input uncertainty would influence the load prediction accuracy. Major conclusions are twofold: first, LSTM performs well in short-term prediction (1 h ahead) but not in long term prediction (24 h ahead), because the sequential information becomes less relevant and accordingly not so useful when the prediction horizon is long. Second, the presence of weather forecast uncertainty deteriorates XGBoost&rsquo;s accuracy and favors LSTM, because the sequential information makes the model more robust to input uncertainty. Training the model with the uncertain rather than accurate weather data could enhance the model&rsquo;s robustness. Our findings have two implications for practice. First, LSTM is recommended for short-term load prediction given that weather forecast uncertainty is unavoidable. Second, XGBoost is recommended for long term prediction, and the model should be trained with the presence of input uncertainty.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Building load prediction informs chiller plant and thermal storage optimization. </LI> <LI>  We used and compared 9 machine learning algorithms and 3 heuristic prediction methods. </LI> <LI>  XGBoost and LSTM are found to be the best shallow and deep learning algorithm. </LI> <LI>  LSTM is better for short term prediction, while XGBoost for long term prediction. </LI> <LI>  It is better to train the model with uncertain rather than accurate weather data. </LI> </UL> </P>"
        },
        {
          "rank": 13,
          "score": 0.6962947845458984,
          "doc_id": "JAKO202401558659086",
          "title": "공장 전력 절감을 위한 인공지능 기반의 에너지 관리 시스템 개발",
          "abstract": "본 연구는 IoT 센싱 기술을 활용하여 구축된 빅데이터 수집 시스템을 통해 제주삼다수 공장에서 생성된 데이터를 활용하여 피크 전력 사용을 예측하는 인공지능 모델을 개발하고 비교 분석하였다. LSTM(Long Short-Term Memory) 모델은 단일 변수 시계열 데이터에서 R<sup>2</sup>=0.98, RMSE=0.039, MAE=0.026으로 가장 높은 예측 정확도를 기록하였으며, XGBoost(eXtreme Gradient Boosting) 모델은 다변량 데이터를 효과적으로 처리하며 R<sup>2</sup>=0.93, RMSE=0.018, MAE=0.013의 성능을 보였다. 연구 과정에서 다양한 데이터 전처리 방법과 특징 조합을 실험적으로 적용하여 모델의 성능을 최적화하였으며, 이를 통해 데이터 전처리와 변수 선택이 예측 정확도에 미치는 영향을 입증하였다. 연구 결과, 최적화된 인공지능 모델을 활용한 피크 전력 예측은 전력 비용 절감과 약 10~15%의 탄소 배출 감소 효과를 달성할 수 있음을 제시하였다. 이는 ESG(환경, 사회, 지배구조) 경영을 목표로 하는 기업들에게 지속 가능성을 실현하기 위한 실질적이고 구체적인 전략을 제공하며, 제조업, 물류, 스마트 팩토리 등 다양한 산업 분야에서 예측 모델의 적용 가능성을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202401558659086&target=NART&cn=JAKO202401558659086",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "공장 전력 절감을 위한 인공지능 기반의 에너지 관리 시스템 개발 공장 전력 절감을 위한 인공지능 기반의 에너지 관리 시스템 개발 공장 전력 절감을 위한 인공지능 기반의 에너지 관리 시스템 개발 본 연구는 IoT 센싱 기술을 활용하여 구축된 빅데이터 수집 시스템을 통해 제주삼다수 공장에서 생성된 데이터를 활용하여 피크 전력 사용을 예측하는 인공지능 모델을 개발하고 비교 분석하였다. LSTM(Long Short-Term Memory) 모델은 단일 변수 시계열 데이터에서 R<sup>2</sup>=0.98, RMSE=0.039, MAE=0.026으로 가장 높은 예측 정확도를 기록하였으며, XGBoost(eXtreme Gradient Boosting) 모델은 다변량 데이터를 효과적으로 처리하며 R<sup>2</sup>=0.93, RMSE=0.018, MAE=0.013의 성능을 보였다. 연구 과정에서 다양한 데이터 전처리 방법과 특징 조합을 실험적으로 적용하여 모델의 성능을 최적화하였으며, 이를 통해 데이터 전처리와 변수 선택이 예측 정확도에 미치는 영향을 입증하였다. 연구 결과, 최적화된 인공지능 모델을 활용한 피크 전력 예측은 전력 비용 절감과 약 10~15%의 탄소 배출 감소 효과를 달성할 수 있음을 제시하였다. 이는 ESG(환경, 사회, 지배구조) 경영을 목표로 하는 기업들에게 지속 가능성을 실현하기 위한 실질적이고 구체적인 전략을 제공하며, 제조업, 물류, 스마트 팩토리 등 다양한 산업 분야에서 예측 모델의 적용 가능성을 확인하였다."
        },
        {
          "rank": 14,
          "score": 0.6942042112350464,
          "doc_id": "NART106279808",
          "title": "Machine learning for site-adaptation and solar radiation forecasting",
          "abstract": "<P><B>Abstract</B></P>  <P>Optimal management for solar energy systems requires quality data to build accurate models for predicting the behavior of solar radiation. Solar irradiance and environmental data are provided by satellite and in-situ measurements. It is usual that satellite measurements present high temporal resolution with limited spatial resolution, and in-situ measurements provide high accuracy but significant missing data. This paper proposes a methodology based on machine learning algorithms that: <I>i)</I> takes the best of both data sources to obtain an improved spatio-temporal resolution, known as site-adaptation; and <I>ii)</I> provides highly accurate forecasting solar-radiation models based on deep learning on the improved data. Through a study case with real data, we show the benefits of using the proposed methodology based on machine and deep learning techniques to integrate data from different sources and to construct precise solar radiation forecasting models in regions where solar energy systems are required. Results show that machine learning models for site-adaptation performed up to 38% better than traditional methods.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Site-adaptation models of solar radiation with machine learning. </LI> <LI>  Machine learning and deep learning for solar radiation forecasting. </LI> <LI>  Improvement of satellite data and ground data. </LI> <LI>  Improvement of spatial-temporal resolution of a database. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106279808&target=NART&cn=NART106279808",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Machine learning for site-adaptation and solar radiation forecasting Machine learning for site-adaptation and solar radiation forecasting Machine learning for site-adaptation and solar radiation forecasting <P><B>Abstract</B></P>  <P>Optimal management for solar energy systems requires quality data to build accurate models for predicting the behavior of solar radiation. Solar irradiance and environmental data are provided by satellite and in-situ measurements. It is usual that satellite measurements present high temporal resolution with limited spatial resolution, and in-situ measurements provide high accuracy but significant missing data. This paper proposes a methodology based on machine learning algorithms that: <I>i)</I> takes the best of both data sources to obtain an improved spatio-temporal resolution, known as site-adaptation; and <I>ii)</I> provides highly accurate forecasting solar-radiation models based on deep learning on the improved data. Through a study case with real data, we show the benefits of using the proposed methodology based on machine and deep learning techniques to integrate data from different sources and to construct precise solar radiation forecasting models in regions where solar energy systems are required. Results show that machine learning models for site-adaptation performed up to 38% better than traditional methods.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Site-adaptation models of solar radiation with machine learning. </LI> <LI>  Machine learning and deep learning for solar radiation forecasting. </LI> <LI>  Improvement of satellite data and ground data. </LI> <LI>  Improvement of spatial-temporal resolution of a database. </LI> </UL> </P>"
        },
        {
          "rank": 15,
          "score": 0.6864114999771118,
          "doc_id": "JAKO202305062334676",
          "title": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측",
          "abstract": "본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202305062334676&target=NART&cn=JAKO202305062334676",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다."
        },
        {
          "rank": 16,
          "score": 0.6859729290008545,
          "doc_id": "ATN0050716044",
          "title": "국민DR 참여활성화를 위한 머신러닝 기반의 전력수요 예측에 관한 연구",
          "abstract": "전력수요예측은 여러 가지 독립변수들이 필요하고 날씨, 기온, 습도, 환경적 영향을 고려하여 딥러닝과 머신러닝을 통하여 예측하는 선행 기술이 다수가 존재한다. 전력수요 예측을 위해서는 많은 인공지능 모델들이 있지만 그중에서 회귀분석과 다층신경망을 구성하여 독립변수에 미치는 종속변수의 추론하는데 오차가 발생하고 이를 해결하기 위해서 기계학습으로 반복하여 오차율을 줄이는 방법을 채택하여 실험을 진행하였다. 실제 전력 사용 데이터를 산업단지 내의 공장의 1년의 데이터를 기준으로 머신러닝을 진행하였다. 이번 연구에서는 독립변수를 채택하지 않고 실제 전력 사용데이터를 가지고 트레이닝 데이터셋을 통하여 다층신경망을 구성하여 기계학습으로 전력예측값을 추론하였으며 일간, 주간, 월간 전력사용량을 예측하였다. 이를 통해 대표적인 전력수요관리인 국민DR 참여 활성화에 대한 방향성을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0050716044&target=NART&cn=ATN0050716044",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "국민DR 참여활성화를 위한 머신러닝 기반의 전력수요 예측에 관한 연구 국민DR 참여활성화를 위한 머신러닝 기반의 전력수요 예측에 관한 연구 국민DR 참여활성화를 위한 머신러닝 기반의 전력수요 예측에 관한 연구 전력수요예측은 여러 가지 독립변수들이 필요하고 날씨, 기온, 습도, 환경적 영향을 고려하여 딥러닝과 머신러닝을 통하여 예측하는 선행 기술이 다수가 존재한다. 전력수요 예측을 위해서는 많은 인공지능 모델들이 있지만 그중에서 회귀분석과 다층신경망을 구성하여 독립변수에 미치는 종속변수의 추론하는데 오차가 발생하고 이를 해결하기 위해서 기계학습으로 반복하여 오차율을 줄이는 방법을 채택하여 실험을 진행하였다. 실제 전력 사용 데이터를 산업단지 내의 공장의 1년의 데이터를 기준으로 머신러닝을 진행하였다. 이번 연구에서는 독립변수를 채택하지 않고 실제 전력 사용데이터를 가지고 트레이닝 데이터셋을 통하여 다층신경망을 구성하여 기계학습으로 전력예측값을 추론하였으며 일간, 주간, 월간 전력사용량을 예측하였다. 이를 통해 대표적인 전력수요관리인 국민DR 참여 활성화에 대한 방향성을 제시한다."
        },
        {
          "rank": 17,
          "score": 0.6826659440994263,
          "doc_id": "NART103136065",
          "title": "Portfolio optimization with return prediction using deep learning and machine learning",
          "abstract": "<P><B>Abstract</B></P>  <P>Integrating return prediction of traditional time series models in portfolio formation can improve the performance of original portfolio optimization model. Since machine learning and deep learning models have shown overwhelming superiority than time series models, this paper combines return prediction in portfolio formation with two machine learning models, i.e., random forest (RF) and support vector regression (SVR), and three deep learning models, i.e., LSTM neural network, deep multilayer perceptron (DMLP) and convolutional neural network. To be specific, this paper first applies these prediction models for stock preselection before portfolio formation. Then, this paper incorporates their predictive results in advancing mean&ndash;variance (MV) and omega portfolio optimization models. In order to present the superiority of these models, portfolio models with autoregressive integrated moving average&rsquo;s return prediction are used as benchmarks. Evaluation is based on historical data of 9 years from 2007 to 2015 of component stocks of China securities 100 index. Experimental results show that MV and omega models with RF return prediction, i.e., RF+MVF and RF+OF, outperform the other models. Further, RF+MVF is superior to RF+OF. Due to the high turnover of these two models, this paper discusses their performance after deducting the transaction fee cased by turnover. Experiments present that RF+MVF still performs the best among MVF models and omega model with SVR prediction (SVR+OF) performs the best among OF models. Moreover, RF+MVF performs better than SVR+OF and high turnover erodes nearly half of their total returns especially for RF+OF and RF+MVF. Therefore, this paper recommends investors to build MVF with RF return prediction for daily trading investment.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Compares the performance of machine learning and deep learning in stock preselection. </LI> <LI>  Combining return prediction of machine learning and deep learning in portfolio formation. </LI> <LI>  Emphasis on advancing portfolio optimization with return prediction. </LI> <LI>  Advanced mean&ndash;variance model with random forest forecasts performs the best. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART103136065&target=NART&cn=NART103136065",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Portfolio optimization with return prediction using deep learning and machine learning Portfolio optimization with return prediction using deep learning and machine learning Portfolio optimization with return prediction using deep learning and machine learning <P><B>Abstract</B></P>  <P>Integrating return prediction of traditional time series models in portfolio formation can improve the performance of original portfolio optimization model. Since machine learning and deep learning models have shown overwhelming superiority than time series models, this paper combines return prediction in portfolio formation with two machine learning models, i.e., random forest (RF) and support vector regression (SVR), and three deep learning models, i.e., LSTM neural network, deep multilayer perceptron (DMLP) and convolutional neural network. To be specific, this paper first applies these prediction models for stock preselection before portfolio formation. Then, this paper incorporates their predictive results in advancing mean&ndash;variance (MV) and omega portfolio optimization models. In order to present the superiority of these models, portfolio models with autoregressive integrated moving average&rsquo;s return prediction are used as benchmarks. Evaluation is based on historical data of 9 years from 2007 to 2015 of component stocks of China securities 100 index. Experimental results show that MV and omega models with RF return prediction, i.e., RF+MVF and RF+OF, outperform the other models. Further, RF+MVF is superior to RF+OF. Due to the high turnover of these two models, this paper discusses their performance after deducting the transaction fee cased by turnover. Experiments present that RF+MVF still performs the best among MVF models and omega model with SVR prediction (SVR+OF) performs the best among OF models. Moreover, RF+MVF performs better than SVR+OF and high turnover erodes nearly half of their total returns especially for RF+OF and RF+MVF. Therefore, this paper recommends investors to build MVF with RF return prediction for daily trading investment.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Compares the performance of machine learning and deep learning in stock preselection. </LI> <LI>  Combining return prediction of machine learning and deep learning in portfolio formation. </LI> <LI>  Emphasis on advancing portfolio optimization with return prediction. </LI> <LI>  Advanced mean&ndash;variance model with random forest forecasts performs the best. </LI> </UL> </P>"
        },
        {
          "rank": 18,
          "score": 0.6758540272712708,
          "doc_id": "JAKO202421251156831",
          "title": "LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석",
          "abstract": "This research is a comprehensive analysis of wind power prediction sensitivity using a Long Short-Term Memory (LSTM) deep learning neural network model, accounting for the inherent uncertainties in wind speed estimation. Utilizing a year's worth of operational data from an operational wind farm, the study forecasts the power output of both individual wind turbines and the farm collectively. Predictions were made daily at intervals of 10 minutes and 1 hour over a span of three months. The model's forecast accuracy was evaluated by comparing the root mean square error (RMSE), normalized RMSE (NRMSE), and correlation coefficients with actual power output data. Moreover, the research investigated how inaccuracies in wind speed inputs affect the power prediction sensitivity of the model. By simulating wind speed errors within a normal distribution range of 1% to 15%, the study analyzed their influence on the accuracy of power predictions. This investigation provided insights into the required wind speed prediction error rate to achieve an 8% power prediction error threshold, meeting the incentive standards for forecasting systems in renewable energy generation.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202421251156831&target=NART&cn=JAKO202421251156831",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 This research is a comprehensive analysis of wind power prediction sensitivity using a Long Short-Term Memory (LSTM) deep learning neural network model, accounting for the inherent uncertainties in wind speed estimation. Utilizing a year's worth of operational data from an operational wind farm, the study forecasts the power output of both individual wind turbines and the farm collectively. Predictions were made daily at intervals of 10 minutes and 1 hour over a span of three months. The model's forecast accuracy was evaluated by comparing the root mean square error (RMSE), normalized RMSE (NRMSE), and correlation coefficients with actual power output data. Moreover, the research investigated how inaccuracies in wind speed inputs affect the power prediction sensitivity of the model. By simulating wind speed errors within a normal distribution range of 1% to 15%, the study analyzed their influence on the accuracy of power predictions. This investigation provided insights into the required wind speed prediction error rate to achieve an 8% power prediction error threshold, meeting the incentive standards for forecasting systems in renewable energy generation."
        },
        {
          "rank": 19,
          "score": 0.6758220195770264,
          "doc_id": "JAKO202520454002736",
          "title": "산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구",
          "abstract": "In industrial settings, inverters play a critical role in maintaining productivity and ensuring stable equipment operation. However, inverter failures can result in production downtime and increased maintenance costs. Traditional fault prediction methods based on physical models and expert experience often struggle to capture complex patterns and adapt to varying operational conditions. To address this, this study evaluates the performance of statistical, machine learning, and deep learning approaches for industrial inverter fault prediction, using operational data from a 90W-class inverter at an automotive parts manufacturer in Daegu, South Korea. The experimental results demonstrate that unsupervised anomaly detection models, particularly Autoencoder and SOM, achieved the highest accuracy. These findings suggest that models capable of detecting deviations from normal operating patterns are more effective for inverter fault prediction than conventional methods. In contrast, SVM and Logistic Regression exhibited limitations in handling time-series complexity. This study highlights the necessity of deploying real-time monitoring and predictive maintenance systems in industrial environments, with future research focusing on hyperparameter optimization and real-time data streaming validation.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202520454002736&target=NART&cn=JAKO202520454002736",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 In industrial settings, inverters play a critical role in maintaining productivity and ensuring stable equipment operation. However, inverter failures can result in production downtime and increased maintenance costs. Traditional fault prediction methods based on physical models and expert experience often struggle to capture complex patterns and adapt to varying operational conditions. To address this, this study evaluates the performance of statistical, machine learning, and deep learning approaches for industrial inverter fault prediction, using operational data from a 90W-class inverter at an automotive parts manufacturer in Daegu, South Korea. The experimental results demonstrate that unsupervised anomaly detection models, particularly Autoencoder and SOM, achieved the highest accuracy. These findings suggest that models capable of detecting deviations from normal operating patterns are more effective for inverter fault prediction than conventional methods. In contrast, SVM and Logistic Regression exhibited limitations in handling time-series complexity. This study highlights the necessity of deploying real-time monitoring and predictive maintenance systems in industrial environments, with future research focusing on hyperparameter optimization and real-time data streaming validation."
        },
        {
          "rank": 20,
          "score": 0.6757974624633789,
          "doc_id": "ATN0048893828",
          "title": "머신러닝을 이용한 우리나라 환율 예측 연구",
          "abstract": "본 연구에서 환율의 예측 능력에 대하여 머신러닝 계열의 방법론과 비-머신러닝 계열의 방법론의 예측 능력을 상고 비교하고자 하였다. 데이터는 2001년부터 2018년을 학습의 기간으로 삼아 2019년을 테스트 하는 실험1과 2001년부터 2017년을 학습 기간으로 삼고 2018년과 2019년을 예측하는 실험2로 나누어 실험을 하였다. 실험1과 실험2 모두에서 머신러닝 계열의 예측이 비-머신러닝 계열의 예측보다 MSE측면에서 우수함을 보였다. 특히 두 실험 모두에서 다층퍼셉트론(MLP)이 매우 우수한 능력을 보였고, KNN을 시계열로 확장을 한 TSFKNN과 신경망을 시계열로 확장을 한 NNETAR 두 가지 모두 유사한 능력을 보였다. 전통적인 비-먼신러닝 계열에서는 충분히 데이터에 대한 특성이 파악이 되지 않아 낮은 수준의 예측 능력을 보이는 것으로 판단이 된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0048893828&target=NART&cn=ATN0048893828",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝을 이용한 우리나라 환율 예측 연구 머신러닝을 이용한 우리나라 환율 예측 연구 머신러닝을 이용한 우리나라 환율 예측 연구 본 연구에서 환율의 예측 능력에 대하여 머신러닝 계열의 방법론과 비-머신러닝 계열의 방법론의 예측 능력을 상고 비교하고자 하였다. 데이터는 2001년부터 2018년을 학습의 기간으로 삼아 2019년을 테스트 하는 실험1과 2001년부터 2017년을 학습 기간으로 삼고 2018년과 2019년을 예측하는 실험2로 나누어 실험을 하였다. 실험1과 실험2 모두에서 머신러닝 계열의 예측이 비-머신러닝 계열의 예측보다 MSE측면에서 우수함을 보였다. 특히 두 실험 모두에서 다층퍼셉트론(MLP)이 매우 우수한 능력을 보였고, KNN을 시계열로 확장을 한 TSFKNN과 신경망을 시계열로 확장을 한 NNETAR 두 가지 모두 유사한 능력을 보였다. 전통적인 비-먼신러닝 계열에서는 충분히 데이터에 대한 특성이 파악이 되지 않아 낮은 수준의 예측 능력을 보이는 것으로 판단이 된다."
        },
        {
          "rank": 21,
          "score": 0.6751551628112793,
          "doc_id": "DIKO0016929656",
          "title": "머신러닝 기법을 사용한 이익예측 모형",
          "abstract": "본 논문에서는 단계적 로짓 회귀(Stepwise Logit Regressioin) 모델과 머신러닝(Machine Learning) 기법인 랜덤 포레스트(Random Forest) 모델, 익스트림 그레이디언트 부스팅(Extreme Gradient Boosting) 모델, 인공신경망(Artificial Neural Network) 모델을 사용하여 미래의 희석주당이익(DEPS)의 증가를 예측하고, 각각의 예측 모델의 정확도를 비교한다. 선행연구에서 이익의 증가를 예측할 수 있도록 하는 중요 변수를 사용하여 본 연구를 진행하고 네 개의 모델에서 중요 변수를 추출하고 선행연구의 중요 변수와 비교한다. 그리고 재무상태표와 손익계산서에서 얻을 수 있는 변수들을 확보하여 총자산과 총매출로 변수를 조정한 후 예측 변수에 추가하여 미래 희석주당이익의 증가를 추가적으로 예측하고, 중요 변수를 추출하고 비교한다.&amp;#xD; 분석 결과 선행연구와 동일하게 랜덤포레스트모델의 예측 정확도가 단계적 로짓 회귀 모델의 예측 정확도보다 높았다. 그리고 랜덤 포레스트 모델의 예측 정확도가 나머지 예측 모델보다 예측 정확도가 가장 높았으며 다음으로 익스트림 그레이디언트 부스팅 모델의 예측 정확도가 높았다. 추가 분석 결과 랜덤 포레스트 모델, 익스트림 그레이디언트 부스팅 모델과 인공신경망 모델의 예측 정확도가 좋았으며, 인공신경망 모델의 경우 예측 변수를 추가하기 전보다 예측 정확도가 가장 큰 폭으로 향상되는 것을 확인하였다.&amp;#xD; 본 연구는 머신러닝 모델을 활용하여 회계이익을 예측할 경우, 기존의 이익 예측 모형인 회귀모형 보다 회계이익을 정확하게 예측할 수 있다는 점을 시사한다. 그리고 머신러닝 모델과 딥러닝 모델을 활용한 연구가 회계학에서도 충분히 활용될 수 있다는 증거를 제시하고, 머신러닝 방법론과 딥러닝 방법론을 회계학에 도입하는데 발판을 마련하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016929656&target=NART&cn=DIKO0016929656",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기법을 사용한 이익예측 모형 머신러닝 기법을 사용한 이익예측 모형 머신러닝 기법을 사용한 이익예측 모형 본 논문에서는 단계적 로짓 회귀(Stepwise Logit Regressioin) 모델과 머신러닝(Machine Learning) 기법인 랜덤 포레스트(Random Forest) 모델, 익스트림 그레이디언트 부스팅(Extreme Gradient Boosting) 모델, 인공신경망(Artificial Neural Network) 모델을 사용하여 미래의 희석주당이익(DEPS)의 증가를 예측하고, 각각의 예측 모델의 정확도를 비교한다. 선행연구에서 이익의 증가를 예측할 수 있도록 하는 중요 변수를 사용하여 본 연구를 진행하고 네 개의 모델에서 중요 변수를 추출하고 선행연구의 중요 변수와 비교한다. 그리고 재무상태표와 손익계산서에서 얻을 수 있는 변수들을 확보하여 총자산과 총매출로 변수를 조정한 후 예측 변수에 추가하여 미래 희석주당이익의 증가를 추가적으로 예측하고, 중요 변수를 추출하고 비교한다.&amp;#xD; 분석 결과 선행연구와 동일하게 랜덤포레스트모델의 예측 정확도가 단계적 로짓 회귀 모델의 예측 정확도보다 높았다. 그리고 랜덤 포레스트 모델의 예측 정확도가 나머지 예측 모델보다 예측 정확도가 가장 높았으며 다음으로 익스트림 그레이디언트 부스팅 모델의 예측 정확도가 높았다. 추가 분석 결과 랜덤 포레스트 모델, 익스트림 그레이디언트 부스팅 모델과 인공신경망 모델의 예측 정확도가 좋았으며, 인공신경망 모델의 경우 예측 변수를 추가하기 전보다 예측 정확도가 가장 큰 폭으로 향상되는 것을 확인하였다.&amp;#xD; 본 연구는 머신러닝 모델을 활용하여 회계이익을 예측할 경우, 기존의 이익 예측 모형인 회귀모형 보다 회계이익을 정확하게 예측할 수 있다는 점을 시사한다. 그리고 머신러닝 모델과 딥러닝 모델을 활용한 연구가 회계학에서도 충분히 활용될 수 있다는 증거를 제시하고, 머신러닝 방법론과 딥러닝 방법론을 회계학에 도입하는데 발판을 마련하고자 한다."
        },
        {
          "rank": 22,
          "score": 0.6738572120666504,
          "doc_id": "JAKO202317233054241",
          "title": "머신러닝 기반 수소 충전소 에너지 수요 예측 모델",
          "abstract": "수소 에너지는 높은 에너지 효율로 열과 전기를 생산하면서도 온실가스와 미세먼지 등 유해물질 배출이 없는 친환경 에너지로서, 전 세계적으로 탄소중립으로의 전환을 위한 핵심으로 주목받고 있다. 특히 스마트 수소에너지는 경제적이고 지속 가능하며, 안전한 미래 스마트 수소에너지 서비스로써 수소 에너지의 기반 시설이 디지털로 통합되어 '데이터' 기반으로 안정적으로 운영되는 서비스를 의미한다. 본 논문에서는 데이터 기반 수소 충전소 수요예측 모델 구현을 위해 강원도 내 설치되어 있는 수소 충전소 3곳(춘천, 속초, 평창)을 선정, 수소 충전소의 수요공급 데이터를 확보하였고, 머신러닝 및 딥러닝 알고리즘 7개를 선정하여 총 27종 입력 데이터(기상데이터+수소 충전소 수요량)로 모델을 학습하였고, 평균 제곱근 오차(RMSE)로 모델을 평가하였다. 이를 통해 본 논문에서는 최적의 수소 에너지 수요공급을 위한 머신러닝 기반 수소 충전소 에너지 수요 예측 모델을 제안한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202317233054241&target=NART&cn=JAKO202317233054241",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기반 수소 충전소 에너지 수요 예측 모델 머신러닝 기반 수소 충전소 에너지 수요 예측 모델 머신러닝 기반 수소 충전소 에너지 수요 예측 모델 수소 에너지는 높은 에너지 효율로 열과 전기를 생산하면서도 온실가스와 미세먼지 등 유해물질 배출이 없는 친환경 에너지로서, 전 세계적으로 탄소중립으로의 전환을 위한 핵심으로 주목받고 있다. 특히 스마트 수소에너지는 경제적이고 지속 가능하며, 안전한 미래 스마트 수소에너지 서비스로써 수소 에너지의 기반 시설이 디지털로 통합되어 '데이터' 기반으로 안정적으로 운영되는 서비스를 의미한다. 본 논문에서는 데이터 기반 수소 충전소 수요예측 모델 구현을 위해 강원도 내 설치되어 있는 수소 충전소 3곳(춘천, 속초, 평창)을 선정, 수소 충전소의 수요공급 데이터를 확보하였고, 머신러닝 및 딥러닝 알고리즘 7개를 선정하여 총 27종 입력 데이터(기상데이터+수소 충전소 수요량)로 모델을 학습하였고, 평균 제곱근 오차(RMSE)로 모델을 평가하였다. 이를 통해 본 논문에서는 최적의 수소 에너지 수요공급을 위한 머신러닝 기반 수소 충전소 에너지 수요 예측 모델을 제안한다."
        },
        {
          "rank": 23,
          "score": 0.6703870296478271,
          "doc_id": "JAKO202408283134560",
          "title": "머신러닝&딥러닝 모델을 활용한 댐 일유입량 예측시 융적설을 고려하기 위한 데이터 전처리에 대한 방법 연구",
          "abstract": "댐유입량 예측에 대하여 데이터 기반 머신러닝 및 딥러닝(Machine Learning & Deep Learning, ML&DL) 분석도구들이 공개되어 다양한 분야에서 ML&DL의 적용연구가 활발히 진행되고 있으며, 모델의 자체 성능향상 뿐만 아니라 모델의 특성을 고려한 데이터의 전처리도 댐유입량을 정확하게 예측하게 하는 중요한 모델성능 향상의 요소라고 할 수 있다. 특히 기존 강우자료는 적설량을 열선 설비를 통하여 녹여 강우량으로 환산되어 있으므로, 융적설에 따른 강우와 유입량의 상관관계를 왜곡하게 된다. 따라서 본연구에서는 소양강댐과 같이 융적설의 영향을 받는 댐유역에 대한 댐일유입량 예측시 겨울에 강설량이 적설이 되어 적게 유출되는 현상과, 봄에 융설로 인하여 무강우나 적은 비에도 많은 유출이 일어나는 물리적 현상을 ML&DL모델로 적용하기 위하여 필요한 강우 데이터의 전처리에 대한 연구를 수행 하였다. 강우계열, 유입량계열을 조합하여 3가지 머신러닝(SVM, RF, LGBM)과 2가지 딥러닝(LSTM, TCN) 모델을 구축하고, 최적 하이퍼파라메터 튜닝을 통하여 적합 모델을 적용하고 한 결과, NSE 0.842~0.894로 높은 수준의 예측성능을 나타내었다. 또한 융적설을 반영한 강우보정 데이터를 만들기 위하여 융적설 모의 알고리즘을 개발하고, 이를 통하여 산정된 보정강우를 머신러닝 및 딥러닝 모델에 적용한 결과 NSE 0.841~0.896 으로 융적설 적용전과 비슷한 높은 수준의 예측 성능을 나타내었으나, 융적설 기간에는 조정된 강우로 학습되어 예측되었을 때 실측유입량에 근접하는 모의결과를 나타내었다. 결론적으로, 융적설이 영향을 미치는 유역에서의 데이터 모델 적용시에는 입력자료 구축시 적설 및 융설이 물리적으로 타당한 강우-유출 반응에 적합하도록 전처리과정이 중요함을 밝혔다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202408283134560&target=NART&cn=JAKO202408283134560",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝&딥러닝 모델을 활용한 댐 일유입량 예측시 융적설을 고려하기 위한 데이터 전처리에 대한 방법 연구 머신러닝&딥러닝 모델을 활용한 댐 일유입량 예측시 융적설을 고려하기 위한 데이터 전처리에 대한 방법 연구 머신러닝&딥러닝 모델을 활용한 댐 일유입량 예측시 융적설을 고려하기 위한 데이터 전처리에 대한 방법 연구 댐유입량 예측에 대하여 데이터 기반 머신러닝 및 딥러닝(Machine Learning & Deep Learning, ML&DL) 분석도구들이 공개되어 다양한 분야에서 ML&DL의 적용연구가 활발히 진행되고 있으며, 모델의 자체 성능향상 뿐만 아니라 모델의 특성을 고려한 데이터의 전처리도 댐유입량을 정확하게 예측하게 하는 중요한 모델성능 향상의 요소라고 할 수 있다. 특히 기존 강우자료는 적설량을 열선 설비를 통하여 녹여 강우량으로 환산되어 있으므로, 융적설에 따른 강우와 유입량의 상관관계를 왜곡하게 된다. 따라서 본연구에서는 소양강댐과 같이 융적설의 영향을 받는 댐유역에 대한 댐일유입량 예측시 겨울에 강설량이 적설이 되어 적게 유출되는 현상과, 봄에 융설로 인하여 무강우나 적은 비에도 많은 유출이 일어나는 물리적 현상을 ML&DL모델로 적용하기 위하여 필요한 강우 데이터의 전처리에 대한 연구를 수행 하였다. 강우계열, 유입량계열을 조합하여 3가지 머신러닝(SVM, RF, LGBM)과 2가지 딥러닝(LSTM, TCN) 모델을 구축하고, 최적 하이퍼파라메터 튜닝을 통하여 적합 모델을 적용하고 한 결과, NSE 0.842~0.894로 높은 수준의 예측성능을 나타내었다. 또한 융적설을 반영한 강우보정 데이터를 만들기 위하여 융적설 모의 알고리즘을 개발하고, 이를 통하여 산정된 보정강우를 머신러닝 및 딥러닝 모델에 적용한 결과 NSE 0.841~0.896 으로 융적설 적용전과 비슷한 높은 수준의 예측 성능을 나타내었으나, 융적설 기간에는 조정된 강우로 학습되어 예측되었을 때 실측유입량에 근접하는 모의결과를 나타내었다. 결론적으로, 융적설이 영향을 미치는 유역에서의 데이터 모델 적용시에는 입력자료 구축시 적설 및 융설이 물리적으로 타당한 강우-유출 반응에 적합하도록 전처리과정이 중요함을 밝혔다."
        },
        {
          "rank": 24,
          "score": 0.6703163981437683,
          "doc_id": "JAKO202518361202534",
          "title": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측",
          "abstract": "본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202518361202534&target=NART&cn=JAKO202518361202534",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다."
        },
        {
          "rank": 25,
          "score": 0.6664005517959595,
          "doc_id": "NART123583722",
          "title": "Sediment load prediction in Johor river: deep learning versus machine learning models",
          "abstract": "<P><B>Abstract</B><P>Sediment transport is a normal phenomenon in rivers and streams, contributing significantly to ecosystem production and preservation by replenishing vital nutrients and preserving aquatic life&rsquo;s natural habitats. Thus, sediment transport prediction through modeling is crucial for predicting flood events, tracking coastal erosion, planning for water supplies, and managing irrigation. The predictability of process-driven models may encounter various restrictions throughout the validation process. Given that data-driven models work on the assumption that the underlying physical process is not requisite, this opens up the avenue for AI-based model as alternative modeling. However, AI-based models, such as ANN and SVM, face problems, such as long-term dependency, which require alternative dynamic procedures. Since their performance as universal function approximation depends on their compatibility with the nature of the problem itself, this study investigated several distinct AI-based models, such as long short-term memory (LSTM), artificial neural network (ANN), and support vector machine (SVM), in predicting sediment transport in the Johor river. The collected historical daily sediment transport data from January 1, 2008, to December 01, 2018, through autocorrelation function, were used as input for the model. The statistical results showed that, despite their ability (deep learning and machine learning) to provide sediment predictions based on historical input datasets, machine learning, such as ANN, might be more prone to overfitting or being trapped in a local optimum than deep learning, evidenced by the worse in all metrics score. With RMSE = 11.395, MAE = 18.094, and <I>R</I>2 = 0.914, LSTM outperformed other models in the comparison.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART123583722&target=NART&cn=NART123583722",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Sediment load prediction in Johor river: deep learning versus machine learning models Sediment load prediction in Johor river: deep learning versus machine learning models Sediment load prediction in Johor river: deep learning versus machine learning models <P><B>Abstract</B><P>Sediment transport is a normal phenomenon in rivers and streams, contributing significantly to ecosystem production and preservation by replenishing vital nutrients and preserving aquatic life&rsquo;s natural habitats. Thus, sediment transport prediction through modeling is crucial for predicting flood events, tracking coastal erosion, planning for water supplies, and managing irrigation. The predictability of process-driven models may encounter various restrictions throughout the validation process. Given that data-driven models work on the assumption that the underlying physical process is not requisite, this opens up the avenue for AI-based model as alternative modeling. However, AI-based models, such as ANN and SVM, face problems, such as long-term dependency, which require alternative dynamic procedures. Since their performance as universal function approximation depends on their compatibility with the nature of the problem itself, this study investigated several distinct AI-based models, such as long short-term memory (LSTM), artificial neural network (ANN), and support vector machine (SVM), in predicting sediment transport in the Johor river. The collected historical daily sediment transport data from January 1, 2008, to December 01, 2018, through autocorrelation function, were used as input for the model. The statistical results showed that, despite their ability (deep learning and machine learning) to provide sediment predictions based on historical input datasets, machine learning, such as ANN, might be more prone to overfitting or being trapped in a local optimum than deep learning, evidenced by the worse in all metrics score. With RMSE = 11.395, MAE = 18.094, and <I>R</I>2 = 0.914, LSTM outperformed other models in the comparison.</P></P>"
        },
        {
          "rank": 26,
          "score": 0.6654180288314819,
          "doc_id": "DIKO0016660298",
          "title": "Parallel CNN-LSTM 기반의 부하예측 및 열병합 발전 이상탐지",
          "abstract": "지구온난화에 따른 환경 변화가 전세계적으로 나타남에 따라 한국에서도 온실가스 배출 감소를 목표로 석탄기반의 화력발전 감축과 이를 재생에너지로 대체하기 위한 방안을 모색하고 있다. 하지만 기존 탄소기반 발전(發電)체제를 기반으로 하는 제도와 인프라, 인력으로 인해 신속한 에너지 체제 전환이 어렵다. 이러한 이유로 열병합 (combined heat and power, CHP) 발전은 기존 화력발전 인프라 활용이 가능하다는 장점과 열과 전력을 동시에 생산하며 일반 복합발전 대비 높은 효율을 가지고 있어 기존 화력발전의 대체체제로 주목받고 있다. &amp;#xD; 하지만 CHP 발전은 높은 발전 효율과 인프라 활용 가능성에 불구하고 단점도 가지고 있는데, 그 중 하나가 잉여 열에너지 발생으로 인한 경제적 손실이다. 전력의 경우 저장이 용이해 추후 판매가 가능한 반면 열 에너지는 고온의 액체와 증기 상태이기 때문에 저장효율이 매우 낮아 손실을 유발한다. 또한 CHP 엔진은 화력발전과 유사하게 액화천연가스를 연소하고 터빈을 작동하는 방식으로 고온∙고압의 환경에서 운영되기 때문에 이상(異常)과 결함이 발생할 가능성이 높다. 엔진 관리가 적절히 이루어지지 않을 경우 연료 소비와 온실가스 배출이 증가할 수 있으며 갑작스러운 운행 중단의 위험이 있기 때문에 이상관리는 필수적이다. 이에 다음 세 연구를 통해 CHP 발전 효율 향상과 경제 손실 감소를 위한 열 부하 예측모델과 엔진 이상탐지 모델을 제안한다.&amp;#xD; 첫 번째, 열 부하 파생 변수와 LSTM을 사용한 부하예측 모델을 제안한다. 열 부하는 수용가의 열 사용량에 영향을 받기 때문에 지역난방 파생변수와 사용자의 사용량에 영향을 주는 기상, 공휴일 정보를 단계적으로 추가하며 모델의 정확도를 높인다. 실험 결과 공휴일 정보를 기반으로 생성한 열 부하 파생변수를 학습한 모델의 성능이 가장 우수하다. 기본 지역난방 관련 변수들만 사용한 모델 대비 열 부하 파생 변수들을 추가한 모델의 MAPE가 18.37에서 15.32로 약 3.05 감소, MSE는 1.31에서 1.23으로 약 0.08 감소하는 것을 확인할 수 있다. 본 연구의 결론을 기반으로 두 번째 연구에서는 파생변수를 확장하고 모델의 네트워크 구조를 고도화한다. &amp;#xD; 두 번째, CNN과 LSTM을 parallel하게 결합하고 attention mechanism을 추가한 parallel CNN-LSTM attention (PCLA) 모델을 제안한다. CHP 발전 로그는 multivariate time series 데이터이므로 temporal feature 뿐만 아닌 spatial feature도 추출해 학습할 필요가 있다. 이를 위해 선행 연구들은 CNN과 LSTM을 serial하게 결합한 모델을 제안했으나, 이에는 input data에서 직접 temporal feature를 추출할 수 없고 CNN에서 추출한 spatial feature도 손상된다는 문제가 있다. 따라서 본 연구에서는 CNN과 LSTM을 parallel하게 결합해 spatial feature와 temporal feature를 추출하고, 이 중 중요한 요인을 집중 학습하기 위해 attention mechanism을 결합한 PCLA를 제안한다. PCLA의 열 부하 예측 우수성을 증명하기 위해 선행 연구들이 제안한 statistical analysis, shallow machine learning, deep learning, 그리고 CNN과 LSTM을 결합한 hybrid deep learning 모델 12개와 비교한다. 이에 더해 첫 번째 연구에서 증명한 파생변수의 효용성에 따라 공휴일, 요일, 시간을 기준으로 열 부하 파생변수를 추가해 모델을 학습한다. 그 결과 PCLA의 성능은 MAE와 MSE가 0.571, 0.662로 가장 낮으며, R-squared 0.942로 가장 높은 것을 확인했다. 이를 통해 CNN과 LSTM을 serial하게 결합한 모델보다 parallel하게 결합한 모델이 multivariate time series 데이터의 spatiotemporal feature를 추출하고 학습하는데 우수하며, attention mechanism을 통해 예측 정확도를 더 높일 수 있음을 증명한다. &amp;#xD; 세 번째, CNN, LSTM, residual block, attention mechanism을 결합한 parallel CNN-LSTM residual attention (PCL-Res-Att) 이상탐지 모델을 제안한다. PCL-Res-Att 모델은 CNN과 LSTM을 사용해 feature를 추출하고 residual block을 사용해 손실된 정보를 보완한다. 그리고 attention mechanism을 사용해 중요한 spatiotemporal feature를 추출해 학습하고 이상탐지 결과를 도출한다. 엔진 이상탐지 모델 학습 데이터인 엔진 시스템 로그 데이터 또한 multivariate time series 데이터이기 때문에 CNN과 LSTM 결합모델을 통한 spatiotemporal feature를 추출하고 학습하는 것이 중요하다. 이를 위해 본 연구에서는 PCLA에 residual block을 결합해 모델의 성능을 고도화한 엔진 이상탐지 모델 PCL-Res-Att을 제안한다. PCL-Res-Att의 이상탐지 성능 우수성을 증명하기 위해 CNN, LSTM, 그리고 CNN과 LSTM의 serial, parallel 결합 모델에 residual block과 attention mechanism을 단계적으로 결합한 9개 모델과 비교한다. 그리고 CHP 엔진 3대, 5개 계통, 총 15 케이스에 모델들을 적용해 실험 결과를 도출한다. 그 결과 PCL-Res-Att의 macro f1-score가 0.951±0.033 (mean ± standard deviation), abnormal f1-score가 0.903±0.064, 그리고 accuracy가 0.999±0.002로 가장 우수하다. 이를 통해 CNN과 LSTM의 parallel한 결합 모델이 multivariate time series 데이터의 spatiotemporal feature를 추출하고 학습하는데 우수함을 증명한다. 또한 attention mechanism 대비 residual block을 통한 손실된 정보 보완이 이상탐지 정확도 향상에 더 기여하지만, 모두 사용했을 때 성능이 크게 향상됨을 확인할 수 있다. &amp;#xD;",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016660298&target=NART&cn=DIKO0016660298",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Parallel CNN-LSTM 기반의 부하예측 및 열병합 발전 이상탐지 Parallel CNN-LSTM 기반의 부하예측 및 열병합 발전 이상탐지 Parallel CNN-LSTM 기반의 부하예측 및 열병합 발전 이상탐지 지구온난화에 따른 환경 변화가 전세계적으로 나타남에 따라 한국에서도 온실가스 배출 감소를 목표로 석탄기반의 화력발전 감축과 이를 재생에너지로 대체하기 위한 방안을 모색하고 있다. 하지만 기존 탄소기반 발전(發電)체제를 기반으로 하는 제도와 인프라, 인력으로 인해 신속한 에너지 체제 전환이 어렵다. 이러한 이유로 열병합 (combined heat and power, CHP) 발전은 기존 화력발전 인프라 활용이 가능하다는 장점과 열과 전력을 동시에 생산하며 일반 복합발전 대비 높은 효율을 가지고 있어 기존 화력발전의 대체체제로 주목받고 있다. &amp;#xD; 하지만 CHP 발전은 높은 발전 효율과 인프라 활용 가능성에 불구하고 단점도 가지고 있는데, 그 중 하나가 잉여 열에너지 발생으로 인한 경제적 손실이다. 전력의 경우 저장이 용이해 추후 판매가 가능한 반면 열 에너지는 고온의 액체와 증기 상태이기 때문에 저장효율이 매우 낮아 손실을 유발한다. 또한 CHP 엔진은 화력발전과 유사하게 액화천연가스를 연소하고 터빈을 작동하는 방식으로 고온∙고압의 환경에서 운영되기 때문에 이상(異常)과 결함이 발생할 가능성이 높다. 엔진 관리가 적절히 이루어지지 않을 경우 연료 소비와 온실가스 배출이 증가할 수 있으며 갑작스러운 운행 중단의 위험이 있기 때문에 이상관리는 필수적이다. 이에 다음 세 연구를 통해 CHP 발전 효율 향상과 경제 손실 감소를 위한 열 부하 예측모델과 엔진 이상탐지 모델을 제안한다.&amp;#xD; 첫 번째, 열 부하 파생 변수와 LSTM을 사용한 부하예측 모델을 제안한다. 열 부하는 수용가의 열 사용량에 영향을 받기 때문에 지역난방 파생변수와 사용자의 사용량에 영향을 주는 기상, 공휴일 정보를 단계적으로 추가하며 모델의 정확도를 높인다. 실험 결과 공휴일 정보를 기반으로 생성한 열 부하 파생변수를 학습한 모델의 성능이 가장 우수하다. 기본 지역난방 관련 변수들만 사용한 모델 대비 열 부하 파생 변수들을 추가한 모델의 MAPE가 18.37에서 15.32로 약 3.05 감소, MSE는 1.31에서 1.23으로 약 0.08 감소하는 것을 확인할 수 있다. 본 연구의 결론을 기반으로 두 번째 연구에서는 파생변수를 확장하고 모델의 네트워크 구조를 고도화한다. &amp;#xD; 두 번째, CNN과 LSTM을 parallel하게 결합하고 attention mechanism을 추가한 parallel CNN-LSTM attention (PCLA) 모델을 제안한다. CHP 발전 로그는 multivariate time series 데이터이므로 temporal feature 뿐만 아닌 spatial feature도 추출해 학습할 필요가 있다. 이를 위해 선행 연구들은 CNN과 LSTM을 serial하게 결합한 모델을 제안했으나, 이에는 input data에서 직접 temporal feature를 추출할 수 없고 CNN에서 추출한 spatial feature도 손상된다는 문제가 있다. 따라서 본 연구에서는 CNN과 LSTM을 parallel하게 결합해 spatial feature와 temporal feature를 추출하고, 이 중 중요한 요인을 집중 학습하기 위해 attention mechanism을 결합한 PCLA를 제안한다. PCLA의 열 부하 예측 우수성을 증명하기 위해 선행 연구들이 제안한 statistical analysis, shallow machine learning, deep learning, 그리고 CNN과 LSTM을 결합한 hybrid deep learning 모델 12개와 비교한다. 이에 더해 첫 번째 연구에서 증명한 파생변수의 효용성에 따라 공휴일, 요일, 시간을 기준으로 열 부하 파생변수를 추가해 모델을 학습한다. 그 결과 PCLA의 성능은 MAE와 MSE가 0.571, 0.662로 가장 낮으며, R-squared 0.942로 가장 높은 것을 확인했다. 이를 통해 CNN과 LSTM을 serial하게 결합한 모델보다 parallel하게 결합한 모델이 multivariate time series 데이터의 spatiotemporal feature를 추출하고 학습하는데 우수하며, attention mechanism을 통해 예측 정확도를 더 높일 수 있음을 증명한다. &amp;#xD; 세 번째, CNN, LSTM, residual block, attention mechanism을 결합한 parallel CNN-LSTM residual attention (PCL-Res-Att) 이상탐지 모델을 제안한다. PCL-Res-Att 모델은 CNN과 LSTM을 사용해 feature를 추출하고 residual block을 사용해 손실된 정보를 보완한다. 그리고 attention mechanism을 사용해 중요한 spatiotemporal feature를 추출해 학습하고 이상탐지 결과를 도출한다. 엔진 이상탐지 모델 학습 데이터인 엔진 시스템 로그 데이터 또한 multivariate time series 데이터이기 때문에 CNN과 LSTM 결합모델을 통한 spatiotemporal feature를 추출하고 학습하는 것이 중요하다. 이를 위해 본 연구에서는 PCLA에 residual block을 결합해 모델의 성능을 고도화한 엔진 이상탐지 모델 PCL-Res-Att을 제안한다. PCL-Res-Att의 이상탐지 성능 우수성을 증명하기 위해 CNN, LSTM, 그리고 CNN과 LSTM의 serial, parallel 결합 모델에 residual block과 attention mechanism을 단계적으로 결합한 9개 모델과 비교한다. 그리고 CHP 엔진 3대, 5개 계통, 총 15 케이스에 모델들을 적용해 실험 결과를 도출한다. 그 결과 PCL-Res-Att의 macro f1-score가 0.951±0.033 (mean ± standard deviation), abnormal f1-score가 0.903±0.064, 그리고 accuracy가 0.999±0.002로 가장 우수하다. 이를 통해 CNN과 LSTM의 parallel한 결합 모델이 multivariate time series 데이터의 spatiotemporal feature를 추출하고 학습하는데 우수함을 증명한다. 또한 attention mechanism 대비 residual block을 통한 손실된 정보 보완이 이상탐지 정확도 향상에 더 기여하지만, 모두 사용했을 때 성능이 크게 향상됨을 확인할 수 있다. &amp;#xD;"
        },
        {
          "rank": 27,
          "score": 0.6629467010498047,
          "doc_id": "ART003240564",
          "title": "Prediction of Electrochemical Characteristics of Metallic Bipolar Plates for PEMFCs Using Machine Learning and Deep Learning Model",
          "abstract": "In this research, machine learning and deep learning models were employed to develop a predictive model for the electrochemical characteristics of 316L stainless steel used in metallic bipolar plates of PEM fuel cells (PEMFCs). The results indicated that hydrogen ion concentration (pH) exhibited the strongest correlation with electrochemical characteristics, as determined through SHAP analysis, Spearman correlation analysis, and the random forest model. Furthermore, the interaction between hydrogen ion concentration and hydrofluoric acid significantly decreased corrosion resistance, while hydrogen peroxide enhanced corrosion resistance by forming an oxide film on the surface. When comparing the performance of the MLP, DNN_Adam, and ResNet_Nadam prediction models, the DNN and ResNet models outperformed the MLP model. This superior performance is attributed to the use of appropriate optimizers (Adam and Nadam) andBayesian optimization techniques to identify the optimal hyperparameters for each model. Notably, the ResNet_Nadam model emerged as the best predictor of electrochemical characteristics, achieving the highest R2 value and showing a difference of less than 10% between training and validation data loss in the learning curve. These findings suggest that a thorough understanding of the independent variable data and the application of optimal parameters can enhance the predictive performance of deep learning models.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART003240564&target=NART&cn=ART003240564",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Prediction of Electrochemical Characteristics of Metallic Bipolar Plates for PEMFCs Using Machine Learning and Deep Learning Model Prediction of Electrochemical Characteristics of Metallic Bipolar Plates for PEMFCs Using Machine Learning and Deep Learning Model Prediction of Electrochemical Characteristics of Metallic Bipolar Plates for PEMFCs Using Machine Learning and Deep Learning Model In this research, machine learning and deep learning models were employed to develop a predictive model for the electrochemical characteristics of 316L stainless steel used in metallic bipolar plates of PEM fuel cells (PEMFCs). The results indicated that hydrogen ion concentration (pH) exhibited the strongest correlation with electrochemical characteristics, as determined through SHAP analysis, Spearman correlation analysis, and the random forest model. Furthermore, the interaction between hydrogen ion concentration and hydrofluoric acid significantly decreased corrosion resistance, while hydrogen peroxide enhanced corrosion resistance by forming an oxide film on the surface. When comparing the performance of the MLP, DNN_Adam, and ResNet_Nadam prediction models, the DNN and ResNet models outperformed the MLP model. This superior performance is attributed to the use of appropriate optimizers (Adam and Nadam) andBayesian optimization techniques to identify the optimal hyperparameters for each model. Notably, the ResNet_Nadam model emerged as the best predictor of electrochemical characteristics, achieving the highest R2 value and showing a difference of less than 10% between training and validation data loss in the learning curve. These findings suggest that a thorough understanding of the independent variable data and the application of optimal parameters can enhance the predictive performance of deep learning models."
        },
        {
          "rank": 28,
          "score": 0.6620736718177795,
          "doc_id": "ART003029446",
          "title": "통계, 머신러닝, 딥러닝 기반 시계열 모델을 이용한 원자재 가격 예측",
          "abstract": "본 연구는 글로벌 물가 상승으로 인한 불안과 불만의 증가에 대응하기 위해 원자재 가격 예측 모델에 관한 연구를 수행하였다. 생활용품 대부분이 원자재로 제작되는 현실에서 원자재 가격의 변동이 소비자 가격에 직접적인 영향을 미치고 이에 대한 예측과 대비가 중요하다. 본 연구에서는 이를 위해 아연, 구리, 알루미늄, 리튬, 니켈 5가지의 원자재를 가격 예측 대상으로 사용하였으며, 전통적인 통계 기반 모델부터 최신 딥러닝 모델까지 총 14개의 모델을 사용하여 시계열 예측 비교 실험을 진행하였다. 실험 결과, 아연은 TiDE 모델, 알루미늄은 Transformer 모델, 리튬은 AutoARIMA 모델, 니켈은 LSTM 모델, 구리는 TBATS 모델이 가장 좋은 성능을 보였다. 본 연구 결과를 통해 다양한 원자재뿐만 아니라 다양한 시계열 기반 가격 예측 분야에서 중요한 이바지를 할 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART003029446&target=NART&cn=ART003029446",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "통계, 머신러닝, 딥러닝 기반 시계열 모델을 이용한 원자재 가격 예측 통계, 머신러닝, 딥러닝 기반 시계열 모델을 이용한 원자재 가격 예측 통계, 머신러닝, 딥러닝 기반 시계열 모델을 이용한 원자재 가격 예측 본 연구는 글로벌 물가 상승으로 인한 불안과 불만의 증가에 대응하기 위해 원자재 가격 예측 모델에 관한 연구를 수행하였다. 생활용품 대부분이 원자재로 제작되는 현실에서 원자재 가격의 변동이 소비자 가격에 직접적인 영향을 미치고 이에 대한 예측과 대비가 중요하다. 본 연구에서는 이를 위해 아연, 구리, 알루미늄, 리튬, 니켈 5가지의 원자재를 가격 예측 대상으로 사용하였으며, 전통적인 통계 기반 모델부터 최신 딥러닝 모델까지 총 14개의 모델을 사용하여 시계열 예측 비교 실험을 진행하였다. 실험 결과, 아연은 TiDE 모델, 알루미늄은 Transformer 모델, 리튬은 AutoARIMA 모델, 니켈은 LSTM 모델, 구리는 TBATS 모델이 가장 좋은 성능을 보였다. 본 연구 결과를 통해 다양한 원자재뿐만 아니라 다양한 시계열 기반 가격 예측 분야에서 중요한 이바지를 할 것으로 기대된다."
        },
        {
          "rank": 29,
          "score": 0.6594250202178955,
          "doc_id": "JAKO202404861562091",
          "title": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교",
          "abstract": "대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202404861562091&target=NART&cn=JAKO202404861562091",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다."
        },
        {
          "rank": 30,
          "score": 0.6584491729736328,
          "doc_id": "NART134433851",
          "title": "E-Commerce Inventory Prediction by Hybridization Deep and Machine Learning",
          "abstract": "<P>Inventory management is crucial for the optimisation of consumer demand and supply chains in e-commerce companies. This is the stage at which precise inventory forecasting becomes necessary for the primary objective of forecasting future demand patterns and stock levels. Traditional forecasting methods often struggle with e-commerce data due to seasonality, sudden changes in customer behaviour, and non-linearity. Machine learning (ML) and deep learning (DL) techniques have become powerful weapons for inventory prediction due to their capability of analysing huge amounts of data with high dimensionality. In highly competitive market environments, e-commerce firms can improve their resource allocation, inventory management, and customer experience. This paper proposes different types of inventories forecasting models and especially evaluates the applicability of sophisticated machine learning algorithms. While we commonly use old methods like Random Forest, ARIMA, and MLPs, they often lack the necessary robustness to non-linearity within inventory data. To address these problems, we introduce a novel method that combines convolutional neural networks (CNN) and XGBoost called CNN-XGBoost, which provides better feature extraction than the conventional prediction model and regression performance. We then compared CNN-XGBoost's performance to traditional forecasting methods (another common approach to contextualizing predictive model performance) using a 52-week simulated dataset in which we mimic patient data growing over time.We used key performance metrics such as R2, mean squared error (MSE), and mean absolute percentage error (MAPE) to assess each model's accuracy. The CNN-XGBoost model performed much better than others with an R2 of 0.78, which means our proposed model can explain more variation in comparison to other competitors, as depicted in the results section. It also had the best MSE of 0.15, indicating better predictive performance. The CNN-XGBoost model demonstrated promising prospects as a powerful inventory forecasting tool for commerce, in spite of its slightly higher MAPE value (0.69), suggesting some vulnerability to outlier data points. This study demonstrates the potential of using a convolutional neural network in combination with gradient boosting techniques to tackle the complexity of stock management issues, as well as the fact that it outperforms based line methods by a large margin.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART134433851&target=NART&cn=NART134433851",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "E-Commerce Inventory Prediction by Hybridization Deep and Machine Learning E-Commerce Inventory Prediction by Hybridization Deep and Machine Learning E-Commerce Inventory Prediction by Hybridization Deep and Machine Learning <P>Inventory management is crucial for the optimisation of consumer demand and supply chains in e-commerce companies. This is the stage at which precise inventory forecasting becomes necessary for the primary objective of forecasting future demand patterns and stock levels. Traditional forecasting methods often struggle with e-commerce data due to seasonality, sudden changes in customer behaviour, and non-linearity. Machine learning (ML) and deep learning (DL) techniques have become powerful weapons for inventory prediction due to their capability of analysing huge amounts of data with high dimensionality. In highly competitive market environments, e-commerce firms can improve their resource allocation, inventory management, and customer experience. This paper proposes different types of inventories forecasting models and especially evaluates the applicability of sophisticated machine learning algorithms. While we commonly use old methods like Random Forest, ARIMA, and MLPs, they often lack the necessary robustness to non-linearity within inventory data. To address these problems, we introduce a novel method that combines convolutional neural networks (CNN) and XGBoost called CNN-XGBoost, which provides better feature extraction than the conventional prediction model and regression performance. We then compared CNN-XGBoost's performance to traditional forecasting methods (another common approach to contextualizing predictive model performance) using a 52-week simulated dataset in which we mimic patient data growing over time.We used key performance metrics such as R2, mean squared error (MSE), and mean absolute percentage error (MAPE) to assess each model's accuracy. The CNN-XGBoost model performed much better than others with an R2 of 0.78, which means our proposed model can explain more variation in comparison to other competitors, as depicted in the results section. It also had the best MSE of 0.15, indicating better predictive performance. The CNN-XGBoost model demonstrated promising prospects as a powerful inventory forecasting tool for commerce, in spite of its slightly higher MAPE value (0.69), suggesting some vulnerability to outlier data points. This study demonstrates the potential of using a convolutional neural network in combination with gradient boosting techniques to tackle the complexity of stock management issues, as well as the fact that it outperforms based line methods by a large margin.</P>"
        },
        {
          "rank": 31,
          "score": 0.6581618189811707,
          "doc_id": "DIKO0014861002",
          "title": "딥 러닝기반 고객평점 예측모델",
          "abstract": "인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014861002&target=NART&cn=DIKO0014861002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다."
        },
        {
          "rank": 32,
          "score": 0.6570261120796204,
          "doc_id": "JAKO201723840540692",
          "title": "빅데이터 통합모형 비교분석",
          "abstract": "빅데이터가 4차 산업혁명의 핵심으로 자리하면서 빅데이터 기반 처리 및 분석 능력이 기업의 미래 경쟁력을 좌우할 전망이다. 빅데이터 처리 및 분석을 위한 RHadoop과 RHIPE 모형은 R과 Hadoop의 통합모형으로 지금까지 각각의 모형에 대해서는 연구가 많이 진행되어 왔으나 두 모형간 비교 연구는 거의 이루어 지지 않았다. 본 논문에서는 대용량의 실제 데이터와 모의실험 데이터에서 다중 회귀 (multiple regression)와 로지스틱 회귀 (logistic regression) 추정을 위한 머신러닝 (machine learning) 알고리즘을 MapReduce 프로그램 구현을 통해 RHadoop과 RHIPE 간의 비교 분석하고자 한다. 구축된 분산 클러스터 (distributed cluster) 하에서 두 모형간 성능 실험 결과, RHIPE은 RHadoop에 비해 대체로 빠른 처리속도를 보인 반면에 설치, 사용면에서 어려움을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201723840540692&target=NART&cn=JAKO201723840540692",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "빅데이터 통합모형 비교분석 빅데이터 통합모형 비교분석 빅데이터 통합모형 비교분석 빅데이터가 4차 산업혁명의 핵심으로 자리하면서 빅데이터 기반 처리 및 분석 능력이 기업의 미래 경쟁력을 좌우할 전망이다. 빅데이터 처리 및 분석을 위한 RHadoop과 RHIPE 모형은 R과 Hadoop의 통합모형으로 지금까지 각각의 모형에 대해서는 연구가 많이 진행되어 왔으나 두 모형간 비교 연구는 거의 이루어 지지 않았다. 본 논문에서는 대용량의 실제 데이터와 모의실험 데이터에서 다중 회귀 (multiple regression)와 로지스틱 회귀 (logistic regression) 추정을 위한 머신러닝 (machine learning) 알고리즘을 MapReduce 프로그램 구현을 통해 RHadoop과 RHIPE 간의 비교 분석하고자 한다. 구축된 분산 클러스터 (distributed cluster) 하에서 두 모형간 성능 실험 결과, RHIPE은 RHadoop에 비해 대체로 빠른 처리속도를 보인 반면에 설치, 사용면에서 어려움을 보였다."
        },
        {
          "rank": 33,
          "score": 0.6569152474403381,
          "doc_id": "ATN0025427236",
          "title": "딥러닝을 이용한 열 수요예측 모델 개발",
          "abstract": "In order to provide stable district heat supplying service to the certain limited residential area, it is the most important to forecast the short-term future demand more accurately and produce and supply heat in efficient way. However, it is very difficult to develop a universal heat demand forecasting model that can be applied to general situations because the factors affecting the heat consumption are very diverse and the consumption patterns are changed according to individual consumers and regional characteristics. In particular, considering all of the various variables that can affect heat demand does not help improve performance in terms of accuracy and versatility. Therefore, this study aims to develop a demand forecasting model using deep learning based on only limited information that can be acquired in real time. A demand forecasting model was developed by learning the artificial neural network of the Tensorflow using past data consisting only of the outdoor temperature of the area and date as input variables. The performance of the proposed model was evaluated by comparing the accuracy of demand predicted with the previous regression model. The proposed heat demand forecasting model in this research showed that it is possible to enhance the accuracy using only limited variables which can be secured in real time. For the demand forecasting in a certain region, the proposed model can be customized by adding some features which can reflect the regional characteristics.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0025427236&target=NART&cn=ATN0025427236",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 이용한 열 수요예측 모델 개발 딥러닝을 이용한 열 수요예측 모델 개발 딥러닝을 이용한 열 수요예측 모델 개발 In order to provide stable district heat supplying service to the certain limited residential area, it is the most important to forecast the short-term future demand more accurately and produce and supply heat in efficient way. However, it is very difficult to develop a universal heat demand forecasting model that can be applied to general situations because the factors affecting the heat consumption are very diverse and the consumption patterns are changed according to individual consumers and regional characteristics. In particular, considering all of the various variables that can affect heat demand does not help improve performance in terms of accuracy and versatility. Therefore, this study aims to develop a demand forecasting model using deep learning based on only limited information that can be acquired in real time. A demand forecasting model was developed by learning the artificial neural network of the Tensorflow using past data consisting only of the outdoor temperature of the area and date as input variables. The performance of the proposed model was evaluated by comparing the accuracy of demand predicted with the previous regression model. The proposed heat demand forecasting model in this research showed that it is possible to enhance the accuracy using only limited variables which can be secured in real time. For the demand forecasting in a certain region, the proposed model can be customized by adding some features which can reflect the regional characteristics."
        },
        {
          "rank": 34,
          "score": 0.6565304398536682,
          "doc_id": "JAKO202013661037911",
          "title": "머신러닝을 이용한 철광석 가격 예측에 대한 연구",
          "abstract": "철광석의 가격은 여러 국가와 기업들의 수요와 공급에 따라서 높은 변동성이 지속되고 있다. 이러한 비즈니스 환경에서 철광석의 가격을 예측하는 것은 중요해졌다. 본 연구는 머신러닝 기법을 이용하여 철광석이 거래되는 시점으로부터 한 달 전에 철광석 거래가격을 미리 예측하는 모형을 개발하고자 하였다. 예측 모형은 시계열 데이터를 활용한 예측 방법론으로 많이 활용되고 있는 시차분포 모형과 다층신경망 (Multi-layer perceptron), 순환신경망 (Recurrent neural network), 그리고 장단기 기억 네트워크 (Long short-term memory)와 같은 딥 러닝(Deep Learning) 모형을 사용하였다. 측정지표를 통해 개별 모형을 비교한 결과에 따르면, LSTM 모형이 예측 오차가 가장 낮은 것으로 나타났다. 또한, 앙상블 기법을 적용한 모형들을 비교한 결과, 시차분포와 LSTM의 앙상블 모형이 예측오차가 가장 낮은 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202013661037911&target=NART&cn=JAKO202013661037911",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝을 이용한 철광석 가격 예측에 대한 연구 머신러닝을 이용한 철광석 가격 예측에 대한 연구 머신러닝을 이용한 철광석 가격 예측에 대한 연구 철광석의 가격은 여러 국가와 기업들의 수요와 공급에 따라서 높은 변동성이 지속되고 있다. 이러한 비즈니스 환경에서 철광석의 가격을 예측하는 것은 중요해졌다. 본 연구는 머신러닝 기법을 이용하여 철광석이 거래되는 시점으로부터 한 달 전에 철광석 거래가격을 미리 예측하는 모형을 개발하고자 하였다. 예측 모형은 시계열 데이터를 활용한 예측 방법론으로 많이 활용되고 있는 시차분포 모형과 다층신경망 (Multi-layer perceptron), 순환신경망 (Recurrent neural network), 그리고 장단기 기억 네트워크 (Long short-term memory)와 같은 딥 러닝(Deep Learning) 모형을 사용하였다. 측정지표를 통해 개별 모형을 비교한 결과에 따르면, LSTM 모형이 예측 오차가 가장 낮은 것으로 나타났다. 또한, 앙상블 기법을 적용한 모형들을 비교한 결과, 시차분포와 LSTM의 앙상블 모형이 예측오차가 가장 낮은 것으로 나타났다."
        },
        {
          "rank": 35,
          "score": 0.656381368637085,
          "doc_id": "NART07374155",
          "title": "Artificial neural networks as applied to long-term demand forecasting",
          "abstract": "<P><B>Abstract</B></P><P>This paper reports on the application of Artificial Neural Networks (ANN) to long-term load forecasting. The ANN model is used to forecast the energy requirements of an electric utility. It is then compared to time series models. The comparison reveals that the ANN produces results that are close to the actual data. The ANN model is then used to forecast the annual peak demand of a Middle Eastern utility up to the year 2006. The results compare favorably with the utility&#x2019;s forecast.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART07374155&target=NART&cn=NART07374155",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Artificial neural networks as applied to long-term demand forecasting Artificial neural networks as applied to long-term demand forecasting Artificial neural networks as applied to long-term demand forecasting <P><B>Abstract</B></P><P>This paper reports on the application of Artificial Neural Networks (ANN) to long-term load forecasting. The ANN model is used to forecast the energy requirements of an electric utility. It is then compared to time series models. The comparison reveals that the ANN produces results that are close to the actual data. The ANN model is then used to forecast the annual peak demand of a Middle Eastern utility up to the year 2006. The results compare favorably with the utility&#x2019;s forecast.</P>"
        },
        {
          "rank": 36,
          "score": 0.6546223163604736,
          "doc_id": "ATN0049124885",
          "title": "머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구",
          "abstract": "본고는 XGBoost(Extreme Gradient Boosting), LSTM(Long Short-Term Memory) 등 다양한 머신러닝 방법을 포함한 총 13개의 모형을 활용하여 한국의 전년동월대비 물가상승률을 예측/분석하고 이를 통해 시계열 지속성과 머신러닝 예측 간의 관계에 관한 시사점을 제시한다. FRED-MD를 참고한 총 93개의 관련 국내외 거시경제/금융 변수들을 설명변수로 사용하고, 2004년 8월에서 2023년 6월까지의 표본을 고려하였다. 전월대비 물가상승률의 경우와는 달리 시계열 지속성(Persistence)이 높은 전년동월대비 물가상승률을 종속변수로 삼고 머신러닝 방법을 적용하여 예측할 때, 머신러닝 모형들의 예측력이 크게 떨어져 오히려 임의보행(Random Walk) 모형보다 예측오차가 더 큰 것으로 나타났다. 머신러닝 기법으로 시계열 지속성이 낮은 전월대비 물가상승률 예측치를 구하고 이를 통해 전년동월대비 물가상승률 예측치를 계산하면 전년동월대비 물가상승률 자체를 예측하는 경우에 비해 전반적으로 예측오차가 줄어드는 것을 확인하였고, 이 방법을 통해 전년동월대비 물가상승률 자체를 임의보행 모형으로 예측할 때보다 통계적으로 유의한 수준으로 우월한 예측치를 구할 수도 있는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0049124885&target=NART&cn=ATN0049124885",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구 머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구 머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구 본고는 XGBoost(Extreme Gradient Boosting), LSTM(Long Short-Term Memory) 등 다양한 머신러닝 방법을 포함한 총 13개의 모형을 활용하여 한국의 전년동월대비 물가상승률을 예측/분석하고 이를 통해 시계열 지속성과 머신러닝 예측 간의 관계에 관한 시사점을 제시한다. FRED-MD를 참고한 총 93개의 관련 국내외 거시경제/금융 변수들을 설명변수로 사용하고, 2004년 8월에서 2023년 6월까지의 표본을 고려하였다. 전월대비 물가상승률의 경우와는 달리 시계열 지속성(Persistence)이 높은 전년동월대비 물가상승률을 종속변수로 삼고 머신러닝 방법을 적용하여 예측할 때, 머신러닝 모형들의 예측력이 크게 떨어져 오히려 임의보행(Random Walk) 모형보다 예측오차가 더 큰 것으로 나타났다. 머신러닝 기법으로 시계열 지속성이 낮은 전월대비 물가상승률 예측치를 구하고 이를 통해 전년동월대비 물가상승률 예측치를 계산하면 전년동월대비 물가상승률 자체를 예측하는 경우에 비해 전반적으로 예측오차가 줄어드는 것을 확인하였고, 이 방법을 통해 전년동월대비 물가상승률 자체를 임의보행 모형으로 예측할 때보다 통계적으로 유의한 수준으로 우월한 예측치를 구할 수도 있는 것으로 나타났다."
        },
        {
          "rank": 37,
          "score": 0.6541521549224854,
          "doc_id": "ATN0051728135",
          "title": "딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구",
          "abstract": "하천 수위 예측의 주요 목적 중 하나는 홍수예경보 발령을 위한 기준으로 활용하는 것이다. 본 연구에서는 딥러닝 기반의 하천 수위 예측 모델을 홍수예경보 측면에서 효과적으로 활용하기 위해 학습데이터를 최적화하고, 딥러닝 모델의 정확도 향상을 평가하기 위해 딥러닝 모델의 자동 설계 및 최적화를 지원하는 AutoKeras를 활용하여 인위적인 요인을 배제한 모델을 구축하였다. 한탄강 상류유역을 대상지역으로 선정하고, 3개의 수위관측소와 유역평균강우 데이터를 구축하였고, 구축된 데이터를 이용하여 수위 변화 여부와 관계없이 강우가 발생한 모든 학습 데이터 셋을 사용한 모델(Model 1)과 일정 수준 이상의 수위 상승 변화가 있는 학습데이터 셋을 사용한 딥러닝 모델(Model 2)을 개발하여 한탄강 상류 한탄대교의 수위 및 홍수 예측 성능을 평가하였다. 실시간 하천 홍수예측 결과, 시계열 수위 예측에서 Model 1이 더 많은 데이터를 활용함으로써 상관계수와 평균제곱근오차(RMSE)에서 다소 우수한 성능을 보였다. 반면, Model 2는 홍수 예측에서 재현율(recall), F1-score, 임계성공지수(CSI) 등의 지표에서 더 뛰어난 성과를 보였다. 본 결과는 학습데이터의 특성과 구성 방식이 딥러닝 모델의 예측 능력에 큰 영향을 미친다는 것을 보여주며, 홍수와 같은 특정 사건을 예측하려면 수위 상승과 같은 핵심 요인 위주의 데이터를 더 집중적으로 학습시킬 필요가 있음을 시사한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0051728135&target=NART&cn=ATN0051728135",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구 딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구 딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구 하천 수위 예측의 주요 목적 중 하나는 홍수예경보 발령을 위한 기준으로 활용하는 것이다. 본 연구에서는 딥러닝 기반의 하천 수위 예측 모델을 홍수예경보 측면에서 효과적으로 활용하기 위해 학습데이터를 최적화하고, 딥러닝 모델의 정확도 향상을 평가하기 위해 딥러닝 모델의 자동 설계 및 최적화를 지원하는 AutoKeras를 활용하여 인위적인 요인을 배제한 모델을 구축하였다. 한탄강 상류유역을 대상지역으로 선정하고, 3개의 수위관측소와 유역평균강우 데이터를 구축하였고, 구축된 데이터를 이용하여 수위 변화 여부와 관계없이 강우가 발생한 모든 학습 데이터 셋을 사용한 모델(Model 1)과 일정 수준 이상의 수위 상승 변화가 있는 학습데이터 셋을 사용한 딥러닝 모델(Model 2)을 개발하여 한탄강 상류 한탄대교의 수위 및 홍수 예측 성능을 평가하였다. 실시간 하천 홍수예측 결과, 시계열 수위 예측에서 Model 1이 더 많은 데이터를 활용함으로써 상관계수와 평균제곱근오차(RMSE)에서 다소 우수한 성능을 보였다. 반면, Model 2는 홍수 예측에서 재현율(recall), F1-score, 임계성공지수(CSI) 등의 지표에서 더 뛰어난 성과를 보였다. 본 결과는 학습데이터의 특성과 구성 방식이 딥러닝 모델의 예측 능력에 큰 영향을 미친다는 것을 보여주며, 홍수와 같은 특정 사건을 예측하려면 수위 상승과 같은 핵심 요인 위주의 데이터를 더 집중적으로 학습시킬 필요가 있음을 시사한다."
        },
        {
          "rank": 38,
          "score": 0.6515759825706482,
          "doc_id": "ATN0038661375",
          "title": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험",
          "abstract": "Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0038661375&target=NART&cn=ATN0038661375",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model."
        },
        {
          "rank": 39,
          "score": 0.649823784828186,
          "doc_id": "NART132071160",
          "title": "Integrating machine learning and deep learning for enhanced supplier risk prediction",
          "abstract": "<P>The importance of anticipating and preventing disruptions is underscored by the increased operational complexity and vulnerability caused by advancements in supply chain management (SCM). This has spurred interest in integrating machine learning (ML) and deep learning (DL) into supply chain risk management (SCRM). In this paper, we introduce a tailored method using ML and DL to improve SCRM by predicting supplier failures, thus boosting efficiency and resilience in SC operations. Our method involves five phases focused on classifying and predicting supplier failures in non-conforming deliveries. This involves forecasting failure quantities and estimating total disruption costs. Initially, data from an automotive company is selected, and appropriate potential features and algorithms are selected, performance metric aligns with case study objectives, facilitating method evaluation are used such as: Precision, recall, F1-score, and accuracy metrics assess classification models, while Mean Squared Error (MSE) is used for regression tasks. Finally, an experimental design optimizes models, assessing success rates of various algorithms and their parameters within the chosen feature space. Experimental results underscore the success of our methodology in model development. In the classification task, the Random Forest (RF) classifier achieved 86% accuracy. When combined with the Gradient Boosting classifier, the ensemble exhibited enhanced accuracy, highlighting the complementary strengths of both algorithms and their synergistic impact, surpassing the performance of RF, Support Vector Regression (SVR), k-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). Noteworthy is the performance in regression tasks, where Linear Regression, ANN, and RF Regressor displayed exceptionally low MSE compared to other models.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART132071160&target=NART&cn=NART132071160",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Integrating machine learning and deep learning for enhanced supplier risk prediction Integrating machine learning and deep learning for enhanced supplier risk prediction Integrating machine learning and deep learning for enhanced supplier risk prediction <P>The importance of anticipating and preventing disruptions is underscored by the increased operational complexity and vulnerability caused by advancements in supply chain management (SCM). This has spurred interest in integrating machine learning (ML) and deep learning (DL) into supply chain risk management (SCRM). In this paper, we introduce a tailored method using ML and DL to improve SCRM by predicting supplier failures, thus boosting efficiency and resilience in SC operations. Our method involves five phases focused on classifying and predicting supplier failures in non-conforming deliveries. This involves forecasting failure quantities and estimating total disruption costs. Initially, data from an automotive company is selected, and appropriate potential features and algorithms are selected, performance metric aligns with case study objectives, facilitating method evaluation are used such as: Precision, recall, F1-score, and accuracy metrics assess classification models, while Mean Squared Error (MSE) is used for regression tasks. Finally, an experimental design optimizes models, assessing success rates of various algorithms and their parameters within the chosen feature space. Experimental results underscore the success of our methodology in model development. In the classification task, the Random Forest (RF) classifier achieved 86% accuracy. When combined with the Gradient Boosting classifier, the ensemble exhibited enhanced accuracy, highlighting the complementary strengths of both algorithms and their synergistic impact, surpassing the performance of RF, Support Vector Regression (SVR), k-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). Noteworthy is the performance in regression tasks, where Linear Regression, ANN, and RF Regressor displayed exceptionally low MSE compared to other models.</P>"
        },
        {
          "rank": 40,
          "score": 0.6490049362182617,
          "doc_id": "JAKO202012758284659",
          "title": "딥러닝을 활용한 다목적댐 유입량 예측",
          "abstract": "최근 데이터 예측 방법으로 인공신경망(Artificial Neural Network, ANN)분야에 대한 관심이 높아졌으며, 그 중 시계열 데이터 예측에 특화된 LSTM(Long Short-Term Memory)모형은 수문 시계열자료의 예측방법으로도 활용되고 있다. 본 연구에서는 구글에서 제공하는 딥러닝 오픈소스 라이브러리인 텐서플로우(TensorFlow)를 활용하여 LSTM모형을 구축하고 금강 상류에 위치한 용담다목적댐의 유입량을 예측하였다. 분석 자료로는 WAMIS에서 제공하는 용담댐의 2006년부터 2018년까지의 시간당 유입량 자료를 사용하였으며, 예측된 유입량과 관측 유입량의 비교를 통하여 평균제곱오차(RMSE), 평균절대오차(MAE), 용적오차(VE)를 계산하고 모형의 학습변수에 따른 정확도를 평가하였다. 분석결과, 모든 모형이 고유량에서의 정확도가 낮은 것으로 나타났으며, 이와 같은 문제를 해결하기 위하여 용담댐 유역의 시간당 강수량 자료를 추가 학습 자료로 활용하여 분석한 결과, 고유량에 대한 예측의 정확도가 높아지는 것을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202012758284659&target=NART&cn=JAKO202012758284659",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 활용한 다목적댐 유입량 예측 딥러닝을 활용한 다목적댐 유입량 예측 딥러닝을 활용한 다목적댐 유입량 예측 최근 데이터 예측 방법으로 인공신경망(Artificial Neural Network, ANN)분야에 대한 관심이 높아졌으며, 그 중 시계열 데이터 예측에 특화된 LSTM(Long Short-Term Memory)모형은 수문 시계열자료의 예측방법으로도 활용되고 있다. 본 연구에서는 구글에서 제공하는 딥러닝 오픈소스 라이브러리인 텐서플로우(TensorFlow)를 활용하여 LSTM모형을 구축하고 금강 상류에 위치한 용담다목적댐의 유입량을 예측하였다. 분석 자료로는 WAMIS에서 제공하는 용담댐의 2006년부터 2018년까지의 시간당 유입량 자료를 사용하였으며, 예측된 유입량과 관측 유입량의 비교를 통하여 평균제곱오차(RMSE), 평균절대오차(MAE), 용적오차(VE)를 계산하고 모형의 학습변수에 따른 정확도를 평가하였다. 분석결과, 모든 모형이 고유량에서의 정확도가 낮은 것으로 나타났으며, 이와 같은 문제를 해결하기 위하여 용담댐 유역의 시간당 강수량 자료를 추가 학습 자료로 활용하여 분석한 결과, 고유량에 대한 예측의 정확도가 높아지는 것을 알 수 있었다."
        },
        {
          "rank": 41,
          "score": 0.6481567621231079,
          "doc_id": "JAKO202108848920380",
          "title": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구",
          "abstract": "The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202108848920380&target=NART&cn=JAKO202108848920380",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity."
        },
        {
          "rank": 42,
          "score": 0.646591305732727,
          "doc_id": "DIKO0016605759",
          "title": "SVM, LSTM, CNN-LSTM과 TCN을 이용한 금 가격 예측",
          "abstract": "금은 불확실한 경제 시대에서 안전한 상품으로 사용되며, 세계의 많은 금융 활동에 영향을 끼치는 세계 경제의 핵심 요소이다. 이러한 금 가격에 대한 정확한 예측은 금 가격 변동에 대한 통찰력을 제공하며 투자자들이 올바른 결정을 내리는 데 도움이 되어 상당한 이익을 얻을 수 있는 기회를 제공할 수 있다. 따라서, 신뢰할 수 있는 금 가격 예측 모델의 개발은 매우 중요하다.&amp;#xD; 최근 금 가격 예측의 선행 연구에서 SVM, LSTM, CNN-LSTM, 그리고 TCN 모델이 높은 성과를 보인 바 있다. 따라서, 본 논문에서는 금 가격 예측을 위해 SVM, LSTM, CNN-LSTM과 TCN 모델을 사용하여 그 예측 성능을 비교한다. 이 연구는 2019년 1월 1일부터 2022년 9월 30일까지의 미국 달러(USD) 기준 일일 금 가격 데이터를 분석에 사용하고, 모델의 성능 평가 지표로 RMSE, MAE와 MAPE를 사용한다.&amp;#xD; 연구 결과는 TCN 모델이 나머지 모델들과 비교하여 가장 낮은 RMSE, MAE, MAPE를 보이며 가장 높은 예측성능을 보였다. 또한, MCS 분석 결과에서도 TCN 모델이 가장 우수한 모델임을 보이며, 금 가격 예측에서 TCN 모델이 차세대 딥러닝 모델로서 충분히 사용 가치가 있음을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016605759&target=NART&cn=DIKO0016605759",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SVM, LSTM, CNN-LSTM과 TCN을 이용한 금 가격 예측 SVM, LSTM, CNN-LSTM과 TCN을 이용한 금 가격 예측 SVM, LSTM, CNN-LSTM과 TCN을 이용한 금 가격 예측 금은 불확실한 경제 시대에서 안전한 상품으로 사용되며, 세계의 많은 금융 활동에 영향을 끼치는 세계 경제의 핵심 요소이다. 이러한 금 가격에 대한 정확한 예측은 금 가격 변동에 대한 통찰력을 제공하며 투자자들이 올바른 결정을 내리는 데 도움이 되어 상당한 이익을 얻을 수 있는 기회를 제공할 수 있다. 따라서, 신뢰할 수 있는 금 가격 예측 모델의 개발은 매우 중요하다.&amp;#xD; 최근 금 가격 예측의 선행 연구에서 SVM, LSTM, CNN-LSTM, 그리고 TCN 모델이 높은 성과를 보인 바 있다. 따라서, 본 논문에서는 금 가격 예측을 위해 SVM, LSTM, CNN-LSTM과 TCN 모델을 사용하여 그 예측 성능을 비교한다. 이 연구는 2019년 1월 1일부터 2022년 9월 30일까지의 미국 달러(USD) 기준 일일 금 가격 데이터를 분석에 사용하고, 모델의 성능 평가 지표로 RMSE, MAE와 MAPE를 사용한다.&amp;#xD; 연구 결과는 TCN 모델이 나머지 모델들과 비교하여 가장 낮은 RMSE, MAE, MAPE를 보이며 가장 높은 예측성능을 보였다. 또한, MCS 분석 결과에서도 TCN 모델이 가장 우수한 모델임을 보이며, 금 가격 예측에서 TCN 모델이 차세대 딥러닝 모델로서 충분히 사용 가치가 있음을 보였다."
        },
        {
          "rank": 43,
          "score": 0.6464430093765259,
          "doc_id": "DIKO0014548904",
          "title": "Mathematical prediction for stock price fluctuation by machine learning and deep learning methods",
          "abstract": "Predicting stock market fluctuation is one of the most important topics in finance.&amp;#xD; In the current study, we will predict the increase and decrease of stock prices based&amp;#xD; on machine learning, deep learning methods and mathematical methods. Among the&amp;#xD; machine learning methods, the ensemble method has higher accuracy than other single&amp;#xD; methods (Logistic Regression, Decision Tree, K-Nearest Neighbors, and Support Vector&amp;#xD; Machines) but has a disadvantage of high computational cost. Because it takes too much&amp;#xD; computation time, the ensemble method has problems with efficiency in applying it to&amp;#xD; the real market, even though it returns high accuracy. In this paper, we will introduce&amp;#xD; Regional Majority Voting, a new method to reduce the CPU-time while maintaining high&amp;#xD; accuracy of Majority Voting. Therefore, through RMV, we solved the efficiency problem&amp;#xD; which was a problem when applying ensemble method to real market.&amp;#xD; Another new prediction method is the Adaptive Data Selection method. Since stock&amp;#xD; data is volatile and influenced by many factors, it is very difficult to predict future changes&amp;#xD; unlike other data. Therefore, it has always been a difficult task to improve accuracy when&amp;#xD; predicting stock prices with general machine learning or deep learning. However, applying&amp;#xD; the ADS method newly proposed in this paper can achieve a significant improvement in&amp;#xD; accuracy. Since the ADS method selects historical data with a pattern similar to the&amp;#xD; predicted time and uses it as training data, it is possible to improve the accuracy of&amp;#xD; prediction of machine learning and deep learning. Using the standard method which&amp;#xD; trains with only the latest data, it interferes with the prediction because it contains the&amp;#xD; elements with pattern which are completely different from the data at the point to be&amp;#xD; predicted. However, since the ADS method removes these disturbing elements, it returns&amp;#xD; more accurate predictions. The data used in all the tests in this paper are based on 7&amp;#xD; national indexes for the past six years.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014548904&target=NART&cn=DIKO0014548904",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Mathematical prediction for stock price fluctuation by machine learning and deep learning methods Mathematical prediction for stock price fluctuation by machine learning and deep learning methods Mathematical prediction for stock price fluctuation by machine learning and deep learning methods Predicting stock market fluctuation is one of the most important topics in finance.&amp;#xD; In the current study, we will predict the increase and decrease of stock prices based&amp;#xD; on machine learning, deep learning methods and mathematical methods. Among the&amp;#xD; machine learning methods, the ensemble method has higher accuracy than other single&amp;#xD; methods (Logistic Regression, Decision Tree, K-Nearest Neighbors, and Support Vector&amp;#xD; Machines) but has a disadvantage of high computational cost. Because it takes too much&amp;#xD; computation time, the ensemble method has problems with efficiency in applying it to&amp;#xD; the real market, even though it returns high accuracy. In this paper, we will introduce&amp;#xD; Regional Majority Voting, a new method to reduce the CPU-time while maintaining high&amp;#xD; accuracy of Majority Voting. Therefore, through RMV, we solved the efficiency problem&amp;#xD; which was a problem when applying ensemble method to real market.&amp;#xD; Another new prediction method is the Adaptive Data Selection method. Since stock&amp;#xD; data is volatile and influenced by many factors, it is very difficult to predict future changes&amp;#xD; unlike other data. Therefore, it has always been a difficult task to improve accuracy when&amp;#xD; predicting stock prices with general machine learning or deep learning. However, applying&amp;#xD; the ADS method newly proposed in this paper can achieve a significant improvement in&amp;#xD; accuracy. Since the ADS method selects historical data with a pattern similar to the&amp;#xD; predicted time and uses it as training data, it is possible to improve the accuracy of&amp;#xD; prediction of machine learning and deep learning. Using the standard method which&amp;#xD; trains with only the latest data, it interferes with the prediction because it contains the&amp;#xD; elements with pattern which are completely different from the data at the point to be&amp;#xD; predicted. However, since the ADS method removes these disturbing elements, it returns&amp;#xD; more accurate predictions. The data used in all the tests in this paper are based on 7&amp;#xD; national indexes for the past six years."
        },
        {
          "rank": 44,
          "score": 0.6464133262634277,
          "doc_id": "ATN0052776138",
          "title": "머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구",
          "abstract": "첨단 무기체계의 도입에 따른 운영유지비 증가와 수리부속 조달환경의 악화로 인해, 정밀한 수요예측의 중요성이 더욱 강조되고 있다. 본 연구는 공군 수리부속의 수요가 소량이며 발생 간격이 불규칙한 특성으로 인해 예측이 어렵다는 점에 착안하여, 기존 통계기반 예측기법의 한계를 극복하고자 머신러닝 및 딥러닝 기반 예측모형을 적용하였다. 국방물자관리체계로 부터 수집한 약 37만 건의 수요 데이터를 유형별(Regular, Intermittent, Erratic, Lumpy)로 분류한 후, Random Forest, XG-Boost, LightGBM, LSTM, N-Beats 5가지 예측모델을 구축하고 성능을 비교하였다. 분석 결과, XG-Boost 모델이 가장 우수한 정확도(79.13%)를 기록하였으며, 그리드 서치를 통한 매개변수 최적화 결과, 품목 기준 최대 81.28%의 예측 정확도를 달성하였다. 본 연구를 통해 세부 품목별 분류 기준 정립, 최적 모델 적용 및 매개변수 튜닝 효율화 등을 통해 공군 수리부속 수요예측의 정확도를 실질적으로 향상시킬 수 있음을 실증적으로 확인하였으며, 이는 대규모 군수 데이터셋에 대한 정량적 분석과 실용적인 예측모형 적용을 통해 현장 활용 가능성이 높은 모델을 제시하였다는 점에서 기존 연구와 차별성을 지닌다. 본 연구의 결과는 향후 공군 및 국방 군수 시스템 전반의 운영 효율성 제고와 자원관리 혁신에 중요한 토대를 제공할 수 있을 것으로 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0052776138&target=NART&cn=ATN0052776138",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 첨단 무기체계의 도입에 따른 운영유지비 증가와 수리부속 조달환경의 악화로 인해, 정밀한 수요예측의 중요성이 더욱 강조되고 있다. 본 연구는 공군 수리부속의 수요가 소량이며 발생 간격이 불규칙한 특성으로 인해 예측이 어렵다는 점에 착안하여, 기존 통계기반 예측기법의 한계를 극복하고자 머신러닝 및 딥러닝 기반 예측모형을 적용하였다. 국방물자관리체계로 부터 수집한 약 37만 건의 수요 데이터를 유형별(Regular, Intermittent, Erratic, Lumpy)로 분류한 후, Random Forest, XG-Boost, LightGBM, LSTM, N-Beats 5가지 예측모델을 구축하고 성능을 비교하였다. 분석 결과, XG-Boost 모델이 가장 우수한 정확도(79.13%)를 기록하였으며, 그리드 서치를 통한 매개변수 최적화 결과, 품목 기준 최대 81.28%의 예측 정확도를 달성하였다. 본 연구를 통해 세부 품목별 분류 기준 정립, 최적 모델 적용 및 매개변수 튜닝 효율화 등을 통해 공군 수리부속 수요예측의 정확도를 실질적으로 향상시킬 수 있음을 실증적으로 확인하였으며, 이는 대규모 군수 데이터셋에 대한 정량적 분석과 실용적인 예측모형 적용을 통해 현장 활용 가능성이 높은 모델을 제시하였다는 점에서 기존 연구와 차별성을 지닌다. 본 연구의 결과는 향후 공군 및 국방 군수 시스템 전반의 운영 효율성 제고와 자원관리 혁신에 중요한 토대를 제공할 수 있을 것으로 기대한다."
        },
        {
          "rank": 45,
          "score": 0.6461352705955505,
          "doc_id": "JAKO201919341608277",
          "title": "앙상블 학습과 온도 변수를 이용한 A 호텔의 전력소모량 예측",
          "abstract": "과거의 전력소모량을 분석하여 미래의 전력소모량을 예측하는 것은 에너지 계획과 정책 결정에 있어 많은 이점을 가져다준다. 기계학습은 최근 전력소모량을 예측하는 분석 방법으로 많이 사용하고 있다. 그중 앙상블 학습은 모형의 과적합 현상을 방지하고 분산을 줄여 예측의 정확성을 높이는 방법으로 알려져 있다. 하지만 일별 데이터에 앙상블 학습을 적용했을 때 분석 방법의 특성으로 인해 피크를 잘 나타내지 못하고 중심값으로 예측하는 단점을 보였다. 본 연구에서는 앙상블 학습 전에 온도 변수와의 상관성을 고려하여 선형모형으로 적합함으로써 앙상블 학습의 단점을 보완한다. 그리고 9개의 모형을 비교한 결과 온도 변수를 선형모형으로 적합하고 랜덤포레스트를 사용한 모형이 결과가 가장 좋음을 보여준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201919341608277&target=NART&cn=JAKO201919341608277",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "앙상블 학습과 온도 변수를 이용한 A 호텔의 전력소모량 예측 앙상블 학습과 온도 변수를 이용한 A 호텔의 전력소모량 예측 앙상블 학습과 온도 변수를 이용한 A 호텔의 전력소모량 예측 과거의 전력소모량을 분석하여 미래의 전력소모량을 예측하는 것은 에너지 계획과 정책 결정에 있어 많은 이점을 가져다준다. 기계학습은 최근 전력소모량을 예측하는 분석 방법으로 많이 사용하고 있다. 그중 앙상블 학습은 모형의 과적합 현상을 방지하고 분산을 줄여 예측의 정확성을 높이는 방법으로 알려져 있다. 하지만 일별 데이터에 앙상블 학습을 적용했을 때 분석 방법의 특성으로 인해 피크를 잘 나타내지 못하고 중심값으로 예측하는 단점을 보였다. 본 연구에서는 앙상블 학습 전에 온도 변수와의 상관성을 고려하여 선형모형으로 적합함으로써 앙상블 학습의 단점을 보완한다. 그리고 9개의 모형을 비교한 결과 온도 변수를 선형모형으로 적합하고 랜덤포레스트를 사용한 모형이 결과가 가장 좋음을 보여준다."
        },
        {
          "rank": 46,
          "score": 0.6459336280822754,
          "doc_id": "JAKO202133472092958",
          "title": "머신러닝과 딥러닝 기법을 이용한 부산 전략산업과 수출에 의한 고용과 소득 예측",
          "abstract": "This paper analyzes the feasibility of using machine learning and deep learning methods to forecast the income and employment using the strategic industries as well as investment, export, and exchange rates. The decision tree, artificial neural network, support vector machine, and deep learning models were used to forecast the income and employment in Busan. The following were the main findings of the comparison of their predictive abilities. First, the decision tree models predict the income and employment well. The forecasting values for the income and employment appeared somewhat differently according to the depth of decision trees and several conditions of strategic industries as well as investment, export, and exchange rates. Second, since the artificial neural network models show that the coefficients are somewhat low and RMSE are somewhat high, these models are not good forecasting the income and employment. Third, the support vector machine models show the high predictive power with the high coefficients of determination and low RMSE. Fourth, the deep neural network models show the higher predictive power with appropriate epochs and batch sizes. Thus, since the machine learning and deep learning models can predict the employment well, we need to adopt the machine learning and deep learning models to forecast the income and employment.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202133472092958&target=NART&cn=JAKO202133472092958",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝과 딥러닝 기법을 이용한 부산 전략산업과 수출에 의한 고용과 소득 예측 머신러닝과 딥러닝 기법을 이용한 부산 전략산업과 수출에 의한 고용과 소득 예측 머신러닝과 딥러닝 기법을 이용한 부산 전략산업과 수출에 의한 고용과 소득 예측 This paper analyzes the feasibility of using machine learning and deep learning methods to forecast the income and employment using the strategic industries as well as investment, export, and exchange rates. The decision tree, artificial neural network, support vector machine, and deep learning models were used to forecast the income and employment in Busan. The following were the main findings of the comparison of their predictive abilities. First, the decision tree models predict the income and employment well. The forecasting values for the income and employment appeared somewhat differently according to the depth of decision trees and several conditions of strategic industries as well as investment, export, and exchange rates. Second, since the artificial neural network models show that the coefficients are somewhat low and RMSE are somewhat high, these models are not good forecasting the income and employment. Third, the support vector machine models show the high predictive power with the high coefficients of determination and low RMSE. Fourth, the deep neural network models show the higher predictive power with appropriate epochs and batch sizes. Thus, since the machine learning and deep learning models can predict the employment well, we need to adopt the machine learning and deep learning models to forecast the income and employment."
        },
        {
          "rank": 47,
          "score": 0.643879771232605,
          "doc_id": "JAKO202116954704821",
          "title": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측",
          "abstract": "본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116954704821&target=NART&cn=JAKO202116954704821",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다."
        },
        {
          "rank": 48,
          "score": 0.6432340145111084,
          "doc_id": "NART111572455",
          "title": "동풍 예측을 위한 딥러닝 기반의 예측 모델",
          "abstract": "Understanding the characteristics of the easterly-related weather phenomena in the eastern coast in Korean Peninsula is very important to analyze abnormal atmospheric phenomena such as heavy rain, heavy snow, and hot-dry wind. As data science techniques have steadily improved, data driven prediction models are becoming more powerful in the quantitative forecasting weather. In this paper, we apply the deep learning based methods to predict the presence or absence of the easterly wind around the Korean peninsula. The DNN, CNN, and LSTM based deep learning approaches for prediction of easterly wind are experimented and compared for the Korean Peninsula and East Sea. Vertical pressure levels of ERA5 data in year 2013 and 2014 are used.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART111572455&target=NART&cn=NART111572455",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "동풍 예측을 위한 딥러닝 기반의 예측 모델 동풍 예측을 위한 딥러닝 기반의 예측 모델 동풍 예측을 위한 딥러닝 기반의 예측 모델 Understanding the characteristics of the easterly-related weather phenomena in the eastern coast in Korean Peninsula is very important to analyze abnormal atmospheric phenomena such as heavy rain, heavy snow, and hot-dry wind. As data science techniques have steadily improved, data driven prediction models are becoming more powerful in the quantitative forecasting weather. In this paper, we apply the deep learning based methods to predict the presence or absence of the easterly wind around the Korean peninsula. The DNN, CNN, and LSTM based deep learning approaches for prediction of easterly wind are experimented and compared for the Korean Peninsula and East Sea. Vertical pressure levels of ERA5 data in year 2013 and 2014 are used."
        },
        {
          "rank": 49,
          "score": 0.6430057883262634,
          "doc_id": "JAKO201718054814596",
          "title": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석",
          "abstract": "딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201718054814596&target=NART&cn=JAKO201718054814596",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다."
        },
        {
          "rank": 50,
          "score": 0.6426259279251099,
          "doc_id": "JAKO200815652408515",
          "title": "Gompertz 소프트웨어 비용 추정 모델",
          "abstract": "본 논문은 소프트웨어 비용추정 모델의 적합성을 평가하고, 가장 적합한 모델을 제시하였다. 먼저, 해당 모델의 함수를 변수변환시켜 선형식으로 만든다. 다음으로 실제 개발 소프트웨어의 비용 데이터가 모델의 선형식에 얼마나 적합한지로 모델의 성능을 평가한다. 모델 성능평가에는 절대오차 대신 상대오차 개념인 MMRE를 적용하였다. 기존의 소프트웨어 비용추정 모델은 Weibull, Gamma와 Rayleigh 함수를 따르고 있다. 본 논문에서는 성장곡선의 일종인 Gompertz 곡선 모델을 제안하였다. 추가로 다른 성장곡선들도 적합성을 검증하였다. 모델 성능평가 결과 Gompertz 성장곡선이 소프트웨어 비용추정 모델로 가장 적합한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200815652408515&target=NART&cn=JAKO200815652408515",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Gompertz 소프트웨어 비용 추정 모델 Gompertz 소프트웨어 비용 추정 모델 Gompertz 소프트웨어 비용 추정 모델 본 논문은 소프트웨어 비용추정 모델의 적합성을 평가하고, 가장 적합한 모델을 제시하였다. 먼저, 해당 모델의 함수를 변수변환시켜 선형식으로 만든다. 다음으로 실제 개발 소프트웨어의 비용 데이터가 모델의 선형식에 얼마나 적합한지로 모델의 성능을 평가한다. 모델 성능평가에는 절대오차 대신 상대오차 개념인 MMRE를 적용하였다. 기존의 소프트웨어 비용추정 모델은 Weibull, Gamma와 Rayleigh 함수를 따르고 있다. 본 논문에서는 성장곡선의 일종인 Gompertz 곡선 모델을 제안하였다. 추가로 다른 성장곡선들도 적합성을 검증하였다. 모델 성능평가 결과 Gompertz 성장곡선이 소프트웨어 비용추정 모델로 가장 적합한 성능을 보였다."
        }
      ]
    },
    {
      "query": "What deep learning and machine learning models were evaluated for electricity demand prediction?",
      "query_meta": {
        "type": "single_hop",
        "index": 0
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8177565336227417,
          "doc_id": "ART002787934",
          "title": "Effective Electricity Demand Prediction via Deep Learning",
          "abstract": "Prediction of electricity demand in homes and buildings can be used to optimize an energy management system by decreasing energy wastage. A time-series prediction system is still a challenging problem in machine learning and deep learning. Our main idea is to compare three methods. For this work, we analyzed an electricity demand prediction system using the current state-of-the-art deep-learning methods with a machine-learning method: error correction with multi-layer perceptron (eMLP) structure, autoregressive integrated moving average (ARIMA) structure, and a proposed structure named CNN-LSTM. For this, we measured and collected electricity demand data in Germany for home appliances. We report the prediction accuracy in terms of the mean square error (MSE) and mean absolute percentage error (MAPE). The experimental result indicates that CNN-LSTM outperforms eMLP and ARIMA in accuracy.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002787934&target=NART&cn=ART002787934",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Effective Electricity Demand Prediction via Deep Learning Effective Electricity Demand Prediction via Deep Learning Effective Electricity Demand Prediction via Deep Learning Prediction of electricity demand in homes and buildings can be used to optimize an energy management system by decreasing energy wastage. A time-series prediction system is still a challenging problem in machine learning and deep learning. Our main idea is to compare three methods. For this work, we analyzed an electricity demand prediction system using the current state-of-the-art deep-learning methods with a machine-learning method: error correction with multi-layer perceptron (eMLP) structure, autoregressive integrated moving average (ARIMA) structure, and a proposed structure named CNN-LSTM. For this, we measured and collected electricity demand data in Germany for home appliances. We report the prediction accuracy in terms of the mean square error (MSE) and mean absolute percentage error (MAPE). The experimental result indicates that CNN-LSTM outperforms eMLP and ARIMA in accuracy."
        },
        {
          "rank": 2,
          "score": 0.8061591386795044,
          "doc_id": "JAKO201726163356540",
          "title": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델",
          "abstract": "본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201726163356540&target=NART&cn=JAKO201726163356540",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다."
        },
        {
          "rank": 3,
          "score": 0.8001939654350281,
          "doc_id": "DIKO0015771393",
          "title": "딥러닝 기술을 이용한 전력 수요 예측 방법",
          "abstract": "정확한 전력 수요 예측은 전력수급시스템의 안정을 위해 중요하다. 또한, 불필요한 비용 및 재난 안전사고를 최소화하기 위해 필수적이다. 그러나 전력 수요는 기후, 시간대, 공휴일 등의 영향을 받아 변동성이 있으며 비선형적인 특성이 있기에 예측에 어려움을 겪는다.&amp;#xD; 본 논문에서는 전력 수요 예측 과정에서 발생하는 불확실성을 최소화하기 위한 전력 수요 예측 모델을 제시한다. 국내 전력 공급업체 중 하나인 ㈜JB의 발전기 전력 데이터를 사용해 발전기 전력 수요 예측 모델을 구현하였으며, AMI(Advanced Metering Infrastructure) 데이터를 사용해 AMI 전력 수요 예측 모델을 구현하였다. &amp;#xD; 발전기 전력 수요 예측에는 전력 수요량에 영향을 줄 수 있는 기상 변수와 공휴일 변수 등을 사용한다. 그리고 LSTM에 Attention Mechanism을 추가한 알고리즘을 사용해 예측 모델을 구현한다. 실험을 통해 성능을 측정한 결과, 제안한 모델이 가장 낮은 평균 제곱근 오차와 절대 평균 백분율 오차를 가지며 우수한 성능을 보인다. 또한, 결과에 영향을 미치는 중요 변수를 확인함으로써 설명이 가능한 모델을 제안한다. &amp;#xD; AMI 전력 수요 예측은 전체 71세대의 전력 사용량을 HDBSCAN 클러스터링을 통해 분석한다. 그리고 클러스터별로 Bayesian Optimization 기법을 적용해 LSTM 알고리즘의 최적 하이퍼 파라미터를 선정한다. 선정한 하이퍼 파라미터를 적용한 클러스터별 예측 모델을 구현한다. 실험을 통해 성능을 측정한 결과, 제안한 모델이 기본 하이퍼 파라미터를 적용한 모델보다 낮은 평균 제곱근 오차를 가지며 우수한 성능을 보인다.&amp;#xD; 본 연구에서 제안하는 방법을 사용했을 때 더욱 정확한 전력 수요 예측을 기대할 수 있으며, 상황에 따른 전력 수요량 예측이 가능하므로 안정적인 전력의 공급, 전력 시스템의 효율적인 운영관리 및 안전 운행을 기대할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015771393&target=NART&cn=DIKO0015771393",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기술을 이용한 전력 수요 예측 방법 딥러닝 기술을 이용한 전력 수요 예측 방법 딥러닝 기술을 이용한 전력 수요 예측 방법 정확한 전력 수요 예측은 전력수급시스템의 안정을 위해 중요하다. 또한, 불필요한 비용 및 재난 안전사고를 최소화하기 위해 필수적이다. 그러나 전력 수요는 기후, 시간대, 공휴일 등의 영향을 받아 변동성이 있으며 비선형적인 특성이 있기에 예측에 어려움을 겪는다.&amp;#xD; 본 논문에서는 전력 수요 예측 과정에서 발생하는 불확실성을 최소화하기 위한 전력 수요 예측 모델을 제시한다. 국내 전력 공급업체 중 하나인 ㈜JB의 발전기 전력 데이터를 사용해 발전기 전력 수요 예측 모델을 구현하였으며, AMI(Advanced Metering Infrastructure) 데이터를 사용해 AMI 전력 수요 예측 모델을 구현하였다. &amp;#xD; 발전기 전력 수요 예측에는 전력 수요량에 영향을 줄 수 있는 기상 변수와 공휴일 변수 등을 사용한다. 그리고 LSTM에 Attention Mechanism을 추가한 알고리즘을 사용해 예측 모델을 구현한다. 실험을 통해 성능을 측정한 결과, 제안한 모델이 가장 낮은 평균 제곱근 오차와 절대 평균 백분율 오차를 가지며 우수한 성능을 보인다. 또한, 결과에 영향을 미치는 중요 변수를 확인함으로써 설명이 가능한 모델을 제안한다. &amp;#xD; AMI 전력 수요 예측은 전체 71세대의 전력 사용량을 HDBSCAN 클러스터링을 통해 분석한다. 그리고 클러스터별로 Bayesian Optimization 기법을 적용해 LSTM 알고리즘의 최적 하이퍼 파라미터를 선정한다. 선정한 하이퍼 파라미터를 적용한 클러스터별 예측 모델을 구현한다. 실험을 통해 성능을 측정한 결과, 제안한 모델이 기본 하이퍼 파라미터를 적용한 모델보다 낮은 평균 제곱근 오차를 가지며 우수한 성능을 보인다.&amp;#xD; 본 연구에서 제안하는 방법을 사용했을 때 더욱 정확한 전력 수요 예측을 기대할 수 있으며, 상황에 따른 전력 수요량 예측이 가능하므로 안정적인 전력의 공급, 전력 시스템의 효율적인 운영관리 및 안전 운행을 기대할 수 있다."
        },
        {
          "rank": 4,
          "score": 0.7943707704544067,
          "doc_id": "NART113695778",
          "title": "Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning",
          "abstract": "<P>Due to severe climate change impact on electricity consumption, as well as new trends in smart grids (such as the use of renewable resources and the advent of prosumers and energy commons), medium-term and long-term electricity load forecasting has become a crucial need. Such forecasts are necessary to support the plans and decisions related to the capacity evaluation of centralized and decentralized power generation systems, demand response strategies, and controlling the operation. To address this problem, the main objective of this study is to develop and compare precise district level models for predicting the electrical load demand based on machine learning techniques including support vector machine (SVM) and Random Forest (RF), and deep learning methods such as non-linear auto-regressive exogenous (NARX) neural network and recurrent neural networks (Long Short-Term Memory-LSTM). A dataset including nine years of historical load demand for Bruce County, Ontario, Canada, fused with the climatic information (temperature and wind speed) are used to train the models after completing the preprocessing and cleaning stages. The results show that by employing deep learning, the model could predict the load demand more accurately than SVM and RF, with an R-Squared of about 0.93-0.96 and Mean Absolute Percentage Error (MAPE) of about 4-10%. The model can be used not only by the municipalities as well as utility companies and power distributors in the management and expansion of electricity grids; but also by the households to make decisions on the adoption of home- and district-scale renewable energy technologies.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART113695778&target=NART&cn=NART113695778",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning <P>Due to severe climate change impact on electricity consumption, as well as new trends in smart grids (such as the use of renewable resources and the advent of prosumers and energy commons), medium-term and long-term electricity load forecasting has become a crucial need. Such forecasts are necessary to support the plans and decisions related to the capacity evaluation of centralized and decentralized power generation systems, demand response strategies, and controlling the operation. To address this problem, the main objective of this study is to develop and compare precise district level models for predicting the electrical load demand based on machine learning techniques including support vector machine (SVM) and Random Forest (RF), and deep learning methods such as non-linear auto-regressive exogenous (NARX) neural network and recurrent neural networks (Long Short-Term Memory-LSTM). A dataset including nine years of historical load demand for Bruce County, Ontario, Canada, fused with the climatic information (temperature and wind speed) are used to train the models after completing the preprocessing and cleaning stages. The results show that by employing deep learning, the model could predict the load demand more accurately than SVM and RF, with an R-Squared of about 0.93-0.96 and Mean Absolute Percentage Error (MAPE) of about 4-10%. The model can be used not only by the municipalities as well as utility companies and power distributors in the management and expansion of electricity grids; but also by the households to make decisions on the adoption of home- and district-scale renewable energy technologies.</P>"
        },
        {
          "rank": 5,
          "score": 0.7927114367485046,
          "doc_id": "NPAP13485205",
          "title": "머신러닝 및 딥러닝 모델의 스태킹 앙상블을 이용한 단기 전력수요 예측에 관한 연구",
          "abstract": "전력수요는 월, 요일 및 시간의 계절성(Seasonality)을 보이는 데이터이다. 각 계절성에 따라 특성이 다르기 때문에, 전력수요를 예측하기 위해서는 계절성의 특성을 고려한 다양한 모델을 선정하고, 병합하는 방법이 필요하다. 본 연구에서는 전력수요의 계절성을 고려한 다양한 예측모델을 병합하여 이용할 수 있도록 스태킹 앙상블 적용하고 실험결과를 기술한다. 또한, 162개 도시의 기상 데이터와 인구 데이터를 예측에 이용하는 방법, Regression 모델과 Time-series모델에 입력하는 특징(Feature)의 전처리 방법, 베이지안 최적화를 이용한 머신러닝 및 딥러닝 모델의 하이퍼파라메터 최적화 방법을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP13485205&target=NART&cn=NPAP13485205",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 및 딥러닝 모델의 스태킹 앙상블을 이용한 단기 전력수요 예측에 관한 연구 머신러닝 및 딥러닝 모델의 스태킹 앙상블을 이용한 단기 전력수요 예측에 관한 연구 머신러닝 및 딥러닝 모델의 스태킹 앙상블을 이용한 단기 전력수요 예측에 관한 연구 전력수요는 월, 요일 및 시간의 계절성(Seasonality)을 보이는 데이터이다. 각 계절성에 따라 특성이 다르기 때문에, 전력수요를 예측하기 위해서는 계절성의 특성을 고려한 다양한 모델을 선정하고, 병합하는 방법이 필요하다. 본 연구에서는 전력수요의 계절성을 고려한 다양한 예측모델을 병합하여 이용할 수 있도록 스태킹 앙상블 적용하고 실험결과를 기술한다. 또한, 162개 도시의 기상 데이터와 인구 데이터를 예측에 이용하는 방법, Regression 모델과 Time-series모델에 입력하는 특징(Feature)의 전처리 방법, 베이지안 최적화를 이용한 머신러닝 및 딥러닝 모델의 하이퍼파라메터 최적화 방법을 제시한다."
        },
        {
          "rank": 6,
          "score": 0.7810021042823792,
          "doc_id": "DIKO0017291669",
          "title": "다중 변수 융합을 통한 Hybrid Gated Fusion 기반 딥러닝 모델을 활용한 전력 수요 예측",
          "abstract": "최근 환경오염으로 인한 기후 이상 현상, 산업구조의 디지털 전환, 에너지 정책 및 인구 변화 등 복합적인 외부 요인으로 인해 전력수요는 과거보다 더 복잡하고 예측이 어려운 양상을 띄고 있다. &amp;#xD; 특히, 우리나라 전력 시장은 하루 전 수요예측 데이터를 기반으로 전력 공급이 이루어지기 때문에, 예측의 정확도가 낮을 경우 불필요한 전력 생산 또는 공급 부족같은 문제로 이어질 수 있다. 이는 발전 비용 및 출력 제어 비용 낭비, 급전 비용 상승으로 인한 전력 요금 인상, 정전 위험 등 많은 손실을 초래하므로 보다 정밀하고 신뢰성 높은 예측 모델이 필요하다. &amp;#xD; 이에 본 연구는 전력 수요 예측의 정확도 한계를 극복하고 전력 수요 패턴의 변동성에 능동적으로 대응하기 위해, 다양한 외생 변수를 통합하고 이를 효과적으로 학습할 수 있는 Hybrid 딥러닝 모델을 제안하고자 한다. &amp;#xD; 특히 시계열 데이터 흐름을 잘 반영하는 LSTM(Long Short-Term Memory)과 전역적 패턴 학습에 특화된 Transformer 의 장점을 동시에 활용하기 위해 두 모델의 구조를 통합한 Hybrid 모델을 구성하였으며, 여러가지 Fusion 기법을 적용하여 두 모델 간 정보를 효과적으로 조합하여 예측의 정확성과 안정성을 동시에 향상시켰다. &amp;#xD; 예측 모델은 전력 사용량, 캘린더 정보, 기온 민감도(CDD/HDD), 대중교통 이용량 등 6 개 외생 변수를 중심으로 설계된 5 가지 시나리오에 따라 학습되었으며, MAE, RMSE, MAPE를 기준으로 성능을 비교하였다.&amp;#xD; 단일 모델은 외생 변수가 없는 경우 높은 오차율을 보인 반면, Hybrid 모델은 모든 시나리오에서 우수한 예측 성능을 보였다. 특히 Gated Fusion 기반 Hybrid 모델은 최종 시나리오에서 MAPE 4.3%로 가장 낮은 오차를 기록하였다. 추가적으로 수행한 잔차 분석, 정규성 검정, 대응표본 t-검정 결과를 통해 해당 모델의 통계적 유의성과 예측 신뢰도를 뒷받침하였다. &amp;#xD; 결론적으로 본 연구는 전력 수요 예측에서 외생 변수 융합과 Hybrid 모델 구조가 실질적인 예측 성능 향상에 기여함을 입증하였으며, 향후 에너지 수급 계획 및 정책 수립 등에 실무적으로 적용 가능한 정교한 수요 예측 모델 개발의 기반을 제공하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017291669&target=NART&cn=DIKO0017291669",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "다중 변수 융합을 통한 Hybrid Gated Fusion 기반 딥러닝 모델을 활용한 전력 수요 예측 다중 변수 융합을 통한 Hybrid Gated Fusion 기반 딥러닝 모델을 활용한 전력 수요 예측 다중 변수 융합을 통한 Hybrid Gated Fusion 기반 딥러닝 모델을 활용한 전력 수요 예측 최근 환경오염으로 인한 기후 이상 현상, 산업구조의 디지털 전환, 에너지 정책 및 인구 변화 등 복합적인 외부 요인으로 인해 전력수요는 과거보다 더 복잡하고 예측이 어려운 양상을 띄고 있다. &amp;#xD; 특히, 우리나라 전력 시장은 하루 전 수요예측 데이터를 기반으로 전력 공급이 이루어지기 때문에, 예측의 정확도가 낮을 경우 불필요한 전력 생산 또는 공급 부족같은 문제로 이어질 수 있다. 이는 발전 비용 및 출력 제어 비용 낭비, 급전 비용 상승으로 인한 전력 요금 인상, 정전 위험 등 많은 손실을 초래하므로 보다 정밀하고 신뢰성 높은 예측 모델이 필요하다. &amp;#xD; 이에 본 연구는 전력 수요 예측의 정확도 한계를 극복하고 전력 수요 패턴의 변동성에 능동적으로 대응하기 위해, 다양한 외생 변수를 통합하고 이를 효과적으로 학습할 수 있는 Hybrid 딥러닝 모델을 제안하고자 한다. &amp;#xD; 특히 시계열 데이터 흐름을 잘 반영하는 LSTM(Long Short-Term Memory)과 전역적 패턴 학습에 특화된 Transformer 의 장점을 동시에 활용하기 위해 두 모델의 구조를 통합한 Hybrid 모델을 구성하였으며, 여러가지 Fusion 기법을 적용하여 두 모델 간 정보를 효과적으로 조합하여 예측의 정확성과 안정성을 동시에 향상시켰다. &amp;#xD; 예측 모델은 전력 사용량, 캘린더 정보, 기온 민감도(CDD/HDD), 대중교통 이용량 등 6 개 외생 변수를 중심으로 설계된 5 가지 시나리오에 따라 학습되었으며, MAE, RMSE, MAPE를 기준으로 성능을 비교하였다.&amp;#xD; 단일 모델은 외생 변수가 없는 경우 높은 오차율을 보인 반면, Hybrid 모델은 모든 시나리오에서 우수한 예측 성능을 보였다. 특히 Gated Fusion 기반 Hybrid 모델은 최종 시나리오에서 MAPE 4.3%로 가장 낮은 오차를 기록하였다. 추가적으로 수행한 잔차 분석, 정규성 검정, 대응표본 t-검정 결과를 통해 해당 모델의 통계적 유의성과 예측 신뢰도를 뒷받침하였다. &amp;#xD; 결론적으로 본 연구는 전력 수요 예측에서 외생 변수 융합과 Hybrid 모델 구조가 실질적인 예측 성능 향상에 기여함을 입증하였으며, 향후 에너지 수급 계획 및 정책 수립 등에 실무적으로 적용 가능한 정교한 수요 예측 모델 개발의 기반을 제공하고자 한다."
        },
        {
          "rank": 7,
          "score": 0.778916597366333,
          "doc_id": "ATN0044029065",
          "title": "TCN 딥러닝 모델을 이용한 최대전력 예측에 관한 연구",
          "abstract": "It is necessary to predict peak load accurately in order to supply electric power and operate the power system stably. Especially,it is more important to predict peak load accurately in winter and summer because peak load is higher than other seasons. If peakload is predicted to be higher than actual peak load, the start-up costs of power plants would increase. It causes economic loss to thecompany. On the other hand, if the peak load is predicted to be lower than the actual peak load, blackout may occur due to a lackof power plants capable of generating electricity. Economic losses and blackouts can be prevented by minimizing the prediction errorof the peak load. In this paper, the latest deep learning model such as TCN is used to minimize the prediction error of peak load. Evenif the same deep learning model is used, there is a difference in performance depending on the hyper-parameters. So, I propose methodsfor optimizing hyper-parameters of TCN for predicting the peak load. Data from 2006 to 2021 were input into the model and trained,and prediction error was tested using data in 2022. It was confirmed that the performance of the deep learning model optimized bythe methods proposed in this study is superior to other deep learning models.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0044029065&target=NART&cn=ATN0044029065",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "TCN 딥러닝 모델을 이용한 최대전력 예측에 관한 연구 TCN 딥러닝 모델을 이용한 최대전력 예측에 관한 연구 TCN 딥러닝 모델을 이용한 최대전력 예측에 관한 연구 It is necessary to predict peak load accurately in order to supply electric power and operate the power system stably. Especially,it is more important to predict peak load accurately in winter and summer because peak load is higher than other seasons. If peakload is predicted to be higher than actual peak load, the start-up costs of power plants would increase. It causes economic loss to thecompany. On the other hand, if the peak load is predicted to be lower than the actual peak load, blackout may occur due to a lackof power plants capable of generating electricity. Economic losses and blackouts can be prevented by minimizing the prediction errorof the peak load. In this paper, the latest deep learning model such as TCN is used to minimize the prediction error of peak load. Evenif the same deep learning model is used, there is a difference in performance depending on the hyper-parameters. So, I propose methodsfor optimizing hyper-parameters of TCN for predicting the peak load. Data from 2006 to 2021 were input into the model and trained,and prediction error was tested using data in 2022. It was confirmed that the performance of the deep learning model optimized bythe methods proposed in this study is superior to other deep learning models."
        },
        {
          "rank": 8,
          "score": 0.7722798585891724,
          "doc_id": "NART125164824",
          "title": "Electricity Consumption Prediction Using Machine Learning",
          "abstract": "<P>The use of electricity has a significant impact on the environment, energy distribution costs, and energy management since it directly impacts these costs. Long-standing techniques have inherent limits in terms of accuracy and scalability when it comes to predicting power usage. It is now feasible to properly anticipate power use using previous data thanks to improvements in machine learning techniques. In this paper, we provide a machine learning-based method for forecasting power use. In this study, we investigate a number of machine learning techniques, including linear regression, K Nearest Neighbours, XGBOOST, random forest, and artificial neural networks(ANN), to forecast power usage. Using historical electricity use data received from a power utility business, we trained and assessed these models. The data is a year&rsquo;s worth of hourly power use that has been pre-processed to address outliers and missing numbers. Various assessment measures, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination (R2), were used to assess the performance of the models [19]. The outcomes demonstrate that the suggested method may accurately forecast power use. The K Nearest Neighbours(KNN) model outperformed all others in terms of performance, with a 90.92% accuracy rate for predicting agricultural production</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART125164824&target=NART&cn=NART125164824",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Electricity Consumption Prediction Using Machine Learning Electricity Consumption Prediction Using Machine Learning Electricity Consumption Prediction Using Machine Learning <P>The use of electricity has a significant impact on the environment, energy distribution costs, and energy management since it directly impacts these costs. Long-standing techniques have inherent limits in terms of accuracy and scalability when it comes to predicting power usage. It is now feasible to properly anticipate power use using previous data thanks to improvements in machine learning techniques. In this paper, we provide a machine learning-based method for forecasting power use. In this study, we investigate a number of machine learning techniques, including linear regression, K Nearest Neighbours, XGBOOST, random forest, and artificial neural networks(ANN), to forecast power usage. Using historical electricity use data received from a power utility business, we trained and assessed these models. The data is a year&rsquo;s worth of hourly power use that has been pre-processed to address outliers and missing numbers. Various assessment measures, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination (R2), were used to assess the performance of the models [19]. The outcomes demonstrate that the suggested method may accurately forecast power use. The K Nearest Neighbours(KNN) model outperformed all others in terms of performance, with a 90.92% accuracy rate for predicting agricultural production</P>"
        },
        {
          "rank": 9,
          "score": 0.772156834602356,
          "doc_id": "NART99478101",
          "title": "Building thermal load prediction through shallow machine learning and deep learning",
          "abstract": "<P><B>Abstract</B></P>  <P>Building thermal load prediction informs the optimization of cooling plant and thermal energy storage. Physics-based prediction models of building thermal load are constrained by the model and input complexity. In this study, we developed 12 data-driven models (7 shallow learning, 2 deep learning, and 3 heuristic methods) to predict building thermal load and compared shallow machine learning and deep learning. The 12 prediction models were compared with the measured cooling demand. It was found XGBoost (Extreme Gradient Boost) and LSTM (Long Short Term Memory) provided the most accurate load prediction in the shallow and deep learning category, and both outperformed the best baseline model, which uses the previous day&rsquo;s data for prediction. Then, we discussed how the prediction horizon and input uncertainty would influence the load prediction accuracy. Major conclusions are twofold: first, LSTM performs well in short-term prediction (1 h ahead) but not in long term prediction (24 h ahead), because the sequential information becomes less relevant and accordingly not so useful when the prediction horizon is long. Second, the presence of weather forecast uncertainty deteriorates XGBoost&rsquo;s accuracy and favors LSTM, because the sequential information makes the model more robust to input uncertainty. Training the model with the uncertain rather than accurate weather data could enhance the model&rsquo;s robustness. Our findings have two implications for practice. First, LSTM is recommended for short-term load prediction given that weather forecast uncertainty is unavoidable. Second, XGBoost is recommended for long term prediction, and the model should be trained with the presence of input uncertainty.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Building load prediction informs chiller plant and thermal storage optimization. </LI> <LI>  We used and compared 9 machine learning algorithms and 3 heuristic prediction methods. </LI> <LI>  XGBoost and LSTM are found to be the best shallow and deep learning algorithm. </LI> <LI>  LSTM is better for short term prediction, while XGBoost for long term prediction. </LI> <LI>  It is better to train the model with uncertain rather than accurate weather data. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART99478101&target=NART&cn=NART99478101",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Building thermal load prediction through shallow machine learning and deep learning Building thermal load prediction through shallow machine learning and deep learning Building thermal load prediction through shallow machine learning and deep learning <P><B>Abstract</B></P>  <P>Building thermal load prediction informs the optimization of cooling plant and thermal energy storage. Physics-based prediction models of building thermal load are constrained by the model and input complexity. In this study, we developed 12 data-driven models (7 shallow learning, 2 deep learning, and 3 heuristic methods) to predict building thermal load and compared shallow machine learning and deep learning. The 12 prediction models were compared with the measured cooling demand. It was found XGBoost (Extreme Gradient Boost) and LSTM (Long Short Term Memory) provided the most accurate load prediction in the shallow and deep learning category, and both outperformed the best baseline model, which uses the previous day&rsquo;s data for prediction. Then, we discussed how the prediction horizon and input uncertainty would influence the load prediction accuracy. Major conclusions are twofold: first, LSTM performs well in short-term prediction (1 h ahead) but not in long term prediction (24 h ahead), because the sequential information becomes less relevant and accordingly not so useful when the prediction horizon is long. Second, the presence of weather forecast uncertainty deteriorates XGBoost&rsquo;s accuracy and favors LSTM, because the sequential information makes the model more robust to input uncertainty. Training the model with the uncertain rather than accurate weather data could enhance the model&rsquo;s robustness. Our findings have two implications for practice. First, LSTM is recommended for short-term load prediction given that weather forecast uncertainty is unavoidable. Second, XGBoost is recommended for long term prediction, and the model should be trained with the presence of input uncertainty.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Building load prediction informs chiller plant and thermal storage optimization. </LI> <LI>  We used and compared 9 machine learning algorithms and 3 heuristic prediction methods. </LI> <LI>  XGBoost and LSTM are found to be the best shallow and deep learning algorithm. </LI> <LI>  LSTM is better for short term prediction, while XGBoost for long term prediction. </LI> <LI>  It is better to train the model with uncertain rather than accurate weather data. </LI> </UL> </P>"
        },
        {
          "rank": 10,
          "score": 0.7664695978164673,
          "doc_id": "ATN0037496660",
          "title": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교",
          "abstract": "Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037496660&target=NART&cn=ATN0037496660",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model."
        },
        {
          "rank": 11,
          "score": 0.7617771029472351,
          "doc_id": "ATN0050716044",
          "title": "국민DR 참여활성화를 위한 머신러닝 기반의 전력수요 예측에 관한 연구",
          "abstract": "전력수요예측은 여러 가지 독립변수들이 필요하고 날씨, 기온, 습도, 환경적 영향을 고려하여 딥러닝과 머신러닝을 통하여 예측하는 선행 기술이 다수가 존재한다. 전력수요 예측을 위해서는 많은 인공지능 모델들이 있지만 그중에서 회귀분석과 다층신경망을 구성하여 독립변수에 미치는 종속변수의 추론하는데 오차가 발생하고 이를 해결하기 위해서 기계학습으로 반복하여 오차율을 줄이는 방법을 채택하여 실험을 진행하였다. 실제 전력 사용 데이터를 산업단지 내의 공장의 1년의 데이터를 기준으로 머신러닝을 진행하였다. 이번 연구에서는 독립변수를 채택하지 않고 실제 전력 사용데이터를 가지고 트레이닝 데이터셋을 통하여 다층신경망을 구성하여 기계학습으로 전력예측값을 추론하였으며 일간, 주간, 월간 전력사용량을 예측하였다. 이를 통해 대표적인 전력수요관리인 국민DR 참여 활성화에 대한 방향성을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0050716044&target=NART&cn=ATN0050716044",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "국민DR 참여활성화를 위한 머신러닝 기반의 전력수요 예측에 관한 연구 국민DR 참여활성화를 위한 머신러닝 기반의 전력수요 예측에 관한 연구 국민DR 참여활성화를 위한 머신러닝 기반의 전력수요 예측에 관한 연구 전력수요예측은 여러 가지 독립변수들이 필요하고 날씨, 기온, 습도, 환경적 영향을 고려하여 딥러닝과 머신러닝을 통하여 예측하는 선행 기술이 다수가 존재한다. 전력수요 예측을 위해서는 많은 인공지능 모델들이 있지만 그중에서 회귀분석과 다층신경망을 구성하여 독립변수에 미치는 종속변수의 추론하는데 오차가 발생하고 이를 해결하기 위해서 기계학습으로 반복하여 오차율을 줄이는 방법을 채택하여 실험을 진행하였다. 실제 전력 사용 데이터를 산업단지 내의 공장의 1년의 데이터를 기준으로 머신러닝을 진행하였다. 이번 연구에서는 독립변수를 채택하지 않고 실제 전력 사용데이터를 가지고 트레이닝 데이터셋을 통하여 다층신경망을 구성하여 기계학습으로 전력예측값을 추론하였으며 일간, 주간, 월간 전력사용량을 예측하였다. 이를 통해 대표적인 전력수요관리인 국민DR 참여 활성화에 대한 방향성을 제시한다."
        },
        {
          "rank": 12,
          "score": 0.7565658092498779,
          "doc_id": "JAKO202305062334676",
          "title": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측",
          "abstract": "본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202305062334676&target=NART&cn=JAKO202305062334676",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다."
        },
        {
          "rank": 13,
          "score": 0.7546472549438477,
          "doc_id": "JAKO202222059037013",
          "title": "기온 데이터를 반영한 전력수요 예측 딥러닝 모델",
          "abstract": "최근 전력수요를 예측하기 위해 통계기반 시계열 분석 기법을 대체하기 위해 딥러닝 기법을 활용한 연구가 활발히 진행되고 있다. 딥러닝 기반 전력수요 예측 연구 결과를 분석한 결과, LSTM 기반 예측 모델의 성능이 우수한 것으로 규명되었으나 장기간의 지역 범위 전력수요 예측에 대해 LSTM 기반 모델의 성능이 충분하지 않음을 확인할 수 있다. 본 연구에서는 기온 데이터를 반영하여 24시간 이전에 전력수요를 예측하는 WaveNet 기반 딥러닝 모델을 개발하여, 실제 사용하고 있는 통계적 시계열 예측 기법의 정확도(MAPE 값 2%)보다 우수한 예측 성능을 달성하는 모델을 개발하고자 한다. 먼저 WaveNet의 핵심 구조인 팽창인과 1차원 합성곱 신경망 구조를 소개하고, 전력수요와 기온 데이터를 입력값으로 모델에 주입하기 위한 데이터 전처리 과정을 제시한다. 다음으로, 개선된 WaveNet 모델을 학습하고 검증하는 방법을 제시한다. 성능 비교 결과, WaveNet 기반 모델에 기온 데이터를 반영한 방법은 전체 검증데이터에 대해 MAPE 값 1.33%를 달성하였고, 동일한 구조의 모델에서 기온 데이터를 반영하지 않는 것(MAPE 값 2.31%)보다 우수한 전력수요 예측 결과를 나타내고 있음을 확인할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202222059037013&target=NART&cn=JAKO202222059037013",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "기온 데이터를 반영한 전력수요 예측 딥러닝 모델 기온 데이터를 반영한 전력수요 예측 딥러닝 모델 기온 데이터를 반영한 전력수요 예측 딥러닝 모델 최근 전력수요를 예측하기 위해 통계기반 시계열 분석 기법을 대체하기 위해 딥러닝 기법을 활용한 연구가 활발히 진행되고 있다. 딥러닝 기반 전력수요 예측 연구 결과를 분석한 결과, LSTM 기반 예측 모델의 성능이 우수한 것으로 규명되었으나 장기간의 지역 범위 전력수요 예측에 대해 LSTM 기반 모델의 성능이 충분하지 않음을 확인할 수 있다. 본 연구에서는 기온 데이터를 반영하여 24시간 이전에 전력수요를 예측하는 WaveNet 기반 딥러닝 모델을 개발하여, 실제 사용하고 있는 통계적 시계열 예측 기법의 정확도(MAPE 값 2%)보다 우수한 예측 성능을 달성하는 모델을 개발하고자 한다. 먼저 WaveNet의 핵심 구조인 팽창인과 1차원 합성곱 신경망 구조를 소개하고, 전력수요와 기온 데이터를 입력값으로 모델에 주입하기 위한 데이터 전처리 과정을 제시한다. 다음으로, 개선된 WaveNet 모델을 학습하고 검증하는 방법을 제시한다. 성능 비교 결과, WaveNet 기반 모델에 기온 데이터를 반영한 방법은 전체 검증데이터에 대해 MAPE 값 1.33%를 달성하였고, 동일한 구조의 모델에서 기온 데이터를 반영하지 않는 것(MAPE 값 2.31%)보다 우수한 전력수요 예측 결과를 나타내고 있음을 확인할 수 있다."
        },
        {
          "rank": 14,
          "score": 0.753998875617981,
          "doc_id": "JAKO202401558659086",
          "title": "공장 전력 절감을 위한 인공지능 기반의 에너지 관리 시스템 개발",
          "abstract": "본 연구는 IoT 센싱 기술을 활용하여 구축된 빅데이터 수집 시스템을 통해 제주삼다수 공장에서 생성된 데이터를 활용하여 피크 전력 사용을 예측하는 인공지능 모델을 개발하고 비교 분석하였다. LSTM(Long Short-Term Memory) 모델은 단일 변수 시계열 데이터에서 R<sup>2</sup>=0.98, RMSE=0.039, MAE=0.026으로 가장 높은 예측 정확도를 기록하였으며, XGBoost(eXtreme Gradient Boosting) 모델은 다변량 데이터를 효과적으로 처리하며 R<sup>2</sup>=0.93, RMSE=0.018, MAE=0.013의 성능을 보였다. 연구 과정에서 다양한 데이터 전처리 방법과 특징 조합을 실험적으로 적용하여 모델의 성능을 최적화하였으며, 이를 통해 데이터 전처리와 변수 선택이 예측 정확도에 미치는 영향을 입증하였다. 연구 결과, 최적화된 인공지능 모델을 활용한 피크 전력 예측은 전력 비용 절감과 약 10~15%의 탄소 배출 감소 효과를 달성할 수 있음을 제시하였다. 이는 ESG(환경, 사회, 지배구조) 경영을 목표로 하는 기업들에게 지속 가능성을 실현하기 위한 실질적이고 구체적인 전략을 제공하며, 제조업, 물류, 스마트 팩토리 등 다양한 산업 분야에서 예측 모델의 적용 가능성을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202401558659086&target=NART&cn=JAKO202401558659086",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "공장 전력 절감을 위한 인공지능 기반의 에너지 관리 시스템 개발 공장 전력 절감을 위한 인공지능 기반의 에너지 관리 시스템 개발 공장 전력 절감을 위한 인공지능 기반의 에너지 관리 시스템 개발 본 연구는 IoT 센싱 기술을 활용하여 구축된 빅데이터 수집 시스템을 통해 제주삼다수 공장에서 생성된 데이터를 활용하여 피크 전력 사용을 예측하는 인공지능 모델을 개발하고 비교 분석하였다. LSTM(Long Short-Term Memory) 모델은 단일 변수 시계열 데이터에서 R<sup>2</sup>=0.98, RMSE=0.039, MAE=0.026으로 가장 높은 예측 정확도를 기록하였으며, XGBoost(eXtreme Gradient Boosting) 모델은 다변량 데이터를 효과적으로 처리하며 R<sup>2</sup>=0.93, RMSE=0.018, MAE=0.013의 성능을 보였다. 연구 과정에서 다양한 데이터 전처리 방법과 특징 조합을 실험적으로 적용하여 모델의 성능을 최적화하였으며, 이를 통해 데이터 전처리와 변수 선택이 예측 정확도에 미치는 영향을 입증하였다. 연구 결과, 최적화된 인공지능 모델을 활용한 피크 전력 예측은 전력 비용 절감과 약 10~15%의 탄소 배출 감소 효과를 달성할 수 있음을 제시하였다. 이는 ESG(환경, 사회, 지배구조) 경영을 목표로 하는 기업들에게 지속 가능성을 실현하기 위한 실질적이고 구체적인 전략을 제공하며, 제조업, 물류, 스마트 팩토리 등 다양한 산업 분야에서 예측 모델의 적용 가능성을 확인하였다."
        },
        {
          "rank": 15,
          "score": 0.7521605491638184,
          "doc_id": "NART133895722",
          "title": "Enhancing Electricity Load Forecasting with Machine Learning and Deep Learning",
          "abstract": "<P>The electricity load forecasting handles the process of determining how much electricity will be available at a given time while maintaining the balance and stability of the power grid. The accuracy of electricity load forecasting plays an important role in ensuring safe operation and improving the reliability of power systems and is a key component in the operational planning and efficient market. For many years, a conventional method has been used by using historical data as input parameters. With swift progress and improvement in technology, which shows more potential due to its accuracy, different methods can be applied depending on the identified model. To enhance the forecast of load, this paper introduces and proposes a framework developed on graph database technology to archive large amounts of data, which collects measured data from electrical substations in Pristina, Kosovo. The data includes electrical and weather parameters collected over a four-year timeframe. The proposed framework is designed to handle short-term load forecasting. Machine learning Linear Regression and deep learning Long Short-Term Memory algorithms are applied to multiple datasets and mean absolute error and root mean square error are calculated. The results show the promising performance and effectiveness of the proposed model, with high accuracy in load forecasting.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART133895722&target=NART&cn=NART133895722",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Enhancing Electricity Load Forecasting with Machine Learning and Deep Learning Enhancing Electricity Load Forecasting with Machine Learning and Deep Learning Enhancing Electricity Load Forecasting with Machine Learning and Deep Learning <P>The electricity load forecasting handles the process of determining how much electricity will be available at a given time while maintaining the balance and stability of the power grid. The accuracy of electricity load forecasting plays an important role in ensuring safe operation and improving the reliability of power systems and is a key component in the operational planning and efficient market. For many years, a conventional method has been used by using historical data as input parameters. With swift progress and improvement in technology, which shows more potential due to its accuracy, different methods can be applied depending on the identified model. To enhance the forecast of load, this paper introduces and proposes a framework developed on graph database technology to archive large amounts of data, which collects measured data from electrical substations in Pristina, Kosovo. The data includes electrical and weather parameters collected over a four-year timeframe. The proposed framework is designed to handle short-term load forecasting. Machine learning Linear Regression and deep learning Long Short-Term Memory algorithms are applied to multiple datasets and mean absolute error and root mean square error are calculated. The results show the promising performance and effectiveness of the proposed model, with high accuracy in load forecasting.</P>"
        },
        {
          "rank": 16,
          "score": 0.7487066984176636,
          "doc_id": "ATN0025427236",
          "title": "딥러닝을 이용한 열 수요예측 모델 개발",
          "abstract": "In order to provide stable district heat supplying service to the certain limited residential area, it is the most important to forecast the short-term future demand more accurately and produce and supply heat in efficient way. However, it is very difficult to develop a universal heat demand forecasting model that can be applied to general situations because the factors affecting the heat consumption are very diverse and the consumption patterns are changed according to individual consumers and regional characteristics. In particular, considering all of the various variables that can affect heat demand does not help improve performance in terms of accuracy and versatility. Therefore, this study aims to develop a demand forecasting model using deep learning based on only limited information that can be acquired in real time. A demand forecasting model was developed by learning the artificial neural network of the Tensorflow using past data consisting only of the outdoor temperature of the area and date as input variables. The performance of the proposed model was evaluated by comparing the accuracy of demand predicted with the previous regression model. The proposed heat demand forecasting model in this research showed that it is possible to enhance the accuracy using only limited variables which can be secured in real time. For the demand forecasting in a certain region, the proposed model can be customized by adding some features which can reflect the regional characteristics.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0025427236&target=NART&cn=ATN0025427236",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 이용한 열 수요예측 모델 개발 딥러닝을 이용한 열 수요예측 모델 개발 딥러닝을 이용한 열 수요예측 모델 개발 In order to provide stable district heat supplying service to the certain limited residential area, it is the most important to forecast the short-term future demand more accurately and produce and supply heat in efficient way. However, it is very difficult to develop a universal heat demand forecasting model that can be applied to general situations because the factors affecting the heat consumption are very diverse and the consumption patterns are changed according to individual consumers and regional characteristics. In particular, considering all of the various variables that can affect heat demand does not help improve performance in terms of accuracy and versatility. Therefore, this study aims to develop a demand forecasting model using deep learning based on only limited information that can be acquired in real time. A demand forecasting model was developed by learning the artificial neural network of the Tensorflow using past data consisting only of the outdoor temperature of the area and date as input variables. The performance of the proposed model was evaluated by comparing the accuracy of demand predicted with the previous regression model. The proposed heat demand forecasting model in this research showed that it is possible to enhance the accuracy using only limited variables which can be secured in real time. For the demand forecasting in a certain region, the proposed model can be customized by adding some features which can reflect the regional characteristics."
        },
        {
          "rank": 17,
          "score": 0.733765721321106,
          "doc_id": "JAKO202421251156831",
          "title": "LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석",
          "abstract": "This research is a comprehensive analysis of wind power prediction sensitivity using a Long Short-Term Memory (LSTM) deep learning neural network model, accounting for the inherent uncertainties in wind speed estimation. Utilizing a year's worth of operational data from an operational wind farm, the study forecasts the power output of both individual wind turbines and the farm collectively. Predictions were made daily at intervals of 10 minutes and 1 hour over a span of three months. The model's forecast accuracy was evaluated by comparing the root mean square error (RMSE), normalized RMSE (NRMSE), and correlation coefficients with actual power output data. Moreover, the research investigated how inaccuracies in wind speed inputs affect the power prediction sensitivity of the model. By simulating wind speed errors within a normal distribution range of 1% to 15%, the study analyzed their influence on the accuracy of power predictions. This investigation provided insights into the required wind speed prediction error rate to achieve an 8% power prediction error threshold, meeting the incentive standards for forecasting systems in renewable energy generation.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202421251156831&target=NART&cn=JAKO202421251156831",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 This research is a comprehensive analysis of wind power prediction sensitivity using a Long Short-Term Memory (LSTM) deep learning neural network model, accounting for the inherent uncertainties in wind speed estimation. Utilizing a year's worth of operational data from an operational wind farm, the study forecasts the power output of both individual wind turbines and the farm collectively. Predictions were made daily at intervals of 10 minutes and 1 hour over a span of three months. The model's forecast accuracy was evaluated by comparing the root mean square error (RMSE), normalized RMSE (NRMSE), and correlation coefficients with actual power output data. Moreover, the research investigated how inaccuracies in wind speed inputs affect the power prediction sensitivity of the model. By simulating wind speed errors within a normal distribution range of 1% to 15%, the study analyzed their influence on the accuracy of power predictions. This investigation provided insights into the required wind speed prediction error rate to achieve an 8% power prediction error threshold, meeting the incentive standards for forecasting systems in renewable energy generation."
        },
        {
          "rank": 18,
          "score": 0.7329273223876953,
          "doc_id": "DIKO0014861002",
          "title": "딥 러닝기반 고객평점 예측모델",
          "abstract": "인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014861002&target=NART&cn=DIKO0014861002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다."
        },
        {
          "rank": 19,
          "score": 0.731716513633728,
          "doc_id": "NART106279808",
          "title": "Machine learning for site-adaptation and solar radiation forecasting",
          "abstract": "<P><B>Abstract</B></P>  <P>Optimal management for solar energy systems requires quality data to build accurate models for predicting the behavior of solar radiation. Solar irradiance and environmental data are provided by satellite and in-situ measurements. It is usual that satellite measurements present high temporal resolution with limited spatial resolution, and in-situ measurements provide high accuracy but significant missing data. This paper proposes a methodology based on machine learning algorithms that: <I>i)</I> takes the best of both data sources to obtain an improved spatio-temporal resolution, known as site-adaptation; and <I>ii)</I> provides highly accurate forecasting solar-radiation models based on deep learning on the improved data. Through a study case with real data, we show the benefits of using the proposed methodology based on machine and deep learning techniques to integrate data from different sources and to construct precise solar radiation forecasting models in regions where solar energy systems are required. Results show that machine learning models for site-adaptation performed up to 38% better than traditional methods.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Site-adaptation models of solar radiation with machine learning. </LI> <LI>  Machine learning and deep learning for solar radiation forecasting. </LI> <LI>  Improvement of satellite data and ground data. </LI> <LI>  Improvement of spatial-temporal resolution of a database. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106279808&target=NART&cn=NART106279808",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Machine learning for site-adaptation and solar radiation forecasting Machine learning for site-adaptation and solar radiation forecasting Machine learning for site-adaptation and solar radiation forecasting <P><B>Abstract</B></P>  <P>Optimal management for solar energy systems requires quality data to build accurate models for predicting the behavior of solar radiation. Solar irradiance and environmental data are provided by satellite and in-situ measurements. It is usual that satellite measurements present high temporal resolution with limited spatial resolution, and in-situ measurements provide high accuracy but significant missing data. This paper proposes a methodology based on machine learning algorithms that: <I>i)</I> takes the best of both data sources to obtain an improved spatio-temporal resolution, known as site-adaptation; and <I>ii)</I> provides highly accurate forecasting solar-radiation models based on deep learning on the improved data. Through a study case with real data, we show the benefits of using the proposed methodology based on machine and deep learning techniques to integrate data from different sources and to construct precise solar radiation forecasting models in regions where solar energy systems are required. Results show that machine learning models for site-adaptation performed up to 38% better than traditional methods.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Site-adaptation models of solar radiation with machine learning. </LI> <LI>  Machine learning and deep learning for solar radiation forecasting. </LI> <LI>  Improvement of satellite data and ground data. </LI> <LI>  Improvement of spatial-temporal resolution of a database. </LI> </UL> </P>"
        },
        {
          "rank": 20,
          "score": 0.7258808016777039,
          "doc_id": "JAKO202520454002736",
          "title": "산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구",
          "abstract": "In industrial settings, inverters play a critical role in maintaining productivity and ensuring stable equipment operation. However, inverter failures can result in production downtime and increased maintenance costs. Traditional fault prediction methods based on physical models and expert experience often struggle to capture complex patterns and adapt to varying operational conditions. To address this, this study evaluates the performance of statistical, machine learning, and deep learning approaches for industrial inverter fault prediction, using operational data from a 90W-class inverter at an automotive parts manufacturer in Daegu, South Korea. The experimental results demonstrate that unsupervised anomaly detection models, particularly Autoencoder and SOM, achieved the highest accuracy. These findings suggest that models capable of detecting deviations from normal operating patterns are more effective for inverter fault prediction than conventional methods. In contrast, SVM and Logistic Regression exhibited limitations in handling time-series complexity. This study highlights the necessity of deploying real-time monitoring and predictive maintenance systems in industrial environments, with future research focusing on hyperparameter optimization and real-time data streaming validation.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202520454002736&target=NART&cn=JAKO202520454002736",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 In industrial settings, inverters play a critical role in maintaining productivity and ensuring stable equipment operation. However, inverter failures can result in production downtime and increased maintenance costs. Traditional fault prediction methods based on physical models and expert experience often struggle to capture complex patterns and adapt to varying operational conditions. To address this, this study evaluates the performance of statistical, machine learning, and deep learning approaches for industrial inverter fault prediction, using operational data from a 90W-class inverter at an automotive parts manufacturer in Daegu, South Korea. The experimental results demonstrate that unsupervised anomaly detection models, particularly Autoencoder and SOM, achieved the highest accuracy. These findings suggest that models capable of detecting deviations from normal operating patterns are more effective for inverter fault prediction than conventional methods. In contrast, SVM and Logistic Regression exhibited limitations in handling time-series complexity. This study highlights the necessity of deploying real-time monitoring and predictive maintenance systems in industrial environments, with future research focusing on hyperparameter optimization and real-time data streaming validation."
        },
        {
          "rank": 21,
          "score": 0.7250416278839111,
          "doc_id": "NART07374155",
          "title": "Artificial neural networks as applied to long-term demand forecasting",
          "abstract": "<P><B>Abstract</B></P><P>This paper reports on the application of Artificial Neural Networks (ANN) to long-term load forecasting. The ANN model is used to forecast the energy requirements of an electric utility. It is then compared to time series models. The comparison reveals that the ANN produces results that are close to the actual data. The ANN model is then used to forecast the annual peak demand of a Middle Eastern utility up to the year 2006. The results compare favorably with the utility&#x2019;s forecast.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART07374155&target=NART&cn=NART07374155",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Artificial neural networks as applied to long-term demand forecasting Artificial neural networks as applied to long-term demand forecasting Artificial neural networks as applied to long-term demand forecasting <P><B>Abstract</B></P><P>This paper reports on the application of Artificial Neural Networks (ANN) to long-term load forecasting. The ANN model is used to forecast the energy requirements of an electric utility. It is then compared to time series models. The comparison reveals that the ANN produces results that are close to the actual data. The ANN model is then used to forecast the annual peak demand of a Middle Eastern utility up to the year 2006. The results compare favorably with the utility&#x2019;s forecast.</P>"
        },
        {
          "rank": 22,
          "score": 0.7203686833381653,
          "doc_id": "JAKO202518361202534",
          "title": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측",
          "abstract": "본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202518361202534&target=NART&cn=JAKO202518361202534",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다."
        },
        {
          "rank": 23,
          "score": 0.7149094343185425,
          "doc_id": "ART003029446",
          "title": "통계, 머신러닝, 딥러닝 기반 시계열 모델을 이용한 원자재 가격 예측",
          "abstract": "본 연구는 글로벌 물가 상승으로 인한 불안과 불만의 증가에 대응하기 위해 원자재 가격 예측 모델에 관한 연구를 수행하였다. 생활용품 대부분이 원자재로 제작되는 현실에서 원자재 가격의 변동이 소비자 가격에 직접적인 영향을 미치고 이에 대한 예측과 대비가 중요하다. 본 연구에서는 이를 위해 아연, 구리, 알루미늄, 리튬, 니켈 5가지의 원자재를 가격 예측 대상으로 사용하였으며, 전통적인 통계 기반 모델부터 최신 딥러닝 모델까지 총 14개의 모델을 사용하여 시계열 예측 비교 실험을 진행하였다. 실험 결과, 아연은 TiDE 모델, 알루미늄은 Transformer 모델, 리튬은 AutoARIMA 모델, 니켈은 LSTM 모델, 구리는 TBATS 모델이 가장 좋은 성능을 보였다. 본 연구 결과를 통해 다양한 원자재뿐만 아니라 다양한 시계열 기반 가격 예측 분야에서 중요한 이바지를 할 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART003029446&target=NART&cn=ART003029446",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "통계, 머신러닝, 딥러닝 기반 시계열 모델을 이용한 원자재 가격 예측 통계, 머신러닝, 딥러닝 기반 시계열 모델을 이용한 원자재 가격 예측 통계, 머신러닝, 딥러닝 기반 시계열 모델을 이용한 원자재 가격 예측 본 연구는 글로벌 물가 상승으로 인한 불안과 불만의 증가에 대응하기 위해 원자재 가격 예측 모델에 관한 연구를 수행하였다. 생활용품 대부분이 원자재로 제작되는 현실에서 원자재 가격의 변동이 소비자 가격에 직접적인 영향을 미치고 이에 대한 예측과 대비가 중요하다. 본 연구에서는 이를 위해 아연, 구리, 알루미늄, 리튬, 니켈 5가지의 원자재를 가격 예측 대상으로 사용하였으며, 전통적인 통계 기반 모델부터 최신 딥러닝 모델까지 총 14개의 모델을 사용하여 시계열 예측 비교 실험을 진행하였다. 실험 결과, 아연은 TiDE 모델, 알루미늄은 Transformer 모델, 리튬은 AutoARIMA 모델, 니켈은 LSTM 모델, 구리는 TBATS 모델이 가장 좋은 성능을 보였다. 본 연구 결과를 통해 다양한 원자재뿐만 아니라 다양한 시계열 기반 가격 예측 분야에서 중요한 이바지를 할 것으로 기대된다."
        },
        {
          "rank": 24,
          "score": 0.7088508605957031,
          "doc_id": "JAKO202317233054241",
          "title": "머신러닝 기반 수소 충전소 에너지 수요 예측 모델",
          "abstract": "수소 에너지는 높은 에너지 효율로 열과 전기를 생산하면서도 온실가스와 미세먼지 등 유해물질 배출이 없는 친환경 에너지로서, 전 세계적으로 탄소중립으로의 전환을 위한 핵심으로 주목받고 있다. 특히 스마트 수소에너지는 경제적이고 지속 가능하며, 안전한 미래 스마트 수소에너지 서비스로써 수소 에너지의 기반 시설이 디지털로 통합되어 '데이터' 기반으로 안정적으로 운영되는 서비스를 의미한다. 본 논문에서는 데이터 기반 수소 충전소 수요예측 모델 구현을 위해 강원도 내 설치되어 있는 수소 충전소 3곳(춘천, 속초, 평창)을 선정, 수소 충전소의 수요공급 데이터를 확보하였고, 머신러닝 및 딥러닝 알고리즘 7개를 선정하여 총 27종 입력 데이터(기상데이터+수소 충전소 수요량)로 모델을 학습하였고, 평균 제곱근 오차(RMSE)로 모델을 평가하였다. 이를 통해 본 논문에서는 최적의 수소 에너지 수요공급을 위한 머신러닝 기반 수소 충전소 에너지 수요 예측 모델을 제안한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202317233054241&target=NART&cn=JAKO202317233054241",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기반 수소 충전소 에너지 수요 예측 모델 머신러닝 기반 수소 충전소 에너지 수요 예측 모델 머신러닝 기반 수소 충전소 에너지 수요 예측 모델 수소 에너지는 높은 에너지 효율로 열과 전기를 생산하면서도 온실가스와 미세먼지 등 유해물질 배출이 없는 친환경 에너지로서, 전 세계적으로 탄소중립으로의 전환을 위한 핵심으로 주목받고 있다. 특히 스마트 수소에너지는 경제적이고 지속 가능하며, 안전한 미래 스마트 수소에너지 서비스로써 수소 에너지의 기반 시설이 디지털로 통합되어 '데이터' 기반으로 안정적으로 운영되는 서비스를 의미한다. 본 논문에서는 데이터 기반 수소 충전소 수요예측 모델 구현을 위해 강원도 내 설치되어 있는 수소 충전소 3곳(춘천, 속초, 평창)을 선정, 수소 충전소의 수요공급 데이터를 확보하였고, 머신러닝 및 딥러닝 알고리즘 7개를 선정하여 총 27종 입력 데이터(기상데이터+수소 충전소 수요량)로 모델을 학습하였고, 평균 제곱근 오차(RMSE)로 모델을 평가하였다. 이를 통해 본 논문에서는 최적의 수소 에너지 수요공급을 위한 머신러닝 기반 수소 충전소 에너지 수요 예측 모델을 제안한다."
        },
        {
          "rank": 25,
          "score": 0.7073089480400085,
          "doc_id": "NART103136065",
          "title": "Portfolio optimization with return prediction using deep learning and machine learning",
          "abstract": "<P><B>Abstract</B></P>  <P>Integrating return prediction of traditional time series models in portfolio formation can improve the performance of original portfolio optimization model. Since machine learning and deep learning models have shown overwhelming superiority than time series models, this paper combines return prediction in portfolio formation with two machine learning models, i.e., random forest (RF) and support vector regression (SVR), and three deep learning models, i.e., LSTM neural network, deep multilayer perceptron (DMLP) and convolutional neural network. To be specific, this paper first applies these prediction models for stock preselection before portfolio formation. Then, this paper incorporates their predictive results in advancing mean&ndash;variance (MV) and omega portfolio optimization models. In order to present the superiority of these models, portfolio models with autoregressive integrated moving average&rsquo;s return prediction are used as benchmarks. Evaluation is based on historical data of 9 years from 2007 to 2015 of component stocks of China securities 100 index. Experimental results show that MV and omega models with RF return prediction, i.e., RF+MVF and RF+OF, outperform the other models. Further, RF+MVF is superior to RF+OF. Due to the high turnover of these two models, this paper discusses their performance after deducting the transaction fee cased by turnover. Experiments present that RF+MVF still performs the best among MVF models and omega model with SVR prediction (SVR+OF) performs the best among OF models. Moreover, RF+MVF performs better than SVR+OF and high turnover erodes nearly half of their total returns especially for RF+OF and RF+MVF. Therefore, this paper recommends investors to build MVF with RF return prediction for daily trading investment.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Compares the performance of machine learning and deep learning in stock preselection. </LI> <LI>  Combining return prediction of machine learning and deep learning in portfolio formation. </LI> <LI>  Emphasis on advancing portfolio optimization with return prediction. </LI> <LI>  Advanced mean&ndash;variance model with random forest forecasts performs the best. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART103136065&target=NART&cn=NART103136065",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Portfolio optimization with return prediction using deep learning and machine learning Portfolio optimization with return prediction using deep learning and machine learning Portfolio optimization with return prediction using deep learning and machine learning <P><B>Abstract</B></P>  <P>Integrating return prediction of traditional time series models in portfolio formation can improve the performance of original portfolio optimization model. Since machine learning and deep learning models have shown overwhelming superiority than time series models, this paper combines return prediction in portfolio formation with two machine learning models, i.e., random forest (RF) and support vector regression (SVR), and three deep learning models, i.e., LSTM neural network, deep multilayer perceptron (DMLP) and convolutional neural network. To be specific, this paper first applies these prediction models for stock preselection before portfolio formation. Then, this paper incorporates their predictive results in advancing mean&ndash;variance (MV) and omega portfolio optimization models. In order to present the superiority of these models, portfolio models with autoregressive integrated moving average&rsquo;s return prediction are used as benchmarks. Evaluation is based on historical data of 9 years from 2007 to 2015 of component stocks of China securities 100 index. Experimental results show that MV and omega models with RF return prediction, i.e., RF+MVF and RF+OF, outperform the other models. Further, RF+MVF is superior to RF+OF. Due to the high turnover of these two models, this paper discusses their performance after deducting the transaction fee cased by turnover. Experiments present that RF+MVF still performs the best among MVF models and omega model with SVR prediction (SVR+OF) performs the best among OF models. Moreover, RF+MVF performs better than SVR+OF and high turnover erodes nearly half of their total returns especially for RF+OF and RF+MVF. Therefore, this paper recommends investors to build MVF with RF return prediction for daily trading investment.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Compares the performance of machine learning and deep learning in stock preselection. </LI> <LI>  Combining return prediction of machine learning and deep learning in portfolio formation. </LI> <LI>  Emphasis on advancing portfolio optimization with return prediction. </LI> <LI>  Advanced mean&ndash;variance model with random forest forecasts performs the best. </LI> </UL> </P>"
        },
        {
          "rank": 26,
          "score": 0.706576943397522,
          "doc_id": "ATN0038661375",
          "title": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험",
          "abstract": "Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0038661375&target=NART&cn=ATN0038661375",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model."
        },
        {
          "rank": 27,
          "score": 0.7060531973838806,
          "doc_id": "NART111572455",
          "title": "동풍 예측을 위한 딥러닝 기반의 예측 모델",
          "abstract": "Understanding the characteristics of the easterly-related weather phenomena in the eastern coast in Korean Peninsula is very important to analyze abnormal atmospheric phenomena such as heavy rain, heavy snow, and hot-dry wind. As data science techniques have steadily improved, data driven prediction models are becoming more powerful in the quantitative forecasting weather. In this paper, we apply the deep learning based methods to predict the presence or absence of the easterly wind around the Korean peninsula. The DNN, CNN, and LSTM based deep learning approaches for prediction of easterly wind are experimented and compared for the Korean Peninsula and East Sea. Vertical pressure levels of ERA5 data in year 2013 and 2014 are used.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART111572455&target=NART&cn=NART111572455",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "동풍 예측을 위한 딥러닝 기반의 예측 모델 동풍 예측을 위한 딥러닝 기반의 예측 모델 동풍 예측을 위한 딥러닝 기반의 예측 모델 Understanding the characteristics of the easterly-related weather phenomena in the eastern coast in Korean Peninsula is very important to analyze abnormal atmospheric phenomena such as heavy rain, heavy snow, and hot-dry wind. As data science techniques have steadily improved, data driven prediction models are becoming more powerful in the quantitative forecasting weather. In this paper, we apply the deep learning based methods to predict the presence or absence of the easterly wind around the Korean peninsula. The DNN, CNN, and LSTM based deep learning approaches for prediction of easterly wind are experimented and compared for the Korean Peninsula and East Sea. Vertical pressure levels of ERA5 data in year 2013 and 2014 are used."
        },
        {
          "rank": 28,
          "score": 0.7052245736122131,
          "doc_id": "JAKO202013661037911",
          "title": "머신러닝을 이용한 철광석 가격 예측에 대한 연구",
          "abstract": "철광석의 가격은 여러 국가와 기업들의 수요와 공급에 따라서 높은 변동성이 지속되고 있다. 이러한 비즈니스 환경에서 철광석의 가격을 예측하는 것은 중요해졌다. 본 연구는 머신러닝 기법을 이용하여 철광석이 거래되는 시점으로부터 한 달 전에 철광석 거래가격을 미리 예측하는 모형을 개발하고자 하였다. 예측 모형은 시계열 데이터를 활용한 예측 방법론으로 많이 활용되고 있는 시차분포 모형과 다층신경망 (Multi-layer perceptron), 순환신경망 (Recurrent neural network), 그리고 장단기 기억 네트워크 (Long short-term memory)와 같은 딥 러닝(Deep Learning) 모형을 사용하였다. 측정지표를 통해 개별 모형을 비교한 결과에 따르면, LSTM 모형이 예측 오차가 가장 낮은 것으로 나타났다. 또한, 앙상블 기법을 적용한 모형들을 비교한 결과, 시차분포와 LSTM의 앙상블 모형이 예측오차가 가장 낮은 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202013661037911&target=NART&cn=JAKO202013661037911",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝을 이용한 철광석 가격 예측에 대한 연구 머신러닝을 이용한 철광석 가격 예측에 대한 연구 머신러닝을 이용한 철광석 가격 예측에 대한 연구 철광석의 가격은 여러 국가와 기업들의 수요와 공급에 따라서 높은 변동성이 지속되고 있다. 이러한 비즈니스 환경에서 철광석의 가격을 예측하는 것은 중요해졌다. 본 연구는 머신러닝 기법을 이용하여 철광석이 거래되는 시점으로부터 한 달 전에 철광석 거래가격을 미리 예측하는 모형을 개발하고자 하였다. 예측 모형은 시계열 데이터를 활용한 예측 방법론으로 많이 활용되고 있는 시차분포 모형과 다층신경망 (Multi-layer perceptron), 순환신경망 (Recurrent neural network), 그리고 장단기 기억 네트워크 (Long short-term memory)와 같은 딥 러닝(Deep Learning) 모형을 사용하였다. 측정지표를 통해 개별 모형을 비교한 결과에 따르면, LSTM 모형이 예측 오차가 가장 낮은 것으로 나타났다. 또한, 앙상블 기법을 적용한 모형들을 비교한 결과, 시차분포와 LSTM의 앙상블 모형이 예측오차가 가장 낮은 것으로 나타났다."
        },
        {
          "rank": 29,
          "score": 0.7040902376174927,
          "doc_id": "DIKO0016660298",
          "title": "Parallel CNN-LSTM 기반의 부하예측 및 열병합 발전 이상탐지",
          "abstract": "지구온난화에 따른 환경 변화가 전세계적으로 나타남에 따라 한국에서도 온실가스 배출 감소를 목표로 석탄기반의 화력발전 감축과 이를 재생에너지로 대체하기 위한 방안을 모색하고 있다. 하지만 기존 탄소기반 발전(發電)체제를 기반으로 하는 제도와 인프라, 인력으로 인해 신속한 에너지 체제 전환이 어렵다. 이러한 이유로 열병합 (combined heat and power, CHP) 발전은 기존 화력발전 인프라 활용이 가능하다는 장점과 열과 전력을 동시에 생산하며 일반 복합발전 대비 높은 효율을 가지고 있어 기존 화력발전의 대체체제로 주목받고 있다. &amp;#xD; 하지만 CHP 발전은 높은 발전 효율과 인프라 활용 가능성에 불구하고 단점도 가지고 있는데, 그 중 하나가 잉여 열에너지 발생으로 인한 경제적 손실이다. 전력의 경우 저장이 용이해 추후 판매가 가능한 반면 열 에너지는 고온의 액체와 증기 상태이기 때문에 저장효율이 매우 낮아 손실을 유발한다. 또한 CHP 엔진은 화력발전과 유사하게 액화천연가스를 연소하고 터빈을 작동하는 방식으로 고온∙고압의 환경에서 운영되기 때문에 이상(異常)과 결함이 발생할 가능성이 높다. 엔진 관리가 적절히 이루어지지 않을 경우 연료 소비와 온실가스 배출이 증가할 수 있으며 갑작스러운 운행 중단의 위험이 있기 때문에 이상관리는 필수적이다. 이에 다음 세 연구를 통해 CHP 발전 효율 향상과 경제 손실 감소를 위한 열 부하 예측모델과 엔진 이상탐지 모델을 제안한다.&amp;#xD; 첫 번째, 열 부하 파생 변수와 LSTM을 사용한 부하예측 모델을 제안한다. 열 부하는 수용가의 열 사용량에 영향을 받기 때문에 지역난방 파생변수와 사용자의 사용량에 영향을 주는 기상, 공휴일 정보를 단계적으로 추가하며 모델의 정확도를 높인다. 실험 결과 공휴일 정보를 기반으로 생성한 열 부하 파생변수를 학습한 모델의 성능이 가장 우수하다. 기본 지역난방 관련 변수들만 사용한 모델 대비 열 부하 파생 변수들을 추가한 모델의 MAPE가 18.37에서 15.32로 약 3.05 감소, MSE는 1.31에서 1.23으로 약 0.08 감소하는 것을 확인할 수 있다. 본 연구의 결론을 기반으로 두 번째 연구에서는 파생변수를 확장하고 모델의 네트워크 구조를 고도화한다. &amp;#xD; 두 번째, CNN과 LSTM을 parallel하게 결합하고 attention mechanism을 추가한 parallel CNN-LSTM attention (PCLA) 모델을 제안한다. CHP 발전 로그는 multivariate time series 데이터이므로 temporal feature 뿐만 아닌 spatial feature도 추출해 학습할 필요가 있다. 이를 위해 선행 연구들은 CNN과 LSTM을 serial하게 결합한 모델을 제안했으나, 이에는 input data에서 직접 temporal feature를 추출할 수 없고 CNN에서 추출한 spatial feature도 손상된다는 문제가 있다. 따라서 본 연구에서는 CNN과 LSTM을 parallel하게 결합해 spatial feature와 temporal feature를 추출하고, 이 중 중요한 요인을 집중 학습하기 위해 attention mechanism을 결합한 PCLA를 제안한다. PCLA의 열 부하 예측 우수성을 증명하기 위해 선행 연구들이 제안한 statistical analysis, shallow machine learning, deep learning, 그리고 CNN과 LSTM을 결합한 hybrid deep learning 모델 12개와 비교한다. 이에 더해 첫 번째 연구에서 증명한 파생변수의 효용성에 따라 공휴일, 요일, 시간을 기준으로 열 부하 파생변수를 추가해 모델을 학습한다. 그 결과 PCLA의 성능은 MAE와 MSE가 0.571, 0.662로 가장 낮으며, R-squared 0.942로 가장 높은 것을 확인했다. 이를 통해 CNN과 LSTM을 serial하게 결합한 모델보다 parallel하게 결합한 모델이 multivariate time series 데이터의 spatiotemporal feature를 추출하고 학습하는데 우수하며, attention mechanism을 통해 예측 정확도를 더 높일 수 있음을 증명한다. &amp;#xD; 세 번째, CNN, LSTM, residual block, attention mechanism을 결합한 parallel CNN-LSTM residual attention (PCL-Res-Att) 이상탐지 모델을 제안한다. PCL-Res-Att 모델은 CNN과 LSTM을 사용해 feature를 추출하고 residual block을 사용해 손실된 정보를 보완한다. 그리고 attention mechanism을 사용해 중요한 spatiotemporal feature를 추출해 학습하고 이상탐지 결과를 도출한다. 엔진 이상탐지 모델 학습 데이터인 엔진 시스템 로그 데이터 또한 multivariate time series 데이터이기 때문에 CNN과 LSTM 결합모델을 통한 spatiotemporal feature를 추출하고 학습하는 것이 중요하다. 이를 위해 본 연구에서는 PCLA에 residual block을 결합해 모델의 성능을 고도화한 엔진 이상탐지 모델 PCL-Res-Att을 제안한다. PCL-Res-Att의 이상탐지 성능 우수성을 증명하기 위해 CNN, LSTM, 그리고 CNN과 LSTM의 serial, parallel 결합 모델에 residual block과 attention mechanism을 단계적으로 결합한 9개 모델과 비교한다. 그리고 CHP 엔진 3대, 5개 계통, 총 15 케이스에 모델들을 적용해 실험 결과를 도출한다. 그 결과 PCL-Res-Att의 macro f1-score가 0.951±0.033 (mean ± standard deviation), abnormal f1-score가 0.903±0.064, 그리고 accuracy가 0.999±0.002로 가장 우수하다. 이를 통해 CNN과 LSTM의 parallel한 결합 모델이 multivariate time series 데이터의 spatiotemporal feature를 추출하고 학습하는데 우수함을 증명한다. 또한 attention mechanism 대비 residual block을 통한 손실된 정보 보완이 이상탐지 정확도 향상에 더 기여하지만, 모두 사용했을 때 성능이 크게 향상됨을 확인할 수 있다. &amp;#xD;",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016660298&target=NART&cn=DIKO0016660298",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Parallel CNN-LSTM 기반의 부하예측 및 열병합 발전 이상탐지 Parallel CNN-LSTM 기반의 부하예측 및 열병합 발전 이상탐지 Parallel CNN-LSTM 기반의 부하예측 및 열병합 발전 이상탐지 지구온난화에 따른 환경 변화가 전세계적으로 나타남에 따라 한국에서도 온실가스 배출 감소를 목표로 석탄기반의 화력발전 감축과 이를 재생에너지로 대체하기 위한 방안을 모색하고 있다. 하지만 기존 탄소기반 발전(發電)체제를 기반으로 하는 제도와 인프라, 인력으로 인해 신속한 에너지 체제 전환이 어렵다. 이러한 이유로 열병합 (combined heat and power, CHP) 발전은 기존 화력발전 인프라 활용이 가능하다는 장점과 열과 전력을 동시에 생산하며 일반 복합발전 대비 높은 효율을 가지고 있어 기존 화력발전의 대체체제로 주목받고 있다. &amp;#xD; 하지만 CHP 발전은 높은 발전 효율과 인프라 활용 가능성에 불구하고 단점도 가지고 있는데, 그 중 하나가 잉여 열에너지 발생으로 인한 경제적 손실이다. 전력의 경우 저장이 용이해 추후 판매가 가능한 반면 열 에너지는 고온의 액체와 증기 상태이기 때문에 저장효율이 매우 낮아 손실을 유발한다. 또한 CHP 엔진은 화력발전과 유사하게 액화천연가스를 연소하고 터빈을 작동하는 방식으로 고온∙고압의 환경에서 운영되기 때문에 이상(異常)과 결함이 발생할 가능성이 높다. 엔진 관리가 적절히 이루어지지 않을 경우 연료 소비와 온실가스 배출이 증가할 수 있으며 갑작스러운 운행 중단의 위험이 있기 때문에 이상관리는 필수적이다. 이에 다음 세 연구를 통해 CHP 발전 효율 향상과 경제 손실 감소를 위한 열 부하 예측모델과 엔진 이상탐지 모델을 제안한다.&amp;#xD; 첫 번째, 열 부하 파생 변수와 LSTM을 사용한 부하예측 모델을 제안한다. 열 부하는 수용가의 열 사용량에 영향을 받기 때문에 지역난방 파생변수와 사용자의 사용량에 영향을 주는 기상, 공휴일 정보를 단계적으로 추가하며 모델의 정확도를 높인다. 실험 결과 공휴일 정보를 기반으로 생성한 열 부하 파생변수를 학습한 모델의 성능이 가장 우수하다. 기본 지역난방 관련 변수들만 사용한 모델 대비 열 부하 파생 변수들을 추가한 모델의 MAPE가 18.37에서 15.32로 약 3.05 감소, MSE는 1.31에서 1.23으로 약 0.08 감소하는 것을 확인할 수 있다. 본 연구의 결론을 기반으로 두 번째 연구에서는 파생변수를 확장하고 모델의 네트워크 구조를 고도화한다. &amp;#xD; 두 번째, CNN과 LSTM을 parallel하게 결합하고 attention mechanism을 추가한 parallel CNN-LSTM attention (PCLA) 모델을 제안한다. CHP 발전 로그는 multivariate time series 데이터이므로 temporal feature 뿐만 아닌 spatial feature도 추출해 학습할 필요가 있다. 이를 위해 선행 연구들은 CNN과 LSTM을 serial하게 결합한 모델을 제안했으나, 이에는 input data에서 직접 temporal feature를 추출할 수 없고 CNN에서 추출한 spatial feature도 손상된다는 문제가 있다. 따라서 본 연구에서는 CNN과 LSTM을 parallel하게 결합해 spatial feature와 temporal feature를 추출하고, 이 중 중요한 요인을 집중 학습하기 위해 attention mechanism을 결합한 PCLA를 제안한다. PCLA의 열 부하 예측 우수성을 증명하기 위해 선행 연구들이 제안한 statistical analysis, shallow machine learning, deep learning, 그리고 CNN과 LSTM을 결합한 hybrid deep learning 모델 12개와 비교한다. 이에 더해 첫 번째 연구에서 증명한 파생변수의 효용성에 따라 공휴일, 요일, 시간을 기준으로 열 부하 파생변수를 추가해 모델을 학습한다. 그 결과 PCLA의 성능은 MAE와 MSE가 0.571, 0.662로 가장 낮으며, R-squared 0.942로 가장 높은 것을 확인했다. 이를 통해 CNN과 LSTM을 serial하게 결합한 모델보다 parallel하게 결합한 모델이 multivariate time series 데이터의 spatiotemporal feature를 추출하고 학습하는데 우수하며, attention mechanism을 통해 예측 정확도를 더 높일 수 있음을 증명한다. &amp;#xD; 세 번째, CNN, LSTM, residual block, attention mechanism을 결합한 parallel CNN-LSTM residual attention (PCL-Res-Att) 이상탐지 모델을 제안한다. PCL-Res-Att 모델은 CNN과 LSTM을 사용해 feature를 추출하고 residual block을 사용해 손실된 정보를 보완한다. 그리고 attention mechanism을 사용해 중요한 spatiotemporal feature를 추출해 학습하고 이상탐지 결과를 도출한다. 엔진 이상탐지 모델 학습 데이터인 엔진 시스템 로그 데이터 또한 multivariate time series 데이터이기 때문에 CNN과 LSTM 결합모델을 통한 spatiotemporal feature를 추출하고 학습하는 것이 중요하다. 이를 위해 본 연구에서는 PCLA에 residual block을 결합해 모델의 성능을 고도화한 엔진 이상탐지 모델 PCL-Res-Att을 제안한다. PCL-Res-Att의 이상탐지 성능 우수성을 증명하기 위해 CNN, LSTM, 그리고 CNN과 LSTM의 serial, parallel 결합 모델에 residual block과 attention mechanism을 단계적으로 결합한 9개 모델과 비교한다. 그리고 CHP 엔진 3대, 5개 계통, 총 15 케이스에 모델들을 적용해 실험 결과를 도출한다. 그 결과 PCL-Res-Att의 macro f1-score가 0.951±0.033 (mean ± standard deviation), abnormal f1-score가 0.903±0.064, 그리고 accuracy가 0.999±0.002로 가장 우수하다. 이를 통해 CNN과 LSTM의 parallel한 결합 모델이 multivariate time series 데이터의 spatiotemporal feature를 추출하고 학습하는데 우수함을 증명한다. 또한 attention mechanism 대비 residual block을 통한 손실된 정보 보완이 이상탐지 정확도 향상에 더 기여하지만, 모두 사용했을 때 성능이 크게 향상됨을 확인할 수 있다. &amp;#xD;"
        },
        {
          "rank": 30,
          "score": 0.7039399743080139,
          "doc_id": "DIKO0016929656",
          "title": "머신러닝 기법을 사용한 이익예측 모형",
          "abstract": "본 논문에서는 단계적 로짓 회귀(Stepwise Logit Regressioin) 모델과 머신러닝(Machine Learning) 기법인 랜덤 포레스트(Random Forest) 모델, 익스트림 그레이디언트 부스팅(Extreme Gradient Boosting) 모델, 인공신경망(Artificial Neural Network) 모델을 사용하여 미래의 희석주당이익(DEPS)의 증가를 예측하고, 각각의 예측 모델의 정확도를 비교한다. 선행연구에서 이익의 증가를 예측할 수 있도록 하는 중요 변수를 사용하여 본 연구를 진행하고 네 개의 모델에서 중요 변수를 추출하고 선행연구의 중요 변수와 비교한다. 그리고 재무상태표와 손익계산서에서 얻을 수 있는 변수들을 확보하여 총자산과 총매출로 변수를 조정한 후 예측 변수에 추가하여 미래 희석주당이익의 증가를 추가적으로 예측하고, 중요 변수를 추출하고 비교한다.&amp;#xD; 분석 결과 선행연구와 동일하게 랜덤포레스트모델의 예측 정확도가 단계적 로짓 회귀 모델의 예측 정확도보다 높았다. 그리고 랜덤 포레스트 모델의 예측 정확도가 나머지 예측 모델보다 예측 정확도가 가장 높았으며 다음으로 익스트림 그레이디언트 부스팅 모델의 예측 정확도가 높았다. 추가 분석 결과 랜덤 포레스트 모델, 익스트림 그레이디언트 부스팅 모델과 인공신경망 모델의 예측 정확도가 좋았으며, 인공신경망 모델의 경우 예측 변수를 추가하기 전보다 예측 정확도가 가장 큰 폭으로 향상되는 것을 확인하였다.&amp;#xD; 본 연구는 머신러닝 모델을 활용하여 회계이익을 예측할 경우, 기존의 이익 예측 모형인 회귀모형 보다 회계이익을 정확하게 예측할 수 있다는 점을 시사한다. 그리고 머신러닝 모델과 딥러닝 모델을 활용한 연구가 회계학에서도 충분히 활용될 수 있다는 증거를 제시하고, 머신러닝 방법론과 딥러닝 방법론을 회계학에 도입하는데 발판을 마련하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016929656&target=NART&cn=DIKO0016929656",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기법을 사용한 이익예측 모형 머신러닝 기법을 사용한 이익예측 모형 머신러닝 기법을 사용한 이익예측 모형 본 논문에서는 단계적 로짓 회귀(Stepwise Logit Regressioin) 모델과 머신러닝(Machine Learning) 기법인 랜덤 포레스트(Random Forest) 모델, 익스트림 그레이디언트 부스팅(Extreme Gradient Boosting) 모델, 인공신경망(Artificial Neural Network) 모델을 사용하여 미래의 희석주당이익(DEPS)의 증가를 예측하고, 각각의 예측 모델의 정확도를 비교한다. 선행연구에서 이익의 증가를 예측할 수 있도록 하는 중요 변수를 사용하여 본 연구를 진행하고 네 개의 모델에서 중요 변수를 추출하고 선행연구의 중요 변수와 비교한다. 그리고 재무상태표와 손익계산서에서 얻을 수 있는 변수들을 확보하여 총자산과 총매출로 변수를 조정한 후 예측 변수에 추가하여 미래 희석주당이익의 증가를 추가적으로 예측하고, 중요 변수를 추출하고 비교한다.&amp;#xD; 분석 결과 선행연구와 동일하게 랜덤포레스트모델의 예측 정확도가 단계적 로짓 회귀 모델의 예측 정확도보다 높았다. 그리고 랜덤 포레스트 모델의 예측 정확도가 나머지 예측 모델보다 예측 정확도가 가장 높았으며 다음으로 익스트림 그레이디언트 부스팅 모델의 예측 정확도가 높았다. 추가 분석 결과 랜덤 포레스트 모델, 익스트림 그레이디언트 부스팅 모델과 인공신경망 모델의 예측 정확도가 좋았으며, 인공신경망 모델의 경우 예측 변수를 추가하기 전보다 예측 정확도가 가장 큰 폭으로 향상되는 것을 확인하였다.&amp;#xD; 본 연구는 머신러닝 모델을 활용하여 회계이익을 예측할 경우, 기존의 이익 예측 모형인 회귀모형 보다 회계이익을 정확하게 예측할 수 있다는 점을 시사한다. 그리고 머신러닝 모델과 딥러닝 모델을 활용한 연구가 회계학에서도 충분히 활용될 수 있다는 증거를 제시하고, 머신러닝 방법론과 딥러닝 방법론을 회계학에 도입하는데 발판을 마련하고자 한다."
        },
        {
          "rank": 31,
          "score": 0.7027282118797302,
          "doc_id": "JAKO202012758284659",
          "title": "딥러닝을 활용한 다목적댐 유입량 예측",
          "abstract": "최근 데이터 예측 방법으로 인공신경망(Artificial Neural Network, ANN)분야에 대한 관심이 높아졌으며, 그 중 시계열 데이터 예측에 특화된 LSTM(Long Short-Term Memory)모형은 수문 시계열자료의 예측방법으로도 활용되고 있다. 본 연구에서는 구글에서 제공하는 딥러닝 오픈소스 라이브러리인 텐서플로우(TensorFlow)를 활용하여 LSTM모형을 구축하고 금강 상류에 위치한 용담다목적댐의 유입량을 예측하였다. 분석 자료로는 WAMIS에서 제공하는 용담댐의 2006년부터 2018년까지의 시간당 유입량 자료를 사용하였으며, 예측된 유입량과 관측 유입량의 비교를 통하여 평균제곱오차(RMSE), 평균절대오차(MAE), 용적오차(VE)를 계산하고 모형의 학습변수에 따른 정확도를 평가하였다. 분석결과, 모든 모형이 고유량에서의 정확도가 낮은 것으로 나타났으며, 이와 같은 문제를 해결하기 위하여 용담댐 유역의 시간당 강수량 자료를 추가 학습 자료로 활용하여 분석한 결과, 고유량에 대한 예측의 정확도가 높아지는 것을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202012758284659&target=NART&cn=JAKO202012758284659",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 활용한 다목적댐 유입량 예측 딥러닝을 활용한 다목적댐 유입량 예측 딥러닝을 활용한 다목적댐 유입량 예측 최근 데이터 예측 방법으로 인공신경망(Artificial Neural Network, ANN)분야에 대한 관심이 높아졌으며, 그 중 시계열 데이터 예측에 특화된 LSTM(Long Short-Term Memory)모형은 수문 시계열자료의 예측방법으로도 활용되고 있다. 본 연구에서는 구글에서 제공하는 딥러닝 오픈소스 라이브러리인 텐서플로우(TensorFlow)를 활용하여 LSTM모형을 구축하고 금강 상류에 위치한 용담다목적댐의 유입량을 예측하였다. 분석 자료로는 WAMIS에서 제공하는 용담댐의 2006년부터 2018년까지의 시간당 유입량 자료를 사용하였으며, 예측된 유입량과 관측 유입량의 비교를 통하여 평균제곱오차(RMSE), 평균절대오차(MAE), 용적오차(VE)를 계산하고 모형의 학습변수에 따른 정확도를 평가하였다. 분석결과, 모든 모형이 고유량에서의 정확도가 낮은 것으로 나타났으며, 이와 같은 문제를 해결하기 위하여 용담댐 유역의 시간당 강수량 자료를 추가 학습 자료로 활용하여 분석한 결과, 고유량에 대한 예측의 정확도가 높아지는 것을 알 수 있었다."
        },
        {
          "rank": 32,
          "score": 0.7023690938949585,
          "doc_id": "JAKO202408283134560",
          "title": "머신러닝&딥러닝 모델을 활용한 댐 일유입량 예측시 융적설을 고려하기 위한 데이터 전처리에 대한 방법 연구",
          "abstract": "댐유입량 예측에 대하여 데이터 기반 머신러닝 및 딥러닝(Machine Learning & Deep Learning, ML&DL) 분석도구들이 공개되어 다양한 분야에서 ML&DL의 적용연구가 활발히 진행되고 있으며, 모델의 자체 성능향상 뿐만 아니라 모델의 특성을 고려한 데이터의 전처리도 댐유입량을 정확하게 예측하게 하는 중요한 모델성능 향상의 요소라고 할 수 있다. 특히 기존 강우자료는 적설량을 열선 설비를 통하여 녹여 강우량으로 환산되어 있으므로, 융적설에 따른 강우와 유입량의 상관관계를 왜곡하게 된다. 따라서 본연구에서는 소양강댐과 같이 융적설의 영향을 받는 댐유역에 대한 댐일유입량 예측시 겨울에 강설량이 적설이 되어 적게 유출되는 현상과, 봄에 융설로 인하여 무강우나 적은 비에도 많은 유출이 일어나는 물리적 현상을 ML&DL모델로 적용하기 위하여 필요한 강우 데이터의 전처리에 대한 연구를 수행 하였다. 강우계열, 유입량계열을 조합하여 3가지 머신러닝(SVM, RF, LGBM)과 2가지 딥러닝(LSTM, TCN) 모델을 구축하고, 최적 하이퍼파라메터 튜닝을 통하여 적합 모델을 적용하고 한 결과, NSE 0.842~0.894로 높은 수준의 예측성능을 나타내었다. 또한 융적설을 반영한 강우보정 데이터를 만들기 위하여 융적설 모의 알고리즘을 개발하고, 이를 통하여 산정된 보정강우를 머신러닝 및 딥러닝 모델에 적용한 결과 NSE 0.841~0.896 으로 융적설 적용전과 비슷한 높은 수준의 예측 성능을 나타내었으나, 융적설 기간에는 조정된 강우로 학습되어 예측되었을 때 실측유입량에 근접하는 모의결과를 나타내었다. 결론적으로, 융적설이 영향을 미치는 유역에서의 데이터 모델 적용시에는 입력자료 구축시 적설 및 융설이 물리적으로 타당한 강우-유출 반응에 적합하도록 전처리과정이 중요함을 밝혔다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202408283134560&target=NART&cn=JAKO202408283134560",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝&딥러닝 모델을 활용한 댐 일유입량 예측시 융적설을 고려하기 위한 데이터 전처리에 대한 방법 연구 머신러닝&딥러닝 모델을 활용한 댐 일유입량 예측시 융적설을 고려하기 위한 데이터 전처리에 대한 방법 연구 머신러닝&딥러닝 모델을 활용한 댐 일유입량 예측시 융적설을 고려하기 위한 데이터 전처리에 대한 방법 연구 댐유입량 예측에 대하여 데이터 기반 머신러닝 및 딥러닝(Machine Learning & Deep Learning, ML&DL) 분석도구들이 공개되어 다양한 분야에서 ML&DL의 적용연구가 활발히 진행되고 있으며, 모델의 자체 성능향상 뿐만 아니라 모델의 특성을 고려한 데이터의 전처리도 댐유입량을 정확하게 예측하게 하는 중요한 모델성능 향상의 요소라고 할 수 있다. 특히 기존 강우자료는 적설량을 열선 설비를 통하여 녹여 강우량으로 환산되어 있으므로, 융적설에 따른 강우와 유입량의 상관관계를 왜곡하게 된다. 따라서 본연구에서는 소양강댐과 같이 융적설의 영향을 받는 댐유역에 대한 댐일유입량 예측시 겨울에 강설량이 적설이 되어 적게 유출되는 현상과, 봄에 융설로 인하여 무강우나 적은 비에도 많은 유출이 일어나는 물리적 현상을 ML&DL모델로 적용하기 위하여 필요한 강우 데이터의 전처리에 대한 연구를 수행 하였다. 강우계열, 유입량계열을 조합하여 3가지 머신러닝(SVM, RF, LGBM)과 2가지 딥러닝(LSTM, TCN) 모델을 구축하고, 최적 하이퍼파라메터 튜닝을 통하여 적합 모델을 적용하고 한 결과, NSE 0.842~0.894로 높은 수준의 예측성능을 나타내었다. 또한 융적설을 반영한 강우보정 데이터를 만들기 위하여 융적설 모의 알고리즘을 개발하고, 이를 통하여 산정된 보정강우를 머신러닝 및 딥러닝 모델에 적용한 결과 NSE 0.841~0.896 으로 융적설 적용전과 비슷한 높은 수준의 예측 성능을 나타내었으나, 융적설 기간에는 조정된 강우로 학습되어 예측되었을 때 실측유입량에 근접하는 모의결과를 나타내었다. 결론적으로, 융적설이 영향을 미치는 유역에서의 데이터 모델 적용시에는 입력자료 구축시 적설 및 융설이 물리적으로 타당한 강우-유출 반응에 적합하도록 전처리과정이 중요함을 밝혔다."
        },
        {
          "rank": 33,
          "score": 0.7020517587661743,
          "doc_id": "ATN0051728135",
          "title": "딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구",
          "abstract": "하천 수위 예측의 주요 목적 중 하나는 홍수예경보 발령을 위한 기준으로 활용하는 것이다. 본 연구에서는 딥러닝 기반의 하천 수위 예측 모델을 홍수예경보 측면에서 효과적으로 활용하기 위해 학습데이터를 최적화하고, 딥러닝 모델의 정확도 향상을 평가하기 위해 딥러닝 모델의 자동 설계 및 최적화를 지원하는 AutoKeras를 활용하여 인위적인 요인을 배제한 모델을 구축하였다. 한탄강 상류유역을 대상지역으로 선정하고, 3개의 수위관측소와 유역평균강우 데이터를 구축하였고, 구축된 데이터를 이용하여 수위 변화 여부와 관계없이 강우가 발생한 모든 학습 데이터 셋을 사용한 모델(Model 1)과 일정 수준 이상의 수위 상승 변화가 있는 학습데이터 셋을 사용한 딥러닝 모델(Model 2)을 개발하여 한탄강 상류 한탄대교의 수위 및 홍수 예측 성능을 평가하였다. 실시간 하천 홍수예측 결과, 시계열 수위 예측에서 Model 1이 더 많은 데이터를 활용함으로써 상관계수와 평균제곱근오차(RMSE)에서 다소 우수한 성능을 보였다. 반면, Model 2는 홍수 예측에서 재현율(recall), F1-score, 임계성공지수(CSI) 등의 지표에서 더 뛰어난 성과를 보였다. 본 결과는 학습데이터의 특성과 구성 방식이 딥러닝 모델의 예측 능력에 큰 영향을 미친다는 것을 보여주며, 홍수와 같은 특정 사건을 예측하려면 수위 상승과 같은 핵심 요인 위주의 데이터를 더 집중적으로 학습시킬 필요가 있음을 시사한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0051728135&target=NART&cn=ATN0051728135",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구 딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구 딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구 하천 수위 예측의 주요 목적 중 하나는 홍수예경보 발령을 위한 기준으로 활용하는 것이다. 본 연구에서는 딥러닝 기반의 하천 수위 예측 모델을 홍수예경보 측면에서 효과적으로 활용하기 위해 학습데이터를 최적화하고, 딥러닝 모델의 정확도 향상을 평가하기 위해 딥러닝 모델의 자동 설계 및 최적화를 지원하는 AutoKeras를 활용하여 인위적인 요인을 배제한 모델을 구축하였다. 한탄강 상류유역을 대상지역으로 선정하고, 3개의 수위관측소와 유역평균강우 데이터를 구축하였고, 구축된 데이터를 이용하여 수위 변화 여부와 관계없이 강우가 발생한 모든 학습 데이터 셋을 사용한 모델(Model 1)과 일정 수준 이상의 수위 상승 변화가 있는 학습데이터 셋을 사용한 딥러닝 모델(Model 2)을 개발하여 한탄강 상류 한탄대교의 수위 및 홍수 예측 성능을 평가하였다. 실시간 하천 홍수예측 결과, 시계열 수위 예측에서 Model 1이 더 많은 데이터를 활용함으로써 상관계수와 평균제곱근오차(RMSE)에서 다소 우수한 성능을 보였다. 반면, Model 2는 홍수 예측에서 재현율(recall), F1-score, 임계성공지수(CSI) 등의 지표에서 더 뛰어난 성과를 보였다. 본 결과는 학습데이터의 특성과 구성 방식이 딥러닝 모델의 예측 능력에 큰 영향을 미친다는 것을 보여주며, 홍수와 같은 특정 사건을 예측하려면 수위 상승과 같은 핵심 요인 위주의 데이터를 더 집중적으로 학습시킬 필요가 있음을 시사한다."
        },
        {
          "rank": 34,
          "score": 0.7005530595779419,
          "doc_id": "NART125976684",
          "title": "A deep learning model for online doctor rating prediction",
          "abstract": "<P><B>Abstract</B><P>Predicting doctor ratings is a critical task in the healthcare industry. A patient usually provides ratings to a few doctors only, leading to the data sparsity issue, which complicates the rating prediction task. The study attempts to improve the prediction methodologies used in the doctor rating prediction systems. The study proposes a novel deep learning (DL) model for online doctor rating prediction based on a hierarchical attention bidirectional long short&#x2010;term memory (ODRP&#x2010;HABiLSTM) network. A hierarchical self&#x2010;attention bidirectional long short&#x2010;term memory (HA&#x2010;BiLSTM) network incorporates a textual review's word and sentence level information. A highway network is used to refine the representations learned by BiLSTM. The resulting latent patient and doctor representations are utilized to predict the online doctor ratings. Experimental findings based on real&#x2010;world doctor reviews from Yelp.com across two medical specialties demonstrate the proposed model's superior performance over state&#x2010;of&#x2010;the&#x2010;art benchmark models. In addition, robustness analysis is used to strengthen the findings.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART125976684&target=NART&cn=NART125976684",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A deep learning model for online doctor rating prediction A deep learning model for online doctor rating prediction A deep learning model for online doctor rating prediction <P><B>Abstract</B><P>Predicting doctor ratings is a critical task in the healthcare industry. A patient usually provides ratings to a few doctors only, leading to the data sparsity issue, which complicates the rating prediction task. The study attempts to improve the prediction methodologies used in the doctor rating prediction systems. The study proposes a novel deep learning (DL) model for online doctor rating prediction based on a hierarchical attention bidirectional long short&#x2010;term memory (ODRP&#x2010;HABiLSTM) network. A hierarchical self&#x2010;attention bidirectional long short&#x2010;term memory (HA&#x2010;BiLSTM) network incorporates a textual review's word and sentence level information. A highway network is used to refine the representations learned by BiLSTM. The resulting latent patient and doctor representations are utilized to predict the online doctor ratings. Experimental findings based on real&#x2010;world doctor reviews from Yelp.com across two medical specialties demonstrate the proposed model's superior performance over state&#x2010;of&#x2010;the&#x2010;art benchmark models. In addition, robustness analysis is used to strengthen the findings.</P></P>"
        },
        {
          "rank": 35,
          "score": 0.6999574899673462,
          "doc_id": "NART123506805",
          "title": "Prediction of Preeclampsia Using Machine Learning and Deep Learning Models: A Review",
          "abstract": "<P>Preeclampsia is one of the illnesses associated with placental dysfunction and pregnancy-induced hypertension, which appears after the first 20 weeks of pregnancy and is marked by proteinuria and hypertension. It can affect pregnant women and limit fetal growth, resulting in low birth weights, a risk factor for neonatal mortality. Approximately 10% of pregnancies worldwide are affected by hypertensive disorders during pregnancy. In this review, we discuss the machine learning and deep learning methods for preeclampsia prediction that were published between 2018 and 2022. Many models have been created using a variety of data types, including demographic and clinical data. We determined the techniques that successfully predicted preeclampsia. The methods that were used the most are random forest, support vector machine, and artificial neural network (ANN). In addition, the prospects and challenges in preeclampsia prediction are discussed to boost the research on artificial intelligence systems, allowing academics and practitioners to improve their methods and advance automated prediction.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART123506805&target=NART&cn=NART123506805",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Prediction of Preeclampsia Using Machine Learning and Deep Learning Models: A Review Prediction of Preeclampsia Using Machine Learning and Deep Learning Models: A Review Prediction of Preeclampsia Using Machine Learning and Deep Learning Models: A Review <P>Preeclampsia is one of the illnesses associated with placental dysfunction and pregnancy-induced hypertension, which appears after the first 20 weeks of pregnancy and is marked by proteinuria and hypertension. It can affect pregnant women and limit fetal growth, resulting in low birth weights, a risk factor for neonatal mortality. Approximately 10% of pregnancies worldwide are affected by hypertensive disorders during pregnancy. In this review, we discuss the machine learning and deep learning methods for preeclampsia prediction that were published between 2018 and 2022. Many models have been created using a variety of data types, including demographic and clinical data. We determined the techniques that successfully predicted preeclampsia. The methods that were used the most are random forest, support vector machine, and artificial neural network (ANN). In addition, the prospects and challenges in preeclampsia prediction are discussed to boost the research on artificial intelligence systems, allowing academics and practitioners to improve their methods and advance automated prediction.</P>"
        },
        {
          "rank": 36,
          "score": 0.699613094329834,
          "doc_id": "JAKO202313759686729",
          "title": "딥러닝과 머신러닝을 이용한 아파트 실거래가 예측",
          "abstract": "코로나 시대 이후 아파트 가격 상승은 비상식적이었다. 이러한 불확실한 부동산 시장에서 가격 예측 연구는 매우 중요하다. 본 논문에서는 다양한 부동산 사이트에서 자료 수집 및 크롤링을 통해 2015년부터 2020년까지 87만개의 방대한 데이터셋을 구축하고 다양한 아파트 정보와 경제지표 등 가능한 많은 변수를 모은 뒤 미래 아파트 매매실거래가격을 예측하는 모델을 만든다. 해당 연구는 먼저 다중 공선성 문제를 변수 제거 및 결합으로 해결하였다. 이후 의미있는 독립변수들을 뽑아내는 전진선택법(Forward Selection), 후진소거법(Backward Elimination), 단계적선택법(Stepwise Selection), L1 Regularization, 주성분분석(PCA) 총 5개의 변수 선택 알고리즘을 사용했다. 또한 심층신경망(DNN), XGBoost, CatBoost, Linear Regression 총 4개의 머신러닝 및 딥러닝 알고리즘을 이용해 하이퍼파라미터 최적화 후 모델을 학습시키고 모형간 예측력을 비교하였다. 추가 실험에서는 DNN의 node와 layer 수를 바꿔가면서 실험을 진행하여 가장 적절한 node와 layer 수를 찾고자 하였다. 결론적으로 가장 성능이 우수한 모델로 2021년의 아파트 매매실거래가격을 예측한 후 실제 2021년 데이터와 비교한 결과 훌륭한 성과를 보였다. 이를 통해 머신러닝과 딥러닝은 다양한 경제 상황 속에서 투자자들이 주택을 구매할 때 올바른 판단을 할 수 있도록 도움을 줄 수 있을 것이라 확신한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202313759686729&target=NART&cn=JAKO202313759686729",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝과 머신러닝을 이용한 아파트 실거래가 예측 딥러닝과 머신러닝을 이용한 아파트 실거래가 예측 딥러닝과 머신러닝을 이용한 아파트 실거래가 예측 코로나 시대 이후 아파트 가격 상승은 비상식적이었다. 이러한 불확실한 부동산 시장에서 가격 예측 연구는 매우 중요하다. 본 논문에서는 다양한 부동산 사이트에서 자료 수집 및 크롤링을 통해 2015년부터 2020년까지 87만개의 방대한 데이터셋을 구축하고 다양한 아파트 정보와 경제지표 등 가능한 많은 변수를 모은 뒤 미래 아파트 매매실거래가격을 예측하는 모델을 만든다. 해당 연구는 먼저 다중 공선성 문제를 변수 제거 및 결합으로 해결하였다. 이후 의미있는 독립변수들을 뽑아내는 전진선택법(Forward Selection), 후진소거법(Backward Elimination), 단계적선택법(Stepwise Selection), L1 Regularization, 주성분분석(PCA) 총 5개의 변수 선택 알고리즘을 사용했다. 또한 심층신경망(DNN), XGBoost, CatBoost, Linear Regression 총 4개의 머신러닝 및 딥러닝 알고리즘을 이용해 하이퍼파라미터 최적화 후 모델을 학습시키고 모형간 예측력을 비교하였다. 추가 실험에서는 DNN의 node와 layer 수를 바꿔가면서 실험을 진행하여 가장 적절한 node와 layer 수를 찾고자 하였다. 결론적으로 가장 성능이 우수한 모델로 2021년의 아파트 매매실거래가격을 예측한 후 실제 2021년 데이터와 비교한 결과 훌륭한 성과를 보였다. 이를 통해 머신러닝과 딥러닝은 다양한 경제 상황 속에서 투자자들이 주택을 구매할 때 올바른 판단을 할 수 있도록 도움을 줄 수 있을 것이라 확신한다."
        },
        {
          "rank": 37,
          "score": 0.6989973187446594,
          "doc_id": "NART123583722",
          "title": "Sediment load prediction in Johor river: deep learning versus machine learning models",
          "abstract": "<P><B>Abstract</B><P>Sediment transport is a normal phenomenon in rivers and streams, contributing significantly to ecosystem production and preservation by replenishing vital nutrients and preserving aquatic life&rsquo;s natural habitats. Thus, sediment transport prediction through modeling is crucial for predicting flood events, tracking coastal erosion, planning for water supplies, and managing irrigation. The predictability of process-driven models may encounter various restrictions throughout the validation process. Given that data-driven models work on the assumption that the underlying physical process is not requisite, this opens up the avenue for AI-based model as alternative modeling. However, AI-based models, such as ANN and SVM, face problems, such as long-term dependency, which require alternative dynamic procedures. Since their performance as universal function approximation depends on their compatibility with the nature of the problem itself, this study investigated several distinct AI-based models, such as long short-term memory (LSTM), artificial neural network (ANN), and support vector machine (SVM), in predicting sediment transport in the Johor river. The collected historical daily sediment transport data from January 1, 2008, to December 01, 2018, through autocorrelation function, were used as input for the model. The statistical results showed that, despite their ability (deep learning and machine learning) to provide sediment predictions based on historical input datasets, machine learning, such as ANN, might be more prone to overfitting or being trapped in a local optimum than deep learning, evidenced by the worse in all metrics score. With RMSE = 11.395, MAE = 18.094, and <I>R</I>2 = 0.914, LSTM outperformed other models in the comparison.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART123583722&target=NART&cn=NART123583722",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Sediment load prediction in Johor river: deep learning versus machine learning models Sediment load prediction in Johor river: deep learning versus machine learning models Sediment load prediction in Johor river: deep learning versus machine learning models <P><B>Abstract</B><P>Sediment transport is a normal phenomenon in rivers and streams, contributing significantly to ecosystem production and preservation by replenishing vital nutrients and preserving aquatic life&rsquo;s natural habitats. Thus, sediment transport prediction through modeling is crucial for predicting flood events, tracking coastal erosion, planning for water supplies, and managing irrigation. The predictability of process-driven models may encounter various restrictions throughout the validation process. Given that data-driven models work on the assumption that the underlying physical process is not requisite, this opens up the avenue for AI-based model as alternative modeling. However, AI-based models, such as ANN and SVM, face problems, such as long-term dependency, which require alternative dynamic procedures. Since their performance as universal function approximation depends on their compatibility with the nature of the problem itself, this study investigated several distinct AI-based models, such as long short-term memory (LSTM), artificial neural network (ANN), and support vector machine (SVM), in predicting sediment transport in the Johor river. The collected historical daily sediment transport data from January 1, 2008, to December 01, 2018, through autocorrelation function, were used as input for the model. The statistical results showed that, despite their ability (deep learning and machine learning) to provide sediment predictions based on historical input datasets, machine learning, such as ANN, might be more prone to overfitting or being trapped in a local optimum than deep learning, evidenced by the worse in all metrics score. With RMSE = 11.395, MAE = 18.094, and <I>R</I>2 = 0.914, LSTM outperformed other models in the comparison.</P></P>"
        },
        {
          "rank": 38,
          "score": 0.6968635320663452,
          "doc_id": "DIKO0017011976",
          "title": "대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형",
          "abstract": "본 연구는 온라인 리뷰의 유용성을 예측하기 위한 모델을 제안하며, 이를 위해 대형 언어 모델과 다양한 딥러닝 기법을 통합적으로 활용하였다. 연구의 시작에서는 온라인 리뷰 및 리뷰 유용성에 대한 이론적 배경을 탐구하였으며, 여러 기존 연구들을 통해 리뷰 유용성에 영향을 미치는 요인들을 정리하였다. 특히, 통계기법, 머신러닝, 딥러닝, 그리고 대형 언어 모델을 중심으로 한 기존의 리뷰 유용성 예측 모형들을 비교 및 분석하였다. 이후, KoBERT와 KoGPT2와 같은 한국어 대형 언어 모델을 기반으로 한 리뷰 유용성 예측모형을 구축하였으며, K-NN 알고리즘으로 통합하여 모델의 성능을 향상시켰다. 실증분석 결과, 본 연구에서 제안한 모델은 기존의 모델들에 비해 높은 예측 성능을 보여주었고, 특히 대형 언어 모델의 통합은 리뷰 유용성 예측의 정확도를 크게 향상시켰다. 이러한 결과는 온라인 리뷰의 품질 및 유용성 평가에 큰 도움을 제공할 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017011976&target=NART&cn=DIKO0017011976",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형 대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형 대형 언어 모델과 딥러닝을 통합한 리뷰 유용성 예측 모형 본 연구는 온라인 리뷰의 유용성을 예측하기 위한 모델을 제안하며, 이를 위해 대형 언어 모델과 다양한 딥러닝 기법을 통합적으로 활용하였다. 연구의 시작에서는 온라인 리뷰 및 리뷰 유용성에 대한 이론적 배경을 탐구하였으며, 여러 기존 연구들을 통해 리뷰 유용성에 영향을 미치는 요인들을 정리하였다. 특히, 통계기법, 머신러닝, 딥러닝, 그리고 대형 언어 모델을 중심으로 한 기존의 리뷰 유용성 예측 모형들을 비교 및 분석하였다. 이후, KoBERT와 KoGPT2와 같은 한국어 대형 언어 모델을 기반으로 한 리뷰 유용성 예측모형을 구축하였으며, K-NN 알고리즘으로 통합하여 모델의 성능을 향상시켰다. 실증분석 결과, 본 연구에서 제안한 모델은 기존의 모델들에 비해 높은 예측 성능을 보여주었고, 특히 대형 언어 모델의 통합은 리뷰 유용성 예측의 정확도를 크게 향상시켰다. 이러한 결과는 온라인 리뷰의 품질 및 유용성 평가에 큰 도움을 제공할 것으로 기대된다."
        },
        {
          "rank": 39,
          "score": 0.6962004899978638,
          "doc_id": "ATN0035906971",
          "title": "딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구",
          "abstract": "In this research, we compare the explanatory power between linear regression model and deep-learning model when estimating stock returns. As predicted, the deep-learning model shows statistically significant improvement over linear regression model, although the improvement is not economically meaningful. We further investigate the effects of deep-learning model using different parameters and pre-processing. The results show that the predictive power of deep-learning model can be worse-off than that of linear model if it fails to select optimal parameters. Especially, it is important to choose adequate deep-learning parameters not to overfit the data, because the accounting data (which is at most quarterly) may not be sufficient enough for the deep model structure. Further, we show that the predictive power using researchers’ domain knowledge is sometimes better off than that relying simply on the deep-learning model. For instance, denomination with total assets brings better results than non-denomination. Another interesting finding is that winsorizing extreme values brings lower explanatory power when we use the deep-learning model. Such finding implies that, by removing extreme values, we may lose useful information in the parameter estimation. The results of this paper will help future research decide whether to utilize deep learning model or linear regression model",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0035906971&target=NART&cn=ATN0035906971",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 In this research, we compare the explanatory power between linear regression model and deep-learning model when estimating stock returns. As predicted, the deep-learning model shows statistically significant improvement over linear regression model, although the improvement is not economically meaningful. We further investigate the effects of deep-learning model using different parameters and pre-processing. The results show that the predictive power of deep-learning model can be worse-off than that of linear model if it fails to select optimal parameters. Especially, it is important to choose adequate deep-learning parameters not to overfit the data, because the accounting data (which is at most quarterly) may not be sufficient enough for the deep model structure. Further, we show that the predictive power using researchers’ domain knowledge is sometimes better off than that relying simply on the deep-learning model. For instance, denomination with total assets brings better results than non-denomination. Another interesting finding is that winsorizing extreme values brings lower explanatory power when we use the deep-learning model. Such finding implies that, by removing extreme values, we may lose useful information in the parameter estimation. The results of this paper will help future research decide whether to utilize deep learning model or linear regression model"
        },
        {
          "rank": 40,
          "score": 0.6941635012626648,
          "doc_id": "NART104866386",
          "title": "딥러닝 기반의 가정용 가스 수요 예측 모델 개발",
          "abstract": "개인 가정의 가스 사용량 측정에는 일반적으로 검침원이 투입되어 수동으로 실행된다. 그러나, 수동적인 방법은 비용이 크고, 실시간으로 가스사용량을 반영할 수 없는 문제가 있다. 최근에는 이러한 문제를 해결하기 위해 원격 검침이 가능한 스마트 가스미터가 개발되고 있으나 가스미터 장치에 자석을 부착하는 등 데이터 수집을 방해하는 악의적인 사용은 스마트 가스미터의 문제점 중 하나로 꼽히고 있다. 이러한 문제는 사용량을 비교할 수 있는 예측 모델이 있다면 악의적인 사용을 감지하는데 도움이 될 것이다. 본 연구진은 문제 해결을 위해 시계열 데이터 학습 모델인 Long short term memory을 사용하여 가스 수요 예측 가능성을 검증하였다. 모델을 검증하기 위해 실제 운영 중인 스마트 가스미터로부터 데이터 수집하였고, 과거 일주일간의 시간에 따른 사용량을 입력, 48시간 뒤의 사용량 값을 출력으로 학습한 모델을 실제 값과 비교하여 검증하였으며 그 결과로 98.08%의 정확도를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART104866386&target=NART&cn=NART104866386",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반의 가정용 가스 수요 예측 모델 개발 딥러닝 기반의 가정용 가스 수요 예측 모델 개발 딥러닝 기반의 가정용 가스 수요 예측 모델 개발 개인 가정의 가스 사용량 측정에는 일반적으로 검침원이 투입되어 수동으로 실행된다. 그러나, 수동적인 방법은 비용이 크고, 실시간으로 가스사용량을 반영할 수 없는 문제가 있다. 최근에는 이러한 문제를 해결하기 위해 원격 검침이 가능한 스마트 가스미터가 개발되고 있으나 가스미터 장치에 자석을 부착하는 등 데이터 수집을 방해하는 악의적인 사용은 스마트 가스미터의 문제점 중 하나로 꼽히고 있다. 이러한 문제는 사용량을 비교할 수 있는 예측 모델이 있다면 악의적인 사용을 감지하는데 도움이 될 것이다. 본 연구진은 문제 해결을 위해 시계열 데이터 학습 모델인 Long short term memory을 사용하여 가스 수요 예측 가능성을 검증하였다. 모델을 검증하기 위해 실제 운영 중인 스마트 가스미터로부터 데이터 수집하였고, 과거 일주일간의 시간에 따른 사용량을 입력, 48시간 뒤의 사용량 값을 출력으로 학습한 모델을 실제 값과 비교하여 검증하였으며 그 결과로 98.08%의 정확도를 얻었다."
        },
        {
          "rank": 41,
          "score": 0.6910699605941772,
          "doc_id": "JAKO201614137727823",
          "title": "딥러닝 기법을 이용한 내일강수 예측",
          "abstract": "정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로 기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본 논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도 사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의 AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를 사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로 사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의 척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201614137727823&target=NART&cn=JAKO201614137727823",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기법을 이용한 내일강수 예측 딥러닝 기법을 이용한 내일강수 예측 딥러닝 기법을 이용한 내일강수 예측 정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로 기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본 논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도 사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의 AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를 사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로 사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의 척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다."
        },
        {
          "rank": 42,
          "score": 0.6894570589065552,
          "doc_id": "JAKO202116954704821",
          "title": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측",
          "abstract": "본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116954704821&target=NART&cn=JAKO202116954704821",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다."
        },
        {
          "rank": 43,
          "score": 0.6885985136032104,
          "doc_id": "DIKO0016605759",
          "title": "SVM, LSTM, CNN-LSTM과 TCN을 이용한 금 가격 예측",
          "abstract": "금은 불확실한 경제 시대에서 안전한 상품으로 사용되며, 세계의 많은 금융 활동에 영향을 끼치는 세계 경제의 핵심 요소이다. 이러한 금 가격에 대한 정확한 예측은 금 가격 변동에 대한 통찰력을 제공하며 투자자들이 올바른 결정을 내리는 데 도움이 되어 상당한 이익을 얻을 수 있는 기회를 제공할 수 있다. 따라서, 신뢰할 수 있는 금 가격 예측 모델의 개발은 매우 중요하다.&amp;#xD; 최근 금 가격 예측의 선행 연구에서 SVM, LSTM, CNN-LSTM, 그리고 TCN 모델이 높은 성과를 보인 바 있다. 따라서, 본 논문에서는 금 가격 예측을 위해 SVM, LSTM, CNN-LSTM과 TCN 모델을 사용하여 그 예측 성능을 비교한다. 이 연구는 2019년 1월 1일부터 2022년 9월 30일까지의 미국 달러(USD) 기준 일일 금 가격 데이터를 분석에 사용하고, 모델의 성능 평가 지표로 RMSE, MAE와 MAPE를 사용한다.&amp;#xD; 연구 결과는 TCN 모델이 나머지 모델들과 비교하여 가장 낮은 RMSE, MAE, MAPE를 보이며 가장 높은 예측성능을 보였다. 또한, MCS 분석 결과에서도 TCN 모델이 가장 우수한 모델임을 보이며, 금 가격 예측에서 TCN 모델이 차세대 딥러닝 모델로서 충분히 사용 가치가 있음을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016605759&target=NART&cn=DIKO0016605759",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SVM, LSTM, CNN-LSTM과 TCN을 이용한 금 가격 예측 SVM, LSTM, CNN-LSTM과 TCN을 이용한 금 가격 예측 SVM, LSTM, CNN-LSTM과 TCN을 이용한 금 가격 예측 금은 불확실한 경제 시대에서 안전한 상품으로 사용되며, 세계의 많은 금융 활동에 영향을 끼치는 세계 경제의 핵심 요소이다. 이러한 금 가격에 대한 정확한 예측은 금 가격 변동에 대한 통찰력을 제공하며 투자자들이 올바른 결정을 내리는 데 도움이 되어 상당한 이익을 얻을 수 있는 기회를 제공할 수 있다. 따라서, 신뢰할 수 있는 금 가격 예측 모델의 개발은 매우 중요하다.&amp;#xD; 최근 금 가격 예측의 선행 연구에서 SVM, LSTM, CNN-LSTM, 그리고 TCN 모델이 높은 성과를 보인 바 있다. 따라서, 본 논문에서는 금 가격 예측을 위해 SVM, LSTM, CNN-LSTM과 TCN 모델을 사용하여 그 예측 성능을 비교한다. 이 연구는 2019년 1월 1일부터 2022년 9월 30일까지의 미국 달러(USD) 기준 일일 금 가격 데이터를 분석에 사용하고, 모델의 성능 평가 지표로 RMSE, MAE와 MAPE를 사용한다.&amp;#xD; 연구 결과는 TCN 모델이 나머지 모델들과 비교하여 가장 낮은 RMSE, MAE, MAPE를 보이며 가장 높은 예측성능을 보였다. 또한, MCS 분석 결과에서도 TCN 모델이 가장 우수한 모델임을 보이며, 금 가격 예측에서 TCN 모델이 차세대 딥러닝 모델로서 충분히 사용 가치가 있음을 보였다."
        },
        {
          "rank": 44,
          "score": 0.688042163848877,
          "doc_id": "NART135912853",
          "title": "A Comprehensive Evaluation of Machine Learning and Deep Learning Models for Churn Prediction",
          "abstract": "<P>Churn prediction has become one of the core concepts in customer relationship management within the insurances, telecom, and internet service provider industries, which is essential in customer retention. Therefore, this study attempts to analyze the effectiveness of the advanced machine learning and deep learning models for churn prediction in the evaluation of the models&rsquo; performance across different sectors. This would help conclude whether the varied patterns of the churn throughout different sectors to the level that affects the model performance and to what extent. The work includes three datasets: namely, insurance churn, internet service provider customer churn, and Telecom churn datasets. The implementation and comparison conducted in this study of models include XGBoost, Convolutional Neural Networks (CNNs), and Ensemble Deep Learning with the pre-trained hybrid approach. The results show that the ensemble deep learning model outperforms other models in terms of accuracy and F1-score, achieving accuracies of up to 95.96% in the insurance churn dataset and of 98.42% in the telecom churn dataset. Moreover, traditional machine learning models like XGBoost also produced competitive results for selected datasets. The proposed deep learning ensembles reveal the strength and possibility for churn prediction and provide a benchmark for future research relevant to customer retention strategies. Also, the proposed ensemble deep learning model shows stable performance across different sectors, which reflects its ability to capture the varied churn patterns of different sectors.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART135912853&target=NART&cn=NART135912853",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "A Comprehensive Evaluation of Machine Learning and Deep Learning Models for Churn Prediction A Comprehensive Evaluation of Machine Learning and Deep Learning Models for Churn Prediction A Comprehensive Evaluation of Machine Learning and Deep Learning Models for Churn Prediction <P>Churn prediction has become one of the core concepts in customer relationship management within the insurances, telecom, and internet service provider industries, which is essential in customer retention. Therefore, this study attempts to analyze the effectiveness of the advanced machine learning and deep learning models for churn prediction in the evaluation of the models&rsquo; performance across different sectors. This would help conclude whether the varied patterns of the churn throughout different sectors to the level that affects the model performance and to what extent. The work includes three datasets: namely, insurance churn, internet service provider customer churn, and Telecom churn datasets. The implementation and comparison conducted in this study of models include XGBoost, Convolutional Neural Networks (CNNs), and Ensemble Deep Learning with the pre-trained hybrid approach. The results show that the ensemble deep learning model outperforms other models in terms of accuracy and F1-score, achieving accuracies of up to 95.96% in the insurance churn dataset and of 98.42% in the telecom churn dataset. Moreover, traditional machine learning models like XGBoost also produced competitive results for selected datasets. The proposed deep learning ensembles reveal the strength and possibility for churn prediction and provide a benchmark for future research relevant to customer retention strategies. Also, the proposed ensemble deep learning model shows stable performance across different sectors, which reflects its ability to capture the varied churn patterns of different sectors.</P>"
        },
        {
          "rank": 45,
          "score": 0.6870097517967224,
          "doc_id": "ATN0049124885",
          "title": "머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구",
          "abstract": "본고는 XGBoost(Extreme Gradient Boosting), LSTM(Long Short-Term Memory) 등 다양한 머신러닝 방법을 포함한 총 13개의 모형을 활용하여 한국의 전년동월대비 물가상승률을 예측/분석하고 이를 통해 시계열 지속성과 머신러닝 예측 간의 관계에 관한 시사점을 제시한다. FRED-MD를 참고한 총 93개의 관련 국내외 거시경제/금융 변수들을 설명변수로 사용하고, 2004년 8월에서 2023년 6월까지의 표본을 고려하였다. 전월대비 물가상승률의 경우와는 달리 시계열 지속성(Persistence)이 높은 전년동월대비 물가상승률을 종속변수로 삼고 머신러닝 방법을 적용하여 예측할 때, 머신러닝 모형들의 예측력이 크게 떨어져 오히려 임의보행(Random Walk) 모형보다 예측오차가 더 큰 것으로 나타났다. 머신러닝 기법으로 시계열 지속성이 낮은 전월대비 물가상승률 예측치를 구하고 이를 통해 전년동월대비 물가상승률 예측치를 계산하면 전년동월대비 물가상승률 자체를 예측하는 경우에 비해 전반적으로 예측오차가 줄어드는 것을 확인하였고, 이 방법을 통해 전년동월대비 물가상승률 자체를 임의보행 모형으로 예측할 때보다 통계적으로 유의한 수준으로 우월한 예측치를 구할 수도 있는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0049124885&target=NART&cn=ATN0049124885",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구 머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구 머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구 본고는 XGBoost(Extreme Gradient Boosting), LSTM(Long Short-Term Memory) 등 다양한 머신러닝 방법을 포함한 총 13개의 모형을 활용하여 한국의 전년동월대비 물가상승률을 예측/분석하고 이를 통해 시계열 지속성과 머신러닝 예측 간의 관계에 관한 시사점을 제시한다. FRED-MD를 참고한 총 93개의 관련 국내외 거시경제/금융 변수들을 설명변수로 사용하고, 2004년 8월에서 2023년 6월까지의 표본을 고려하였다. 전월대비 물가상승률의 경우와는 달리 시계열 지속성(Persistence)이 높은 전년동월대비 물가상승률을 종속변수로 삼고 머신러닝 방법을 적용하여 예측할 때, 머신러닝 모형들의 예측력이 크게 떨어져 오히려 임의보행(Random Walk) 모형보다 예측오차가 더 큰 것으로 나타났다. 머신러닝 기법으로 시계열 지속성이 낮은 전월대비 물가상승률 예측치를 구하고 이를 통해 전년동월대비 물가상승률 예측치를 계산하면 전년동월대비 물가상승률 자체를 예측하는 경우에 비해 전반적으로 예측오차가 줄어드는 것을 확인하였고, 이 방법을 통해 전년동월대비 물가상승률 자체를 임의보행 모형으로 예측할 때보다 통계적으로 유의한 수준으로 우월한 예측치를 구할 수도 있는 것으로 나타났다."
        },
        {
          "rank": 46,
          "score": 0.6864522695541382,
          "doc_id": "ART003001921",
          "title": "Prediction Model of Inclination to Visit Jeju Tourist Attractions based on CNN Deep Learning",
          "abstract": "Sentiment analysis can be applied to all texts generated from websites, blogs, messengers, etc. The study fulfills an artificial intelligence sentiment analysis estimating visiting evaluation opinions (reviews) and visitor ratings, and suggests a deep learning model which foretells either an affirmative or a negative inclination for new reviews. This study operates review big data about Jeju tourist attractions which are extracted from Google from October 1st, 2021 to November 30th, 2021. The normalization data used in the propensity prediction modeling of this study were divided into training data and test data at a 7.5:2.5 ratio, and the CNN classification neural network was used for learning. The predictive model of the research indicates an accuracy of approximately 84.72%, which shows that it can upgrade performance in the future as evaluating its error rate and learning precision.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART003001921&target=NART&cn=ART003001921",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Prediction Model of Inclination to Visit Jeju Tourist Attractions based on CNN Deep Learning Prediction Model of Inclination to Visit Jeju Tourist Attractions based on CNN Deep Learning Prediction Model of Inclination to Visit Jeju Tourist Attractions based on CNN Deep Learning Sentiment analysis can be applied to all texts generated from websites, blogs, messengers, etc. The study fulfills an artificial intelligence sentiment analysis estimating visiting evaluation opinions (reviews) and visitor ratings, and suggests a deep learning model which foretells either an affirmative or a negative inclination for new reviews. This study operates review big data about Jeju tourist attractions which are extracted from Google from October 1st, 2021 to November 30th, 2021. The normalization data used in the propensity prediction modeling of this study were divided into training data and test data at a 7.5:2.5 ratio, and the CNN classification neural network was used for learning. The predictive model of the research indicates an accuracy of approximately 84.72%, which shows that it can upgrade performance in the future as evaluating its error rate and learning precision."
        },
        {
          "rank": 47,
          "score": 0.6854442358016968,
          "doc_id": "JAKO202133472092958",
          "title": "머신러닝과 딥러닝 기법을 이용한 부산 전략산업과 수출에 의한 고용과 소득 예측",
          "abstract": "This paper analyzes the feasibility of using machine learning and deep learning methods to forecast the income and employment using the strategic industries as well as investment, export, and exchange rates. The decision tree, artificial neural network, support vector machine, and deep learning models were used to forecast the income and employment in Busan. The following were the main findings of the comparison of their predictive abilities. First, the decision tree models predict the income and employment well. The forecasting values for the income and employment appeared somewhat differently according to the depth of decision trees and several conditions of strategic industries as well as investment, export, and exchange rates. Second, since the artificial neural network models show that the coefficients are somewhat low and RMSE are somewhat high, these models are not good forecasting the income and employment. Third, the support vector machine models show the high predictive power with the high coefficients of determination and low RMSE. Fourth, the deep neural network models show the higher predictive power with appropriate epochs and batch sizes. Thus, since the machine learning and deep learning models can predict the employment well, we need to adopt the machine learning and deep learning models to forecast the income and employment.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202133472092958&target=NART&cn=JAKO202133472092958",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝과 딥러닝 기법을 이용한 부산 전략산업과 수출에 의한 고용과 소득 예측 머신러닝과 딥러닝 기법을 이용한 부산 전략산업과 수출에 의한 고용과 소득 예측 머신러닝과 딥러닝 기법을 이용한 부산 전략산업과 수출에 의한 고용과 소득 예측 This paper analyzes the feasibility of using machine learning and deep learning methods to forecast the income and employment using the strategic industries as well as investment, export, and exchange rates. The decision tree, artificial neural network, support vector machine, and deep learning models were used to forecast the income and employment in Busan. The following were the main findings of the comparison of their predictive abilities. First, the decision tree models predict the income and employment well. The forecasting values for the income and employment appeared somewhat differently according to the depth of decision trees and several conditions of strategic industries as well as investment, export, and exchange rates. Second, since the artificial neural network models show that the coefficients are somewhat low and RMSE are somewhat high, these models are not good forecasting the income and employment. Third, the support vector machine models show the high predictive power with the high coefficients of determination and low RMSE. Fourth, the deep neural network models show the higher predictive power with appropriate epochs and batch sizes. Thus, since the machine learning and deep learning models can predict the employment well, we need to adopt the machine learning and deep learning models to forecast the income and employment."
        },
        {
          "rank": 48,
          "score": 0.6848295331001282,
          "doc_id": "JAKO202131559470407",
          "title": "빅데이터를 활용한 인공지능 주식 예측 분석",
          "abstract": "저금리 시대의 도래로 인해 많은 투자자들이 주식 시장으로 몰리고 있다. 과거의 주식 시장은 사람들이 기업 분석 및 각자의 투자기법을 통해 노동 집약적으로 주식 투자가 이루어졌다면 최근 들어 인공지능 및 데이터를 활용하여 주식 투자가 널리 이용되고 있는 실정이다. 인공지능을 통해 주식 예측의 성공률은 현재 높지 않아 다양한 인공지능 모델을 통해 주식 예측률을 높이는 시도를 하고 있다. 본 연구에서는 다양한 인공지능 모델에 대해 살펴보고 각 모델들간의 장단점 및 예측률을 파악하고자 한다. 이를 위해, 본 연구에서는 주식예측 인공지능 프로그램으로 인공신경망(ANN), 심층 학습 또는 딥 러닝(DNN), k-최근접 이웃 알고리즘(k-NN), 합성곱 신경망(CNN), 순환 신경망(RNN), LSTM에 대해 살펴보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202131559470407&target=NART&cn=JAKO202131559470407",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "빅데이터를 활용한 인공지능 주식 예측 분석 빅데이터를 활용한 인공지능 주식 예측 분석 빅데이터를 활용한 인공지능 주식 예측 분석 저금리 시대의 도래로 인해 많은 투자자들이 주식 시장으로 몰리고 있다. 과거의 주식 시장은 사람들이 기업 분석 및 각자의 투자기법을 통해 노동 집약적으로 주식 투자가 이루어졌다면 최근 들어 인공지능 및 데이터를 활용하여 주식 투자가 널리 이용되고 있는 실정이다. 인공지능을 통해 주식 예측의 성공률은 현재 높지 않아 다양한 인공지능 모델을 통해 주식 예측률을 높이는 시도를 하고 있다. 본 연구에서는 다양한 인공지능 모델에 대해 살펴보고 각 모델들간의 장단점 및 예측률을 파악하고자 한다. 이를 위해, 본 연구에서는 주식예측 인공지능 프로그램으로 인공신경망(ANN), 심층 학습 또는 딥 러닝(DNN), k-최근접 이웃 알고리즘(k-NN), 합성곱 신경망(CNN), 순환 신경망(RNN), LSTM에 대해 살펴보고자 한다."
        },
        {
          "rank": 49,
          "score": 0.6840638518333435,
          "doc_id": "JAKO202011236744256",
          "title": "암호화폐 가격 예측을 위한 딥러닝 앙상블 모델링 : Deep 4-LSTM Ensemble Model",
          "abstract": "As the blockchain technology attracts attention, interest in cryptocurrency that is received as a reward is also increasing. Currently, investments and transactions are continuing with the expectation and increasing value of cryptocurrency. Accordingly, prediction for cryptocurrency price has been attempted through artificial intelligence technology and social sentiment analysis. The purpose of this paper is to develop a deep learning ensemble model for predicting the price fluctuations and one-day lag price of cryptocurrency based on the design science research method. This paper intends to perform predictive modeling on Ethereum among cryptocurrencies to make predictions more efficiently and accurately than existing models. Therefore, it collects data for five years related to Ethereum price and performs pre-processing through customized functions. In the model development stage, four LSTM models, which are efficient for time series data processing, are utilized to build an ensemble model with the optimal combination of hyperparameters found in the experimental process. Then, based on the performance evaluation scale, the superiority of the model is evaluated through comparison with other deep learning models. The results of this paper have a practical contribution that can be used as a model that shows high performance and predictive rate for cryptocurrency price prediction and price fluctuations. Besides, it shows academic contribution in that it improves the quality of research by following scientific design research procedures that solve scientific problems and create and evaluate new and innovative products in the field of information systems.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202011236744256&target=NART&cn=JAKO202011236744256",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "암호화폐 가격 예측을 위한 딥러닝 앙상블 모델링 : Deep 4-LSTM Ensemble Model 암호화폐 가격 예측을 위한 딥러닝 앙상블 모델링 : Deep 4-LSTM Ensemble Model 암호화폐 가격 예측을 위한 딥러닝 앙상블 모델링 : Deep 4-LSTM Ensemble Model As the blockchain technology attracts attention, interest in cryptocurrency that is received as a reward is also increasing. Currently, investments and transactions are continuing with the expectation and increasing value of cryptocurrency. Accordingly, prediction for cryptocurrency price has been attempted through artificial intelligence technology and social sentiment analysis. The purpose of this paper is to develop a deep learning ensemble model for predicting the price fluctuations and one-day lag price of cryptocurrency based on the design science research method. This paper intends to perform predictive modeling on Ethereum among cryptocurrencies to make predictions more efficiently and accurately than existing models. Therefore, it collects data for five years related to Ethereum price and performs pre-processing through customized functions. In the model development stage, four LSTM models, which are efficient for time series data processing, are utilized to build an ensemble model with the optimal combination of hyperparameters found in the experimental process. Then, based on the performance evaluation scale, the superiority of the model is evaluated through comparison with other deep learning models. The results of this paper have a practical contribution that can be used as a model that shows high performance and predictive rate for cryptocurrency price prediction and price fluctuations. Besides, it shows academic contribution in that it improves the quality of research by following scientific design research procedures that solve scientific problems and create and evaluate new and innovative products in the field of information systems."
        },
        {
          "rank": 50,
          "score": 0.6817255020141602,
          "doc_id": "JAKO202108848920380",
          "title": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구",
          "abstract": "The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202108848920380&target=NART&cn=JAKO202108848920380",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity."
        }
      ]
    },
    {
      "query": "What is the Mean Squared Error (MSE) for each of the evaluated models?",
      "query_meta": {
        "type": "single_hop",
        "index": 1
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.6792014837265015,
          "doc_id": "ATN0035909826",
          "title": "머신러닝 모델을 이용한 석산 개발 발파진동 예측",
          "abstract": "In this study, a model was developed to predict the peak particle velocity (PPV) that affects people and the surrounding environment during blasting. Four machine learning models using the k-nearest neighbors (kNN), classification and regression tree (CART), support vector regression (SVR), and particle swarm optimization (PSO)-SVR algorithms were developed and compared with each other to predict the PPV. Mt. Yogmang located in Changwon-si, Gyeongsangnam-do was selected as a study area, and 1048 blasting data were acquired to train the machine learning models. The blasting data consisted of hole length, burden, spacing, maximum charge per delay, powder factor, number of holes, ratio of emulsion, monitoring distance and PPV. To evaluate the performance of the trained models, the mean absolute error (MAE), mean square error (MSE), and root mean square error (RMSE) were used. The PSO-SVR model showed superior performance with MAE, MSE and RMSE of 0.0348, 0.0021 and 0.0458, respectively. Finally, a method was proposed to predict the degree of influence on the surrounding environment using the developed machine learning models.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0035909826&target=NART&cn=ATN0035909826",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 모델을 이용한 석산 개발 발파진동 예측 머신러닝 모델을 이용한 석산 개발 발파진동 예측 머신러닝 모델을 이용한 석산 개발 발파진동 예측 In this study, a model was developed to predict the peak particle velocity (PPV) that affects people and the surrounding environment during blasting. Four machine learning models using the k-nearest neighbors (kNN), classification and regression tree (CART), support vector regression (SVR), and particle swarm optimization (PSO)-SVR algorithms were developed and compared with each other to predict the PPV. Mt. Yogmang located in Changwon-si, Gyeongsangnam-do was selected as a study area, and 1048 blasting data were acquired to train the machine learning models. The blasting data consisted of hole length, burden, spacing, maximum charge per delay, powder factor, number of holes, ratio of emulsion, monitoring distance and PPV. To evaluate the performance of the trained models, the mean absolute error (MAE), mean square error (MSE), and root mean square error (RMSE) were used. The PSO-SVR model showed superior performance with MAE, MSE and RMSE of 0.0348, 0.0021 and 0.0458, respectively. Finally, a method was proposed to predict the degree of influence on the surrounding environment using the developed machine learning models."
        },
        {
          "rank": 2,
          "score": 0.6573114395141602,
          "doc_id": "ATN0027036789",
          "title": "인공신경망과 상관도 행렬을 이용한 폐광지역 지반침하 위험도지수 개발",
          "abstract": "A MSH(Mine Subsidence Hazard) index has been developed to estimate the subsidence possibility of an abandoned mine by means of an artificial neural network (ANN) and an interaction matrix. For this, 19 influence factors of mine subsidence were determined through a literature study, and the influence factors for 287 subsidence points at 38 abandoned mines were selected first, of which 108 points (34 abandoned mines) having high data quality were used for learning of ANN. Each influence factor of which range is widely distributed was classified into seven ranges to use it as a learning data of ANN. Interaction matrixes were constructed by an ANN analysis, and a MSH index for each subsidence point was calculated from influence factors as well as influence weights that are from the interaction matrix. The result shows that MSH index ranges between 37 and 66. The average of MSH index for Non-coal mines is 54, which is 13% higher than the one for coal mines of which average MSH index is 47.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0027036789&target=NART&cn=ATN0027036789",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "인공신경망과 상관도 행렬을 이용한 폐광지역 지반침하 위험도지수 개발 인공신경망과 상관도 행렬을 이용한 폐광지역 지반침하 위험도지수 개발 인공신경망과 상관도 행렬을 이용한 폐광지역 지반침하 위험도지수 개발 A MSH(Mine Subsidence Hazard) index has been developed to estimate the subsidence possibility of an abandoned mine by means of an artificial neural network (ANN) and an interaction matrix. For this, 19 influence factors of mine subsidence were determined through a literature study, and the influence factors for 287 subsidence points at 38 abandoned mines were selected first, of which 108 points (34 abandoned mines) having high data quality were used for learning of ANN. Each influence factor of which range is widely distributed was classified into seven ranges to use it as a learning data of ANN. Interaction matrixes were constructed by an ANN analysis, and a MSH index for each subsidence point was calculated from influence factors as well as influence weights that are from the interaction matrix. The result shows that MSH index ranges between 37 and 66. The average of MSH index for Non-coal mines is 54, which is 13% higher than the one for coal mines of which average MSH index is 47."
        },
        {
          "rank": 3,
          "score": 0.648781955242157,
          "doc_id": "JAKO201726163356540",
          "title": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델",
          "abstract": "본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201726163356540&target=NART&cn=JAKO201726163356540",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다."
        },
        {
          "rank": 4,
          "score": 0.6486749649047852,
          "doc_id": "JAKO200815652408515",
          "title": "Gompertz 소프트웨어 비용 추정 모델",
          "abstract": "본 논문은 소프트웨어 비용추정 모델의 적합성을 평가하고, 가장 적합한 모델을 제시하였다. 먼저, 해당 모델의 함수를 변수변환시켜 선형식으로 만든다. 다음으로 실제 개발 소프트웨어의 비용 데이터가 모델의 선형식에 얼마나 적합한지로 모델의 성능을 평가한다. 모델 성능평가에는 절대오차 대신 상대오차 개념인 MMRE를 적용하였다. 기존의 소프트웨어 비용추정 모델은 Weibull, Gamma와 Rayleigh 함수를 따르고 있다. 본 논문에서는 성장곡선의 일종인 Gompertz 곡선 모델을 제안하였다. 추가로 다른 성장곡선들도 적합성을 검증하였다. 모델 성능평가 결과 Gompertz 성장곡선이 소프트웨어 비용추정 모델로 가장 적합한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200815652408515&target=NART&cn=JAKO200815652408515",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Gompertz 소프트웨어 비용 추정 모델 Gompertz 소프트웨어 비용 추정 모델 Gompertz 소프트웨어 비용 추정 모델 본 논문은 소프트웨어 비용추정 모델의 적합성을 평가하고, 가장 적합한 모델을 제시하였다. 먼저, 해당 모델의 함수를 변수변환시켜 선형식으로 만든다. 다음으로 실제 개발 소프트웨어의 비용 데이터가 모델의 선형식에 얼마나 적합한지로 모델의 성능을 평가한다. 모델 성능평가에는 절대오차 대신 상대오차 개념인 MMRE를 적용하였다. 기존의 소프트웨어 비용추정 모델은 Weibull, Gamma와 Rayleigh 함수를 따르고 있다. 본 논문에서는 성장곡선의 일종인 Gompertz 곡선 모델을 제안하였다. 추가로 다른 성장곡선들도 적합성을 검증하였다. 모델 성능평가 결과 Gompertz 성장곡선이 소프트웨어 비용추정 모델로 가장 적합한 성능을 보였다."
        },
        {
          "rank": 5,
          "score": 0.6485890746116638,
          "doc_id": "ART001624002",
          "title": "Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model",
          "abstract": "In this paper, we introduce an integrated performance evaluation model defined by the three indices (evaluation index for each evaluation stage, performance viewpoint, and performance type) and analyze the utilization and satisfaction of the seven public u-IT services implemented from 2008 to 2009 using the proposed model. From the performance results of the public u-IT services, it was found out that most of the research goals initially set by the public u-IT service support projects were satisfied. Furthermore, we suggest improvement directions and promotion strategies of future projects.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART001624002&target=NART&cn=ART001624002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model In this paper, we introduce an integrated performance evaluation model defined by the three indices (evaluation index for each evaluation stage, performance viewpoint, and performance type) and analyze the utilization and satisfaction of the seven public u-IT services implemented from 2008 to 2009 using the proposed model. From the performance results of the public u-IT services, it was found out that most of the research goals initially set by the public u-IT service support projects were satisfied. Furthermore, we suggest improvement directions and promotion strategies of future projects."
        },
        {
          "rank": 6,
          "score": 0.6464701890945435,
          "doc_id": "NART57626890",
          "title": "PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化",
          "abstract": "<P>This study presents an application of the Particle Swarm Optimization (PSO) algorithm to the parameter optimization of rainfall-runoff models. Six global optimization algorithms, the shuffled complex evolution method (SCE-UA), modified SCE-UA, modified SCE-UA with initial value, PSO, modified PSO and modified PSO with initial value, were applied to parameter optimization on four kinds of series tank models. Performance comparison of there algorithms was evaluated and it can be concluded that SCE-UA and PSO show comparable performance in most cases. In addition, PSO is more effective than SCE-UA under the following conditions. 1) the model has large number of parameters, 2) the model has wide range of parameters, 3) calibration period is too short, 4) observation data contains large uncertainty. The modified PSO with initial value shows the most effective and stable performance.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART57626890&target=NART&cn=NART57626890",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化 PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化 PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化 <P>This study presents an application of the Particle Swarm Optimization (PSO) algorithm to the parameter optimization of rainfall-runoff models. Six global optimization algorithms, the shuffled complex evolution method (SCE-UA), modified SCE-UA, modified SCE-UA with initial value, PSO, modified PSO and modified PSO with initial value, were applied to parameter optimization on four kinds of series tank models. Performance comparison of there algorithms was evaluated and it can be concluded that SCE-UA and PSO show comparable performance in most cases. In addition, PSO is more effective than SCE-UA under the following conditions. 1) the model has large number of parameters, 2) the model has wide range of parameters, 3) calibration period is too short, 4) observation data contains large uncertainty. The modified PSO with initial value shows the most effective and stable performance.</P>"
        },
        {
          "rank": 7,
          "score": 0.6459258794784546,
          "doc_id": "NART38095312",
          "title": "Evaluation Model of Data Warehouse System",
          "abstract": "<P>By dividing the evaluation index system for data warehouse system into several layers,we can get a three-dimensional evaluation index matrix and setup an evaluation model of DW.Taking advantages of improved entropy-based TOPSIS algorithm, the weights of indices are optimized.We apply distance-method in the multi-objectives decision-mak-ing, and give a comprehensive evaluation of DW by using the distances from ideal and negative ideal solutions.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART38095312&target=NART&cn=NART38095312",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Evaluation Model of Data Warehouse System Evaluation Model of Data Warehouse System Evaluation Model of Data Warehouse System <P>By dividing the evaluation index system for data warehouse system into several layers,we can get a three-dimensional evaluation index matrix and setup an evaluation model of DW.Taking advantages of improved entropy-based TOPSIS algorithm, the weights of indices are optimized.We apply distance-method in the multi-objectives decision-mak-ing, and give a comprehensive evaluation of DW by using the distances from ideal and negative ideal solutions.</P>"
        },
        {
          "rank": 8,
          "score": 0.6435977220535278,
          "doc_id": "JAKO200708410645481",
          "title": "효율적인 신경망 부싱모델을 위한 신경망 구성 최적화",
          "abstract": "A bushing component of a vehicle suspension system is tested to capture the nonlinear behavior of rubber bushing element using the MTS 3-axes rubber test machine. The results of the tests are used to model the artificial neural network bushing model. The performances from the neural network model usually are dependent on the structure of the neural network. In this paper, maximum error, peak error, root mean square error, and error-to-signal ratio are employed to evaluate the performances of the neural network bushing model. A simple simulation is carried out to show the usefulness of the developed procedure.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200708410645481&target=NART&cn=JAKO200708410645481",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효율적인 신경망 부싱모델을 위한 신경망 구성 최적화 효율적인 신경망 부싱모델을 위한 신경망 구성 최적화 효율적인 신경망 부싱모델을 위한 신경망 구성 최적화 A bushing component of a vehicle suspension system is tested to capture the nonlinear behavior of rubber bushing element using the MTS 3-axes rubber test machine. The results of the tests are used to model the artificial neural network bushing model. The performances from the neural network model usually are dependent on the structure of the neural network. In this paper, maximum error, peak error, root mean square error, and error-to-signal ratio are employed to evaluate the performances of the neural network bushing model. A simple simulation is carried out to show the usefulness of the developed procedure."
        },
        {
          "rank": 9,
          "score": 0.6365176439285278,
          "doc_id": "ATN0048893828",
          "title": "머신러닝을 이용한 우리나라 환율 예측 연구",
          "abstract": "본 연구에서 환율의 예측 능력에 대하여 머신러닝 계열의 방법론과 비-머신러닝 계열의 방법론의 예측 능력을 상고 비교하고자 하였다. 데이터는 2001년부터 2018년을 학습의 기간으로 삼아 2019년을 테스트 하는 실험1과 2001년부터 2017년을 학습 기간으로 삼고 2018년과 2019년을 예측하는 실험2로 나누어 실험을 하였다. 실험1과 실험2 모두에서 머신러닝 계열의 예측이 비-머신러닝 계열의 예측보다 MSE측면에서 우수함을 보였다. 특히 두 실험 모두에서 다층퍼셉트론(MLP)이 매우 우수한 능력을 보였고, KNN을 시계열로 확장을 한 TSFKNN과 신경망을 시계열로 확장을 한 NNETAR 두 가지 모두 유사한 능력을 보였다. 전통적인 비-먼신러닝 계열에서는 충분히 데이터에 대한 특성이 파악이 되지 않아 낮은 수준의 예측 능력을 보이는 것으로 판단이 된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0048893828&target=NART&cn=ATN0048893828",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝을 이용한 우리나라 환율 예측 연구 머신러닝을 이용한 우리나라 환율 예측 연구 머신러닝을 이용한 우리나라 환율 예측 연구 본 연구에서 환율의 예측 능력에 대하여 머신러닝 계열의 방법론과 비-머신러닝 계열의 방법론의 예측 능력을 상고 비교하고자 하였다. 데이터는 2001년부터 2018년을 학습의 기간으로 삼아 2019년을 테스트 하는 실험1과 2001년부터 2017년을 학습 기간으로 삼고 2018년과 2019년을 예측하는 실험2로 나누어 실험을 하였다. 실험1과 실험2 모두에서 머신러닝 계열의 예측이 비-머신러닝 계열의 예측보다 MSE측면에서 우수함을 보였다. 특히 두 실험 모두에서 다층퍼셉트론(MLP)이 매우 우수한 능력을 보였고, KNN을 시계열로 확장을 한 TSFKNN과 신경망을 시계열로 확장을 한 NNETAR 두 가지 모두 유사한 능력을 보였다. 전통적인 비-먼신러닝 계열에서는 충분히 데이터에 대한 특성이 파악이 되지 않아 낮은 수준의 예측 능력을 보이는 것으로 판단이 된다."
        },
        {
          "rank": 10,
          "score": 0.636052131652832,
          "doc_id": "ATN0037469990",
          "title": "특징 벡터의 effectiveness 요소 기반의 가중치 SVM 분류기 연구",
          "abstract": "In this paper, we proposed a new SVM model for classification based on analysis of weight and effectiveness of a certain feature vector. The standard SVM approach used a concept of minimization of geometric soft margin between input feature vector and classification function. Although the classical approach builds a classification function to pro-vide efficient decision boundary, this model is easily affected by outliers because each support vector cannot reflect structural or distributional properties of input data. To overcome defects of the classical SVM approach, a new model is derived from weighting scheme based on geometrical relation of nearby feature vectors and data distribution of input feature vectors. The proposed model was verified by using the effectiveness factor that helps to build classification function to provide effective global decision boundary. We evaluated our weighted SVM model using effectiveness factor for multi-class classification and achieved higher accuracy than classical SVM model using the MNIST dataset.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037469990&target=NART&cn=ATN0037469990",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "특징 벡터의 effectiveness 요소 기반의 가중치 SVM 분류기 연구 특징 벡터의 effectiveness 요소 기반의 가중치 SVM 분류기 연구 특징 벡터의 effectiveness 요소 기반의 가중치 SVM 분류기 연구 In this paper, we proposed a new SVM model for classification based on analysis of weight and effectiveness of a certain feature vector. The standard SVM approach used a concept of minimization of geometric soft margin between input feature vector and classification function. Although the classical approach builds a classification function to pro-vide efficient decision boundary, this model is easily affected by outliers because each support vector cannot reflect structural or distributional properties of input data. To overcome defects of the classical SVM approach, a new model is derived from weighting scheme based on geometrical relation of nearby feature vectors and data distribution of input feature vectors. The proposed model was verified by using the effectiveness factor that helps to build classification function to provide effective global decision boundary. We evaluated our weighted SVM model using effectiveness factor for multi-class classification and achieved higher accuracy than classical SVM model using the MNIST dataset."
        },
        {
          "rank": 11,
          "score": 0.6348894834518433,
          "doc_id": "JAKO202117563196990",
          "title": "약물유전체학에서 약물반응 예측모형과 변수선택 방법",
          "abstract": "약물유전체학 연구의 주요 목표는 고차원의 유전 변수를 기반으로 개인의 약물 반응성을 예측하는 것이다. 변수의 개수가 많기 때문에 변수의 개수를 줄이기 위해서는 변수 선택이 필요하며, 선택된 변수들은 머신러닝 알고리즘을 사용하여 예측 모델을 구축하는데 사용된다. 본 연구에서는 400명의 뇌전증 환자의 차세대 염기서열 분석 데이터에 로지스틱 회귀, ReliefF, TurF, 랜덤 포레스트, LASSO의 조합과 같은 여러 가지 혼합 변수 선택 방법을 적용하였다. 선택된 변수들에 랜덤포레스트, 그래디언트 부스팅, 서포트벡터머신을 포함한 머신러닝 방법들을 적용했고 스태킹을 통해 앙상블 모형을 구축하였다. 본 연구의 결과는 랜덤포레스트와 ReliefF의 혼합 변수 선택 방법을 이용한 스태킹 모형이 다른 모형보다 더 좋은 성능을 보인다는 것을 보여주었다. 5-폴드 교차 검증을 기반으로 하여 적합한 최적 모형의 평균 검증 정확도는 0.727이고 평균 검증 AUC 값은 0.761로 나타났다. 또한, 동일한 변수를 사용할 때 스태킹 모델이 단일 머신러닝 예측 모델보다 성능이 우수한 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202117563196990&target=NART&cn=JAKO202117563196990",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "약물유전체학에서 약물반응 예측모형과 변수선택 방법 약물유전체학에서 약물반응 예측모형과 변수선택 방법 약물유전체학에서 약물반응 예측모형과 변수선택 방법 약물유전체학 연구의 주요 목표는 고차원의 유전 변수를 기반으로 개인의 약물 반응성을 예측하는 것이다. 변수의 개수가 많기 때문에 변수의 개수를 줄이기 위해서는 변수 선택이 필요하며, 선택된 변수들은 머신러닝 알고리즘을 사용하여 예측 모델을 구축하는데 사용된다. 본 연구에서는 400명의 뇌전증 환자의 차세대 염기서열 분석 데이터에 로지스틱 회귀, ReliefF, TurF, 랜덤 포레스트, LASSO의 조합과 같은 여러 가지 혼합 변수 선택 방법을 적용하였다. 선택된 변수들에 랜덤포레스트, 그래디언트 부스팅, 서포트벡터머신을 포함한 머신러닝 방법들을 적용했고 스태킹을 통해 앙상블 모형을 구축하였다. 본 연구의 결과는 랜덤포레스트와 ReliefF의 혼합 변수 선택 방법을 이용한 스태킹 모형이 다른 모형보다 더 좋은 성능을 보인다는 것을 보여주었다. 5-폴드 교차 검증을 기반으로 하여 적합한 최적 모형의 평균 검증 정확도는 0.727이고 평균 검증 AUC 값은 0.761로 나타났다. 또한, 동일한 변수를 사용할 때 스태킹 모델이 단일 머신러닝 예측 모델보다 성능이 우수한 것으로 나타났다."
        },
        {
          "rank": 12,
          "score": 0.633965253829956,
          "doc_id": "JAKO202421251156831",
          "title": "LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석",
          "abstract": "This research is a comprehensive analysis of wind power prediction sensitivity using a Long Short-Term Memory (LSTM) deep learning neural network model, accounting for the inherent uncertainties in wind speed estimation. Utilizing a year's worth of operational data from an operational wind farm, the study forecasts the power output of both individual wind turbines and the farm collectively. Predictions were made daily at intervals of 10 minutes and 1 hour over a span of three months. The model's forecast accuracy was evaluated by comparing the root mean square error (RMSE), normalized RMSE (NRMSE), and correlation coefficients with actual power output data. Moreover, the research investigated how inaccuracies in wind speed inputs affect the power prediction sensitivity of the model. By simulating wind speed errors within a normal distribution range of 1% to 15%, the study analyzed their influence on the accuracy of power predictions. This investigation provided insights into the required wind speed prediction error rate to achieve an 8% power prediction error threshold, meeting the incentive standards for forecasting systems in renewable energy generation.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202421251156831&target=NART&cn=JAKO202421251156831",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 This research is a comprehensive analysis of wind power prediction sensitivity using a Long Short-Term Memory (LSTM) deep learning neural network model, accounting for the inherent uncertainties in wind speed estimation. Utilizing a year's worth of operational data from an operational wind farm, the study forecasts the power output of both individual wind turbines and the farm collectively. Predictions were made daily at intervals of 10 minutes and 1 hour over a span of three months. The model's forecast accuracy was evaluated by comparing the root mean square error (RMSE), normalized RMSE (NRMSE), and correlation coefficients with actual power output data. Moreover, the research investigated how inaccuracies in wind speed inputs affect the power prediction sensitivity of the model. By simulating wind speed errors within a normal distribution range of 1% to 15%, the study analyzed their influence on the accuracy of power predictions. This investigation provided insights into the required wind speed prediction error rate to achieve an 8% power prediction error threshold, meeting the incentive standards for forecasting systems in renewable energy generation."
        },
        {
          "rank": 13,
          "score": 0.6318411827087402,
          "doc_id": "ATN0038661375",
          "title": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험",
          "abstract": "Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0038661375&target=NART&cn=ATN0038661375",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model."
        },
        {
          "rank": 14,
          "score": 0.6308404207229614,
          "doc_id": "ATN0030181675",
          "title": "머신러닝 기법을 적용한 지가 예측 연구",
          "abstract": "The existing officially assessed individual land price is calculated as the multiple regression result of the land price index table which analyzed the officially assessed reference land price. The main purpose of this study is to suggest the nonlinear simulation method considering the characteristics of the land and the local spatial pattern by using the machine learning. The simulation model was performed by combining the 100m vector areal unit based cadastral map and the digital elevation model(DEM). The accuracy of prediction was 75.8%(1996) and 70.3%(2005) in decision trees of C5.0 algorithm. In the case of the support vector machine(SVM) model, prediction accuracy was 63.5%(1996) and 54.3%(2005). In the machine learning simulation, officially assessed individual land price grade was converted into nominal type data classified into 10 grades considering the distribution of officially assessed individual land price of Yongin city. Independent variables were constructed from 30,354 vector areal unit using geographical distances from hazardous aversion facilities, roads, schools, railways, parks, mountains, and land use. The machine learning simulation method of this study can be applied to the analysis of the local land characteristics for calculating the officially assessed individual land price.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0030181675&target=NART&cn=ATN0030181675",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기법을 적용한 지가 예측 연구 머신러닝 기법을 적용한 지가 예측 연구 머신러닝 기법을 적용한 지가 예측 연구 The existing officially assessed individual land price is calculated as the multiple regression result of the land price index table which analyzed the officially assessed reference land price. The main purpose of this study is to suggest the nonlinear simulation method considering the characteristics of the land and the local spatial pattern by using the machine learning. The simulation model was performed by combining the 100m vector areal unit based cadastral map and the digital elevation model(DEM). The accuracy of prediction was 75.8%(1996) and 70.3%(2005) in decision trees of C5.0 algorithm. In the case of the support vector machine(SVM) model, prediction accuracy was 63.5%(1996) and 54.3%(2005). In the machine learning simulation, officially assessed individual land price grade was converted into nominal type data classified into 10 grades considering the distribution of officially assessed individual land price of Yongin city. Independent variables were constructed from 30,354 vector areal unit using geographical distances from hazardous aversion facilities, roads, schools, railways, parks, mountains, and land use. The machine learning simulation method of this study can be applied to the analysis of the local land characteristics for calculating the officially assessed individual land price."
        },
        {
          "rank": 15,
          "score": 0.6308355331420898,
          "doc_id": "JAKO202305758375548",
          "title": "머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교",
          "abstract": "전국 대학생의 중도 탈락 비율의 증가는 학생 개인 뿐만 아니라 대학과 사회에 심각한 부정적 영향을 끼친다. 본 연구에서는 중도 탈락이 예상되는 학생을 사전에 식별하기 위하여, 각 대학의 학사관리 시스템에서 손쉽게 얻을 수 있는 학적 데이터를 기반으로 머신러닝 분야의 결정트리, 랜덤 포레스트, 로지스틱 회귀 및 딥러닝 기반의 중도 탈락 예측 모델을 구축하고, 그 성능을 비교&#x00B7;분석하였다. 분석 결과 로지스틱 회귀 기반 예측 모델의 재현율이 가장 높았으나 f-1 및 auc 값이 낮은 한계를 보였고, 랜덤 포레스트 기반의 예측 모델의 경우 재현율을 제외한 다른 모든 지표에서 가장 우수한 성능을 보였다. 또한 예측 기간에 따른 예측 모델의 성능을 확인하기 위하여 예측 기간을 단기(1개 학기 이내), 중기(2개 학기 이내) 및 장기(3개 학기 이내)로 나누어 분석해 본 결과, 장기 예측 시 가장 높은 예측력을 보였다. 본 연구를 통해 각 대학은 중도 탈락이 예상되는 학생들을 조기에 식별하고, 이들에 대한 집중 관리를 통해 중도 탈락 비율을 줄이며 나아가 대학 재정 안정화에 기여할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202305758375548&target=NART&cn=JAKO202305758375548",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교 머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교 머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교 전국 대학생의 중도 탈락 비율의 증가는 학생 개인 뿐만 아니라 대학과 사회에 심각한 부정적 영향을 끼친다. 본 연구에서는 중도 탈락이 예상되는 학생을 사전에 식별하기 위하여, 각 대학의 학사관리 시스템에서 손쉽게 얻을 수 있는 학적 데이터를 기반으로 머신러닝 분야의 결정트리, 랜덤 포레스트, 로지스틱 회귀 및 딥러닝 기반의 중도 탈락 예측 모델을 구축하고, 그 성능을 비교&#x00B7;분석하였다. 분석 결과 로지스틱 회귀 기반 예측 모델의 재현율이 가장 높았으나 f-1 및 auc 값이 낮은 한계를 보였고, 랜덤 포레스트 기반의 예측 모델의 경우 재현율을 제외한 다른 모든 지표에서 가장 우수한 성능을 보였다. 또한 예측 기간에 따른 예측 모델의 성능을 확인하기 위하여 예측 기간을 단기(1개 학기 이내), 중기(2개 학기 이내) 및 장기(3개 학기 이내)로 나누어 분석해 본 결과, 장기 예측 시 가장 높은 예측력을 보였다. 본 연구를 통해 각 대학은 중도 탈락이 예상되는 학생들을 조기에 식별하고, 이들에 대한 집중 관리를 통해 중도 탈락 비율을 줄이며 나아가 대학 재정 안정화에 기여할 수 있을 것으로 기대된다."
        },
        {
          "rank": 16,
          "score": 0.6279961466789246,
          "doc_id": "JAKO202109651162667",
          "title": "신경망기법을 활용한 선박 가치평가 모델 개발",
          "abstract": "본 연구의 목적은 Neural Network Regression 모델을 활용하여 선박의 가치평가 모델을 개발하는 것이다. 가치평가의 대상은 중고 VLCC선이며, 선행연구를 통해 선박의 가치 변화를 유발하는 주요 요인들을 선별하여 변수를 설정하고, 2000년 1월부터 2020년 8월까지의 해당 데이터를 확보하였다. 변수의 안정성을 판단하기 위해 다중 공선성 검사를 수행하여 최종적으로 6개의 독립변수와 1개의 종속변수를 선정하고 연구 구조를 설계하였다. 이를 바탕으로 Linear Regression, Neural Network Regression, Random Forest Algorithm을 활용하여 총 9개의 시뮬레이션 모델을 설계하였다. 또한 각 모델간의 비교검증을 통해 평가결과의 정확성을 제고시켰다. 평가 결과, VLCC실제값과의 비교를 통해 2층으로 구성된 Hidden Layer의 Neural Network Regression 모델이 가장 정확도가 높은 것으로 나타났다. 본 연구의 시사점은 첫째, 기존 정형화된 평가기법에서 벗어나 기계학습기반 모델을 선박가치평가에 적용하였다는 점이다. 둘째, 해운시장 변화요인을 동태적 관점에서 분석하고 예측함으로써 연구결과의 객관성을 제고시켰다고 할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202109651162667&target=NART&cn=JAKO202109651162667",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망기법을 활용한 선박 가치평가 모델 개발 신경망기법을 활용한 선박 가치평가 모델 개발 신경망기법을 활용한 선박 가치평가 모델 개발 본 연구의 목적은 Neural Network Regression 모델을 활용하여 선박의 가치평가 모델을 개발하는 것이다. 가치평가의 대상은 중고 VLCC선이며, 선행연구를 통해 선박의 가치 변화를 유발하는 주요 요인들을 선별하여 변수를 설정하고, 2000년 1월부터 2020년 8월까지의 해당 데이터를 확보하였다. 변수의 안정성을 판단하기 위해 다중 공선성 검사를 수행하여 최종적으로 6개의 독립변수와 1개의 종속변수를 선정하고 연구 구조를 설계하였다. 이를 바탕으로 Linear Regression, Neural Network Regression, Random Forest Algorithm을 활용하여 총 9개의 시뮬레이션 모델을 설계하였다. 또한 각 모델간의 비교검증을 통해 평가결과의 정확성을 제고시켰다. 평가 결과, VLCC실제값과의 비교를 통해 2층으로 구성된 Hidden Layer의 Neural Network Regression 모델이 가장 정확도가 높은 것으로 나타났다. 본 연구의 시사점은 첫째, 기존 정형화된 평가기법에서 벗어나 기계학습기반 모델을 선박가치평가에 적용하였다는 점이다. 둘째, 해운시장 변화요인을 동태적 관점에서 분석하고 예측함으로써 연구결과의 객관성을 제고시켰다고 할 수 있다."
        },
        {
          "rank": 17,
          "score": 0.6279041767120361,
          "doc_id": "JAKO202123157167812",
          "title": "은닉 마르코프 모델을 이용한 국가별 주가지수 예측",
          "abstract": "은닉 마르코프 모델(hidden Markov model, HMM)은 은닉된 상태와 관찰 가능한 결과의 두 가지 요소로 이루어진 통계적 모형으로 확률론적 접근이 가능하고, 다양한 수학적인 구조를 가지고 있어 여러 분야에서 활발하게 사용되고 있다. 특히 금융 분야의 시계열 데이터에 응용되어 다양한 연구가 진행되고 있다. 본 연구는 HMM 이론을 국내 KOSPI200 주가지수와 더불어 NIKKEI225, HSI, S&P500, FTSE100과 같은 해외 주가지수 예측에 적용해 보고자 한다. 또한, 최근 인공지능 분야의 발전으로 인해 주식 가격 예측에 빈번하게 사용되는 서포트 벡터 회귀(support vector regression, SVR) 결과와 어떤 차이가 있는지 비교하여 살펴보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202123157167812&target=NART&cn=JAKO202123157167812",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마르코프 모델을 이용한 국가별 주가지수 예측 은닉 마르코프 모델을 이용한 국가별 주가지수 예측 은닉 마르코프 모델을 이용한 국가별 주가지수 예측 은닉 마르코프 모델(hidden Markov model, HMM)은 은닉된 상태와 관찰 가능한 결과의 두 가지 요소로 이루어진 통계적 모형으로 확률론적 접근이 가능하고, 다양한 수학적인 구조를 가지고 있어 여러 분야에서 활발하게 사용되고 있다. 특히 금융 분야의 시계열 데이터에 응용되어 다양한 연구가 진행되고 있다. 본 연구는 HMM 이론을 국내 KOSPI200 주가지수와 더불어 NIKKEI225, HSI, S&P500, FTSE100과 같은 해외 주가지수 예측에 적용해 보고자 한다. 또한, 최근 인공지능 분야의 발전으로 인해 주식 가격 예측에 빈번하게 사용되는 서포트 벡터 회귀(support vector regression, SVR) 결과와 어떤 차이가 있는지 비교하여 살펴보고자 한다."
        },
        {
          "rank": 18,
          "score": 0.6251870393753052,
          "doc_id": "ATN0037496660",
          "title": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교",
          "abstract": "Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037496660&target=NART&cn=ATN0037496660",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model."
        },
        {
          "rank": 19,
          "score": 0.6246568560600281,
          "doc_id": "ATN0007263119",
          "title": "교육경쟁력의 구조적 영향요인 관계분석",
          "abstract": "This study aims to identify the relationships among different factors which affect educational competitiveness, and to suggest policy implications for educational policy makers to be able to raise the level of their educational competitiveness. PISA score as an indicator representing the educational competitiveness of OECD countries was selected, and this included a number of independent variables, such as per capita GDP, total public expenditure on education as a percentage of GDP, and total per capita public expenditure on education (US dollars), affecting educational competitiveness. This study employed the structural equation modeling approach to analyze the complex causal relationships among the factors affecting educational competitiveness. The research results show that the significant factors affecting PISA are: edusys (educational system), puptec (pupil–teacher ratio), and privat exp (total expenditure on education by private source as a percentage of GDP), and that the most influential factor affecting PISA directly is edusys (the extent to which the education system meets the needs of a competitive economy). Finally, the study suggests that each country should endeavor to enhance its own educational competitiveness, considering how the factors associated with this relate to each other.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0007263119&target=NART&cn=ATN0007263119",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "교육경쟁력의 구조적 영향요인 관계분석 교육경쟁력의 구조적 영향요인 관계분석 교육경쟁력의 구조적 영향요인 관계분석 This study aims to identify the relationships among different factors which affect educational competitiveness, and to suggest policy implications for educational policy makers to be able to raise the level of their educational competitiveness. PISA score as an indicator representing the educational competitiveness of OECD countries was selected, and this included a number of independent variables, such as per capita GDP, total public expenditure on education as a percentage of GDP, and total per capita public expenditure on education (US dollars), affecting educational competitiveness. This study employed the structural equation modeling approach to analyze the complex causal relationships among the factors affecting educational competitiveness. The research results show that the significant factors affecting PISA are: edusys (educational system), puptec (pupil–teacher ratio), and privat exp (total expenditure on education by private source as a percentage of GDP), and that the most influential factor affecting PISA directly is edusys (the extent to which the education system meets the needs of a competitive economy). Finally, the study suggests that each country should endeavor to enhance its own educational competitiveness, considering how the factors associated with this relate to each other."
        },
        {
          "rank": 20,
          "score": 0.6226783990859985,
          "doc_id": "ART002787934",
          "title": "Effective Electricity Demand Prediction via Deep Learning",
          "abstract": "Prediction of electricity demand in homes and buildings can be used to optimize an energy management system by decreasing energy wastage. A time-series prediction system is still a challenging problem in machine learning and deep learning. Our main idea is to compare three methods. For this work, we analyzed an electricity demand prediction system using the current state-of-the-art deep-learning methods with a machine-learning method: error correction with multi-layer perceptron (eMLP) structure, autoregressive integrated moving average (ARIMA) structure, and a proposed structure named CNN-LSTM. For this, we measured and collected electricity demand data in Germany for home appliances. We report the prediction accuracy in terms of the mean square error (MSE) and mean absolute percentage error (MAPE). The experimental result indicates that CNN-LSTM outperforms eMLP and ARIMA in accuracy.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002787934&target=NART&cn=ART002787934",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Effective Electricity Demand Prediction via Deep Learning Effective Electricity Demand Prediction via Deep Learning Effective Electricity Demand Prediction via Deep Learning Prediction of electricity demand in homes and buildings can be used to optimize an energy management system by decreasing energy wastage. A time-series prediction system is still a challenging problem in machine learning and deep learning. Our main idea is to compare three methods. For this work, we analyzed an electricity demand prediction system using the current state-of-the-art deep-learning methods with a machine-learning method: error correction with multi-layer perceptron (eMLP) structure, autoregressive integrated moving average (ARIMA) structure, and a proposed structure named CNN-LSTM. For this, we measured and collected electricity demand data in Germany for home appliances. We report the prediction accuracy in terms of the mean square error (MSE) and mean absolute percentage error (MAPE). The experimental result indicates that CNN-LSTM outperforms eMLP and ARIMA in accuracy."
        },
        {
          "rank": 21,
          "score": 0.6223585605621338,
          "doc_id": "JAKO201707851605473",
          "title": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표",
          "abstract": "본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201707851605473&target=NART&cn=JAKO201707851605473",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다."
        },
        {
          "rank": 22,
          "score": 0.6199476718902588,
          "doc_id": "JAKO201104642095152",
          "title": "입자 군집 최적화를 이용한 FCM 기반 퍼지 모델의 동정 방법론",
          "abstract": "In this study, we introduce a identification methodology for FCM-based fuzzy model. The two underlying design mechanisms of such networks involve Fuzzy C-Means (FCM) clustering method and Particle Swarm Optimization(PSO). The proposed algorithm is based on FCM clustering method for efficient processing of data and the optimization of model was carried out using PSO. The premise part of fuzzy rules does not construct as any fixed membership functions such as triangular, gaussian, ellipsoidal because we build up the premise part of fuzzy rules using FCM. As a result, the proposed model can lead to the compact architecture of network. In this study, as the consequence part of fuzzy rules, we are able to use four types of polynomials such as simplified, linear, quadratic, modified quadratic. In addition, a Weighted Least Square Estimation to estimate the coefficients of polynomials, which are the consequent parts of fuzzy model, can decouple each fuzzy rule from the other fuzzy rules. Therefore, a local learning capability and an interpretability of the proposed fuzzy model are improved. Also, the parameters of the proposed fuzzy model such as a fuzzification coefficient of FCM clustering, the number of clusters of FCM clustering, and the polynomial type of the consequent part of fuzzy rules are adjusted using PSO. The proposed model is illustrated with the use of Automobile Miles per Gallon(MPG) and Boston housing called Machine Learning dataset. A comparative analysis reveals that the proposed FCM-based fuzzy model exhibits higher accuracy and superb predictive capability in comparison to some previous models available in the literature.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201104642095152&target=NART&cn=JAKO201104642095152",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "입자 군집 최적화를 이용한 FCM 기반 퍼지 모델의 동정 방법론 입자 군집 최적화를 이용한 FCM 기반 퍼지 모델의 동정 방법론 입자 군집 최적화를 이용한 FCM 기반 퍼지 모델의 동정 방법론 In this study, we introduce a identification methodology for FCM-based fuzzy model. The two underlying design mechanisms of such networks involve Fuzzy C-Means (FCM) clustering method and Particle Swarm Optimization(PSO). The proposed algorithm is based on FCM clustering method for efficient processing of data and the optimization of model was carried out using PSO. The premise part of fuzzy rules does not construct as any fixed membership functions such as triangular, gaussian, ellipsoidal because we build up the premise part of fuzzy rules using FCM. As a result, the proposed model can lead to the compact architecture of network. In this study, as the consequence part of fuzzy rules, we are able to use four types of polynomials such as simplified, linear, quadratic, modified quadratic. In addition, a Weighted Least Square Estimation to estimate the coefficients of polynomials, which are the consequent parts of fuzzy model, can decouple each fuzzy rule from the other fuzzy rules. Therefore, a local learning capability and an interpretability of the proposed fuzzy model are improved. Also, the parameters of the proposed fuzzy model such as a fuzzification coefficient of FCM clustering, the number of clusters of FCM clustering, and the polynomial type of the consequent part of fuzzy rules are adjusted using PSO. The proposed model is illustrated with the use of Automobile Miles per Gallon(MPG) and Boston housing called Machine Learning dataset. A comparative analysis reveals that the proposed FCM-based fuzzy model exhibits higher accuracy and superb predictive capability in comparison to some previous models available in the literature."
        },
        {
          "rank": 23,
          "score": 0.6188324689865112,
          "doc_id": "NART103136065",
          "title": "Portfolio optimization with return prediction using deep learning and machine learning",
          "abstract": "<P><B>Abstract</B></P>  <P>Integrating return prediction of traditional time series models in portfolio formation can improve the performance of original portfolio optimization model. Since machine learning and deep learning models have shown overwhelming superiority than time series models, this paper combines return prediction in portfolio formation with two machine learning models, i.e., random forest (RF) and support vector regression (SVR), and three deep learning models, i.e., LSTM neural network, deep multilayer perceptron (DMLP) and convolutional neural network. To be specific, this paper first applies these prediction models for stock preselection before portfolio formation. Then, this paper incorporates their predictive results in advancing mean&ndash;variance (MV) and omega portfolio optimization models. In order to present the superiority of these models, portfolio models with autoregressive integrated moving average&rsquo;s return prediction are used as benchmarks. Evaluation is based on historical data of 9 years from 2007 to 2015 of component stocks of China securities 100 index. Experimental results show that MV and omega models with RF return prediction, i.e., RF+MVF and RF+OF, outperform the other models. Further, RF+MVF is superior to RF+OF. Due to the high turnover of these two models, this paper discusses their performance after deducting the transaction fee cased by turnover. Experiments present that RF+MVF still performs the best among MVF models and omega model with SVR prediction (SVR+OF) performs the best among OF models. Moreover, RF+MVF performs better than SVR+OF and high turnover erodes nearly half of their total returns especially for RF+OF and RF+MVF. Therefore, this paper recommends investors to build MVF with RF return prediction for daily trading investment.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Compares the performance of machine learning and deep learning in stock preselection. </LI> <LI>  Combining return prediction of machine learning and deep learning in portfolio formation. </LI> <LI>  Emphasis on advancing portfolio optimization with return prediction. </LI> <LI>  Advanced mean&ndash;variance model with random forest forecasts performs the best. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART103136065&target=NART&cn=NART103136065",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Portfolio optimization with return prediction using deep learning and machine learning Portfolio optimization with return prediction using deep learning and machine learning Portfolio optimization with return prediction using deep learning and machine learning <P><B>Abstract</B></P>  <P>Integrating return prediction of traditional time series models in portfolio formation can improve the performance of original portfolio optimization model. Since machine learning and deep learning models have shown overwhelming superiority than time series models, this paper combines return prediction in portfolio formation with two machine learning models, i.e., random forest (RF) and support vector regression (SVR), and three deep learning models, i.e., LSTM neural network, deep multilayer perceptron (DMLP) and convolutional neural network. To be specific, this paper first applies these prediction models for stock preselection before portfolio formation. Then, this paper incorporates their predictive results in advancing mean&ndash;variance (MV) and omega portfolio optimization models. In order to present the superiority of these models, portfolio models with autoregressive integrated moving average&rsquo;s return prediction are used as benchmarks. Evaluation is based on historical data of 9 years from 2007 to 2015 of component stocks of China securities 100 index. Experimental results show that MV and omega models with RF return prediction, i.e., RF+MVF and RF+OF, outperform the other models. Further, RF+MVF is superior to RF+OF. Due to the high turnover of these two models, this paper discusses their performance after deducting the transaction fee cased by turnover. Experiments present that RF+MVF still performs the best among MVF models and omega model with SVR prediction (SVR+OF) performs the best among OF models. Moreover, RF+MVF performs better than SVR+OF and high turnover erodes nearly half of their total returns especially for RF+OF and RF+MVF. Therefore, this paper recommends investors to build MVF with RF return prediction for daily trading investment.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Compares the performance of machine learning and deep learning in stock preselection. </LI> <LI>  Combining return prediction of machine learning and deep learning in portfolio formation. </LI> <LI>  Emphasis on advancing portfolio optimization with return prediction. </LI> <LI>  Advanced mean&ndash;variance model with random forest forecasts performs the best. </LI> </UL> </P>"
        },
        {
          "rank": 24,
          "score": 0.6185247302055359,
          "doc_id": "DIKO0013372384",
          "title": "앙상블 SVM을 이용한 기업 부도 예측 모형",
          "abstract": "Bankruptcy prediction has been an important topic in the accounting and finance field for a long time. Several data mining techniques have been used for bankruptcy prediction. However, there are many limits for application to real classification problem with a single model. This study proposes ensemble SVM (support vector machine) model which assembles different SVM models with each different kernel functions. Our ensemble model is made and evaluated by v-fold cross-validation approach. The top performing models are recruited into the ensemble. The classification is then carried out using the majority voting opinion of the ensemble. The performance of the ensemble SVM classifier is investigated in terms of accuracy, error rate, sensitivity, specificity, ROC curve, and AUC to compare with single SVM classifiers based on two financial ratios datasets and simulation datasets. The results confirmed the advantages of our method: It is being robust while providing good performance.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0013372384&target=NART&cn=DIKO0013372384",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "앙상블 SVM을 이용한 기업 부도 예측 모형 앙상블 SVM을 이용한 기업 부도 예측 모형 앙상블 SVM을 이용한 기업 부도 예측 모형 Bankruptcy prediction has been an important topic in the accounting and finance field for a long time. Several data mining techniques have been used for bankruptcy prediction. However, there are many limits for application to real classification problem with a single model. This study proposes ensemble SVM (support vector machine) model which assembles different SVM models with each different kernel functions. Our ensemble model is made and evaluated by v-fold cross-validation approach. The top performing models are recruited into the ensemble. The classification is then carried out using the majority voting opinion of the ensemble. The performance of the ensemble SVM classifier is investigated in terms of accuracy, error rate, sensitivity, specificity, ROC curve, and AUC to compare with single SVM classifiers based on two financial ratios datasets and simulation datasets. The results confirmed the advantages of our method: It is being robust while providing good performance."
        },
        {
          "rank": 25,
          "score": 0.6138328909873962,
          "doc_id": "JAKO199215875841266",
          "title": "음성 인식 신경망을 위한 음성 파라키터들의 성능 비교",
          "abstract": "음성 인식에 신경망 모델을 적용하는 많은 연구들이 있었지만, 주된 관심은 음성인식에 적합한 구조와 학습 방법이었다.  그러나 음성인식에 신경망 모델을 적용한 시스템의 효율 향상은 모델 자체의 구조뿐 아니라, 신경망 모델의 입력으로 어떤 음성 파라미터를 사용하는가에 따라서도 큰 영향을 받는다.  본 논문은 기존 음성인식에 신경망 모델을 적용한 많은 연구들에서 사용한 음성 파라미터를 살펴보고, 대표적인 음성 파라미터 6개를 선정하여, 같은 데이타와 같은 신경망 모델 하에서 어떻게 성능이 달라지는지를 분석한다.  인식 실험에 있어서는 한국어 파열음 9개에 대한 8개 데이터 집합과 모음 8개에 대한 18개 데이터 집합을 음성 파라미터로 하고 신경망 모델은 순환 신경망 모델을 사용하여 노드의 수를 일정하게 한뒤 다양한 입력 파라미터의 성능을 비교하였다.  그 결과 선형 예측 계수로부터 얻어진 delta cepstrum의 음성 파라미터가 가장 좋은 성능을 보였으며 이때 인식률은 같은 학습 데이터에 대해 파열음 100.0%, 모음 95.1%이었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199215875841266&target=NART&cn=JAKO199215875841266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식에 신경망 모델을 적용하는 많은 연구들이 있었지만, 주된 관심은 음성인식에 적합한 구조와 학습 방법이었다.  그러나 음성인식에 신경망 모델을 적용한 시스템의 효율 향상은 모델 자체의 구조뿐 아니라, 신경망 모델의 입력으로 어떤 음성 파라미터를 사용하는가에 따라서도 큰 영향을 받는다.  본 논문은 기존 음성인식에 신경망 모델을 적용한 많은 연구들에서 사용한 음성 파라미터를 살펴보고, 대표적인 음성 파라미터 6개를 선정하여, 같은 데이타와 같은 신경망 모델 하에서 어떻게 성능이 달라지는지를 분석한다.  인식 실험에 있어서는 한국어 파열음 9개에 대한 8개 데이터 집합과 모음 8개에 대한 18개 데이터 집합을 음성 파라미터로 하고 신경망 모델은 순환 신경망 모델을 사용하여 노드의 수를 일정하게 한뒤 다양한 입력 파라미터의 성능을 비교하였다.  그 결과 선형 예측 계수로부터 얻어진 delta cepstrum의 음성 파라미터가 가장 좋은 성능을 보였으며 이때 인식률은 같은 학습 데이터에 대해 파열음 100.0%, 모음 95.1%이었다."
        },
        {
          "rank": 26,
          "score": 0.6132830381393433,
          "doc_id": "JAKO202128557368138",
          "title": "인공지능을 활용한 기계학습 앙상블 모델 개발",
          "abstract": "To predict mechanical properties of secondary hardening martensitic steels, a machine learning ensemble model was established. Based on ANN(Artificial Neural Network) architecture, some kinds of methods was considered to optimize the model. In particular, interaction features, which can reflect interactions between chemical compositions and processing conditions of real alloy system, was considered by means of feature engineering, and then K-Fold cross validation coupled with bagging ensemble were investigated to reduce R2_score and a factor indicating average learning errors owing to biased experimental database.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202128557368138&target=NART&cn=JAKO202128557368138",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "인공지능을 활용한 기계학습 앙상블 모델 개발 인공지능을 활용한 기계학습 앙상블 모델 개발 인공지능을 활용한 기계학습 앙상블 모델 개발 To predict mechanical properties of secondary hardening martensitic steels, a machine learning ensemble model was established. Based on ANN(Artificial Neural Network) architecture, some kinds of methods was considered to optimize the model. In particular, interaction features, which can reflect interactions between chemical compositions and processing conditions of real alloy system, was considered by means of feature engineering, and then K-Fold cross validation coupled with bagging ensemble were investigated to reduce R2_score and a factor indicating average learning errors owing to biased experimental database."
        },
        {
          "rank": 27,
          "score": 0.6124721765518188,
          "doc_id": "DIKO0014861002",
          "title": "딥 러닝기반 고객평점 예측모델",
          "abstract": "인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014861002&target=NART&cn=DIKO0014861002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다."
        },
        {
          "rank": 28,
          "score": 0.6119720935821533,
          "doc_id": "JAKO199428062536801",
          "title": "환경영향평가서 분석",
          "abstract": "The study is to analyze the contents of Environmental Impact Statement(EIS) and supplementary EIS prepared from 1981 to 1992. The contents are project area, project cost, EIS volume, project term, assessment term, EIS preparation cost, land use plan, and kinds of predictive model concerning air quality, water quality, noise and vibration etc. by project type. Data are collected with EIS analysis checklist and analyzed by <TEX>$SPSS/PC^+$</TEX>.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199428062536801&target=NART&cn=JAKO199428062536801",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "환경영향평가서 분석 환경영향평가서 분석 환경영향평가서 분석 The study is to analyze the contents of Environmental Impact Statement(EIS) and supplementary EIS prepared from 1981 to 1992. The contents are project area, project cost, EIS volume, project term, assessment term, EIS preparation cost, land use plan, and kinds of predictive model concerning air quality, water quality, noise and vibration etc. by project type. Data are collected with EIS analysis checklist and analyzed by <TEX>$SPSS/PC^+$</TEX>."
        },
        {
          "rank": 29,
          "score": 0.611920952796936,
          "doc_id": "JAKO202106763002129",
          "title": "머신러닝 기반의 안전도 데이터 필터링 모델",
          "abstract": "Customized services to a sleep induction for better sleepcare are more effective because of different satisfaction levels to users. The EOG data measured at the frontal lobe when a person blinks his eyes can be used as biometric data because it has different values for each person. The accuracy of measurement is degraded by a noise source, such as toss and turn. Therefore, it is necessary to analyze the noisy data and remove them from normal EOG by filtering. There are low-pass filtering and high-pass filtering as filtering using a frequency band. However, since filtering within a frequency band range is also required for more effective performance, we propose a machine learning model for the filtering of EOG data in this paper as the second filtering method. In addition, optimal values of parameters such as the depth of the hidden layer, the number of nodes of the hidden layer, the activation function, and the dropout were found through experiments, to improve the performance of the machine learning filtering model, and the filtering performance of 95.7% was obtained. Eventually, it is expected that it can be used for effective user identification services by using filtering model for EOG data.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202106763002129&target=NART&cn=JAKO202106763002129",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기반의 안전도 데이터 필터링 모델 머신러닝 기반의 안전도 데이터 필터링 모델 머신러닝 기반의 안전도 데이터 필터링 모델 Customized services to a sleep induction for better sleepcare are more effective because of different satisfaction levels to users. The EOG data measured at the frontal lobe when a person blinks his eyes can be used as biometric data because it has different values for each person. The accuracy of measurement is degraded by a noise source, such as toss and turn. Therefore, it is necessary to analyze the noisy data and remove them from normal EOG by filtering. There are low-pass filtering and high-pass filtering as filtering using a frequency band. However, since filtering within a frequency band range is also required for more effective performance, we propose a machine learning model for the filtering of EOG data in this paper as the second filtering method. In addition, optimal values of parameters such as the depth of the hidden layer, the number of nodes of the hidden layer, the activation function, and the dropout were found through experiments, to improve the performance of the machine learning filtering model, and the filtering performance of 95.7% was obtained. Eventually, it is expected that it can be used for effective user identification services by using filtering model for EOG data."
        },
        {
          "rank": 30,
          "score": 0.610206663608551,
          "doc_id": "DIKO0016929656",
          "title": "머신러닝 기법을 사용한 이익예측 모형",
          "abstract": "본 논문에서는 단계적 로짓 회귀(Stepwise Logit Regressioin) 모델과 머신러닝(Machine Learning) 기법인 랜덤 포레스트(Random Forest) 모델, 익스트림 그레이디언트 부스팅(Extreme Gradient Boosting) 모델, 인공신경망(Artificial Neural Network) 모델을 사용하여 미래의 희석주당이익(DEPS)의 증가를 예측하고, 각각의 예측 모델의 정확도를 비교한다. 선행연구에서 이익의 증가를 예측할 수 있도록 하는 중요 변수를 사용하여 본 연구를 진행하고 네 개의 모델에서 중요 변수를 추출하고 선행연구의 중요 변수와 비교한다. 그리고 재무상태표와 손익계산서에서 얻을 수 있는 변수들을 확보하여 총자산과 총매출로 변수를 조정한 후 예측 변수에 추가하여 미래 희석주당이익의 증가를 추가적으로 예측하고, 중요 변수를 추출하고 비교한다.&amp;#xD; 분석 결과 선행연구와 동일하게 랜덤포레스트모델의 예측 정확도가 단계적 로짓 회귀 모델의 예측 정확도보다 높았다. 그리고 랜덤 포레스트 모델의 예측 정확도가 나머지 예측 모델보다 예측 정확도가 가장 높았으며 다음으로 익스트림 그레이디언트 부스팅 모델의 예측 정확도가 높았다. 추가 분석 결과 랜덤 포레스트 모델, 익스트림 그레이디언트 부스팅 모델과 인공신경망 모델의 예측 정확도가 좋았으며, 인공신경망 모델의 경우 예측 변수를 추가하기 전보다 예측 정확도가 가장 큰 폭으로 향상되는 것을 확인하였다.&amp;#xD; 본 연구는 머신러닝 모델을 활용하여 회계이익을 예측할 경우, 기존의 이익 예측 모형인 회귀모형 보다 회계이익을 정확하게 예측할 수 있다는 점을 시사한다. 그리고 머신러닝 모델과 딥러닝 모델을 활용한 연구가 회계학에서도 충분히 활용될 수 있다는 증거를 제시하고, 머신러닝 방법론과 딥러닝 방법론을 회계학에 도입하는데 발판을 마련하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0016929656&target=NART&cn=DIKO0016929656",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기법을 사용한 이익예측 모형 머신러닝 기법을 사용한 이익예측 모형 머신러닝 기법을 사용한 이익예측 모형 본 논문에서는 단계적 로짓 회귀(Stepwise Logit Regressioin) 모델과 머신러닝(Machine Learning) 기법인 랜덤 포레스트(Random Forest) 모델, 익스트림 그레이디언트 부스팅(Extreme Gradient Boosting) 모델, 인공신경망(Artificial Neural Network) 모델을 사용하여 미래의 희석주당이익(DEPS)의 증가를 예측하고, 각각의 예측 모델의 정확도를 비교한다. 선행연구에서 이익의 증가를 예측할 수 있도록 하는 중요 변수를 사용하여 본 연구를 진행하고 네 개의 모델에서 중요 변수를 추출하고 선행연구의 중요 변수와 비교한다. 그리고 재무상태표와 손익계산서에서 얻을 수 있는 변수들을 확보하여 총자산과 총매출로 변수를 조정한 후 예측 변수에 추가하여 미래 희석주당이익의 증가를 추가적으로 예측하고, 중요 변수를 추출하고 비교한다.&amp;#xD; 분석 결과 선행연구와 동일하게 랜덤포레스트모델의 예측 정확도가 단계적 로짓 회귀 모델의 예측 정확도보다 높았다. 그리고 랜덤 포레스트 모델의 예측 정확도가 나머지 예측 모델보다 예측 정확도가 가장 높았으며 다음으로 익스트림 그레이디언트 부스팅 모델의 예측 정확도가 높았다. 추가 분석 결과 랜덤 포레스트 모델, 익스트림 그레이디언트 부스팅 모델과 인공신경망 모델의 예측 정확도가 좋았으며, 인공신경망 모델의 경우 예측 변수를 추가하기 전보다 예측 정확도가 가장 큰 폭으로 향상되는 것을 확인하였다.&amp;#xD; 본 연구는 머신러닝 모델을 활용하여 회계이익을 예측할 경우, 기존의 이익 예측 모형인 회귀모형 보다 회계이익을 정확하게 예측할 수 있다는 점을 시사한다. 그리고 머신러닝 모델과 딥러닝 모델을 활용한 연구가 회계학에서도 충분히 활용될 수 있다는 증거를 제시하고, 머신러닝 방법론과 딥러닝 방법론을 회계학에 도입하는데 발판을 마련하고자 한다."
        },
        {
          "rank": 31,
          "score": 0.6096890568733215,
          "doc_id": "NART118745898",
          "title": "An Email Spam Filtering Model Using Ensemble of Machine Learning Techniques",
          "abstract": "<P>The growth of spam emails is on the increase responsible for larger portions of the global email traffics. Aside the annoyance and the time wasted sifting through the unwanted messages; spam emails can also cause immeasurable harms through malicious software capable of damaging systems and compromising confidential information. The risks of filtering spam emails is that sometimes, legitimate mails are marked as spam, yet the results of not filtering spam are the constant flood of spam clogs on networks that adversely impacts users inboxes while draining valuable resources on the networks such as bandwidth and storage capacity, productivity loss and interfere with the expedient delivery of legitimate emails. Several researchers had worked on the design of models for spam email filtering using different techniques, however the detection accuracy of these models have also become subject of discussions. This study developed spam email filtering model using Ensemble of Decision Tree, Support Vector Machine and Multilayer Perceptron (DT-SVM-MLP) technique as a solution approach to solving issues of low spam emails detection accuracy. The ensemble model was trained using forward propagation training technique and the performance was evaluated using five performance metrics of Accuracy, False Positive (FP) Rate, Precision, Recall and F-Measure.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART118745898&target=NART&cn=NART118745898",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "An Email Spam Filtering Model Using Ensemble of Machine Learning Techniques An Email Spam Filtering Model Using Ensemble of Machine Learning Techniques An Email Spam Filtering Model Using Ensemble of Machine Learning Techniques <P>The growth of spam emails is on the increase responsible for larger portions of the global email traffics. Aside the annoyance and the time wasted sifting through the unwanted messages; spam emails can also cause immeasurable harms through malicious software capable of damaging systems and compromising confidential information. The risks of filtering spam emails is that sometimes, legitimate mails are marked as spam, yet the results of not filtering spam are the constant flood of spam clogs on networks that adversely impacts users inboxes while draining valuable resources on the networks such as bandwidth and storage capacity, productivity loss and interfere with the expedient delivery of legitimate emails. Several researchers had worked on the design of models for spam email filtering using different techniques, however the detection accuracy of these models have also become subject of discussions. This study developed spam email filtering model using Ensemble of Decision Tree, Support Vector Machine and Multilayer Perceptron (DT-SVM-MLP) technique as a solution approach to solving issues of low spam emails detection accuracy. The ensemble model was trained using forward propagation training technique and the performance was evaluated using five performance metrics of Accuracy, False Positive (FP) Rate, Precision, Recall and F-Measure.</P>"
        },
        {
          "rank": 32,
          "score": 0.6082441210746765,
          "doc_id": "JAKO202320150299733",
          "title": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가",
          "abstract": "본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202320150299733&target=NART&cn=JAKO202320150299733",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다."
        },
        {
          "rank": 33,
          "score": 0.6064902544021606,
          "doc_id": "NART123583722",
          "title": "Sediment load prediction in Johor river: deep learning versus machine learning models",
          "abstract": "<P><B>Abstract</B><P>Sediment transport is a normal phenomenon in rivers and streams, contributing significantly to ecosystem production and preservation by replenishing vital nutrients and preserving aquatic life&rsquo;s natural habitats. Thus, sediment transport prediction through modeling is crucial for predicting flood events, tracking coastal erosion, planning for water supplies, and managing irrigation. The predictability of process-driven models may encounter various restrictions throughout the validation process. Given that data-driven models work on the assumption that the underlying physical process is not requisite, this opens up the avenue for AI-based model as alternative modeling. However, AI-based models, such as ANN and SVM, face problems, such as long-term dependency, which require alternative dynamic procedures. Since their performance as universal function approximation depends on their compatibility with the nature of the problem itself, this study investigated several distinct AI-based models, such as long short-term memory (LSTM), artificial neural network (ANN), and support vector machine (SVM), in predicting sediment transport in the Johor river. The collected historical daily sediment transport data from January 1, 2008, to December 01, 2018, through autocorrelation function, were used as input for the model. The statistical results showed that, despite their ability (deep learning and machine learning) to provide sediment predictions based on historical input datasets, machine learning, such as ANN, might be more prone to overfitting or being trapped in a local optimum than deep learning, evidenced by the worse in all metrics score. With RMSE = 11.395, MAE = 18.094, and <I>R</I>2 = 0.914, LSTM outperformed other models in the comparison.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART123583722&target=NART&cn=NART123583722",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Sediment load prediction in Johor river: deep learning versus machine learning models Sediment load prediction in Johor river: deep learning versus machine learning models Sediment load prediction in Johor river: deep learning versus machine learning models <P><B>Abstract</B><P>Sediment transport is a normal phenomenon in rivers and streams, contributing significantly to ecosystem production and preservation by replenishing vital nutrients and preserving aquatic life&rsquo;s natural habitats. Thus, sediment transport prediction through modeling is crucial for predicting flood events, tracking coastal erosion, planning for water supplies, and managing irrigation. The predictability of process-driven models may encounter various restrictions throughout the validation process. Given that data-driven models work on the assumption that the underlying physical process is not requisite, this opens up the avenue for AI-based model as alternative modeling. However, AI-based models, such as ANN and SVM, face problems, such as long-term dependency, which require alternative dynamic procedures. Since their performance as universal function approximation depends on their compatibility with the nature of the problem itself, this study investigated several distinct AI-based models, such as long short-term memory (LSTM), artificial neural network (ANN), and support vector machine (SVM), in predicting sediment transport in the Johor river. The collected historical daily sediment transport data from January 1, 2008, to December 01, 2018, through autocorrelation function, were used as input for the model. The statistical results showed that, despite their ability (deep learning and machine learning) to provide sediment predictions based on historical input datasets, machine learning, such as ANN, might be more prone to overfitting or being trapped in a local optimum than deep learning, evidenced by the worse in all metrics score. With RMSE = 11.395, MAE = 18.094, and <I>R</I>2 = 0.914, LSTM outperformed other models in the comparison.</P></P>"
        },
        {
          "rank": 34,
          "score": 0.6064177751541138,
          "doc_id": "JAKO202305062334676",
          "title": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측",
          "abstract": "본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202305062334676&target=NART&cn=JAKO202305062334676",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다."
        },
        {
          "rank": 35,
          "score": 0.6058825254440308,
          "doc_id": "JAKO202130053169360",
          "title": "비정형, 정형 데이터의 이미지 학습을 활용한 시장예측",
          "abstract": "금융 시계열 분석은 현대 사회의 경제적, 사회적으로 매우 중요한 역할을 하며 세계 발전에 영향을 미치는 중요한 과제지만 많은 잡음(noise)과 불확실성 등의 어려움으로 인해 금융 시계열 분석 예측은 어려운 연구 주제이다. 본 논문에서는 비정형 데이터와 정형 데이터를 함께 이미지로 변환하여 시장을 예측 하는 방법(MPIL)을 제안한다. 시장 예측을 위해 n일 기간의 비정형 데이터인 SNS, 뉴스 데이터를 감정분석하고 정형 데이터인 시장 데이터를 GADF 알고리즘으로 이미지 변환하고 이미지 학습을 통해 n+1일의 가격을 예측하는 초단기 시장을 예측한다. MPIL은 평균 정확도 56%로 기존 시장예측에 사용되던 감정분석을 활용하여 LSTM으로 시장을 예측하는 모델 평균 정확도 50%보다 높은 정확도를 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202130053169360&target=NART&cn=JAKO202130053169360",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "비정형, 정형 데이터의 이미지 학습을 활용한 시장예측 비정형, 정형 데이터의 이미지 학습을 활용한 시장예측 비정형, 정형 데이터의 이미지 학습을 활용한 시장예측 금융 시계열 분석은 현대 사회의 경제적, 사회적으로 매우 중요한 역할을 하며 세계 발전에 영향을 미치는 중요한 과제지만 많은 잡음(noise)과 불확실성 등의 어려움으로 인해 금융 시계열 분석 예측은 어려운 연구 주제이다. 본 논문에서는 비정형 데이터와 정형 데이터를 함께 이미지로 변환하여 시장을 예측 하는 방법(MPIL)을 제안한다. 시장 예측을 위해 n일 기간의 비정형 데이터인 SNS, 뉴스 데이터를 감정분석하고 정형 데이터인 시장 데이터를 GADF 알고리즘으로 이미지 변환하고 이미지 학습을 통해 n+1일의 가격을 예측하는 초단기 시장을 예측한다. MPIL은 평균 정확도 56%로 기존 시장예측에 사용되던 감정분석을 활용하여 LSTM으로 시장을 예측하는 모델 평균 정확도 50%보다 높은 정확도를 보였다."
        },
        {
          "rank": 36,
          "score": 0.6055231690406799,
          "doc_id": "JAKO202517036003495",
          "title": "사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크",
          "abstract": "본 연구는 사이버 복원력을 정량적으로 평가하기 위한 지표 선정 프레임워크를 제시한다. 정보보호 및 개인정보보호 관리체계 인증기관을 기반으로 다양한 서비스 유형의 특성을 분석하고 복원력 측정을 위한 지표 후보군을 도출한다. 선정된 후보 지표는 본 연구에서 정의한 객관성, 재현성, 확장성, 실용성, 복원력 반영성의 다섯가지 핵심 특성을 기준으로 타당성을 검증한다. 또한 지표간 상호 배타성(ME)과 전체 포괄성(CE) 원칙을 보완 기준으로 적용하여 중복성을 제거하고 평가 체계의 범용성을 확보하였다. 최종적으로 12개의 정량적 평가 지표를 선정하였으며 이 중 웹서비스와 같은 트랜잭션 기반 시스템에서의 초당 처리 건수를 나타내는 TPS 지표를 중심으로, TCP SYN flooding 공격 상황에서의 지표의 변화와 해석방법에 대한 실증적 분석을 수행하였다. 제안한 지표선정 프레임워크와 정량적 평가지표는 사이버 복원력을 객관적으로 측정할 수 있는 평가 체계 수립에 기여하며, 지속적으로 변화하는 사이버 위협에 효과적인 대응과 개선을 위한 기준을 제공한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202517036003495&target=NART&cn=JAKO202517036003495",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크 사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크 사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크 본 연구는 사이버 복원력을 정량적으로 평가하기 위한 지표 선정 프레임워크를 제시한다. 정보보호 및 개인정보보호 관리체계 인증기관을 기반으로 다양한 서비스 유형의 특성을 분석하고 복원력 측정을 위한 지표 후보군을 도출한다. 선정된 후보 지표는 본 연구에서 정의한 객관성, 재현성, 확장성, 실용성, 복원력 반영성의 다섯가지 핵심 특성을 기준으로 타당성을 검증한다. 또한 지표간 상호 배타성(ME)과 전체 포괄성(CE) 원칙을 보완 기준으로 적용하여 중복성을 제거하고 평가 체계의 범용성을 확보하였다. 최종적으로 12개의 정량적 평가 지표를 선정하였으며 이 중 웹서비스와 같은 트랜잭션 기반 시스템에서의 초당 처리 건수를 나타내는 TPS 지표를 중심으로, TCP SYN flooding 공격 상황에서의 지표의 변화와 해석방법에 대한 실증적 분석을 수행하였다. 제안한 지표선정 프레임워크와 정량적 평가지표는 사이버 복원력을 객관적으로 측정할 수 있는 평가 체계 수립에 기여하며, 지속적으로 변화하는 사이버 위협에 효과적인 대응과 개선을 위한 기준을 제공한다."
        },
        {
          "rank": 37,
          "score": 0.6054874658584595,
          "doc_id": "ATN0026857341",
          "title": "인공신경망 모델의 가중치와 편의를 이용한 테트라포드의 안정수 계산 방법",
          "abstract": "Tetrapod is one of the most widely used concrete armor units for rubble mound breakwaters. The calculation of the stability number of Tetrapods is necessary to determine the optimal weight of Tetrapods. Many empirical formulas have been developed to calculate the stability number of Tetrapods, from the Hudson formula in 1950s to the recent one developed by Suh and Kang. They were developed by using the regression analysis to determine the coefficients of an assumed formula using the experimental data. Recently, software engineering (or machine learning) methods are introduced as a large amount of experimental data becomes available, e.g. artificial neural network (ANN) models for rock armors. However, these methods are seldom used probably because they did not significantly improve the accuracy compared with the empirical formula and/or the engineers are not familiar with them. In this study, we propose an explicit method to calculate the stability number of Tetrapods using the weights and biases of an ANN model. This method can be used by an engineer who has basic knowledge of matrix operation without requiring knowledge of ANN, and it is more accurate than previous empirical formulas.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0026857341&target=NART&cn=ATN0026857341",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "인공신경망 모델의 가중치와 편의를 이용한 테트라포드의 안정수 계산 방법 인공신경망 모델의 가중치와 편의를 이용한 테트라포드의 안정수 계산 방법 인공신경망 모델의 가중치와 편의를 이용한 테트라포드의 안정수 계산 방법 Tetrapod is one of the most widely used concrete armor units for rubble mound breakwaters. The calculation of the stability number of Tetrapods is necessary to determine the optimal weight of Tetrapods. Many empirical formulas have been developed to calculate the stability number of Tetrapods, from the Hudson formula in 1950s to the recent one developed by Suh and Kang. They were developed by using the regression analysis to determine the coefficients of an assumed formula using the experimental data. Recently, software engineering (or machine learning) methods are introduced as a large amount of experimental data becomes available, e.g. artificial neural network (ANN) models for rock armors. However, these methods are seldom used probably because they did not significantly improve the accuracy compared with the empirical formula and/or the engineers are not familiar with them. In this study, we propose an explicit method to calculate the stability number of Tetrapods using the weights and biases of an ANN model. This method can be used by an engineer who has basic knowledge of matrix operation without requiring knowledge of ANN, and it is more accurate than previous empirical formulas."
        },
        {
          "rank": 38,
          "score": 0.6054724454879761,
          "doc_id": "NART111572458",
          "title": "광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교",
          "abstract": "This study examines the classification characteristics of various machine learning classifiers for pain assessment using photoplethysmogram. The presence of pain was assessed using waveform characteristics derived from photoplethysmogram obtained from 73 patients before and after surgery. Classification performance was evaluated using logistic regression, random forest, multi-layer perceptron, and 1-D convolutional neural network, and was validated with nested k-fold cross validation. As a result, pain classification accuracy was highest in order of logistic regression, convolutional neural network, multi-layer perceptron, and random forest classifier. In addition, logistic regression, random forest, multi-layer perceptron, and convolutional neural network were shown to be robust to overfitting in order.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART111572458&target=NART&cn=NART111572458",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교 광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교 광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교 This study examines the classification characteristics of various machine learning classifiers for pain assessment using photoplethysmogram. The presence of pain was assessed using waveform characteristics derived from photoplethysmogram obtained from 73 patients before and after surgery. Classification performance was evaluated using logistic regression, random forest, multi-layer perceptron, and 1-D convolutional neural network, and was validated with nested k-fold cross validation. As a result, pain classification accuracy was highest in order of logistic regression, convolutional neural network, multi-layer perceptron, and random forest classifier. In addition, logistic regression, random forest, multi-layer perceptron, and convolutional neural network were shown to be robust to overfitting in order."
        },
        {
          "rank": 39,
          "score": 0.6052000522613525,
          "doc_id": "NART131648944",
          "title": "Comprehensive hepatotoxicity prediction: ensemble model integrating machine learning and deep learning",
          "abstract": "<P><B>Background</B></P><P>Chemicals may lead to acute liver injuries, posing a serious threat to human health. Achieving the precise safety profile of a compound is challenging due to the complex and expensive testing procedures. In silico approaches will aid in identifying the potential risk of drug candidates in the initial stage of drug development and thus mitigating the developmental cost.</P><P><B>Methods</B></P><P>In current studies, QSAR models were developed for hepatotoxicity predictions using the ensemble strategy to integrate machine learning (ML) and deep learning (DL) algorithms using various molecular features. A large dataset of 2588 chemicals and drugs was randomly divided into training (80%) and test (20%) sets, followed by the training of individual base models using diverse machine learning or deep learning based on three different kinds of descriptors and fingerprints. Feature selection approaches were employed to proceed with model optimizations based on the model performance. Hybrid ensemble approaches were further utilized to determine the method with the best performance.</P><P><B>Results</B></P><P>The voting ensemble classifier emerged as the optimal model, achieving an excellent prediction accuracy of 80.26%, AUC of 82.84%, and recall of over 93% followed by bagging and stacking ensemble classifiers method. The model was further verified by an external test set, internal 10-fold cross-validation, and rigorous benchmark training, exhibiting much better reliability than the published models.</P><P><B>Conclusion</B></P><P>The proposed ensemble model offers a dependable assessment with a good performance for the prediction regarding the risk of chemicals and drugs to induce liver damage.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART131648944&target=NART&cn=NART131648944",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Comprehensive hepatotoxicity prediction: ensemble model integrating machine learning and deep learning Comprehensive hepatotoxicity prediction: ensemble model integrating machine learning and deep learning Comprehensive hepatotoxicity prediction: ensemble model integrating machine learning and deep learning <P><B>Background</B></P><P>Chemicals may lead to acute liver injuries, posing a serious threat to human health. Achieving the precise safety profile of a compound is challenging due to the complex and expensive testing procedures. In silico approaches will aid in identifying the potential risk of drug candidates in the initial stage of drug development and thus mitigating the developmental cost.</P><P><B>Methods</B></P><P>In current studies, QSAR models were developed for hepatotoxicity predictions using the ensemble strategy to integrate machine learning (ML) and deep learning (DL) algorithms using various molecular features. A large dataset of 2588 chemicals and drugs was randomly divided into training (80%) and test (20%) sets, followed by the training of individual base models using diverse machine learning or deep learning based on three different kinds of descriptors and fingerprints. Feature selection approaches were employed to proceed with model optimizations based on the model performance. Hybrid ensemble approaches were further utilized to determine the method with the best performance.</P><P><B>Results</B></P><P>The voting ensemble classifier emerged as the optimal model, achieving an excellent prediction accuracy of 80.26%, AUC of 82.84%, and recall of over 93% followed by bagging and stacking ensemble classifiers method. The model was further verified by an external test set, internal 10-fold cross-validation, and rigorous benchmark training, exhibiting much better reliability than the published models.</P><P><B>Conclusion</B></P><P>The proposed ensemble model offers a dependable assessment with a good performance for the prediction regarding the risk of chemicals and drugs to induce liver damage.</P>"
        },
        {
          "rank": 40,
          "score": 0.6050255298614502,
          "doc_id": "NART125164824",
          "title": "Electricity Consumption Prediction Using Machine Learning",
          "abstract": "<P>The use of electricity has a significant impact on the environment, energy distribution costs, and energy management since it directly impacts these costs. Long-standing techniques have inherent limits in terms of accuracy and scalability when it comes to predicting power usage. It is now feasible to properly anticipate power use using previous data thanks to improvements in machine learning techniques. In this paper, we provide a machine learning-based method for forecasting power use. In this study, we investigate a number of machine learning techniques, including linear regression, K Nearest Neighbours, XGBOOST, random forest, and artificial neural networks(ANN), to forecast power usage. Using historical electricity use data received from a power utility business, we trained and assessed these models. The data is a year&rsquo;s worth of hourly power use that has been pre-processed to address outliers and missing numbers. Various assessment measures, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination (R2), were used to assess the performance of the models [19]. The outcomes demonstrate that the suggested method may accurately forecast power use. The K Nearest Neighbours(KNN) model outperformed all others in terms of performance, with a 90.92% accuracy rate for predicting agricultural production</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART125164824&target=NART&cn=NART125164824",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Electricity Consumption Prediction Using Machine Learning Electricity Consumption Prediction Using Machine Learning Electricity Consumption Prediction Using Machine Learning <P>The use of electricity has a significant impact on the environment, energy distribution costs, and energy management since it directly impacts these costs. Long-standing techniques have inherent limits in terms of accuracy and scalability when it comes to predicting power usage. It is now feasible to properly anticipate power use using previous data thanks to improvements in machine learning techniques. In this paper, we provide a machine learning-based method for forecasting power use. In this study, we investigate a number of machine learning techniques, including linear regression, K Nearest Neighbours, XGBOOST, random forest, and artificial neural networks(ANN), to forecast power usage. Using historical electricity use data received from a power utility business, we trained and assessed these models. The data is a year&rsquo;s worth of hourly power use that has been pre-processed to address outliers and missing numbers. Various assessment measures, including Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination (R2), were used to assess the performance of the models [19]. The outcomes demonstrate that the suggested method may accurately forecast power use. The K Nearest Neighbours(KNN) model outperformed all others in terms of performance, with a 90.92% accuracy rate for predicting agricultural production</P>"
        },
        {
          "rank": 41,
          "score": 0.6039630770683289,
          "doc_id": "ATN0030211826",
          "title": "Logistic Lasso를 이용한 에너지․환경산업 기업부도 예측",
          "abstract": "Since energy and environment industries have much publicity,predicting the industries’ corporate bankruptcy in advance is importantin that it can predict and prepare for the possibility of sudden supplyshocks, thereby alleviating the overall economic shock anddeteriorating public welfare. This study analyzes prediction of corporatebankruptcy in energy and environment industries. The logit model isused as basic estimation model, in which 51 financial variablesfrequently used in the previous studies are considered as initialexplanatory variables. And we compare the predictive power betweentwo models, one with and another without the variables multiplied bythe industries' dummy variable as additional variables. On the otherhand, Lasso(Tibshirani, 1996), a model shrinkage method, is applied toalleviate the problem of over-fitting and multicollinearity due to manyexplanatory variables used in the model. The results showed that thelogistic lasso model including the dummy variables of the energyenvironment industry was the best in all predictive measures.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0030211826&target=NART&cn=ATN0030211826",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Logistic Lasso를 이용한 에너지․환경산업 기업부도 예측 Logistic Lasso를 이용한 에너지․환경산업 기업부도 예측 Logistic Lasso를 이용한 에너지․환경산업 기업부도 예측 Since energy and environment industries have much publicity,predicting the industries’ corporate bankruptcy in advance is importantin that it can predict and prepare for the possibility of sudden supplyshocks, thereby alleviating the overall economic shock anddeteriorating public welfare. This study analyzes prediction of corporatebankruptcy in energy and environment industries. The logit model isused as basic estimation model, in which 51 financial variablesfrequently used in the previous studies are considered as initialexplanatory variables. And we compare the predictive power betweentwo models, one with and another without the variables multiplied bythe industries' dummy variable as additional variables. On the otherhand, Lasso(Tibshirani, 1996), a model shrinkage method, is applied toalleviate the problem of over-fitting and multicollinearity due to manyexplanatory variables used in the model. The results showed that thelogistic lasso model including the dummy variables of the energyenvironment industry was the best in all predictive measures."
        },
        {
          "rank": 42,
          "score": 0.6033962368965149,
          "doc_id": "JAKO202108848920380",
          "title": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구",
          "abstract": "The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202108848920380&target=NART&cn=JAKO202108848920380",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity."
        },
        {
          "rank": 43,
          "score": 0.6032546162605286,
          "doc_id": "JAKO199911921383665",
          "title": "회귀신경망을 이용한 음성인식에 관한 연구",
          "abstract": "본 논문은 회귀신경망을 이용한 음성인식에 관한 연구이다. 예측형 신경망으로 음절단위로 모델링한 후 미지의 입력음성에 대하여 예측오차가 최소가 되는 모델을 인식결과로 한다. 이를 위해서 예측형으로 구성된 신경망에 음성의 시변성을 신경망 내부에 흡수시키기 위해서 회귀구조의 동적인 신경망인 회귀예측신경망을 구성하고 Elman과 Jordan이 제안한 회귀구조에 따라 인식성능을 서로 비교하였다. 음성DB는 ETRI의 샘돌이 음성 데이터를 사용하였다. 그리고, 신경망의 최적모델을 구하기 위하여 예측차수와 은닉층 유니트 수의 변화에 따른 인식률의 변화와 문맥층에서 자기회귀계수를 두어 이전의 값들이 문맥층에서 누적되도록 하였을 경우에 대한 인식률의 변화를 비교하였다. 실험결과, 최적의 예측차수, 은닉층 유니트수, 자기회귀계수는 신경망의 구조에 따라 차이가 나타났으며, 전반적으로 Jordan망이 Elman망보다 인식률이 높았으며, 자기회귀계수에 대한 영향은 신경망의 구조와 계수값에 따라 불규칙하게 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199911921383665&target=NART&cn=JAKO199911921383665",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망을 이용한 음성인식에 관한 연구 회귀신경망을 이용한 음성인식에 관한 연구 회귀신경망을 이용한 음성인식에 관한 연구 본 논문은 회귀신경망을 이용한 음성인식에 관한 연구이다. 예측형 신경망으로 음절단위로 모델링한 후 미지의 입력음성에 대하여 예측오차가 최소가 되는 모델을 인식결과로 한다. 이를 위해서 예측형으로 구성된 신경망에 음성의 시변성을 신경망 내부에 흡수시키기 위해서 회귀구조의 동적인 신경망인 회귀예측신경망을 구성하고 Elman과 Jordan이 제안한 회귀구조에 따라 인식성능을 서로 비교하였다. 음성DB는 ETRI의 샘돌이 음성 데이터를 사용하였다. 그리고, 신경망의 최적모델을 구하기 위하여 예측차수와 은닉층 유니트 수의 변화에 따른 인식률의 변화와 문맥층에서 자기회귀계수를 두어 이전의 값들이 문맥층에서 누적되도록 하였을 경우에 대한 인식률의 변화를 비교하였다. 실험결과, 최적의 예측차수, 은닉층 유니트수, 자기회귀계수는 신경망의 구조에 따라 차이가 나타났으며, 전반적으로 Jordan망이 Elman망보다 인식률이 높았으며, 자기회귀계수에 대한 영향은 신경망의 구조와 계수값에 따라 불규칙하게 나타났다."
        },
        {
          "rank": 44,
          "score": 0.6023030877113342,
          "doc_id": "JAKO202518361202534",
          "title": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측",
          "abstract": "본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202518361202534&target=NART&cn=JAKO202518361202534",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다."
        },
        {
          "rank": 45,
          "score": 0.6022085547447205,
          "doc_id": "JAKO201336161064414",
          "title": "앙상블 SVM 모형을 이용한 기업 부도 예측",
          "abstract": "기업의 부도를 예측하는 것은 회계나 재무 분야에서 중요한 연구주제이다. 지금까지 기업 부도예측을 위해 여러 가지 데이터마이닝 기법들이 적용되었으나 주로 단일 모형을 사용함으로서 복잡한 분류 문제에의 적용에 한계를 갖고 있었다. 본 논문에서는 최근에 각광받고 있는 SVM (support vector machine) 모형들을 결합한 앙상블 SVM 모형 (ensemble SVM model)을 부도예측에 사용하고자 한다. 제안된 앙상블 모형은 v-조각 교차 타당성 (v-fold cross-validation)에 의해 얻어진 여러 가지 모형 중에서 성능이 좋은 상위 k개의 단일 모형으로 구성하고 과반수 투표 방식 (majority voting)을 사용하여 미지의 클래스를 분류한다. 본 논문에서 제안된 앙상블 SVM 모형의 성능을 평가하기 위해 실제 기업의 재무비율 자료와 모의실험자료를 가지고 실험하였고, 실험결과 제안된 앙상블 모형이 여러 가지 평가척도 하에서 단일 SVM 모형들보다 좋은 성능을 보임을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201336161064414&target=NART&cn=JAKO201336161064414",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "앙상블 SVM 모형을 이용한 기업 부도 예측 앙상블 SVM 모형을 이용한 기업 부도 예측 앙상블 SVM 모형을 이용한 기업 부도 예측 기업의 부도를 예측하는 것은 회계나 재무 분야에서 중요한 연구주제이다. 지금까지 기업 부도예측을 위해 여러 가지 데이터마이닝 기법들이 적용되었으나 주로 단일 모형을 사용함으로서 복잡한 분류 문제에의 적용에 한계를 갖고 있었다. 본 논문에서는 최근에 각광받고 있는 SVM (support vector machine) 모형들을 결합한 앙상블 SVM 모형 (ensemble SVM model)을 부도예측에 사용하고자 한다. 제안된 앙상블 모형은 v-조각 교차 타당성 (v-fold cross-validation)에 의해 얻어진 여러 가지 모형 중에서 성능이 좋은 상위 k개의 단일 모형으로 구성하고 과반수 투표 방식 (majority voting)을 사용하여 미지의 클래스를 분류한다. 본 논문에서 제안된 앙상블 SVM 모형의 성능을 평가하기 위해 실제 기업의 재무비율 자료와 모의실험자료를 가지고 실험하였고, 실험결과 제안된 앙상블 모형이 여러 가지 평가척도 하에서 단일 SVM 모형들보다 좋은 성능을 보임을 알 수 있었다."
        },
        {
          "rank": 46,
          "score": 0.6021984815597534,
          "doc_id": "NART132071160",
          "title": "Integrating machine learning and deep learning for enhanced supplier risk prediction",
          "abstract": "<P>The importance of anticipating and preventing disruptions is underscored by the increased operational complexity and vulnerability caused by advancements in supply chain management (SCM). This has spurred interest in integrating machine learning (ML) and deep learning (DL) into supply chain risk management (SCRM). In this paper, we introduce a tailored method using ML and DL to improve SCRM by predicting supplier failures, thus boosting efficiency and resilience in SC operations. Our method involves five phases focused on classifying and predicting supplier failures in non-conforming deliveries. This involves forecasting failure quantities and estimating total disruption costs. Initially, data from an automotive company is selected, and appropriate potential features and algorithms are selected, performance metric aligns with case study objectives, facilitating method evaluation are used such as: Precision, recall, F1-score, and accuracy metrics assess classification models, while Mean Squared Error (MSE) is used for regression tasks. Finally, an experimental design optimizes models, assessing success rates of various algorithms and their parameters within the chosen feature space. Experimental results underscore the success of our methodology in model development. In the classification task, the Random Forest (RF) classifier achieved 86% accuracy. When combined with the Gradient Boosting classifier, the ensemble exhibited enhanced accuracy, highlighting the complementary strengths of both algorithms and their synergistic impact, surpassing the performance of RF, Support Vector Regression (SVR), k-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). Noteworthy is the performance in regression tasks, where Linear Regression, ANN, and RF Regressor displayed exceptionally low MSE compared to other models.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART132071160&target=NART&cn=NART132071160",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Integrating machine learning and deep learning for enhanced supplier risk prediction Integrating machine learning and deep learning for enhanced supplier risk prediction Integrating machine learning and deep learning for enhanced supplier risk prediction <P>The importance of anticipating and preventing disruptions is underscored by the increased operational complexity and vulnerability caused by advancements in supply chain management (SCM). This has spurred interest in integrating machine learning (ML) and deep learning (DL) into supply chain risk management (SCRM). In this paper, we introduce a tailored method using ML and DL to improve SCRM by predicting supplier failures, thus boosting efficiency and resilience in SC operations. Our method involves five phases focused on classifying and predicting supplier failures in non-conforming deliveries. This involves forecasting failure quantities and estimating total disruption costs. Initially, data from an automotive company is selected, and appropriate potential features and algorithms are selected, performance metric aligns with case study objectives, facilitating method evaluation are used such as: Precision, recall, F1-score, and accuracy metrics assess classification models, while Mean Squared Error (MSE) is used for regression tasks. Finally, an experimental design optimizes models, assessing success rates of various algorithms and their parameters within the chosen feature space. Experimental results underscore the success of our methodology in model development. In the classification task, the Random Forest (RF) classifier achieved 86% accuracy. When combined with the Gradient Boosting classifier, the ensemble exhibited enhanced accuracy, highlighting the complementary strengths of both algorithms and their synergistic impact, surpassing the performance of RF, Support Vector Regression (SVR), k-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). Noteworthy is the performance in regression tasks, where Linear Regression, ANN, and RF Regressor displayed exceptionally low MSE compared to other models.</P>"
        },
        {
          "rank": 47,
          "score": 0.6013496518135071,
          "doc_id": "ATN0035906971",
          "title": "딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구",
          "abstract": "In this research, we compare the explanatory power between linear regression model and deep-learning model when estimating stock returns. As predicted, the deep-learning model shows statistically significant improvement over linear regression model, although the improvement is not economically meaningful. We further investigate the effects of deep-learning model using different parameters and pre-processing. The results show that the predictive power of deep-learning model can be worse-off than that of linear model if it fails to select optimal parameters. Especially, it is important to choose adequate deep-learning parameters not to overfit the data, because the accounting data (which is at most quarterly) may not be sufficient enough for the deep model structure. Further, we show that the predictive power using researchers’ domain knowledge is sometimes better off than that relying simply on the deep-learning model. For instance, denomination with total assets brings better results than non-denomination. Another interesting finding is that winsorizing extreme values brings lower explanatory power when we use the deep-learning model. Such finding implies that, by removing extreme values, we may lose useful information in the parameter estimation. The results of this paper will help future research decide whether to utilize deep learning model or linear regression model",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0035906971&target=NART&cn=ATN0035906971",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 In this research, we compare the explanatory power between linear regression model and deep-learning model when estimating stock returns. As predicted, the deep-learning model shows statistically significant improvement over linear regression model, although the improvement is not economically meaningful. We further investigate the effects of deep-learning model using different parameters and pre-processing. The results show that the predictive power of deep-learning model can be worse-off than that of linear model if it fails to select optimal parameters. Especially, it is important to choose adequate deep-learning parameters not to overfit the data, because the accounting data (which is at most quarterly) may not be sufficient enough for the deep model structure. Further, we show that the predictive power using researchers’ domain knowledge is sometimes better off than that relying simply on the deep-learning model. For instance, denomination with total assets brings better results than non-denomination. Another interesting finding is that winsorizing extreme values brings lower explanatory power when we use the deep-learning model. Such finding implies that, by removing extreme values, we may lose useful information in the parameter estimation. The results of this paper will help future research decide whether to utilize deep learning model or linear regression model"
        },
        {
          "rank": 48,
          "score": 0.6009066104888916,
          "doc_id": "ATN0049124885",
          "title": "머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구",
          "abstract": "본고는 XGBoost(Extreme Gradient Boosting), LSTM(Long Short-Term Memory) 등 다양한 머신러닝 방법을 포함한 총 13개의 모형을 활용하여 한국의 전년동월대비 물가상승률을 예측/분석하고 이를 통해 시계열 지속성과 머신러닝 예측 간의 관계에 관한 시사점을 제시한다. FRED-MD를 참고한 총 93개의 관련 국내외 거시경제/금융 변수들을 설명변수로 사용하고, 2004년 8월에서 2023년 6월까지의 표본을 고려하였다. 전월대비 물가상승률의 경우와는 달리 시계열 지속성(Persistence)이 높은 전년동월대비 물가상승률을 종속변수로 삼고 머신러닝 방법을 적용하여 예측할 때, 머신러닝 모형들의 예측력이 크게 떨어져 오히려 임의보행(Random Walk) 모형보다 예측오차가 더 큰 것으로 나타났다. 머신러닝 기법으로 시계열 지속성이 낮은 전월대비 물가상승률 예측치를 구하고 이를 통해 전년동월대비 물가상승률 예측치를 계산하면 전년동월대비 물가상승률 자체를 예측하는 경우에 비해 전반적으로 예측오차가 줄어드는 것을 확인하였고, 이 방법을 통해 전년동월대비 물가상승률 자체를 임의보행 모형으로 예측할 때보다 통계적으로 유의한 수준으로 우월한 예측치를 구할 수도 있는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0049124885&target=NART&cn=ATN0049124885",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구 머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구 머신러닝 기법을 이용한 한국 인플레이션 예측:시계열 지속성과 머신러닝 예측의 관계에 관한 사례 연구 본고는 XGBoost(Extreme Gradient Boosting), LSTM(Long Short-Term Memory) 등 다양한 머신러닝 방법을 포함한 총 13개의 모형을 활용하여 한국의 전년동월대비 물가상승률을 예측/분석하고 이를 통해 시계열 지속성과 머신러닝 예측 간의 관계에 관한 시사점을 제시한다. FRED-MD를 참고한 총 93개의 관련 국내외 거시경제/금융 변수들을 설명변수로 사용하고, 2004년 8월에서 2023년 6월까지의 표본을 고려하였다. 전월대비 물가상승률의 경우와는 달리 시계열 지속성(Persistence)이 높은 전년동월대비 물가상승률을 종속변수로 삼고 머신러닝 방법을 적용하여 예측할 때, 머신러닝 모형들의 예측력이 크게 떨어져 오히려 임의보행(Random Walk) 모형보다 예측오차가 더 큰 것으로 나타났다. 머신러닝 기법으로 시계열 지속성이 낮은 전월대비 물가상승률 예측치를 구하고 이를 통해 전년동월대비 물가상승률 예측치를 계산하면 전년동월대비 물가상승률 자체를 예측하는 경우에 비해 전반적으로 예측오차가 줄어드는 것을 확인하였고, 이 방법을 통해 전년동월대비 물가상승률 자체를 임의보행 모형으로 예측할 때보다 통계적으로 유의한 수준으로 우월한 예측치를 구할 수도 있는 것으로 나타났다."
        },
        {
          "rank": 49,
          "score": 0.6008574962615967,
          "doc_id": "NART118947969",
          "title": "Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction",
          "abstract": "<P>Recent growth in crop genomic and trait data have opened opportunities for the application of novel approaches to accelerate crop improvement. Machine learning and deep learning are at the forefront of prediction-based data analysis. However, few approaches for genotype to phenotype prediction compare machine learning with deep learning and further interpret the models that support the predictions. This study uses genome wide molecular markers and traits across 1110 soybean individuals to develop accurate prediction models. For 13/14 sets of predictions, XGBoost or random forest outperformed deep learning models in prediction performance. Top ranked SNPs by F-score were identified from XGBoost, and with further investigation found overlap with significantly associated loci identified from GWAS and previous literature. Feature importance rankings were used to reduce marker input by up to 90%, and subsequent models maintained or improved their prediction performance. These findings support interpretable machine learning as an approach for genomic based prediction of traits in soybean and other crops.</P><P><B>Supplementary Information</B></P><P>The online version contains supplementary material available at 10.1186/s12870-022-03559-z.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART118947969&target=NART&cn=NART118947969",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction <P>Recent growth in crop genomic and trait data have opened opportunities for the application of novel approaches to accelerate crop improvement. Machine learning and deep learning are at the forefront of prediction-based data analysis. However, few approaches for genotype to phenotype prediction compare machine learning with deep learning and further interpret the models that support the predictions. This study uses genome wide molecular markers and traits across 1110 soybean individuals to develop accurate prediction models. For 13/14 sets of predictions, XGBoost or random forest outperformed deep learning models in prediction performance. Top ranked SNPs by F-score were identified from XGBoost, and with further investigation found overlap with significantly associated loci identified from GWAS and previous literature. Feature importance rankings were used to reduce marker input by up to 90%, and subsequent models maintained or improved their prediction performance. These findings support interpretable machine learning as an approach for genomic based prediction of traits in soybean and other crops.</P><P><B>Supplementary Information</B></P><P>The online version contains supplementary material available at 10.1186/s12870-022-03559-z.</P>"
        },
        {
          "rank": 50,
          "score": 0.6007606983184814,
          "doc_id": "ART002897789",
          "title": "Modified Artificial Neural Networks and Support Vector Regression to Predict Lateral Pressure Exerted by Fresh Concrete on Formwork",
          "abstract": "In this study, a modified Artificial Neural Network (ANN) and Support Vector Regression (SVR) with three different optimization algorithms (Genetic, Salp Swarm and Grasshopper) were used to establish an accurate and easy-to-use module to predict the lateral pressure exerted by fresh concrete on formwork based on three main inputs, namely mix proportions (cement content, w/c, coarse aggregates, fine aggregates and admixture agent), casting rate, and height of specimens. The data have been obtained from 30 previously piloted experimental studies (resulted 113 samples). Achieved results for the model including all the input data provide the most excellent prediction of the exerted lateral pressure. Additionally, having different magnitudes of powder volume, aggregate volume and fluid content in the mix exposes different rising and descending in the lateral pressure outcomes. The results indicate that each model has its own advantages and disadvantages; however, the root mean square error values of the SVR models are lower than that of the ANN model. Additionally, the proposed models have been validated and all of them can accurately predict the lateral pressure of fresh concrete on the panel of the formwork.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002897789&target=NART&cn=ART002897789",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Modified Artificial Neural Networks and Support Vector Regression to Predict Lateral Pressure Exerted by Fresh Concrete on Formwork Modified Artificial Neural Networks and Support Vector Regression to Predict Lateral Pressure Exerted by Fresh Concrete on Formwork Modified Artificial Neural Networks and Support Vector Regression to Predict Lateral Pressure Exerted by Fresh Concrete on Formwork In this study, a modified Artificial Neural Network (ANN) and Support Vector Regression (SVR) with three different optimization algorithms (Genetic, Salp Swarm and Grasshopper) were used to establish an accurate and easy-to-use module to predict the lateral pressure exerted by fresh concrete on formwork based on three main inputs, namely mix proportions (cement content, w/c, coarse aggregates, fine aggregates and admixture agent), casting rate, and height of specimens. The data have been obtained from 30 previously piloted experimental studies (resulted 113 samples). Achieved results for the model including all the input data provide the most excellent prediction of the exerted lateral pressure. Additionally, having different magnitudes of powder volume, aggregate volume and fluid content in the mix exposes different rising and descending in the lateral pressure outcomes. The results indicate that each model has its own advantages and disadvantages; however, the root mean square error values of the SVR models are lower than that of the ANN model. Additionally, the proposed models have been validated and all of them can accurately predict the lateral pressure of fresh concrete on the panel of the formwork."
        }
      ]
    },
    {
      "query": "What is the Mean Absolute Percentage Error (MAPE) for each of the evaluated models?",
      "query_meta": {
        "type": "single_hop",
        "index": 2
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.6531235575675964,
          "doc_id": "ATN0035909826",
          "title": "머신러닝 모델을 이용한 석산 개발 발파진동 예측",
          "abstract": "In this study, a model was developed to predict the peak particle velocity (PPV) that affects people and the surrounding environment during blasting. Four machine learning models using the k-nearest neighbors (kNN), classification and regression tree (CART), support vector regression (SVR), and particle swarm optimization (PSO)-SVR algorithms were developed and compared with each other to predict the PPV. Mt. Yogmang located in Changwon-si, Gyeongsangnam-do was selected as a study area, and 1048 blasting data were acquired to train the machine learning models. The blasting data consisted of hole length, burden, spacing, maximum charge per delay, powder factor, number of holes, ratio of emulsion, monitoring distance and PPV. To evaluate the performance of the trained models, the mean absolute error (MAE), mean square error (MSE), and root mean square error (RMSE) were used. The PSO-SVR model showed superior performance with MAE, MSE and RMSE of 0.0348, 0.0021 and 0.0458, respectively. Finally, a method was proposed to predict the degree of influence on the surrounding environment using the developed machine learning models.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0035909826&target=NART&cn=ATN0035909826",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 모델을 이용한 석산 개발 발파진동 예측 머신러닝 모델을 이용한 석산 개발 발파진동 예측 머신러닝 모델을 이용한 석산 개발 발파진동 예측 In this study, a model was developed to predict the peak particle velocity (PPV) that affects people and the surrounding environment during blasting. Four machine learning models using the k-nearest neighbors (kNN), classification and regression tree (CART), support vector regression (SVR), and particle swarm optimization (PSO)-SVR algorithms were developed and compared with each other to predict the PPV. Mt. Yogmang located in Changwon-si, Gyeongsangnam-do was selected as a study area, and 1048 blasting data were acquired to train the machine learning models. The blasting data consisted of hole length, burden, spacing, maximum charge per delay, powder factor, number of holes, ratio of emulsion, monitoring distance and PPV. To evaluate the performance of the trained models, the mean absolute error (MAE), mean square error (MSE), and root mean square error (RMSE) were used. The PSO-SVR model showed superior performance with MAE, MSE and RMSE of 0.0348, 0.0021 and 0.0458, respectively. Finally, a method was proposed to predict the degree of influence on the surrounding environment using the developed machine learning models."
        },
        {
          "rank": 2,
          "score": 0.6420742869377136,
          "doc_id": "JAKO200815652408515",
          "title": "Gompertz 소프트웨어 비용 추정 모델",
          "abstract": "본 논문은 소프트웨어 비용추정 모델의 적합성을 평가하고, 가장 적합한 모델을 제시하였다. 먼저, 해당 모델의 함수를 변수변환시켜 선형식으로 만든다. 다음으로 실제 개발 소프트웨어의 비용 데이터가 모델의 선형식에 얼마나 적합한지로 모델의 성능을 평가한다. 모델 성능평가에는 절대오차 대신 상대오차 개념인 MMRE를 적용하였다. 기존의 소프트웨어 비용추정 모델은 Weibull, Gamma와 Rayleigh 함수를 따르고 있다. 본 논문에서는 성장곡선의 일종인 Gompertz 곡선 모델을 제안하였다. 추가로 다른 성장곡선들도 적합성을 검증하였다. 모델 성능평가 결과 Gompertz 성장곡선이 소프트웨어 비용추정 모델로 가장 적합한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200815652408515&target=NART&cn=JAKO200815652408515",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Gompertz 소프트웨어 비용 추정 모델 Gompertz 소프트웨어 비용 추정 모델 Gompertz 소프트웨어 비용 추정 모델 본 논문은 소프트웨어 비용추정 모델의 적합성을 평가하고, 가장 적합한 모델을 제시하였다. 먼저, 해당 모델의 함수를 변수변환시켜 선형식으로 만든다. 다음으로 실제 개발 소프트웨어의 비용 데이터가 모델의 선형식에 얼마나 적합한지로 모델의 성능을 평가한다. 모델 성능평가에는 절대오차 대신 상대오차 개념인 MMRE를 적용하였다. 기존의 소프트웨어 비용추정 모델은 Weibull, Gamma와 Rayleigh 함수를 따르고 있다. 본 논문에서는 성장곡선의 일종인 Gompertz 곡선 모델을 제안하였다. 추가로 다른 성장곡선들도 적합성을 검증하였다. 모델 성능평가 결과 Gompertz 성장곡선이 소프트웨어 비용추정 모델로 가장 적합한 성능을 보였다."
        },
        {
          "rank": 3,
          "score": 0.6396507024765015,
          "doc_id": "ATN0049468747",
          "title": "AHP 방법론을 적용한 FMEA 치명도 평가 사례연구",
          "abstract": "Failure mode and effects analysis(FMEA) is popular approach applied to examine potential failures in equipment designs and maintenance of equipments. Risk Priority Number(RPN) is used as an index for the criticality of fault modes in FMEA and RCM(reliability centered maintenance). Traditional RPN approach does not have much credit as a index for the criticality because it does not reflect their experience and the governing logic is apart from their knowledge. Multiple Criteria Decision Method(MCDM) is a proven approach applied to evaluate multiple conflicting criteria in decision making. MCDM can be applied as a tool for RPN evaluation. This study was carried out to investigate application of Analytical Hierarchy Process(AHP) in evaluating RPN.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0049468747&target=NART&cn=ATN0049468747",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "AHP 방법론을 적용한 FMEA 치명도 평가 사례연구 AHP 방법론을 적용한 FMEA 치명도 평가 사례연구 AHP 방법론을 적용한 FMEA 치명도 평가 사례연구 Failure mode and effects analysis(FMEA) is popular approach applied to examine potential failures in equipment designs and maintenance of equipments. Risk Priority Number(RPN) is used as an index for the criticality of fault modes in FMEA and RCM(reliability centered maintenance). Traditional RPN approach does not have much credit as a index for the criticality because it does not reflect their experience and the governing logic is apart from their knowledge. Multiple Criteria Decision Method(MCDM) is a proven approach applied to evaluate multiple conflicting criteria in decision making. MCDM can be applied as a tool for RPN evaluation. This study was carried out to investigate application of Analytical Hierarchy Process(AHP) in evaluating RPN."
        },
        {
          "rank": 4,
          "score": 0.6359399557113647,
          "doc_id": "JAKO201707851605473",
          "title": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표",
          "abstract": "본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201707851605473&target=NART&cn=JAKO201707851605473",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 효과적인 음성 인식 평가를 위한 심층 신경망 기반의 음성 인식 성능 지표 본 논문에서는 음성 데이터베이스를 평가하기 위해 여러 가지의 음성 특성 지표 추출 알고리즘을 설명하고 심층 신경망 기반의 새로운 음성 성능 지표 생성 방법을 제안한다. 선행 연구에서는 효과적인 음성 인식 성능 지표를 생성하기 위해 대표적인 음성 인식 성능 지표인 단어 오인식률(Word Error Rate, WER)과 상관도가 높은 여러 가지 음성 특성 지표들을 조합하여 새로운 성능 지표를 생성하였다. 생성된 음성 성능 지표는 다양한 잡음 환경에서 각 음성 특성 지표를 단독으로 사용할 때보다 단어 오인식률과 높은 상관도를 나타내어 음성 인식 성능을 예측하는데 효과적임을 입증 하였다. 본 논문에서는 심층 신경망을 기반으로 한 음성 특성 지표 추출 방법에 대해 설명하며 선행 연구에서 조합에 사용한 GMM(Gaussian Mixture Model) 음향 모델 확률 값을 심층 신경망 학습을 통해 추출한 확률 값으로 대체해 조합함으로써 단어 오인식률과 보다 높은 상관도를 갖는 것을 확인한다."
        },
        {
          "rank": 5,
          "score": 0.6325387358665466,
          "doc_id": "ATN0030181675",
          "title": "머신러닝 기법을 적용한 지가 예측 연구",
          "abstract": "The existing officially assessed individual land price is calculated as the multiple regression result of the land price index table which analyzed the officially assessed reference land price. The main purpose of this study is to suggest the nonlinear simulation method considering the characteristics of the land and the local spatial pattern by using the machine learning. The simulation model was performed by combining the 100m vector areal unit based cadastral map and the digital elevation model(DEM). The accuracy of prediction was 75.8%(1996) and 70.3%(2005) in decision trees of C5.0 algorithm. In the case of the support vector machine(SVM) model, prediction accuracy was 63.5%(1996) and 54.3%(2005). In the machine learning simulation, officially assessed individual land price grade was converted into nominal type data classified into 10 grades considering the distribution of officially assessed individual land price of Yongin city. Independent variables were constructed from 30,354 vector areal unit using geographical distances from hazardous aversion facilities, roads, schools, railways, parks, mountains, and land use. The machine learning simulation method of this study can be applied to the analysis of the local land characteristics for calculating the officially assessed individual land price.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0030181675&target=NART&cn=ATN0030181675",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기법을 적용한 지가 예측 연구 머신러닝 기법을 적용한 지가 예측 연구 머신러닝 기법을 적용한 지가 예측 연구 The existing officially assessed individual land price is calculated as the multiple regression result of the land price index table which analyzed the officially assessed reference land price. The main purpose of this study is to suggest the nonlinear simulation method considering the characteristics of the land and the local spatial pattern by using the machine learning. The simulation model was performed by combining the 100m vector areal unit based cadastral map and the digital elevation model(DEM). The accuracy of prediction was 75.8%(1996) and 70.3%(2005) in decision trees of C5.0 algorithm. In the case of the support vector machine(SVM) model, prediction accuracy was 63.5%(1996) and 54.3%(2005). In the machine learning simulation, officially assessed individual land price grade was converted into nominal type data classified into 10 grades considering the distribution of officially assessed individual land price of Yongin city. Independent variables were constructed from 30,354 vector areal unit using geographical distances from hazardous aversion facilities, roads, schools, railways, parks, mountains, and land use. The machine learning simulation method of this study can be applied to the analysis of the local land characteristics for calculating the officially assessed individual land price."
        },
        {
          "rank": 6,
          "score": 0.6306313276290894,
          "doc_id": "ART001624002",
          "title": "Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model",
          "abstract": "In this paper, we introduce an integrated performance evaluation model defined by the three indices (evaluation index for each evaluation stage, performance viewpoint, and performance type) and analyze the utilization and satisfaction of the seven public u-IT services implemented from 2008 to 2009 using the proposed model. From the performance results of the public u-IT services, it was found out that most of the research goals initially set by the public u-IT service support projects were satisfied. Furthermore, we suggest improvement directions and promotion strategies of future projects.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART001624002&target=NART&cn=ART001624002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model Performance Evaluation for Public u-IT Services using a Proposed Three-dimensional Model In this paper, we introduce an integrated performance evaluation model defined by the three indices (evaluation index for each evaluation stage, performance viewpoint, and performance type) and analyze the utilization and satisfaction of the seven public u-IT services implemented from 2008 to 2009 using the proposed model. From the performance results of the public u-IT services, it was found out that most of the research goals initially set by the public u-IT service support projects were satisfied. Furthermore, we suggest improvement directions and promotion strategies of future projects."
        },
        {
          "rank": 7,
          "score": 0.6254422664642334,
          "doc_id": "JAKO202305758375548",
          "title": "머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교",
          "abstract": "전국 대학생의 중도 탈락 비율의 증가는 학생 개인 뿐만 아니라 대학과 사회에 심각한 부정적 영향을 끼친다. 본 연구에서는 중도 탈락이 예상되는 학생을 사전에 식별하기 위하여, 각 대학의 학사관리 시스템에서 손쉽게 얻을 수 있는 학적 데이터를 기반으로 머신러닝 분야의 결정트리, 랜덤 포레스트, 로지스틱 회귀 및 딥러닝 기반의 중도 탈락 예측 모델을 구축하고, 그 성능을 비교&#x00B7;분석하였다. 분석 결과 로지스틱 회귀 기반 예측 모델의 재현율이 가장 높았으나 f-1 및 auc 값이 낮은 한계를 보였고, 랜덤 포레스트 기반의 예측 모델의 경우 재현율을 제외한 다른 모든 지표에서 가장 우수한 성능을 보였다. 또한 예측 기간에 따른 예측 모델의 성능을 확인하기 위하여 예측 기간을 단기(1개 학기 이내), 중기(2개 학기 이내) 및 장기(3개 학기 이내)로 나누어 분석해 본 결과, 장기 예측 시 가장 높은 예측력을 보였다. 본 연구를 통해 각 대학은 중도 탈락이 예상되는 학생들을 조기에 식별하고, 이들에 대한 집중 관리를 통해 중도 탈락 비율을 줄이며 나아가 대학 재정 안정화에 기여할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202305758375548&target=NART&cn=JAKO202305758375548",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교 머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교 머신러닝 기반 대학생 중도 탈락 예측 모델의 성능 비교 전국 대학생의 중도 탈락 비율의 증가는 학생 개인 뿐만 아니라 대학과 사회에 심각한 부정적 영향을 끼친다. 본 연구에서는 중도 탈락이 예상되는 학생을 사전에 식별하기 위하여, 각 대학의 학사관리 시스템에서 손쉽게 얻을 수 있는 학적 데이터를 기반으로 머신러닝 분야의 결정트리, 랜덤 포레스트, 로지스틱 회귀 및 딥러닝 기반의 중도 탈락 예측 모델을 구축하고, 그 성능을 비교&#x00B7;분석하였다. 분석 결과 로지스틱 회귀 기반 예측 모델의 재현율이 가장 높았으나 f-1 및 auc 값이 낮은 한계를 보였고, 랜덤 포레스트 기반의 예측 모델의 경우 재현율을 제외한 다른 모든 지표에서 가장 우수한 성능을 보였다. 또한 예측 기간에 따른 예측 모델의 성능을 확인하기 위하여 예측 기간을 단기(1개 학기 이내), 중기(2개 학기 이내) 및 장기(3개 학기 이내)로 나누어 분석해 본 결과, 장기 예측 시 가장 높은 예측력을 보였다. 본 연구를 통해 각 대학은 중도 탈락이 예상되는 학생들을 조기에 식별하고, 이들에 대한 집중 관리를 통해 중도 탈락 비율을 줄이며 나아가 대학 재정 안정화에 기여할 수 있을 것으로 기대된다."
        },
        {
          "rank": 8,
          "score": 0.6246206164360046,
          "doc_id": "ATN0027036789",
          "title": "인공신경망과 상관도 행렬을 이용한 폐광지역 지반침하 위험도지수 개발",
          "abstract": "A MSH(Mine Subsidence Hazard) index has been developed to estimate the subsidence possibility of an abandoned mine by means of an artificial neural network (ANN) and an interaction matrix. For this, 19 influence factors of mine subsidence were determined through a literature study, and the influence factors for 287 subsidence points at 38 abandoned mines were selected first, of which 108 points (34 abandoned mines) having high data quality were used for learning of ANN. Each influence factor of which range is widely distributed was classified into seven ranges to use it as a learning data of ANN. Interaction matrixes were constructed by an ANN analysis, and a MSH index for each subsidence point was calculated from influence factors as well as influence weights that are from the interaction matrix. The result shows that MSH index ranges between 37 and 66. The average of MSH index for Non-coal mines is 54, which is 13% higher than the one for coal mines of which average MSH index is 47.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0027036789&target=NART&cn=ATN0027036789",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "인공신경망과 상관도 행렬을 이용한 폐광지역 지반침하 위험도지수 개발 인공신경망과 상관도 행렬을 이용한 폐광지역 지반침하 위험도지수 개발 인공신경망과 상관도 행렬을 이용한 폐광지역 지반침하 위험도지수 개발 A MSH(Mine Subsidence Hazard) index has been developed to estimate the subsidence possibility of an abandoned mine by means of an artificial neural network (ANN) and an interaction matrix. For this, 19 influence factors of mine subsidence were determined through a literature study, and the influence factors for 287 subsidence points at 38 abandoned mines were selected first, of which 108 points (34 abandoned mines) having high data quality were used for learning of ANN. Each influence factor of which range is widely distributed was classified into seven ranges to use it as a learning data of ANN. Interaction matrixes were constructed by an ANN analysis, and a MSH index for each subsidence point was calculated from influence factors as well as influence weights that are from the interaction matrix. The result shows that MSH index ranges between 37 and 66. The average of MSH index for Non-coal mines is 54, which is 13% higher than the one for coal mines of which average MSH index is 47."
        },
        {
          "rank": 9,
          "score": 0.6239428520202637,
          "doc_id": "JAKO201934340925802",
          "title": "안면 정보를 이용한 나이브 베이즈 기반 고중성지방혈증 예측 모델",
          "abstract": "최근에 이르러, 기계학습 및 데이터마이닝은 수많은 질병 예측 및 진단에 활용되고 있다. 만성질환은 전체 사망률의 약 80%를 차지하는 질병으로, 점점 증가하는 추세이다. 만성질환 관련 예측 모델을 연구한 기존 연구들은 예측 모델을 구성하는 데이터로 혈당, 혈압, 인슐린 수치 등의 건강검진 수준의 데이터를 이용한다. 본 논문은 만성질환의 위험 요인인 이상지질혈증과 안면 정보의 연관성을 검증하고, 기계학습 기반 안면 정보를 이용한 이상지질혈증 예측 모델을 세계 최초로 개발한다. 본 연구는 5390명의 임상 데이터 중 안면 정보와 중성지방혈증 정보를 바탕으로 수행하였다. 중성지방혈증은 이상지질혈증을 판단하는 척도이다. 연구의 결과로 얼굴의 하악(mandibular) 간의 거리를 나타내는 FD_43_143_aD(p<0.0001, Area Under the receiver operating characteristics Curve(AUC)=0.652) 와 고중성지방혈증이 매우 높은 연관성을 가진 것을 밝혀냈고, 이를 기반으로 구축한 모델은 0.662의 AUC값을 획득하였다. 이러한 연구결과는 향후 질병 역학 및 대중 보건 영역의 스크리닝 단계에서 안면정보만으로 다양할 질병을 예측할 수 있는 기반을 제공할 수 있을 것이다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201934340925802&target=NART&cn=JAKO201934340925802",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "안면 정보를 이용한 나이브 베이즈 기반 고중성지방혈증 예측 모델 안면 정보를 이용한 나이브 베이즈 기반 고중성지방혈증 예측 모델 안면 정보를 이용한 나이브 베이즈 기반 고중성지방혈증 예측 모델 최근에 이르러, 기계학습 및 데이터마이닝은 수많은 질병 예측 및 진단에 활용되고 있다. 만성질환은 전체 사망률의 약 80%를 차지하는 질병으로, 점점 증가하는 추세이다. 만성질환 관련 예측 모델을 연구한 기존 연구들은 예측 모델을 구성하는 데이터로 혈당, 혈압, 인슐린 수치 등의 건강검진 수준의 데이터를 이용한다. 본 논문은 만성질환의 위험 요인인 이상지질혈증과 안면 정보의 연관성을 검증하고, 기계학습 기반 안면 정보를 이용한 이상지질혈증 예측 모델을 세계 최초로 개발한다. 본 연구는 5390명의 임상 데이터 중 안면 정보와 중성지방혈증 정보를 바탕으로 수행하였다. 중성지방혈증은 이상지질혈증을 판단하는 척도이다. 연구의 결과로 얼굴의 하악(mandibular) 간의 거리를 나타내는 FD_43_143_aD(p<0.0001, Area Under the receiver operating characteristics Curve(AUC)=0.652) 와 고중성지방혈증이 매우 높은 연관성을 가진 것을 밝혀냈고, 이를 기반으로 구축한 모델은 0.662의 AUC값을 획득하였다. 이러한 연구결과는 향후 질병 역학 및 대중 보건 영역의 스크리닝 단계에서 안면정보만으로 다양할 질병을 예측할 수 있는 기반을 제공할 수 있을 것이다."
        },
        {
          "rank": 10,
          "score": 0.6232813596725464,
          "doc_id": "NART38095312",
          "title": "Evaluation Model of Data Warehouse System",
          "abstract": "<P>By dividing the evaluation index system for data warehouse system into several layers,we can get a three-dimensional evaluation index matrix and setup an evaluation model of DW.Taking advantages of improved entropy-based TOPSIS algorithm, the weights of indices are optimized.We apply distance-method in the multi-objectives decision-mak-ing, and give a comprehensive evaluation of DW by using the distances from ideal and negative ideal solutions.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART38095312&target=NART&cn=NART38095312",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Evaluation Model of Data Warehouse System Evaluation Model of Data Warehouse System Evaluation Model of Data Warehouse System <P>By dividing the evaluation index system for data warehouse system into several layers,we can get a three-dimensional evaluation index matrix and setup an evaluation model of DW.Taking advantages of improved entropy-based TOPSIS algorithm, the weights of indices are optimized.We apply distance-method in the multi-objectives decision-mak-ing, and give a comprehensive evaluation of DW by using the distances from ideal and negative ideal solutions.</P>"
        },
        {
          "rank": 11,
          "score": 0.6229398250579834,
          "doc_id": "NART17510385",
          "title": "Hidden-articulator Markov models for speech recognition",
          "abstract": "<P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART17510385&target=NART&cn=NART17510385",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition Hidden-articulator Markov models for speech recognition <P><B>Abstract</B></P><P>Most existing automatic speech recognition systems today do not explicitly use knowledge about human speech production. We show that the incorporation of articulatory knowledge into these systems is a promising direction for speech recognition, with the potential for lower error rates and more robust performance. To this end, we introduce the Hidden-Articulator Markov model (HAMM), a model which directly integrates articulatory information into speech recognition.</P><P>The HAMM is an extension of the articulatory-feature model introduced by Erler in 1996. We extend the model by using diphone units, developing a new technique for model initialization, and constructing a novel articulatory feature mapping. We also introduce a method to decrease the number of parameters, making the HAMM comparable in size to standard HMMs. We demonstrate that the HAMM can reasonably predict the movement of articulators, which results in a decreased word error rate (WER). The articulatory knowledge also proves useful in noisy acoustic conditions. When combined with a standard model, the HAMM reduces WER 28&#x2013;35% relative to the standard model alone.</P>"
        },
        {
          "rank": 12,
          "score": 0.620491623878479,
          "doc_id": "ART002832678",
          "title": "Deep learning-guided attenuation correction in the image domain for myocardial perfusion SPECT imaging",
          "abstract": "We investigate the accuracy of direct attenuation correction (AC) in the image domain for myocardial perfusion SPECT (single-photon emission computed tomography) imaging (MPI-SPECT) using residual (ResNet) and UNet deep convolutional neural networks. MPI-SPECT 99mTc-sestamibi images of 99 patients were retrospectively included. UNet and ResNet networks were trained using non-attenuation-corrected SPECT images as input, whereas CT-based attenuation-corrected (CT-AC) SPECT images served as reference. Chang’s calculated AC approach considering a uniform attenuation coefficient within the body contour was also implemented. Clinical and quantitative evaluations of the proposed methods were performed considering SPECT CT-AC images of 19 subjects (external validation set) as reference. Image-derived metrics, including the voxel-wise mean error (ME), mean absolute error, relative error, structural similarity index (SSI), and peak signal-to-noise ratio, as well as clinical relevant indices, such as total perfusion deficit (TPD), were utilized. Overall, AC SPECT images generated using the deep learning networks exhibited good agreement with SPECT CT-AC images, substantially outperforming Chang’s method. The ResNet and UNet models resulted in an ME of −6.99 ± 16.72 and −4.41 ± 11.8 and an SSI of 0.99 ± 0.04 and 0.98 ± 0.05, respectively. Chang’s approach led to ME and SSI of 25.52 ± 33.98 and 0.93 ± 0.09, respectively. Similarly, the clinical evaluation revealed a mean TPD of 12.78 ± 9.22% and 12.57 ± 8.93% for ResNet and UNet models, respectively, compared to 12.84 ± 8.63% obtained from SPECT CT-AC images. Conversely, Chang’s approach led to a mean TPD of 16.68 ± 11.24%. The deep learning AC methods have the potential to achieve reliable AC in MPI-SPECT imaging.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002832678&target=NART&cn=ART002832678",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Deep learning-guided attenuation correction in the image domain for myocardial perfusion SPECT imaging Deep learning-guided attenuation correction in the image domain for myocardial perfusion SPECT imaging Deep learning-guided attenuation correction in the image domain for myocardial perfusion SPECT imaging We investigate the accuracy of direct attenuation correction (AC) in the image domain for myocardial perfusion SPECT (single-photon emission computed tomography) imaging (MPI-SPECT) using residual (ResNet) and UNet deep convolutional neural networks. MPI-SPECT 99mTc-sestamibi images of 99 patients were retrospectively included. UNet and ResNet networks were trained using non-attenuation-corrected SPECT images as input, whereas CT-based attenuation-corrected (CT-AC) SPECT images served as reference. Chang’s calculated AC approach considering a uniform attenuation coefficient within the body contour was also implemented. Clinical and quantitative evaluations of the proposed methods were performed considering SPECT CT-AC images of 19 subjects (external validation set) as reference. Image-derived metrics, including the voxel-wise mean error (ME), mean absolute error, relative error, structural similarity index (SSI), and peak signal-to-noise ratio, as well as clinical relevant indices, such as total perfusion deficit (TPD), were utilized. Overall, AC SPECT images generated using the deep learning networks exhibited good agreement with SPECT CT-AC images, substantially outperforming Chang’s method. The ResNet and UNet models resulted in an ME of −6.99 ± 16.72 and −4.41 ± 11.8 and an SSI of 0.99 ± 0.04 and 0.98 ± 0.05, respectively. Chang’s approach led to ME and SSI of 25.52 ± 33.98 and 0.93 ± 0.09, respectively. Similarly, the clinical evaluation revealed a mean TPD of 12.78 ± 9.22% and 12.57 ± 8.93% for ResNet and UNet models, respectively, compared to 12.84 ± 8.63% obtained from SPECT CT-AC images. Conversely, Chang’s approach led to a mean TPD of 16.68 ± 11.24%. The deep learning AC methods have the potential to achieve reliable AC in MPI-SPECT imaging."
        },
        {
          "rank": 13,
          "score": 0.6200371980667114,
          "doc_id": "JAKO202009252092311",
          "title": "공공 빅데이터 플랫폼 성과평가 모형",
          "abstract": "본 연구는 공공데이터 개방에 있어 공공데이터 제공자의 데이터 기여 측면과 공공데이터 사용자의 데이터 활용 측면을 고려하여 공공데이터 플랫폼 성과측정을 위한 프레임워크를 개발하였다. 본 연구는 NIST(2018)의 빅데이터 참조 아키텍처와 Neely et al.(2001)의 성과 프리즘을 기반으로 공공 빅데이터 플랫폼 성과평가 모형의 5개 영역을 제시하였다. 구체적으로, 공공데이터 플랫폼 성과평가 영역은 이해관계자 기여, 빅데이터 거버넌스 역량, 빅데이터 서비스 역량, 빅데이터 정보기술(IT) 역량, 그리고 이해관계자 만족으로 구성된다. 본 연구에서 제시한 공공 빅데이터 플랫폼 성과평가 모형의 5개 영역과 24개 평가지표에 대한 측정 문항은 총 75개 항목으로 구성되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202009252092311&target=NART&cn=JAKO202009252092311",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "공공 빅데이터 플랫폼 성과평가 모형 공공 빅데이터 플랫폼 성과평가 모형 공공 빅데이터 플랫폼 성과평가 모형 본 연구는 공공데이터 개방에 있어 공공데이터 제공자의 데이터 기여 측면과 공공데이터 사용자의 데이터 활용 측면을 고려하여 공공데이터 플랫폼 성과측정을 위한 프레임워크를 개발하였다. 본 연구는 NIST(2018)의 빅데이터 참조 아키텍처와 Neely et al.(2001)의 성과 프리즘을 기반으로 공공 빅데이터 플랫폼 성과평가 모형의 5개 영역을 제시하였다. 구체적으로, 공공데이터 플랫폼 성과평가 영역은 이해관계자 기여, 빅데이터 거버넌스 역량, 빅데이터 서비스 역량, 빅데이터 정보기술(IT) 역량, 그리고 이해관계자 만족으로 구성된다. 본 연구에서 제시한 공공 빅데이터 플랫폼 성과평가 모형의 5개 영역과 24개 평가지표에 대한 측정 문항은 총 75개 항목으로 구성되었다."
        },
        {
          "rank": 14,
          "score": 0.6193464994430542,
          "doc_id": "JAKO201723840540692",
          "title": "빅데이터 통합모형 비교분석",
          "abstract": "빅데이터가 4차 산업혁명의 핵심으로 자리하면서 빅데이터 기반 처리 및 분석 능력이 기업의 미래 경쟁력을 좌우할 전망이다. 빅데이터 처리 및 분석을 위한 RHadoop과 RHIPE 모형은 R과 Hadoop의 통합모형으로 지금까지 각각의 모형에 대해서는 연구가 많이 진행되어 왔으나 두 모형간 비교 연구는 거의 이루어 지지 않았다. 본 논문에서는 대용량의 실제 데이터와 모의실험 데이터에서 다중 회귀 (multiple regression)와 로지스틱 회귀 (logistic regression) 추정을 위한 머신러닝 (machine learning) 알고리즘을 MapReduce 프로그램 구현을 통해 RHadoop과 RHIPE 간의 비교 분석하고자 한다. 구축된 분산 클러스터 (distributed cluster) 하에서 두 모형간 성능 실험 결과, RHIPE은 RHadoop에 비해 대체로 빠른 처리속도를 보인 반면에 설치, 사용면에서 어려움을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201723840540692&target=NART&cn=JAKO201723840540692",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "빅데이터 통합모형 비교분석 빅데이터 통합모형 비교분석 빅데이터 통합모형 비교분석 빅데이터가 4차 산업혁명의 핵심으로 자리하면서 빅데이터 기반 처리 및 분석 능력이 기업의 미래 경쟁력을 좌우할 전망이다. 빅데이터 처리 및 분석을 위한 RHadoop과 RHIPE 모형은 R과 Hadoop의 통합모형으로 지금까지 각각의 모형에 대해서는 연구가 많이 진행되어 왔으나 두 모형간 비교 연구는 거의 이루어 지지 않았다. 본 논문에서는 대용량의 실제 데이터와 모의실험 데이터에서 다중 회귀 (multiple regression)와 로지스틱 회귀 (logistic regression) 추정을 위한 머신러닝 (machine learning) 알고리즘을 MapReduce 프로그램 구현을 통해 RHadoop과 RHIPE 간의 비교 분석하고자 한다. 구축된 분산 클러스터 (distributed cluster) 하에서 두 모형간 성능 실험 결과, RHIPE은 RHadoop에 비해 대체로 빠른 처리속도를 보인 반면에 설치, 사용면에서 어려움을 보였다."
        },
        {
          "rank": 15,
          "score": 0.6169478893280029,
          "doc_id": "JAKO200921640756800",
          "title": "은닉 마르코프 모델 기반 동작 인식 방법",
          "abstract": "본 논문은 비전 기반 동작 인식 방법으로 모범 동작의 유형을 모형화하고 이를 이용하여 사용자의 동작을 인식하고 모범동작과 사용자의 동작간의 유사도를 측정하는 방법을 제안한다. 동작 인식을 위하여 은닉 마르코프 모델 기반의 유형화 기법을 통하여 모범 동작의 유형 모델을 구성하고 이를 이용하여 사용자의 동작을 인식한다. 유사도 측정을 위하여 편집 거리 알고리즘을 응용하여 모범 동작과 사용자 동작의 유사도를 측정하고 점수 표기가 가능하도록 하였다. 본 논문에서 제안하는 동작 인식 처리 방법은 평균 93% 이상의 높은 인식율을 보였다. 본 연구의 결과는 동작 인식 기반 게임, 자세인식, 동작의 반복 훈련 및 훈련 달성도 측정을 요하는 재활훈련 시스템 등에 활용 가능하다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200921640756800&target=NART&cn=JAKO200921640756800",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마르코프 모델 기반 동작 인식 방법 은닉 마르코프 모델 기반 동작 인식 방법 은닉 마르코프 모델 기반 동작 인식 방법 본 논문은 비전 기반 동작 인식 방법으로 모범 동작의 유형을 모형화하고 이를 이용하여 사용자의 동작을 인식하고 모범동작과 사용자의 동작간의 유사도를 측정하는 방법을 제안한다. 동작 인식을 위하여 은닉 마르코프 모델 기반의 유형화 기법을 통하여 모범 동작의 유형 모델을 구성하고 이를 이용하여 사용자의 동작을 인식한다. 유사도 측정을 위하여 편집 거리 알고리즘을 응용하여 모범 동작과 사용자 동작의 유사도를 측정하고 점수 표기가 가능하도록 하였다. 본 논문에서 제안하는 동작 인식 처리 방법은 평균 93% 이상의 높은 인식율을 보였다. 본 연구의 결과는 동작 인식 기반 게임, 자세인식, 동작의 반복 훈련 및 훈련 달성도 측정을 요하는 재활훈련 시스템 등에 활용 가능하다."
        },
        {
          "rank": 16,
          "score": 0.614920437335968,
          "doc_id": "JAKO201424635095344",
          "title": "AHP기법을 활용한 임도의 재해위험 등급 구분에 관한 연구",
          "abstract": "본 연구는 임도의 재해위험 등급을 구분하기 위한 평가표를 제시하고자 수행되었으며, 이를 위해 임도의 재해에 영향을 미치는 인자들을 선정하고 산림공학 전문가들을 대상으로 AHP 설문조사를 실시하였다. 평가항목은 전문가집단의 자문을 통해 크게 자연환경, 인문 사회환경, 임도의 구성요소로 구분하였으며, 총 21개의 평가인자로 구성하였다. 설문조사결과를 바탕으로 각 평가항목에 대한 가중치 분석을 실시하였으며, 분석결과 임도의 구성요소 항목 중 배수시설이 가장 높은 가중치를 점하고 있는 것으로 나타났다. 한편, AHP 분석결과를 바탕으로 각각의 평가항목에 대한 평가기준과 평가점수를 부여하여 평가표를 작성하였다. 도출된 평가표를 현장조사에 적용시킨 결과 최고점은 78.8점, 최저점은 42.7점, 평균점수는 61.8점으로 나타났다. 산정된 점수를 바탕으로 전문가 집단의 회의를 통해 임도의 재해위험 등급을 4등급으로 구분하여 제시하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201424635095344&target=NART&cn=JAKO201424635095344",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "AHP기법을 활용한 임도의 재해위험 등급 구분에 관한 연구 AHP기법을 활용한 임도의 재해위험 등급 구분에 관한 연구 AHP기법을 활용한 임도의 재해위험 등급 구분에 관한 연구 본 연구는 임도의 재해위험 등급을 구분하기 위한 평가표를 제시하고자 수행되었으며, 이를 위해 임도의 재해에 영향을 미치는 인자들을 선정하고 산림공학 전문가들을 대상으로 AHP 설문조사를 실시하였다. 평가항목은 전문가집단의 자문을 통해 크게 자연환경, 인문 사회환경, 임도의 구성요소로 구분하였으며, 총 21개의 평가인자로 구성하였다. 설문조사결과를 바탕으로 각 평가항목에 대한 가중치 분석을 실시하였으며, 분석결과 임도의 구성요소 항목 중 배수시설이 가장 높은 가중치를 점하고 있는 것으로 나타났다. 한편, AHP 분석결과를 바탕으로 각각의 평가항목에 대한 평가기준과 평가점수를 부여하여 평가표를 작성하였다. 도출된 평가표를 현장조사에 적용시킨 결과 최고점은 78.8점, 최저점은 42.7점, 평균점수는 61.8점으로 나타났다. 산정된 점수를 바탕으로 전문가 집단의 회의를 통해 임도의 재해위험 등급을 4등급으로 구분하여 제시하였다."
        },
        {
          "rank": 17,
          "score": 0.6145459413528442,
          "doc_id": "JAKO201726163356540",
          "title": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델",
          "abstract": "본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201726163356540&target=NART&cn=JAKO201726163356540",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다."
        },
        {
          "rank": 18,
          "score": 0.6132601499557495,
          "doc_id": "JAKO200708410645481",
          "title": "효율적인 신경망 부싱모델을 위한 신경망 구성 최적화",
          "abstract": "A bushing component of a vehicle suspension system is tested to capture the nonlinear behavior of rubber bushing element using the MTS 3-axes rubber test machine. The results of the tests are used to model the artificial neural network bushing model. The performances from the neural network model usually are dependent on the structure of the neural network. In this paper, maximum error, peak error, root mean square error, and error-to-signal ratio are employed to evaluate the performances of the neural network bushing model. A simple simulation is carried out to show the usefulness of the developed procedure.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200708410645481&target=NART&cn=JAKO200708410645481",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효율적인 신경망 부싱모델을 위한 신경망 구성 최적화 효율적인 신경망 부싱모델을 위한 신경망 구성 최적화 효율적인 신경망 부싱모델을 위한 신경망 구성 최적화 A bushing component of a vehicle suspension system is tested to capture the nonlinear behavior of rubber bushing element using the MTS 3-axes rubber test machine. The results of the tests are used to model the artificial neural network bushing model. The performances from the neural network model usually are dependent on the structure of the neural network. In this paper, maximum error, peak error, root mean square error, and error-to-signal ratio are employed to evaluate the performances of the neural network bushing model. A simple simulation is carried out to show the usefulness of the developed procedure."
        },
        {
          "rank": 19,
          "score": 0.6131737232208252,
          "doc_id": "JAKO202518361202534",
          "title": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측",
          "abstract": "본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202518361202534&target=NART&cn=JAKO202518361202534",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다."
        },
        {
          "rank": 20,
          "score": 0.6119848489761353,
          "doc_id": "ART002787934",
          "title": "Effective Electricity Demand Prediction via Deep Learning",
          "abstract": "Prediction of electricity demand in homes and buildings can be used to optimize an energy management system by decreasing energy wastage. A time-series prediction system is still a challenging problem in machine learning and deep learning. Our main idea is to compare three methods. For this work, we analyzed an electricity demand prediction system using the current state-of-the-art deep-learning methods with a machine-learning method: error correction with multi-layer perceptron (eMLP) structure, autoregressive integrated moving average (ARIMA) structure, and a proposed structure named CNN-LSTM. For this, we measured and collected electricity demand data in Germany for home appliances. We report the prediction accuracy in terms of the mean square error (MSE) and mean absolute percentage error (MAPE). The experimental result indicates that CNN-LSTM outperforms eMLP and ARIMA in accuracy.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002787934&target=NART&cn=ART002787934",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Effective Electricity Demand Prediction via Deep Learning Effective Electricity Demand Prediction via Deep Learning Effective Electricity Demand Prediction via Deep Learning Prediction of electricity demand in homes and buildings can be used to optimize an energy management system by decreasing energy wastage. A time-series prediction system is still a challenging problem in machine learning and deep learning. Our main idea is to compare three methods. For this work, we analyzed an electricity demand prediction system using the current state-of-the-art deep-learning methods with a machine-learning method: error correction with multi-layer perceptron (eMLP) structure, autoregressive integrated moving average (ARIMA) structure, and a proposed structure named CNN-LSTM. For this, we measured and collected electricity demand data in Germany for home appliances. We report the prediction accuracy in terms of the mean square error (MSE) and mean absolute percentage error (MAPE). The experimental result indicates that CNN-LSTM outperforms eMLP and ARIMA in accuracy."
        },
        {
          "rank": 21,
          "score": 0.6113315224647522,
          "doc_id": "JAKO202517036003495",
          "title": "사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크",
          "abstract": "본 연구는 사이버 복원력을 정량적으로 평가하기 위한 지표 선정 프레임워크를 제시한다. 정보보호 및 개인정보보호 관리체계 인증기관을 기반으로 다양한 서비스 유형의 특성을 분석하고 복원력 측정을 위한 지표 후보군을 도출한다. 선정된 후보 지표는 본 연구에서 정의한 객관성, 재현성, 확장성, 실용성, 복원력 반영성의 다섯가지 핵심 특성을 기준으로 타당성을 검증한다. 또한 지표간 상호 배타성(ME)과 전체 포괄성(CE) 원칙을 보완 기준으로 적용하여 중복성을 제거하고 평가 체계의 범용성을 확보하였다. 최종적으로 12개의 정량적 평가 지표를 선정하였으며 이 중 웹서비스와 같은 트랜잭션 기반 시스템에서의 초당 처리 건수를 나타내는 TPS 지표를 중심으로, TCP SYN flooding 공격 상황에서의 지표의 변화와 해석방법에 대한 실증적 분석을 수행하였다. 제안한 지표선정 프레임워크와 정량적 평가지표는 사이버 복원력을 객관적으로 측정할 수 있는 평가 체계 수립에 기여하며, 지속적으로 변화하는 사이버 위협에 효과적인 대응과 개선을 위한 기준을 제공한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202517036003495&target=NART&cn=JAKO202517036003495",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크 사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크 사이버 복원력 정량적 평가를 위한 지표 선정 프레임워크 본 연구는 사이버 복원력을 정량적으로 평가하기 위한 지표 선정 프레임워크를 제시한다. 정보보호 및 개인정보보호 관리체계 인증기관을 기반으로 다양한 서비스 유형의 특성을 분석하고 복원력 측정을 위한 지표 후보군을 도출한다. 선정된 후보 지표는 본 연구에서 정의한 객관성, 재현성, 확장성, 실용성, 복원력 반영성의 다섯가지 핵심 특성을 기준으로 타당성을 검증한다. 또한 지표간 상호 배타성(ME)과 전체 포괄성(CE) 원칙을 보완 기준으로 적용하여 중복성을 제거하고 평가 체계의 범용성을 확보하였다. 최종적으로 12개의 정량적 평가 지표를 선정하였으며 이 중 웹서비스와 같은 트랜잭션 기반 시스템에서의 초당 처리 건수를 나타내는 TPS 지표를 중심으로, TCP SYN flooding 공격 상황에서의 지표의 변화와 해석방법에 대한 실증적 분석을 수행하였다. 제안한 지표선정 프레임워크와 정량적 평가지표는 사이버 복원력을 객관적으로 측정할 수 있는 평가 체계 수립에 기여하며, 지속적으로 변화하는 사이버 위협에 효과적인 대응과 개선을 위한 기준을 제공한다."
        },
        {
          "rank": 22,
          "score": 0.6111594438552856,
          "doc_id": "JAKO202123157167812",
          "title": "은닉 마르코프 모델을 이용한 국가별 주가지수 예측",
          "abstract": "은닉 마르코프 모델(hidden Markov model, HMM)은 은닉된 상태와 관찰 가능한 결과의 두 가지 요소로 이루어진 통계적 모형으로 확률론적 접근이 가능하고, 다양한 수학적인 구조를 가지고 있어 여러 분야에서 활발하게 사용되고 있다. 특히 금융 분야의 시계열 데이터에 응용되어 다양한 연구가 진행되고 있다. 본 연구는 HMM 이론을 국내 KOSPI200 주가지수와 더불어 NIKKEI225, HSI, S&P500, FTSE100과 같은 해외 주가지수 예측에 적용해 보고자 한다. 또한, 최근 인공지능 분야의 발전으로 인해 주식 가격 예측에 빈번하게 사용되는 서포트 벡터 회귀(support vector regression, SVR) 결과와 어떤 차이가 있는지 비교하여 살펴보고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202123157167812&target=NART&cn=JAKO202123157167812",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마르코프 모델을 이용한 국가별 주가지수 예측 은닉 마르코프 모델을 이용한 국가별 주가지수 예측 은닉 마르코프 모델을 이용한 국가별 주가지수 예측 은닉 마르코프 모델(hidden Markov model, HMM)은 은닉된 상태와 관찰 가능한 결과의 두 가지 요소로 이루어진 통계적 모형으로 확률론적 접근이 가능하고, 다양한 수학적인 구조를 가지고 있어 여러 분야에서 활발하게 사용되고 있다. 특히 금융 분야의 시계열 데이터에 응용되어 다양한 연구가 진행되고 있다. 본 연구는 HMM 이론을 국내 KOSPI200 주가지수와 더불어 NIKKEI225, HSI, S&P500, FTSE100과 같은 해외 주가지수 예측에 적용해 보고자 한다. 또한, 최근 인공지능 분야의 발전으로 인해 주식 가격 예측에 빈번하게 사용되는 서포트 벡터 회귀(support vector regression, SVR) 결과와 어떤 차이가 있는지 비교하여 살펴보고자 한다."
        },
        {
          "rank": 23,
          "score": 0.6104317903518677,
          "doc_id": "JAKO202320150299733",
          "title": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가",
          "abstract": "본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202320150299733&target=NART&cn=JAKO202320150299733",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다."
        },
        {
          "rank": 24,
          "score": 0.6091861724853516,
          "doc_id": "NART57626890",
          "title": "PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化",
          "abstract": "<P>This study presents an application of the Particle Swarm Optimization (PSO) algorithm to the parameter optimization of rainfall-runoff models. Six global optimization algorithms, the shuffled complex evolution method (SCE-UA), modified SCE-UA, modified SCE-UA with initial value, PSO, modified PSO and modified PSO with initial value, were applied to parameter optimization on four kinds of series tank models. Performance comparison of there algorithms was evaluated and it can be concluded that SCE-UA and PSO show comparable performance in most cases. In addition, PSO is more effective than SCE-UA under the following conditions. 1) the model has large number of parameters, 2) the model has wide range of parameters, 3) calibration period is too short, 4) observation data contains large uncertainty. The modified PSO with initial value shows the most effective and stable performance.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART57626890&target=NART&cn=NART57626890",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化 PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化 PSOアルゴリズムによる流出モデルパラメ&#x30fc;タの最適化 <P>This study presents an application of the Particle Swarm Optimization (PSO) algorithm to the parameter optimization of rainfall-runoff models. Six global optimization algorithms, the shuffled complex evolution method (SCE-UA), modified SCE-UA, modified SCE-UA with initial value, PSO, modified PSO and modified PSO with initial value, were applied to parameter optimization on four kinds of series tank models. Performance comparison of there algorithms was evaluated and it can be concluded that SCE-UA and PSO show comparable performance in most cases. In addition, PSO is more effective than SCE-UA under the following conditions. 1) the model has large number of parameters, 2) the model has wide range of parameters, 3) calibration period is too short, 4) observation data contains large uncertainty. The modified PSO with initial value shows the most effective and stable performance.</P>"
        },
        {
          "rank": 25,
          "score": 0.6081986427307129,
          "doc_id": "JAKO202128557368138",
          "title": "인공지능을 활용한 기계학습 앙상블 모델 개발",
          "abstract": "To predict mechanical properties of secondary hardening martensitic steels, a machine learning ensemble model was established. Based on ANN(Artificial Neural Network) architecture, some kinds of methods was considered to optimize the model. In particular, interaction features, which can reflect interactions between chemical compositions and processing conditions of real alloy system, was considered by means of feature engineering, and then K-Fold cross validation coupled with bagging ensemble were investigated to reduce R2_score and a factor indicating average learning errors owing to biased experimental database.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202128557368138&target=NART&cn=JAKO202128557368138",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "인공지능을 활용한 기계학습 앙상블 모델 개발 인공지능을 활용한 기계학습 앙상블 모델 개발 인공지능을 활용한 기계학습 앙상블 모델 개발 To predict mechanical properties of secondary hardening martensitic steels, a machine learning ensemble model was established. Based on ANN(Artificial Neural Network) architecture, some kinds of methods was considered to optimize the model. In particular, interaction features, which can reflect interactions between chemical compositions and processing conditions of real alloy system, was considered by means of feature engineering, and then K-Fold cross validation coupled with bagging ensemble were investigated to reduce R2_score and a factor indicating average learning errors owing to biased experimental database."
        },
        {
          "rank": 26,
          "score": 0.6081003546714783,
          "doc_id": "JAKO199215875841266",
          "title": "음성 인식 신경망을 위한 음성 파라키터들의 성능 비교",
          "abstract": "음성 인식에 신경망 모델을 적용하는 많은 연구들이 있었지만, 주된 관심은 음성인식에 적합한 구조와 학습 방법이었다.  그러나 음성인식에 신경망 모델을 적용한 시스템의 효율 향상은 모델 자체의 구조뿐 아니라, 신경망 모델의 입력으로 어떤 음성 파라미터를 사용하는가에 따라서도 큰 영향을 받는다.  본 논문은 기존 음성인식에 신경망 모델을 적용한 많은 연구들에서 사용한 음성 파라미터를 살펴보고, 대표적인 음성 파라미터 6개를 선정하여, 같은 데이타와 같은 신경망 모델 하에서 어떻게 성능이 달라지는지를 분석한다.  인식 실험에 있어서는 한국어 파열음 9개에 대한 8개 데이터 집합과 모음 8개에 대한 18개 데이터 집합을 음성 파라미터로 하고 신경망 모델은 순환 신경망 모델을 사용하여 노드의 수를 일정하게 한뒤 다양한 입력 파라미터의 성능을 비교하였다.  그 결과 선형 예측 계수로부터 얻어진 delta cepstrum의 음성 파라미터가 가장 좋은 성능을 보였으며 이때 인식률은 같은 학습 데이터에 대해 파열음 100.0%, 모음 95.1%이었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199215875841266&target=NART&cn=JAKO199215875841266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식 신경망을 위한 음성 파라키터들의 성능 비교 음성 인식에 신경망 모델을 적용하는 많은 연구들이 있었지만, 주된 관심은 음성인식에 적합한 구조와 학습 방법이었다.  그러나 음성인식에 신경망 모델을 적용한 시스템의 효율 향상은 모델 자체의 구조뿐 아니라, 신경망 모델의 입력으로 어떤 음성 파라미터를 사용하는가에 따라서도 큰 영향을 받는다.  본 논문은 기존 음성인식에 신경망 모델을 적용한 많은 연구들에서 사용한 음성 파라미터를 살펴보고, 대표적인 음성 파라미터 6개를 선정하여, 같은 데이타와 같은 신경망 모델 하에서 어떻게 성능이 달라지는지를 분석한다.  인식 실험에 있어서는 한국어 파열음 9개에 대한 8개 데이터 집합과 모음 8개에 대한 18개 데이터 집합을 음성 파라미터로 하고 신경망 모델은 순환 신경망 모델을 사용하여 노드의 수를 일정하게 한뒤 다양한 입력 파라미터의 성능을 비교하였다.  그 결과 선형 예측 계수로부터 얻어진 delta cepstrum의 음성 파라미터가 가장 좋은 성능을 보였으며 이때 인식률은 같은 학습 데이터에 대해 파열음 100.0%, 모음 95.1%이었다."
        },
        {
          "rank": 27,
          "score": 0.6079541444778442,
          "doc_id": "JAKO202117563196990",
          "title": "약물유전체학에서 약물반응 예측모형과 변수선택 방법",
          "abstract": "약물유전체학 연구의 주요 목표는 고차원의 유전 변수를 기반으로 개인의 약물 반응성을 예측하는 것이다. 변수의 개수가 많기 때문에 변수의 개수를 줄이기 위해서는 변수 선택이 필요하며, 선택된 변수들은 머신러닝 알고리즘을 사용하여 예측 모델을 구축하는데 사용된다. 본 연구에서는 400명의 뇌전증 환자의 차세대 염기서열 분석 데이터에 로지스틱 회귀, ReliefF, TurF, 랜덤 포레스트, LASSO의 조합과 같은 여러 가지 혼합 변수 선택 방법을 적용하였다. 선택된 변수들에 랜덤포레스트, 그래디언트 부스팅, 서포트벡터머신을 포함한 머신러닝 방법들을 적용했고 스태킹을 통해 앙상블 모형을 구축하였다. 본 연구의 결과는 랜덤포레스트와 ReliefF의 혼합 변수 선택 방법을 이용한 스태킹 모형이 다른 모형보다 더 좋은 성능을 보인다는 것을 보여주었다. 5-폴드 교차 검증을 기반으로 하여 적합한 최적 모형의 평균 검증 정확도는 0.727이고 평균 검증 AUC 값은 0.761로 나타났다. 또한, 동일한 변수를 사용할 때 스태킹 모델이 단일 머신러닝 예측 모델보다 성능이 우수한 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202117563196990&target=NART&cn=JAKO202117563196990",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "약물유전체학에서 약물반응 예측모형과 변수선택 방법 약물유전체학에서 약물반응 예측모형과 변수선택 방법 약물유전체학에서 약물반응 예측모형과 변수선택 방법 약물유전체학 연구의 주요 목표는 고차원의 유전 변수를 기반으로 개인의 약물 반응성을 예측하는 것이다. 변수의 개수가 많기 때문에 변수의 개수를 줄이기 위해서는 변수 선택이 필요하며, 선택된 변수들은 머신러닝 알고리즘을 사용하여 예측 모델을 구축하는데 사용된다. 본 연구에서는 400명의 뇌전증 환자의 차세대 염기서열 분석 데이터에 로지스틱 회귀, ReliefF, TurF, 랜덤 포레스트, LASSO의 조합과 같은 여러 가지 혼합 변수 선택 방법을 적용하였다. 선택된 변수들에 랜덤포레스트, 그래디언트 부스팅, 서포트벡터머신을 포함한 머신러닝 방법들을 적용했고 스태킹을 통해 앙상블 모형을 구축하였다. 본 연구의 결과는 랜덤포레스트와 ReliefF의 혼합 변수 선택 방법을 이용한 스태킹 모형이 다른 모형보다 더 좋은 성능을 보인다는 것을 보여주었다. 5-폴드 교차 검증을 기반으로 하여 적합한 최적 모형의 평균 검증 정확도는 0.727이고 평균 검증 AUC 값은 0.761로 나타났다. 또한, 동일한 변수를 사용할 때 스태킹 모델이 단일 머신러닝 예측 모델보다 성능이 우수한 것으로 나타났다."
        },
        {
          "rank": 28,
          "score": 0.6051317453384399,
          "doc_id": "ATN0037496660",
          "title": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교",
          "abstract": "Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037496660&target=NART&cn=ATN0037496660",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model."
        },
        {
          "rank": 29,
          "score": 0.6047903299331665,
          "doc_id": "ATN0037469990",
          "title": "특징 벡터의 effectiveness 요소 기반의 가중치 SVM 분류기 연구",
          "abstract": "In this paper, we proposed a new SVM model for classification based on analysis of weight and effectiveness of a certain feature vector. The standard SVM approach used a concept of minimization of geometric soft margin between input feature vector and classification function. Although the classical approach builds a classification function to pro-vide efficient decision boundary, this model is easily affected by outliers because each support vector cannot reflect structural or distributional properties of input data. To overcome defects of the classical SVM approach, a new model is derived from weighting scheme based on geometrical relation of nearby feature vectors and data distribution of input feature vectors. The proposed model was verified by using the effectiveness factor that helps to build classification function to provide effective global decision boundary. We evaluated our weighted SVM model using effectiveness factor for multi-class classification and achieved higher accuracy than classical SVM model using the MNIST dataset.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037469990&target=NART&cn=ATN0037469990",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "특징 벡터의 effectiveness 요소 기반의 가중치 SVM 분류기 연구 특징 벡터의 effectiveness 요소 기반의 가중치 SVM 분류기 연구 특징 벡터의 effectiveness 요소 기반의 가중치 SVM 분류기 연구 In this paper, we proposed a new SVM model for classification based on analysis of weight and effectiveness of a certain feature vector. The standard SVM approach used a concept of minimization of geometric soft margin between input feature vector and classification function. Although the classical approach builds a classification function to pro-vide efficient decision boundary, this model is easily affected by outliers because each support vector cannot reflect structural or distributional properties of input data. To overcome defects of the classical SVM approach, a new model is derived from weighting scheme based on geometrical relation of nearby feature vectors and data distribution of input feature vectors. The proposed model was verified by using the effectiveness factor that helps to build classification function to provide effective global decision boundary. We evaluated our weighted SVM model using effectiveness factor for multi-class classification and achieved higher accuracy than classical SVM model using the MNIST dataset."
        },
        {
          "rank": 30,
          "score": 0.6025445461273193,
          "doc_id": "JAKO202109651162667",
          "title": "신경망기법을 활용한 선박 가치평가 모델 개발",
          "abstract": "본 연구의 목적은 Neural Network Regression 모델을 활용하여 선박의 가치평가 모델을 개발하는 것이다. 가치평가의 대상은 중고 VLCC선이며, 선행연구를 통해 선박의 가치 변화를 유발하는 주요 요인들을 선별하여 변수를 설정하고, 2000년 1월부터 2020년 8월까지의 해당 데이터를 확보하였다. 변수의 안정성을 판단하기 위해 다중 공선성 검사를 수행하여 최종적으로 6개의 독립변수와 1개의 종속변수를 선정하고 연구 구조를 설계하였다. 이를 바탕으로 Linear Regression, Neural Network Regression, Random Forest Algorithm을 활용하여 총 9개의 시뮬레이션 모델을 설계하였다. 또한 각 모델간의 비교검증을 통해 평가결과의 정확성을 제고시켰다. 평가 결과, VLCC실제값과의 비교를 통해 2층으로 구성된 Hidden Layer의 Neural Network Regression 모델이 가장 정확도가 높은 것으로 나타났다. 본 연구의 시사점은 첫째, 기존 정형화된 평가기법에서 벗어나 기계학습기반 모델을 선박가치평가에 적용하였다는 점이다. 둘째, 해운시장 변화요인을 동태적 관점에서 분석하고 예측함으로써 연구결과의 객관성을 제고시켰다고 할 수 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202109651162667&target=NART&cn=JAKO202109651162667",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "신경망기법을 활용한 선박 가치평가 모델 개발 신경망기법을 활용한 선박 가치평가 모델 개발 신경망기법을 활용한 선박 가치평가 모델 개발 본 연구의 목적은 Neural Network Regression 모델을 활용하여 선박의 가치평가 모델을 개발하는 것이다. 가치평가의 대상은 중고 VLCC선이며, 선행연구를 통해 선박의 가치 변화를 유발하는 주요 요인들을 선별하여 변수를 설정하고, 2000년 1월부터 2020년 8월까지의 해당 데이터를 확보하였다. 변수의 안정성을 판단하기 위해 다중 공선성 검사를 수행하여 최종적으로 6개의 독립변수와 1개의 종속변수를 선정하고 연구 구조를 설계하였다. 이를 바탕으로 Linear Regression, Neural Network Regression, Random Forest Algorithm을 활용하여 총 9개의 시뮬레이션 모델을 설계하였다. 또한 각 모델간의 비교검증을 통해 평가결과의 정확성을 제고시켰다. 평가 결과, VLCC실제값과의 비교를 통해 2층으로 구성된 Hidden Layer의 Neural Network Regression 모델이 가장 정확도가 높은 것으로 나타났다. 본 연구의 시사점은 첫째, 기존 정형화된 평가기법에서 벗어나 기계학습기반 모델을 선박가치평가에 적용하였다는 점이다. 둘째, 해운시장 변화요인을 동태적 관점에서 분석하고 예측함으로써 연구결과의 객관성을 제고시켰다고 할 수 있다."
        },
        {
          "rank": 31,
          "score": 0.6019911766052246,
          "doc_id": "JAKO201104642095152",
          "title": "입자 군집 최적화를 이용한 FCM 기반 퍼지 모델의 동정 방법론",
          "abstract": "In this study, we introduce a identification methodology for FCM-based fuzzy model. The two underlying design mechanisms of such networks involve Fuzzy C-Means (FCM) clustering method and Particle Swarm Optimization(PSO). The proposed algorithm is based on FCM clustering method for efficient processing of data and the optimization of model was carried out using PSO. The premise part of fuzzy rules does not construct as any fixed membership functions such as triangular, gaussian, ellipsoidal because we build up the premise part of fuzzy rules using FCM. As a result, the proposed model can lead to the compact architecture of network. In this study, as the consequence part of fuzzy rules, we are able to use four types of polynomials such as simplified, linear, quadratic, modified quadratic. In addition, a Weighted Least Square Estimation to estimate the coefficients of polynomials, which are the consequent parts of fuzzy model, can decouple each fuzzy rule from the other fuzzy rules. Therefore, a local learning capability and an interpretability of the proposed fuzzy model are improved. Also, the parameters of the proposed fuzzy model such as a fuzzification coefficient of FCM clustering, the number of clusters of FCM clustering, and the polynomial type of the consequent part of fuzzy rules are adjusted using PSO. The proposed model is illustrated with the use of Automobile Miles per Gallon(MPG) and Boston housing called Machine Learning dataset. A comparative analysis reveals that the proposed FCM-based fuzzy model exhibits higher accuracy and superb predictive capability in comparison to some previous models available in the literature.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201104642095152&target=NART&cn=JAKO201104642095152",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "입자 군집 최적화를 이용한 FCM 기반 퍼지 모델의 동정 방법론 입자 군집 최적화를 이용한 FCM 기반 퍼지 모델의 동정 방법론 입자 군집 최적화를 이용한 FCM 기반 퍼지 모델의 동정 방법론 In this study, we introduce a identification methodology for FCM-based fuzzy model. The two underlying design mechanisms of such networks involve Fuzzy C-Means (FCM) clustering method and Particle Swarm Optimization(PSO). The proposed algorithm is based on FCM clustering method for efficient processing of data and the optimization of model was carried out using PSO. The premise part of fuzzy rules does not construct as any fixed membership functions such as triangular, gaussian, ellipsoidal because we build up the premise part of fuzzy rules using FCM. As a result, the proposed model can lead to the compact architecture of network. In this study, as the consequence part of fuzzy rules, we are able to use four types of polynomials such as simplified, linear, quadratic, modified quadratic. In addition, a Weighted Least Square Estimation to estimate the coefficients of polynomials, which are the consequent parts of fuzzy model, can decouple each fuzzy rule from the other fuzzy rules. Therefore, a local learning capability and an interpretability of the proposed fuzzy model are improved. Also, the parameters of the proposed fuzzy model such as a fuzzification coefficient of FCM clustering, the number of clusters of FCM clustering, and the polynomial type of the consequent part of fuzzy rules are adjusted using PSO. The proposed model is illustrated with the use of Automobile Miles per Gallon(MPG) and Boston housing called Machine Learning dataset. A comparative analysis reveals that the proposed FCM-based fuzzy model exhibits higher accuracy and superb predictive capability in comparison to some previous models available in the literature."
        },
        {
          "rank": 32,
          "score": 0.6009407043457031,
          "doc_id": "JAKO202108848920380",
          "title": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구",
          "abstract": "The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202108848920380&target=NART&cn=JAKO202108848920380",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity."
        },
        {
          "rank": 33,
          "score": 0.6001458168029785,
          "doc_id": "NART118745898",
          "title": "An Email Spam Filtering Model Using Ensemble of Machine Learning Techniques",
          "abstract": "<P>The growth of spam emails is on the increase responsible for larger portions of the global email traffics. Aside the annoyance and the time wasted sifting through the unwanted messages; spam emails can also cause immeasurable harms through malicious software capable of damaging systems and compromising confidential information. The risks of filtering spam emails is that sometimes, legitimate mails are marked as spam, yet the results of not filtering spam are the constant flood of spam clogs on networks that adversely impacts users inboxes while draining valuable resources on the networks such as bandwidth and storage capacity, productivity loss and interfere with the expedient delivery of legitimate emails. Several researchers had worked on the design of models for spam email filtering using different techniques, however the detection accuracy of these models have also become subject of discussions. This study developed spam email filtering model using Ensemble of Decision Tree, Support Vector Machine and Multilayer Perceptron (DT-SVM-MLP) technique as a solution approach to solving issues of low spam emails detection accuracy. The ensemble model was trained using forward propagation training technique and the performance was evaluated using five performance metrics of Accuracy, False Positive (FP) Rate, Precision, Recall and F-Measure.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART118745898&target=NART&cn=NART118745898",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "An Email Spam Filtering Model Using Ensemble of Machine Learning Techniques An Email Spam Filtering Model Using Ensemble of Machine Learning Techniques An Email Spam Filtering Model Using Ensemble of Machine Learning Techniques <P>The growth of spam emails is on the increase responsible for larger portions of the global email traffics. Aside the annoyance and the time wasted sifting through the unwanted messages; spam emails can also cause immeasurable harms through malicious software capable of damaging systems and compromising confidential information. The risks of filtering spam emails is that sometimes, legitimate mails are marked as spam, yet the results of not filtering spam are the constant flood of spam clogs on networks that adversely impacts users inboxes while draining valuable resources on the networks such as bandwidth and storage capacity, productivity loss and interfere with the expedient delivery of legitimate emails. Several researchers had worked on the design of models for spam email filtering using different techniques, however the detection accuracy of these models have also become subject of discussions. This study developed spam email filtering model using Ensemble of Decision Tree, Support Vector Machine and Multilayer Perceptron (DT-SVM-MLP) technique as a solution approach to solving issues of low spam emails detection accuracy. The ensemble model was trained using forward propagation training technique and the performance was evaluated using five performance metrics of Accuracy, False Positive (FP) Rate, Precision, Recall and F-Measure.</P>"
        },
        {
          "rank": 34,
          "score": 0.5999797582626343,
          "doc_id": "NART131648944",
          "title": "Comprehensive hepatotoxicity prediction: ensemble model integrating machine learning and deep learning",
          "abstract": "<P><B>Background</B></P><P>Chemicals may lead to acute liver injuries, posing a serious threat to human health. Achieving the precise safety profile of a compound is challenging due to the complex and expensive testing procedures. In silico approaches will aid in identifying the potential risk of drug candidates in the initial stage of drug development and thus mitigating the developmental cost.</P><P><B>Methods</B></P><P>In current studies, QSAR models were developed for hepatotoxicity predictions using the ensemble strategy to integrate machine learning (ML) and deep learning (DL) algorithms using various molecular features. A large dataset of 2588 chemicals and drugs was randomly divided into training (80%) and test (20%) sets, followed by the training of individual base models using diverse machine learning or deep learning based on three different kinds of descriptors and fingerprints. Feature selection approaches were employed to proceed with model optimizations based on the model performance. Hybrid ensemble approaches were further utilized to determine the method with the best performance.</P><P><B>Results</B></P><P>The voting ensemble classifier emerged as the optimal model, achieving an excellent prediction accuracy of 80.26%, AUC of 82.84%, and recall of over 93% followed by bagging and stacking ensemble classifiers method. The model was further verified by an external test set, internal 10-fold cross-validation, and rigorous benchmark training, exhibiting much better reliability than the published models.</P><P><B>Conclusion</B></P><P>The proposed ensemble model offers a dependable assessment with a good performance for the prediction regarding the risk of chemicals and drugs to induce liver damage.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART131648944&target=NART&cn=NART131648944",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Comprehensive hepatotoxicity prediction: ensemble model integrating machine learning and deep learning Comprehensive hepatotoxicity prediction: ensemble model integrating machine learning and deep learning Comprehensive hepatotoxicity prediction: ensemble model integrating machine learning and deep learning <P><B>Background</B></P><P>Chemicals may lead to acute liver injuries, posing a serious threat to human health. Achieving the precise safety profile of a compound is challenging due to the complex and expensive testing procedures. In silico approaches will aid in identifying the potential risk of drug candidates in the initial stage of drug development and thus mitigating the developmental cost.</P><P><B>Methods</B></P><P>In current studies, QSAR models were developed for hepatotoxicity predictions using the ensemble strategy to integrate machine learning (ML) and deep learning (DL) algorithms using various molecular features. A large dataset of 2588 chemicals and drugs was randomly divided into training (80%) and test (20%) sets, followed by the training of individual base models using diverse machine learning or deep learning based on three different kinds of descriptors and fingerprints. Feature selection approaches were employed to proceed with model optimizations based on the model performance. Hybrid ensemble approaches were further utilized to determine the method with the best performance.</P><P><B>Results</B></P><P>The voting ensemble classifier emerged as the optimal model, achieving an excellent prediction accuracy of 80.26%, AUC of 82.84%, and recall of over 93% followed by bagging and stacking ensemble classifiers method. The model was further verified by an external test set, internal 10-fold cross-validation, and rigorous benchmark training, exhibiting much better reliability than the published models.</P><P><B>Conclusion</B></P><P>The proposed ensemble model offers a dependable assessment with a good performance for the prediction regarding the risk of chemicals and drugs to induce liver damage.</P>"
        },
        {
          "rank": 35,
          "score": 0.5998318195343018,
          "doc_id": "NART111572458",
          "title": "광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교",
          "abstract": "This study examines the classification characteristics of various machine learning classifiers for pain assessment using photoplethysmogram. The presence of pain was assessed using waveform characteristics derived from photoplethysmogram obtained from 73 patients before and after surgery. Classification performance was evaluated using logistic regression, random forest, multi-layer perceptron, and 1-D convolutional neural network, and was validated with nested k-fold cross validation. As a result, pain classification accuracy was highest in order of logistic regression, convolutional neural network, multi-layer perceptron, and random forest classifier. In addition, logistic regression, random forest, multi-layer perceptron, and convolutional neural network were shown to be robust to overfitting in order.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART111572458&target=NART&cn=NART111572458",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교 광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교 광용적맥파 및 머신러닝 기반 통증 평가 분류기 성능 비교 This study examines the classification characteristics of various machine learning classifiers for pain assessment using photoplethysmogram. The presence of pain was assessed using waveform characteristics derived from photoplethysmogram obtained from 73 patients before and after surgery. Classification performance was evaluated using logistic regression, random forest, multi-layer perceptron, and 1-D convolutional neural network, and was validated with nested k-fold cross validation. As a result, pain classification accuracy was highest in order of logistic regression, convolutional neural network, multi-layer perceptron, and random forest classifier. In addition, logistic regression, random forest, multi-layer perceptron, and convolutional neural network were shown to be robust to overfitting in order."
        },
        {
          "rank": 36,
          "score": 0.5997373461723328,
          "doc_id": "JAKO202302557624224",
          "title": "평점 예측 모델 개발을 위한 관광지 만족도 정량 지수 구축: 제주도 관광지 리뷰를 중심으로",
          "abstract": "코로나19 팬데믹 이후 관광 산업이 회복되면서 많은 관광객들이 다양한 플랫폼을 활용하고 리뷰를 남기고 있지만, 대량의 데이터 속에서 유용한 정보를 찾기 어려워 아직도 여행지 선정 과정에서 많은 시간과 비용이 낭비되고 있다. 이에 따라 많은 연구들이 진행되고 있지만, 평점이 없거나 플랫폼별로 다른 형태의 평점 제공으로 인해 연구에 한계를 가지고 있으며, 평점과 리뷰 내용이 일치하지 않는 경우도 있어 추천 모델 구축에 어려움을 주고 있다. 본 연구에서는 이러한 문제를 해결하기 위해 7,104개의 제주도 지역 관광지 리뷰를 활용하여 제주도에 특화된 관광지 만족도 정량 지수를 개발하고 이를 활용하여 '평점 예측 모델'을 구축하였다. 모델의 성능을 확인하기 위해 실험 데이터 700건의 평점을 본 연구에서 개발된 모델과 LSTM을 활용하여 예측 하였으며, 제안된 모델이 LSTM 보다 약 4.67% 높은 73.87%의 가중 정확도로 성능이 더 우수한 것을 확인하였다. 본 연구의 결과를 통해 평점과 리뷰 내용 사이의 불일치 문제를 해결하고, 평점이 없는 리뷰나 다양한 형태의 평점을 정형할 수 있으며, 다른 도메인에 적용하여 여행의 모든 분야에서 신뢰할 수 있는 평점 지표를 제공할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202302557624224&target=NART&cn=JAKO202302557624224",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "평점 예측 모델 개발을 위한 관광지 만족도 정량 지수 구축: 제주도 관광지 리뷰를 중심으로 평점 예측 모델 개발을 위한 관광지 만족도 정량 지수 구축: 제주도 관광지 리뷰를 중심으로 평점 예측 모델 개발을 위한 관광지 만족도 정량 지수 구축: 제주도 관광지 리뷰를 중심으로 코로나19 팬데믹 이후 관광 산업이 회복되면서 많은 관광객들이 다양한 플랫폼을 활용하고 리뷰를 남기고 있지만, 대량의 데이터 속에서 유용한 정보를 찾기 어려워 아직도 여행지 선정 과정에서 많은 시간과 비용이 낭비되고 있다. 이에 따라 많은 연구들이 진행되고 있지만, 평점이 없거나 플랫폼별로 다른 형태의 평점 제공으로 인해 연구에 한계를 가지고 있으며, 평점과 리뷰 내용이 일치하지 않는 경우도 있어 추천 모델 구축에 어려움을 주고 있다. 본 연구에서는 이러한 문제를 해결하기 위해 7,104개의 제주도 지역 관광지 리뷰를 활용하여 제주도에 특화된 관광지 만족도 정량 지수를 개발하고 이를 활용하여 '평점 예측 모델'을 구축하였다. 모델의 성능을 확인하기 위해 실험 데이터 700건의 평점을 본 연구에서 개발된 모델과 LSTM을 활용하여 예측 하였으며, 제안된 모델이 LSTM 보다 약 4.67% 높은 73.87%의 가중 정확도로 성능이 더 우수한 것을 확인하였다. 본 연구의 결과를 통해 평점과 리뷰 내용 사이의 불일치 문제를 해결하고, 평점이 없는 리뷰나 다양한 형태의 평점을 정형할 수 있으며, 다른 도메인에 적용하여 여행의 모든 분야에서 신뢰할 수 있는 평점 지표를 제공할 수 있을 것으로 기대된다."
        },
        {
          "rank": 37,
          "score": 0.5985884666442871,
          "doc_id": "NART118947969",
          "title": "Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction",
          "abstract": "<P>Recent growth in crop genomic and trait data have opened opportunities for the application of novel approaches to accelerate crop improvement. Machine learning and deep learning are at the forefront of prediction-based data analysis. However, few approaches for genotype to phenotype prediction compare machine learning with deep learning and further interpret the models that support the predictions. This study uses genome wide molecular markers and traits across 1110 soybean individuals to develop accurate prediction models. For 13/14 sets of predictions, XGBoost or random forest outperformed deep learning models in prediction performance. Top ranked SNPs by F-score were identified from XGBoost, and with further investigation found overlap with significantly associated loci identified from GWAS and previous literature. Feature importance rankings were used to reduce marker input by up to 90%, and subsequent models maintained or improved their prediction performance. These findings support interpretable machine learning as an approach for genomic based prediction of traits in soybean and other crops.</P><P><B>Supplementary Information</B></P><P>The online version contains supplementary material available at 10.1186/s12870-022-03559-z.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART118947969&target=NART&cn=NART118947969",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction <P>Recent growth in crop genomic and trait data have opened opportunities for the application of novel approaches to accelerate crop improvement. Machine learning and deep learning are at the forefront of prediction-based data analysis. However, few approaches for genotype to phenotype prediction compare machine learning with deep learning and further interpret the models that support the predictions. This study uses genome wide molecular markers and traits across 1110 soybean individuals to develop accurate prediction models. For 13/14 sets of predictions, XGBoost or random forest outperformed deep learning models in prediction performance. Top ranked SNPs by F-score were identified from XGBoost, and with further investigation found overlap with significantly associated loci identified from GWAS and previous literature. Feature importance rankings were used to reduce marker input by up to 90%, and subsequent models maintained or improved their prediction performance. These findings support interpretable machine learning as an approach for genomic based prediction of traits in soybean and other crops.</P><P><B>Supplementary Information</B></P><P>The online version contains supplementary material available at 10.1186/s12870-022-03559-z.</P>"
        },
        {
          "rank": 38,
          "score": 0.5983573794364929,
          "doc_id": "NPAP07942137",
          "title": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP07942137&target=NART&cn=NPAP07942137",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 음성 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용. 데이터에 대하여 Elman망예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 99.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 39,
          "score": 0.5979056358337402,
          "doc_id": "NART113695778",
          "title": "Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning",
          "abstract": "<P>Due to severe climate change impact on electricity consumption, as well as new trends in smart grids (such as the use of renewable resources and the advent of prosumers and energy commons), medium-term and long-term electricity load forecasting has become a crucial need. Such forecasts are necessary to support the plans and decisions related to the capacity evaluation of centralized and decentralized power generation systems, demand response strategies, and controlling the operation. To address this problem, the main objective of this study is to develop and compare precise district level models for predicting the electrical load demand based on machine learning techniques including support vector machine (SVM) and Random Forest (RF), and deep learning methods such as non-linear auto-regressive exogenous (NARX) neural network and recurrent neural networks (Long Short-Term Memory-LSTM). A dataset including nine years of historical load demand for Bruce County, Ontario, Canada, fused with the climatic information (temperature and wind speed) are used to train the models after completing the preprocessing and cleaning stages. The results show that by employing deep learning, the model could predict the load demand more accurately than SVM and RF, with an R-Squared of about 0.93-0.96 and Mean Absolute Percentage Error (MAPE) of about 4-10%. The model can be used not only by the municipalities as well as utility companies and power distributors in the management and expansion of electricity grids; but also by the households to make decisions on the adoption of home- and district-scale renewable energy technologies.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART113695778&target=NART&cn=NART113695778",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning Medium-Term Regional Electricity Load Forecasting through Machine Learning and Deep Learning <P>Due to severe climate change impact on electricity consumption, as well as new trends in smart grids (such as the use of renewable resources and the advent of prosumers and energy commons), medium-term and long-term electricity load forecasting has become a crucial need. Such forecasts are necessary to support the plans and decisions related to the capacity evaluation of centralized and decentralized power generation systems, demand response strategies, and controlling the operation. To address this problem, the main objective of this study is to develop and compare precise district level models for predicting the electrical load demand based on machine learning techniques including support vector machine (SVM) and Random Forest (RF), and deep learning methods such as non-linear auto-regressive exogenous (NARX) neural network and recurrent neural networks (Long Short-Term Memory-LSTM). A dataset including nine years of historical load demand for Bruce County, Ontario, Canada, fused with the climatic information (temperature and wind speed) are used to train the models after completing the preprocessing and cleaning stages. The results show that by employing deep learning, the model could predict the load demand more accurately than SVM and RF, with an R-Squared of about 0.93-0.96 and Mean Absolute Percentage Error (MAPE) of about 4-10%. The model can be used not only by the municipalities as well as utility companies and power distributors in the management and expansion of electricity grids; but also by the households to make decisions on the adoption of home- and district-scale renewable energy technologies.</P>"
        },
        {
          "rank": 40,
          "score": 0.5974851846694946,
          "doc_id": "JAKO201911338887557",
          "title": "잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법",
          "abstract": "본 논문에서는 잡음 환경에서 효과적인 음성 인식을 위하여 DNN(Deep Neural Network) 기반의 잡음 오염 함수 예측을 이용한 음향 모델 적응 기법을 제안한다. 깨끗한 음성과 잡음 정보를 입력으로 하고 오염된 음성에 대한 특징 벡터를 출력으로 하는 DNN을 학습하여 비선형 관계를 갖는 잡음 오염 함수를 예측한다. 예측된 잡음 오염 함수를 음향모델의 평균 벡터에 적용하여 잡음 환경에 적응된 음향 모델을 생성한다. Aurora 2.0 데이터를 이용한 음성 인식 성능 평가에서 본 논문에서 제안한 모델 적응 기법이 기존의 전처리, 모델 적응 기법에 비해 일치, 불일치 잡음 환경에서 모두 평균적으로 우수한 성능을 나타낸다. 특히 불일치 잡음 환경에서 평균 오류율이 15.87 %의 상대 향상률을 나타낸다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201911338887557&target=NART&cn=JAKO201911338887557",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 본 논문에서는 잡음 환경에서 효과적인 음성 인식을 위하여 DNN(Deep Neural Network) 기반의 잡음 오염 함수 예측을 이용한 음향 모델 적응 기법을 제안한다. 깨끗한 음성과 잡음 정보를 입력으로 하고 오염된 음성에 대한 특징 벡터를 출력으로 하는 DNN을 학습하여 비선형 관계를 갖는 잡음 오염 함수를 예측한다. 예측된 잡음 오염 함수를 음향모델의 평균 벡터에 적용하여 잡음 환경에 적응된 음향 모델을 생성한다. Aurora 2.0 데이터를 이용한 음성 인식 성능 평가에서 본 논문에서 제안한 모델 적응 기법이 기존의 전처리, 모델 적응 기법에 비해 일치, 불일치 잡음 환경에서 모두 평균적으로 우수한 성능을 나타낸다. 특히 불일치 잡음 환경에서 평균 오류율이 15.87 %의 상대 향상률을 나타낸다."
        },
        {
          "rank": 41,
          "score": 0.5973669290542603,
          "doc_id": "NART103136065",
          "title": "Portfolio optimization with return prediction using deep learning and machine learning",
          "abstract": "<P><B>Abstract</B></P>  <P>Integrating return prediction of traditional time series models in portfolio formation can improve the performance of original portfolio optimization model. Since machine learning and deep learning models have shown overwhelming superiority than time series models, this paper combines return prediction in portfolio formation with two machine learning models, i.e., random forest (RF) and support vector regression (SVR), and three deep learning models, i.e., LSTM neural network, deep multilayer perceptron (DMLP) and convolutional neural network. To be specific, this paper first applies these prediction models for stock preselection before portfolio formation. Then, this paper incorporates their predictive results in advancing mean&ndash;variance (MV) and omega portfolio optimization models. In order to present the superiority of these models, portfolio models with autoregressive integrated moving average&rsquo;s return prediction are used as benchmarks. Evaluation is based on historical data of 9 years from 2007 to 2015 of component stocks of China securities 100 index. Experimental results show that MV and omega models with RF return prediction, i.e., RF+MVF and RF+OF, outperform the other models. Further, RF+MVF is superior to RF+OF. Due to the high turnover of these two models, this paper discusses their performance after deducting the transaction fee cased by turnover. Experiments present that RF+MVF still performs the best among MVF models and omega model with SVR prediction (SVR+OF) performs the best among OF models. Moreover, RF+MVF performs better than SVR+OF and high turnover erodes nearly half of their total returns especially for RF+OF and RF+MVF. Therefore, this paper recommends investors to build MVF with RF return prediction for daily trading investment.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Compares the performance of machine learning and deep learning in stock preselection. </LI> <LI>  Combining return prediction of machine learning and deep learning in portfolio formation. </LI> <LI>  Emphasis on advancing portfolio optimization with return prediction. </LI> <LI>  Advanced mean&ndash;variance model with random forest forecasts performs the best. </LI> </UL> </P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART103136065&target=NART&cn=NART103136065",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Portfolio optimization with return prediction using deep learning and machine learning Portfolio optimization with return prediction using deep learning and machine learning Portfolio optimization with return prediction using deep learning and machine learning <P><B>Abstract</B></P>  <P>Integrating return prediction of traditional time series models in portfolio formation can improve the performance of original portfolio optimization model. Since machine learning and deep learning models have shown overwhelming superiority than time series models, this paper combines return prediction in portfolio formation with two machine learning models, i.e., random forest (RF) and support vector regression (SVR), and three deep learning models, i.e., LSTM neural network, deep multilayer perceptron (DMLP) and convolutional neural network. To be specific, this paper first applies these prediction models for stock preselection before portfolio formation. Then, this paper incorporates their predictive results in advancing mean&ndash;variance (MV) and omega portfolio optimization models. In order to present the superiority of these models, portfolio models with autoregressive integrated moving average&rsquo;s return prediction are used as benchmarks. Evaluation is based on historical data of 9 years from 2007 to 2015 of component stocks of China securities 100 index. Experimental results show that MV and omega models with RF return prediction, i.e., RF+MVF and RF+OF, outperform the other models. Further, RF+MVF is superior to RF+OF. Due to the high turnover of these two models, this paper discusses their performance after deducting the transaction fee cased by turnover. Experiments present that RF+MVF still performs the best among MVF models and omega model with SVR prediction (SVR+OF) performs the best among OF models. Moreover, RF+MVF performs better than SVR+OF and high turnover erodes nearly half of their total returns especially for RF+OF and RF+MVF. Therefore, this paper recommends investors to build MVF with RF return prediction for daily trading investment.</P>   <P><B>Highlights</B></P>  <P> <UL> <LI>  Compares the performance of machine learning and deep learning in stock preselection. </LI> <LI>  Combining return prediction of machine learning and deep learning in portfolio formation. </LI> <LI>  Emphasis on advancing portfolio optimization with return prediction. </LI> <LI>  Advanced mean&ndash;variance model with random forest forecasts performs the best. </LI> </UL> </P>"
        },
        {
          "rank": 42,
          "score": 0.5970866680145264,
          "doc_id": "DIKO0017198883",
          "title": "Multi-Modal Review Helpfulness Prediction Considering the Consistency Between Review Text and Rating",
          "abstract": "전자상거래 환경에서 온라인 리뷰는 소비자들의 구매 의사결정 과정에서 핵심적인 역할을 수행하며, 방대한 리뷰 중에서 유용한 리뷰를 효율적으로 탐색하는 것은 소비자와 전자상거래 플랫폼 모두에게 중요한 과제가 되고 있다. 기존 연구들은 리뷰 텍스트와 평점 간의 일관성을 분석하여 유용성을 예측하려는 다양한 시도를 해왔으며, 이러한 연구는 소비자 신뢰도를 높이고 유용성을 향상시키는 데 기여해왔다. 그러나 시각적 정보인 리뷰 이미지가 제공하는 보완적 데이터를 충분히 반영하지 못한 한계가 존재하며, 데이터 일관성 여부에 따른 예측 모델의 성능 차이를 체계적으로 분석한 연구는 매우 부족한 상황이다. 특히, 데이터의 일관성 여부는 리뷰 유용성 예측의 정확도와 신뢰성에 중요한 영향을 미칠 수 있음에도 불구하고, 이를 다룬 실증적 연구는 거의 이루어지지 않았다.&amp;#xD; 본 연구에서는 리뷰 텍스트와 평점의 일관성을 학습하고, 이를 이미지 정보와 결합하여 리뷰 유용성을 예측할 수 있는 새로운 모델인 MRHP-CCR(Multimodal Review Helpfulness Prediction Considering the Consistency of Review)을 제안한다. 본 모델은 사전학습된 RoBERTa와 VGG-16을 활용하여 텍스트와 이미지에서 각각의 특징을 추출하며, Co-attention 메커니즘을 통해 텍스트와 평점 간의 상호작용을 효과적으로 학습하여 데이터의 일관성을 반영한다. 이를 통해 리뷰 텍스트와 평점 간의 상호작용뿐만 아니라 시각적 특징이 유용성 예측 성능을 향상시키는 데 어떻게 기여하는지를 검증한다. 제안된 모델은 다양한 데이터 일관성 조건에서도 높은 예측 성능을 보여, 전자상거래 환경에서 신뢰성 있는 리뷰 유용성 평가를 가능하게 한다.&amp;#xD; 본 연구는 리뷰 텍스트, 평점, 이미지 간의 통합적 상호작용이 유용성 예측에서 중요한 역할을 한다는 점을 강조하며, 데이터 일관성이 모델 성능에 미치는 영향을 체계적으로 검토하였다. 이를 통해 전자상거래 플랫폼에서 소비자들의 구매 결정을 효과적으로 지원할 수 있는 유용한 정보를 제공하며, 데이터 일관성과 멀티모달 정보가 결합된 환경에서의 예측 성능 향상 가능성을 입증하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0017198883&target=NART&cn=DIKO0017198883",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Multi-Modal Review Helpfulness Prediction Considering the Consistency Between Review Text and Rating Multi-Modal Review Helpfulness Prediction Considering the Consistency Between Review Text and Rating Multi-Modal Review Helpfulness Prediction Considering the Consistency Between Review Text and Rating 전자상거래 환경에서 온라인 리뷰는 소비자들의 구매 의사결정 과정에서 핵심적인 역할을 수행하며, 방대한 리뷰 중에서 유용한 리뷰를 효율적으로 탐색하는 것은 소비자와 전자상거래 플랫폼 모두에게 중요한 과제가 되고 있다. 기존 연구들은 리뷰 텍스트와 평점 간의 일관성을 분석하여 유용성을 예측하려는 다양한 시도를 해왔으며, 이러한 연구는 소비자 신뢰도를 높이고 유용성을 향상시키는 데 기여해왔다. 그러나 시각적 정보인 리뷰 이미지가 제공하는 보완적 데이터를 충분히 반영하지 못한 한계가 존재하며, 데이터 일관성 여부에 따른 예측 모델의 성능 차이를 체계적으로 분석한 연구는 매우 부족한 상황이다. 특히, 데이터의 일관성 여부는 리뷰 유용성 예측의 정확도와 신뢰성에 중요한 영향을 미칠 수 있음에도 불구하고, 이를 다룬 실증적 연구는 거의 이루어지지 않았다.&amp;#xD; 본 연구에서는 리뷰 텍스트와 평점의 일관성을 학습하고, 이를 이미지 정보와 결합하여 리뷰 유용성을 예측할 수 있는 새로운 모델인 MRHP-CCR(Multimodal Review Helpfulness Prediction Considering the Consistency of Review)을 제안한다. 본 모델은 사전학습된 RoBERTa와 VGG-16을 활용하여 텍스트와 이미지에서 각각의 특징을 추출하며, Co-attention 메커니즘을 통해 텍스트와 평점 간의 상호작용을 효과적으로 학습하여 데이터의 일관성을 반영한다. 이를 통해 리뷰 텍스트와 평점 간의 상호작용뿐만 아니라 시각적 특징이 유용성 예측 성능을 향상시키는 데 어떻게 기여하는지를 검증한다. 제안된 모델은 다양한 데이터 일관성 조건에서도 높은 예측 성능을 보여, 전자상거래 환경에서 신뢰성 있는 리뷰 유용성 평가를 가능하게 한다.&amp;#xD; 본 연구는 리뷰 텍스트, 평점, 이미지 간의 통합적 상호작용이 유용성 예측에서 중요한 역할을 한다는 점을 강조하며, 데이터 일관성이 모델 성능에 미치는 영향을 체계적으로 검토하였다. 이를 통해 전자상거래 플랫폼에서 소비자들의 구매 결정을 효과적으로 지원할 수 있는 유용한 정보를 제공하며, 데이터 일관성과 멀티모달 정보가 결합된 환경에서의 예측 성능 향상 가능성을 입증하였다."
        },
        {
          "rank": 43,
          "score": 0.5959875583648682,
          "doc_id": "ATN0052776138",
          "title": "머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구",
          "abstract": "첨단 무기체계의 도입에 따른 운영유지비 증가와 수리부속 조달환경의 악화로 인해, 정밀한 수요예측의 중요성이 더욱 강조되고 있다. 본 연구는 공군 수리부속의 수요가 소량이며 발생 간격이 불규칙한 특성으로 인해 예측이 어렵다는 점에 착안하여, 기존 통계기반 예측기법의 한계를 극복하고자 머신러닝 및 딥러닝 기반 예측모형을 적용하였다. 국방물자관리체계로 부터 수집한 약 37만 건의 수요 데이터를 유형별(Regular, Intermittent, Erratic, Lumpy)로 분류한 후, Random Forest, XG-Boost, LightGBM, LSTM, N-Beats 5가지 예측모델을 구축하고 성능을 비교하였다. 분석 결과, XG-Boost 모델이 가장 우수한 정확도(79.13%)를 기록하였으며, 그리드 서치를 통한 매개변수 최적화 결과, 품목 기준 최대 81.28%의 예측 정확도를 달성하였다. 본 연구를 통해 세부 품목별 분류 기준 정립, 최적 모델 적용 및 매개변수 튜닝 효율화 등을 통해 공군 수리부속 수요예측의 정확도를 실질적으로 향상시킬 수 있음을 실증적으로 확인하였으며, 이는 대규모 군수 데이터셋에 대한 정량적 분석과 실용적인 예측모형 적용을 통해 현장 활용 가능성이 높은 모델을 제시하였다는 점에서 기존 연구와 차별성을 지닌다. 본 연구의 결과는 향후 공군 및 국방 군수 시스템 전반의 운영 효율성 제고와 자원관리 혁신에 중요한 토대를 제공할 수 있을 것으로 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0052776138&target=NART&cn=ATN0052776138",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 첨단 무기체계의 도입에 따른 운영유지비 증가와 수리부속 조달환경의 악화로 인해, 정밀한 수요예측의 중요성이 더욱 강조되고 있다. 본 연구는 공군 수리부속의 수요가 소량이며 발생 간격이 불규칙한 특성으로 인해 예측이 어렵다는 점에 착안하여, 기존 통계기반 예측기법의 한계를 극복하고자 머신러닝 및 딥러닝 기반 예측모형을 적용하였다. 국방물자관리체계로 부터 수집한 약 37만 건의 수요 데이터를 유형별(Regular, Intermittent, Erratic, Lumpy)로 분류한 후, Random Forest, XG-Boost, LightGBM, LSTM, N-Beats 5가지 예측모델을 구축하고 성능을 비교하였다. 분석 결과, XG-Boost 모델이 가장 우수한 정확도(79.13%)를 기록하였으며, 그리드 서치를 통한 매개변수 최적화 결과, 품목 기준 최대 81.28%의 예측 정확도를 달성하였다. 본 연구를 통해 세부 품목별 분류 기준 정립, 최적 모델 적용 및 매개변수 튜닝 효율화 등을 통해 공군 수리부속 수요예측의 정확도를 실질적으로 향상시킬 수 있음을 실증적으로 확인하였으며, 이는 대규모 군수 데이터셋에 대한 정량적 분석과 실용적인 예측모형 적용을 통해 현장 활용 가능성이 높은 모델을 제시하였다는 점에서 기존 연구와 차별성을 지닌다. 본 연구의 결과는 향후 공군 및 국방 군수 시스템 전반의 운영 효율성 제고와 자원관리 혁신에 중요한 토대를 제공할 수 있을 것으로 기대한다."
        },
        {
          "rank": 44,
          "score": 0.5957062244415283,
          "doc_id": "ATN0026857341",
          "title": "인공신경망 모델의 가중치와 편의를 이용한 테트라포드의 안정수 계산 방법",
          "abstract": "Tetrapod is one of the most widely used concrete armor units for rubble mound breakwaters. The calculation of the stability number of Tetrapods is necessary to determine the optimal weight of Tetrapods. Many empirical formulas have been developed to calculate the stability number of Tetrapods, from the Hudson formula in 1950s to the recent one developed by Suh and Kang. They were developed by using the regression analysis to determine the coefficients of an assumed formula using the experimental data. Recently, software engineering (or machine learning) methods are introduced as a large amount of experimental data becomes available, e.g. artificial neural network (ANN) models for rock armors. However, these methods are seldom used probably because they did not significantly improve the accuracy compared with the empirical formula and/or the engineers are not familiar with them. In this study, we propose an explicit method to calculate the stability number of Tetrapods using the weights and biases of an ANN model. This method can be used by an engineer who has basic knowledge of matrix operation without requiring knowledge of ANN, and it is more accurate than previous empirical formulas.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0026857341&target=NART&cn=ATN0026857341",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "인공신경망 모델의 가중치와 편의를 이용한 테트라포드의 안정수 계산 방법 인공신경망 모델의 가중치와 편의를 이용한 테트라포드의 안정수 계산 방법 인공신경망 모델의 가중치와 편의를 이용한 테트라포드의 안정수 계산 방법 Tetrapod is one of the most widely used concrete armor units for rubble mound breakwaters. The calculation of the stability number of Tetrapods is necessary to determine the optimal weight of Tetrapods. Many empirical formulas have been developed to calculate the stability number of Tetrapods, from the Hudson formula in 1950s to the recent one developed by Suh and Kang. They were developed by using the regression analysis to determine the coefficients of an assumed formula using the experimental data. Recently, software engineering (or machine learning) methods are introduced as a large amount of experimental data becomes available, e.g. artificial neural network (ANN) models for rock armors. However, these methods are seldom used probably because they did not significantly improve the accuracy compared with the empirical formula and/or the engineers are not familiar with them. In this study, we propose an explicit method to calculate the stability number of Tetrapods using the weights and biases of an ANN model. This method can be used by an engineer who has basic knowledge of matrix operation without requiring knowledge of ANN, and it is more accurate than previous empirical formulas."
        },
        {
          "rank": 45,
          "score": 0.5956214070320129,
          "doc_id": "JAKO202332657687316",
          "title": "유아 인공지능 교육을 위한 인공지능 핵심 역량 요소 구성 연구",
          "abstract": "본 연구는 유아 인공지능 교육을 위해 유아 인공지능 역량 요소 및 하위 요소를 구성하는 것을 목적으로 하고 있다. 연구의 목적을 달성하기 위해 문헌 분석과 전문가 델파이 조사를 사용하였다. 문헌 분석을 위해 검색을 통해 국내 자료 4편, 국외 자료 3편을 수집하였다. 수집된 자료를 분석하여 4개의 요소와 25개의 하위 요소를 구성하였다. 최초로 구성된 요소는 인공지능 이해(하위 요소 5개), 인공지능 사고(하위 요소 6개), 인공지능 활용(하위 요소 8개), 인공지능 가치(하위 요소 6)가 도출 되었다. 최초 구성된 요소를 전문가 델파이로 검증하였고, 전문가들은 역량 요소와 하위 요소는 수용할 만한 수준이지만 하위 요소들이 보완되어야 한다는 의견을 제시하였다. 이에 본 연구는 전문가들의 의견을 수렴하여 수정하였다. 수정된 요소는 인공지능 이해(하위 요소 6개), 인공지능 사고(하위 요소 2개), 인공지능 활용(하위 요소 6개), 인공지능 가치(하위 요소 6)로 구성되었다. 수정된 요소는 전문가 델파이 조사를 수행하였고, 그 결과 타당한 것으로 검증되었다. 이에 본 연구는 수정된 요소를 최종 요소로 제안하였다. 본 연구의 결과는 유아 인공지능 교육과정을 구성하는데 중요한 근거를 제시한다는 것에서 많은 시사점을 가진다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202332657687316&target=NART&cn=JAKO202332657687316",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "유아 인공지능 교육을 위한 인공지능 핵심 역량 요소 구성 연구 유아 인공지능 교육을 위한 인공지능 핵심 역량 요소 구성 연구 유아 인공지능 교육을 위한 인공지능 핵심 역량 요소 구성 연구 본 연구는 유아 인공지능 교육을 위해 유아 인공지능 역량 요소 및 하위 요소를 구성하는 것을 목적으로 하고 있다. 연구의 목적을 달성하기 위해 문헌 분석과 전문가 델파이 조사를 사용하였다. 문헌 분석을 위해 검색을 통해 국내 자료 4편, 국외 자료 3편을 수집하였다. 수집된 자료를 분석하여 4개의 요소와 25개의 하위 요소를 구성하였다. 최초로 구성된 요소는 인공지능 이해(하위 요소 5개), 인공지능 사고(하위 요소 6개), 인공지능 활용(하위 요소 8개), 인공지능 가치(하위 요소 6)가 도출 되었다. 최초 구성된 요소를 전문가 델파이로 검증하였고, 전문가들은 역량 요소와 하위 요소는 수용할 만한 수준이지만 하위 요소들이 보완되어야 한다는 의견을 제시하였다. 이에 본 연구는 전문가들의 의견을 수렴하여 수정하였다. 수정된 요소는 인공지능 이해(하위 요소 6개), 인공지능 사고(하위 요소 2개), 인공지능 활용(하위 요소 6개), 인공지능 가치(하위 요소 6)로 구성되었다. 수정된 요소는 전문가 델파이 조사를 수행하였고, 그 결과 타당한 것으로 검증되었다. 이에 본 연구는 수정된 요소를 최종 요소로 제안하였다. 본 연구의 결과는 유아 인공지능 교육과정을 구성하는데 중요한 근거를 제시한다는 것에서 많은 시사점을 가진다."
        },
        {
          "rank": 46,
          "score": 0.5956149101257324,
          "doc_id": "JAKO202106763002129",
          "title": "머신러닝 기반의 안전도 데이터 필터링 모델",
          "abstract": "Customized services to a sleep induction for better sleepcare are more effective because of different satisfaction levels to users. The EOG data measured at the frontal lobe when a person blinks his eyes can be used as biometric data because it has different values for each person. The accuracy of measurement is degraded by a noise source, such as toss and turn. Therefore, it is necessary to analyze the noisy data and remove them from normal EOG by filtering. There are low-pass filtering and high-pass filtering as filtering using a frequency band. However, since filtering within a frequency band range is also required for more effective performance, we propose a machine learning model for the filtering of EOG data in this paper as the second filtering method. In addition, optimal values of parameters such as the depth of the hidden layer, the number of nodes of the hidden layer, the activation function, and the dropout were found through experiments, to improve the performance of the machine learning filtering model, and the filtering performance of 95.7% was obtained. Eventually, it is expected that it can be used for effective user identification services by using filtering model for EOG data.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202106763002129&target=NART&cn=JAKO202106763002129",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝 기반의 안전도 데이터 필터링 모델 머신러닝 기반의 안전도 데이터 필터링 모델 머신러닝 기반의 안전도 데이터 필터링 모델 Customized services to a sleep induction for better sleepcare are more effective because of different satisfaction levels to users. The EOG data measured at the frontal lobe when a person blinks his eyes can be used as biometric data because it has different values for each person. The accuracy of measurement is degraded by a noise source, such as toss and turn. Therefore, it is necessary to analyze the noisy data and remove them from normal EOG by filtering. There are low-pass filtering and high-pass filtering as filtering using a frequency band. However, since filtering within a frequency band range is also required for more effective performance, we propose a machine learning model for the filtering of EOG data in this paper as the second filtering method. In addition, optimal values of parameters such as the depth of the hidden layer, the number of nodes of the hidden layer, the activation function, and the dropout were found through experiments, to improve the performance of the machine learning filtering model, and the filtering performance of 95.7% was obtained. Eventually, it is expected that it can be used for effective user identification services by using filtering model for EOG data."
        },
        {
          "rank": 47,
          "score": 0.5955916047096252,
          "doc_id": "JAKO201326952134359",
          "title": "은닉 마르코프 모델을 이용한 동영상 기반 낙상 인식 알고리듬",
          "abstract": "동영상에서 추출한 변수값을 은닉 마르코프 모델(Hidden Markov Model; HMM)에 적용한 새로운 낙상 인식 알고리듬을 제안한다. 개인간 낙상 양식의 차이나 유사 낙상을 실제 낙상과 구분하기 위한 기계 학습 방법으로 HMM알고리듬을 사용하였다. 비디오의 낙상 특징 변수를 얻기 위해 동영상의 광류를 구한 후 이를 주성분 분석 방식에 적용하여 움직임을 정량화하였다. 주성분 분석으로 얻어진 전체 움직임 벡터의 각도, 장단축의 비, 속도등의 조합으로 새로운 여러 종류의 낙상 특징 변수를 정의한 후 이를 HMM에 적용하여 결과를 비교, 분석하였다. 이들 변수들 중에 각도에 의해 얻어진 변수가 가장 좋은 결과를 보여 본 실험에서 91.5%의 민감도(성공 감지율)와 88.01% 의 특이도(실패 감지율)를 나타내었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201326952134359&target=NART&cn=JAKO201326952134359",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "은닉 마르코프 모델을 이용한 동영상 기반 낙상 인식 알고리듬 은닉 마르코프 모델을 이용한 동영상 기반 낙상 인식 알고리듬 은닉 마르코프 모델을 이용한 동영상 기반 낙상 인식 알고리듬 동영상에서 추출한 변수값을 은닉 마르코프 모델(Hidden Markov Model; HMM)에 적용한 새로운 낙상 인식 알고리듬을 제안한다. 개인간 낙상 양식의 차이나 유사 낙상을 실제 낙상과 구분하기 위한 기계 학습 방법으로 HMM알고리듬을 사용하였다. 비디오의 낙상 특징 변수를 얻기 위해 동영상의 광류를 구한 후 이를 주성분 분석 방식에 적용하여 움직임을 정량화하였다. 주성분 분석으로 얻어진 전체 움직임 벡터의 각도, 장단축의 비, 속도등의 조합으로 새로운 여러 종류의 낙상 특징 변수를 정의한 후 이를 HMM에 적용하여 결과를 비교, 분석하였다. 이들 변수들 중에 각도에 의해 얻어진 변수가 가장 좋은 결과를 보여 본 실험에서 91.5%의 민감도(성공 감지율)와 88.01% 의 특이도(실패 감지율)를 나타내었다."
        },
        {
          "rank": 48,
          "score": 0.5953959226608276,
          "doc_id": "JAKO200111921140843",
          "title": "회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구",
          "abstract": "본문에서는 예측형 회귀신경망과 HMM (Hidden Markov Model)의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경 망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용 데이터에 대하여 Elman망 예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 98.5%로 우수한 결과를 얻었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200111921140843&target=NART&cn=JAKO200111921140843",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 회귀신경망 예측 HMM을 이용한 숫자음 인식에 관한 연구 본문에서는 예측형 회귀신경망과 HMM (Hidden Markov Model)의 하이브리드 네트워크인 회귀신경망 예측 HMM을 구성하였다. 회귀신경망 예측 HMM은 예측형 회귀신경망을 HMM의 각 상태마다 예측기로 정의하여 일정치인 평균벡터 대신에 과거의 특징벡터의 영향을 받아 동적으로 변화하는 신경 망에 의한 예측치를 이용하므로 학습패턴 설정자체가 시변성을 반영하는 동적 네트워크의 특성을 가진다. 따라서 음성과 같은 시계열 패턴의 인식에 유리하다. 회귀신경망 예측 HMM은 예측형 회귀신경망의 구조에 따라 Elman망 예측 HMM과 Jordan망 예측 HMM으로 구분하였다. 실험에서는 회귀신경망 예측 HMM의 상태수를 4, 5, 6으로 증가시켜 각 상태 수별로 예측차수 및 중간층 유니트 수의 변화에 따른 인식성능을 조사하였다. 실험결과 평가용 데이터에 대하여 Elman망 예측 HMM은 상태수가 6이고, 예측차수가 3차, 중간층 유니트의 수가 15차원일 때, Jordan망 예측 HMM의 경우 상태수가 5이고, 예측차수가 3차, 중간층 유니트의 수가 10차원일 때 각각 98.5%로 우수한 결과를 얻었다."
        },
        {
          "rank": 49,
          "score": 0.595228910446167,
          "doc_id": "DIKO0013973515",
          "title": "DTG 실 주행데이터 및 공간정보를 이용한 빅데이터 기반 연료소모량 추정 모델 연구",
          "abstract": "한국에서 교통사고를 줄이는 목적으로 디지털운행기록계(DTG) 장치가 모든 상용차량에 의무 장착이 되어 GPS 궤적을 포함한 많은 양의 운행기록 데이터가 생성되고 있다. 지금까지 이러한 DTG 데이터는 주로 교통안전운전을 유도하는 용도로 사용되어왔다. 그러나 DTG데이터는 연료소모량을 줄일 수 있는 에코드라이빙 분야 등 다양한 분야로 활용도를 확대할 수 있다. 본 연구는 실제 운행한 대규모 DTG운행 데이터를 공간 빅데이터 분석을 통해 연료소모량을 추정할 수 있는 모델을 수립하는 연구이다. 기존의 연료소모량 추정 모델은 주로 실험실 환경 또는 일부 지역에 국한되어 검증한 모델로 실제 운행되는 상용차량 대상으로 분석할 경우 수집하기 어려운 변수들이 많고, 한국의 전국을 운행한 다양한 운행기록 데이터를 적용하면 정확도가 낮아지는 문제점이 있다. 본 연구에서는 이러한 문제를 개선하기 위해 DTG데이터와 공간 빅데이터를 이용한 연료소모량 추정 모델인 SBiFEM(Spatial Big Data Fuel Estimation Model)을 제안하였다. SBiFEM은 운전자 운행특성 및 도로의 다양한 환경정보를 반영하기 위해 운행 패턴과 공간 빅데이터 맵매칭 기술을 이용해 다양한 도로정보들을 입력변수로 추가 활용하는 분석 방법이다. 본 연구에서는 전처리, 공간빅데이터, 운행 패턴 생성에 이르는 전 과정에서 빅데이터 Map Reduce를 활용하는 공간빅데이터 분석 프로세스를 다음과 같이 수립하고 구현, 검증을 하였다. 첫째로, 전처리 기술 고도화이다. 센싱 데이터 및 GPS공간데이터 특성상 많은 양의 이상치 데이터(Outlier)가 존재하고 있어 이를 정제하지 않으면 분석결과도 신뢰할 수 없다. 이를 위해 통계기법 및 운행 패턴 분석을 이용한 필터링 기법을 연구하였다. 둘째로, 대량의 GPS궤적 데이터를 이용한 공간빅데이터 분석 체계이다. 차량 이동 데이터를 분석하기 위해서는 차량이 어느 도로에 위치 했는지를 알기 위한 전자지도와 매칭하는 맵매칭 기술이 필수적이다. 대부분의 맵매칭 기술은 실시간 내비게이션 용도로 주로 사용되어왔기 때문에 빠른 분석 속도를 요하는 빅데이터 분석에 적합하지 않다. 본 연구에서는 대량의 GPS궤적데이터를 전자지도에 매칭하는 빅데이터 맵매칭 기술을 연구하였다. 셋째로, 연료소모량을 추정하는 모델이다. 기존 추정 모델과의 차별점은 실제 운행환경을 반영하기 위한 변수들을 추가했다는 것이다. 한 종류는 차량 운행특성을 반영하는 운행 패턴이고, 또 한 종류는 전국의 다양한 도로 환경 정보이다. 이러한 도로 환경정보를 반영하기 위해 맵매칭 공간빅데이터 프로세스를 이용하였다.  SBiFEM 빅데이터 분석시스템 테스트 및 검증은 5대의 분산 서버에 Hadoop, HBase와 Map Reduce프로그램을 수행하여 성능 검증을 하였다. 또한 연료소모량 추정 모델 검증은 대표적인 연료소모량 및 배출가스 추정 모델인 SIDRA, VSP, VT-Micro등을 DTG데이터를 적용하여 분석한 결과와 본 연구의 모델을 적용한 결과를 비교하여 검증하였다. 비교결과 추정 값과 실제 값을 비교한 Correlation이 0.9169, MAPE(평균절대백분비오차)가 0.1846으로 본 연구의 모델 적용시 가장 우수하게 나와 본 논문에서 개발한 모델의 우수성을 검증하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0013973515&target=NART&cn=DIKO0013973515",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "DTG 실 주행데이터 및 공간정보를 이용한 빅데이터 기반 연료소모량 추정 모델 연구 DTG 실 주행데이터 및 공간정보를 이용한 빅데이터 기반 연료소모량 추정 모델 연구 DTG 실 주행데이터 및 공간정보를 이용한 빅데이터 기반 연료소모량 추정 모델 연구 한국에서 교통사고를 줄이는 목적으로 디지털운행기록계(DTG) 장치가 모든 상용차량에 의무 장착이 되어 GPS 궤적을 포함한 많은 양의 운행기록 데이터가 생성되고 있다. 지금까지 이러한 DTG 데이터는 주로 교통안전운전을 유도하는 용도로 사용되어왔다. 그러나 DTG데이터는 연료소모량을 줄일 수 있는 에코드라이빙 분야 등 다양한 분야로 활용도를 확대할 수 있다. 본 연구는 실제 운행한 대규모 DTG운행 데이터를 공간 빅데이터 분석을 통해 연료소모량을 추정할 수 있는 모델을 수립하는 연구이다. 기존의 연료소모량 추정 모델은 주로 실험실 환경 또는 일부 지역에 국한되어 검증한 모델로 실제 운행되는 상용차량 대상으로 분석할 경우 수집하기 어려운 변수들이 많고, 한국의 전국을 운행한 다양한 운행기록 데이터를 적용하면 정확도가 낮아지는 문제점이 있다. 본 연구에서는 이러한 문제를 개선하기 위해 DTG데이터와 공간 빅데이터를 이용한 연료소모량 추정 모델인 SBiFEM(Spatial Big Data Fuel Estimation Model)을 제안하였다. SBiFEM은 운전자 운행특성 및 도로의 다양한 환경정보를 반영하기 위해 운행 패턴과 공간 빅데이터 맵매칭 기술을 이용해 다양한 도로정보들을 입력변수로 추가 활용하는 분석 방법이다. 본 연구에서는 전처리, 공간빅데이터, 운행 패턴 생성에 이르는 전 과정에서 빅데이터 Map Reduce를 활용하는 공간빅데이터 분석 프로세스를 다음과 같이 수립하고 구현, 검증을 하였다. 첫째로, 전처리 기술 고도화이다. 센싱 데이터 및 GPS공간데이터 특성상 많은 양의 이상치 데이터(Outlier)가 존재하고 있어 이를 정제하지 않으면 분석결과도 신뢰할 수 없다. 이를 위해 통계기법 및 운행 패턴 분석을 이용한 필터링 기법을 연구하였다. 둘째로, 대량의 GPS궤적 데이터를 이용한 공간빅데이터 분석 체계이다. 차량 이동 데이터를 분석하기 위해서는 차량이 어느 도로에 위치 했는지를 알기 위한 전자지도와 매칭하는 맵매칭 기술이 필수적이다. 대부분의 맵매칭 기술은 실시간 내비게이션 용도로 주로 사용되어왔기 때문에 빠른 분석 속도를 요하는 빅데이터 분석에 적합하지 않다. 본 연구에서는 대량의 GPS궤적데이터를 전자지도에 매칭하는 빅데이터 맵매칭 기술을 연구하였다. 셋째로, 연료소모량을 추정하는 모델이다. 기존 추정 모델과의 차별점은 실제 운행환경을 반영하기 위한 변수들을 추가했다는 것이다. 한 종류는 차량 운행특성을 반영하는 운행 패턴이고, 또 한 종류는 전국의 다양한 도로 환경 정보이다. 이러한 도로 환경정보를 반영하기 위해 맵매칭 공간빅데이터 프로세스를 이용하였다.  SBiFEM 빅데이터 분석시스템 테스트 및 검증은 5대의 분산 서버에 Hadoop, HBase와 Map Reduce프로그램을 수행하여 성능 검증을 하였다. 또한 연료소모량 추정 모델 검증은 대표적인 연료소모량 및 배출가스 추정 모델인 SIDRA, VSP, VT-Micro등을 DTG데이터를 적용하여 분석한 결과와 본 연구의 모델을 적용한 결과를 비교하여 검증하였다. 비교결과 추정 값과 실제 값을 비교한 Correlation이 0.9169, MAPE(평균절대백분비오차)가 0.1846으로 본 연구의 모델 적용시 가장 우수하게 나와 본 논문에서 개발한 모델의 우수성을 검증하였다."
        },
        {
          "rank": 50,
          "score": 0.5945483446121216,
          "doc_id": "JAKO202421251156831",
          "title": "LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석",
          "abstract": "This research is a comprehensive analysis of wind power prediction sensitivity using a Long Short-Term Memory (LSTM) deep learning neural network model, accounting for the inherent uncertainties in wind speed estimation. Utilizing a year's worth of operational data from an operational wind farm, the study forecasts the power output of both individual wind turbines and the farm collectively. Predictions were made daily at intervals of 10 minutes and 1 hour over a span of three months. The model's forecast accuracy was evaluated by comparing the root mean square error (RMSE), normalized RMSE (NRMSE), and correlation coefficients with actual power output data. Moreover, the research investigated how inaccuracies in wind speed inputs affect the power prediction sensitivity of the model. By simulating wind speed errors within a normal distribution range of 1% to 15%, the study analyzed their influence on the accuracy of power predictions. This investigation provided insights into the required wind speed prediction error rate to achieve an 8% power prediction error threshold, meeting the incentive standards for forecasting systems in renewable energy generation.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202421251156831&target=NART&cn=JAKO202421251156831",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 LSTM 딥러닝 신경망 모델을 이용한 풍력발전단지 풍속 오차에 따른 출력 예측 민감도 분석 This research is a comprehensive analysis of wind power prediction sensitivity using a Long Short-Term Memory (LSTM) deep learning neural network model, accounting for the inherent uncertainties in wind speed estimation. Utilizing a year's worth of operational data from an operational wind farm, the study forecasts the power output of both individual wind turbines and the farm collectively. Predictions were made daily at intervals of 10 minutes and 1 hour over a span of three months. The model's forecast accuracy was evaluated by comparing the root mean square error (RMSE), normalized RMSE (NRMSE), and correlation coefficients with actual power output data. Moreover, the research investigated how inaccuracies in wind speed inputs affect the power prediction sensitivity of the model. By simulating wind speed errors within a normal distribution range of 1% to 15%, the study analyzed their influence on the accuracy of power predictions. This investigation provided insights into the required wind speed prediction error rate to achieve an 8% power prediction error threshold, meeting the incentive standards for forecasting systems in renewable energy generation."
        }
      ]
    }
  ],
  "meta": {
    "model": "gemini-2.5-flash",
    "temperature": 0.2
  }
}