{
  "id": "row_000028",
  "model_name": "Alibaba-NLP/gte-multilingual-base",
  "timestamp_kst": "2025-09-09T00:13:36.320935+09:00",
  "trial_id": "f10f1dff",
  "queries": [
    {
      "query": "DBN 기반 딥 러닝을 이용한 기업부도 예측과 기존 SVM 방법 간의 성능 차이, 특히 부도기업 예측 민감도 향상 결과를 간단히 정리해 주실 수 있나요?",
      "query_meta": {
        "type": "original"
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8596502542495728,
          "doc_id": "DIKO0014169472",
          "title": "딥러닝 알고리즘에 기반한 기업부도 예측",
          "abstract": "기업의 부도는 국가경제에 막대한 손실을 입히며, 해당기업의 이해관계자들 모두에게 경제적 손실을 초래하고 사회적 부를 감소시킨다. 따라서 기업의 부도를 좀 더 정확하게 예측하는 것은 사회적·경제적 측면에서 매우 중요한 연구라 할 수 있다. &amp;#xD; 이에 최근 이미지 인식, 음성 인식, 자연어 처리 등 여러 분야에서 우수한 예측력을 보여주고 있는 딥러닝(Deep Learning)을 기업부도예측에 이용하고자 하며, 본 논문에서는 기업부도예측 방법으로 여러 딥러닝 알고리즘 중 DBN(Deep Belief Network)을 제안한다. 기존에 사용되던 분석기법 대비 우수성을 확인하기 위해 최근까지 기업부도예측에서 연구되고 있는 SVM(Support Vector Machine)과 비교하고자 하였으며, 1999년부터 2015년 사이에 국내 코스닥·코스피에 상장된 비금융업의 기업데이터를 이용하였다. 건실기업의 수는 1669개, 부도기업의 수는 495개이며, 한국은행의 기업경영분석에서 소개된 재무비율 변수를 이용하여 분석을 진행하였다. 분석결과 DBN이 SVM보다 여러 평가척도에서 더 좋은 성능을 보였다. 특히 시험데이터에 대해 부도기업을 부도기업으로 예측하는 민감도에서 5%이상의 더 뛰어난 성능을 보였으며, 이에 기업부도예측분야에 딥러닝의 적용가능성을 확인해 볼 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014169472&target=NART&cn=DIKO0014169472",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 알고리즘에 기반한 기업부도 예측 딥러닝 알고리즘에 기반한 기업부도 예측 딥러닝 알고리즘에 기반한 기업부도 예측 기업의 부도는 국가경제에 막대한 손실을 입히며, 해당기업의 이해관계자들 모두에게 경제적 손실을 초래하고 사회적 부를 감소시킨다. 따라서 기업의 부도를 좀 더 정확하게 예측하는 것은 사회적·경제적 측면에서 매우 중요한 연구라 할 수 있다. &amp;#xD; 이에 최근 이미지 인식, 음성 인식, 자연어 처리 등 여러 분야에서 우수한 예측력을 보여주고 있는 딥러닝(Deep Learning)을 기업부도예측에 이용하고자 하며, 본 논문에서는 기업부도예측 방법으로 여러 딥러닝 알고리즘 중 DBN(Deep Belief Network)을 제안한다. 기존에 사용되던 분석기법 대비 우수성을 확인하기 위해 최근까지 기업부도예측에서 연구되고 있는 SVM(Support Vector Machine)과 비교하고자 하였으며, 1999년부터 2015년 사이에 국내 코스닥·코스피에 상장된 비금융업의 기업데이터를 이용하였다. 건실기업의 수는 1669개, 부도기업의 수는 495개이며, 한국은행의 기업경영분석에서 소개된 재무비율 변수를 이용하여 분석을 진행하였다. 분석결과 DBN이 SVM보다 여러 평가척도에서 더 좋은 성능을 보였다. 특히 시험데이터에 대해 부도기업을 부도기업으로 예측하는 민감도에서 5%이상의 더 뛰어난 성능을 보였으며, 이에 기업부도예측분야에 딥러닝의 적용가능성을 확인해 볼 수 있었다."
        },
        {
          "rank": 2,
          "score": 0.7567682862281799,
          "doc_id": "JAKO201614137727823",
          "title": "딥러닝 기법을 이용한 내일강수 예측",
          "abstract": "정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로 기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본 논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도 사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의 AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를 사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로 사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의 척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201614137727823&target=NART&cn=JAKO201614137727823",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기법을 이용한 내일강수 예측 딥러닝 기법을 이용한 내일강수 예측 딥러닝 기법을 이용한 내일강수 예측 정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로 기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본 논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도 사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의 AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를 사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로 사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의 척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다."
        },
        {
          "rank": 3,
          "score": 0.7453463673591614,
          "doc_id": "JAKO201403359939237",
          "title": "개선된 배깅 앙상블을 활용한 기업부도예측",
          "abstract": "기업의 부도 예측은 재무 및 회계 분야에서 매우 중요한 연구 주제이다. 기업의 부도로 인해 발생하는 비용이 매우 크기 때문에 부도 예측의 정확성은 금융기관으로서는 매우 중요한 일이다. 최근에는 여러 개의 모형을 결합하는 앙상블 모형을 부도 예측에 적용해 보려는 연구가 큰 관심을 끌고 있다. 앙상블 모형은 개별 모형보다 더 좋은 성과를 내기 위해 여러 개의 분류기를 결합하는 것이다. 이와 같은 앙상블 분류기는 분류기의 일반화 성능을 개선하는 데 매우 유용한 것으로 알려져 있다. 본 논문은 부도 예측 모형의 성과 개선에 관한 연구이다. 이를 위해 사례 선택(Instance Selection)을 활용한 배깅(Bagging) 모형을 제안하였다. 사례 선택은 원 데이터에서 가장 대표성 있고 관련성 높은 데이터를 선택하고 예측 모형에 악영향을 줄 수 있는 불필요한 데이터를 제거하는 것으로 이를 통해 예측 성과 개선도 기대할 수 있다. 배깅은 학습데이터에 변화를 줌으로써 기저 분류기들을 다양화시키는 앙상블 기법으로 단순하면서도 성과가 매우 좋은 것으로 알려져 있다. 사례 선택과 배깅은 각각 모형의 성과를 개선시킬 수 있는 잠재력이 있지만 이들 두 기법의 결합에 관한 연구는 아직까지 없는 것이 현실이다. 본 연구에서는 부도 예측 모형의 성과를 개선하기 위해 사례 선택과 배깅을 연결하는 새로운 모형을 제안하였다. 최적의 사례 선택을 위해 유전자 알고리즘이 사용되었으며, 이를 통해 최적의 사례 선택 조합을 찾고 이 결과를 배깅 앙상블 모형에 전달하여 새로운 형태의 배깅 앙상블 모형을 구성하게 된다. 본 연구에서 제안한 새로운 앙상블 모형의 성과를 검증하기 위해 ROC 커브, AUC, 예측정확도 등과 같은 성과지표를 사용해 다양한 모형과 비교 분석해 보았다. 실제 기업데이터를 사용해 실험한 결과 본 논문에서 제안한 새로운 형태의 모형이 가장 좋은 성과를 보임을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201403359939237&target=NART&cn=JAKO201403359939237",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "개선된 배깅 앙상블을 활용한 기업부도예측 개선된 배깅 앙상블을 활용한 기업부도예측 개선된 배깅 앙상블을 활용한 기업부도예측 기업의 부도 예측은 재무 및 회계 분야에서 매우 중요한 연구 주제이다. 기업의 부도로 인해 발생하는 비용이 매우 크기 때문에 부도 예측의 정확성은 금융기관으로서는 매우 중요한 일이다. 최근에는 여러 개의 모형을 결합하는 앙상블 모형을 부도 예측에 적용해 보려는 연구가 큰 관심을 끌고 있다. 앙상블 모형은 개별 모형보다 더 좋은 성과를 내기 위해 여러 개의 분류기를 결합하는 것이다. 이와 같은 앙상블 분류기는 분류기의 일반화 성능을 개선하는 데 매우 유용한 것으로 알려져 있다. 본 논문은 부도 예측 모형의 성과 개선에 관한 연구이다. 이를 위해 사례 선택(Instance Selection)을 활용한 배깅(Bagging) 모형을 제안하였다. 사례 선택은 원 데이터에서 가장 대표성 있고 관련성 높은 데이터를 선택하고 예측 모형에 악영향을 줄 수 있는 불필요한 데이터를 제거하는 것으로 이를 통해 예측 성과 개선도 기대할 수 있다. 배깅은 학습데이터에 변화를 줌으로써 기저 분류기들을 다양화시키는 앙상블 기법으로 단순하면서도 성과가 매우 좋은 것으로 알려져 있다. 사례 선택과 배깅은 각각 모형의 성과를 개선시킬 수 있는 잠재력이 있지만 이들 두 기법의 결합에 관한 연구는 아직까지 없는 것이 현실이다. 본 연구에서는 부도 예측 모형의 성과를 개선하기 위해 사례 선택과 배깅을 연결하는 새로운 모형을 제안하였다. 최적의 사례 선택을 위해 유전자 알고리즘이 사용되었으며, 이를 통해 최적의 사례 선택 조합을 찾고 이 결과를 배깅 앙상블 모형에 전달하여 새로운 형태의 배깅 앙상블 모형을 구성하게 된다. 본 연구에서 제안한 새로운 앙상블 모형의 성과를 검증하기 위해 ROC 커브, AUC, 예측정확도 등과 같은 성과지표를 사용해 다양한 모형과 비교 분석해 보았다. 실제 기업데이터를 사용해 실험한 결과 본 논문에서 제안한 새로운 형태의 모형이 가장 좋은 성과를 보임을 알 수 있었다."
        },
        {
          "rank": 4,
          "score": 0.7333786487579346,
          "doc_id": "JAKO201336161064414",
          "title": "앙상블 SVM 모형을 이용한 기업 부도 예측",
          "abstract": "기업의 부도를 예측하는 것은 회계나 재무 분야에서 중요한 연구주제이다. 지금까지 기업 부도예측을 위해 여러 가지 데이터마이닝 기법들이 적용되었으나 주로 단일 모형을 사용함으로서 복잡한 분류 문제에의 적용에 한계를 갖고 있었다. 본 논문에서는 최근에 각광받고 있는 SVM (support vector machine) 모형들을 결합한 앙상블 SVM 모형 (ensemble SVM model)을 부도예측에 사용하고자 한다. 제안된 앙상블 모형은 v-조각 교차 타당성 (v-fold cross-validation)에 의해 얻어진 여러 가지 모형 중에서 성능이 좋은 상위 k개의 단일 모형으로 구성하고 과반수 투표 방식 (majority voting)을 사용하여 미지의 클래스를 분류한다. 본 논문에서 제안된 앙상블 SVM 모형의 성능을 평가하기 위해 실제 기업의 재무비율 자료와 모의실험자료를 가지고 실험하였고, 실험결과 제안된 앙상블 모형이 여러 가지 평가척도 하에서 단일 SVM 모형들보다 좋은 성능을 보임을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201336161064414&target=NART&cn=JAKO201336161064414",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "앙상블 SVM 모형을 이용한 기업 부도 예측 앙상블 SVM 모형을 이용한 기업 부도 예측 앙상블 SVM 모형을 이용한 기업 부도 예측 기업의 부도를 예측하는 것은 회계나 재무 분야에서 중요한 연구주제이다. 지금까지 기업 부도예측을 위해 여러 가지 데이터마이닝 기법들이 적용되었으나 주로 단일 모형을 사용함으로서 복잡한 분류 문제에의 적용에 한계를 갖고 있었다. 본 논문에서는 최근에 각광받고 있는 SVM (support vector machine) 모형들을 결합한 앙상블 SVM 모형 (ensemble SVM model)을 부도예측에 사용하고자 한다. 제안된 앙상블 모형은 v-조각 교차 타당성 (v-fold cross-validation)에 의해 얻어진 여러 가지 모형 중에서 성능이 좋은 상위 k개의 단일 모형으로 구성하고 과반수 투표 방식 (majority voting)을 사용하여 미지의 클래스를 분류한다. 본 논문에서 제안된 앙상블 SVM 모형의 성능을 평가하기 위해 실제 기업의 재무비율 자료와 모의실험자료를 가지고 실험하였고, 실험결과 제안된 앙상블 모형이 여러 가지 평가척도 하에서 단일 SVM 모형들보다 좋은 성능을 보임을 알 수 있었다."
        },
        {
          "rank": 5,
          "score": 0.7255827188491821,
          "doc_id": "JAKO201510534325002",
          "title": "퍼지이론과 SVM 결합을 통한 기업부도예측 최적화",
          "abstract": "기업부도예측은 재무 분야에 있어 중요한 연구주제 중 하나로 1960년대 이후부터 꾸준히 연구되어져 왔다. 국내의 경우, IMF 사태 이후 기업부도예측에 관한 중요성이 강조되고 있다. 이에 본 연구에서는 보다 정확한 기업부도예측을 위해 높은 예측력과 동시에 과적합화의 문제를 해결한다고 알려진 SVM(Support Vector Machine)을 기반으로 퍼지이론(fuzzy theory)을 활용해 입력변수를 확장하고, 유전자 알고리즘(GA, Genetic Algorithm)을 이용해 유사 혹은 유사최적의 입력변수집합과 파라미터를 탐색하는 새로운 융합모형을 제시한다. 제안모형의 유용성을 검증하기 위하여 H은행의 비외감 중공업 기업 데이터를 이용하여 실험을 수행하였으며, 비교모형으로는 로짓분석, 판별분석, 의사결정나무, 사례기반추론, 인공신경망, SVM을 선정하였다. 실험결과, 제안모형이 모든 비교모형들에 비해 우수한 예측력을 보이는 것으로 나타났다. 본 연구는 우수한 예측 성능을 가진 다기법 융합 모형을 새롭게 제안하여, 부도예측 분야에 학술적, 실무적으로 기여할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201510534325002&target=NART&cn=JAKO201510534325002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "퍼지이론과 SVM 결합을 통한 기업부도예측 최적화 퍼지이론과 SVM 결합을 통한 기업부도예측 최적화 퍼지이론과 SVM 결합을 통한 기업부도예측 최적화 기업부도예측은 재무 분야에 있어 중요한 연구주제 중 하나로 1960년대 이후부터 꾸준히 연구되어져 왔다. 국내의 경우, IMF 사태 이후 기업부도예측에 관한 중요성이 강조되고 있다. 이에 본 연구에서는 보다 정확한 기업부도예측을 위해 높은 예측력과 동시에 과적합화의 문제를 해결한다고 알려진 SVM(Support Vector Machine)을 기반으로 퍼지이론(fuzzy theory)을 활용해 입력변수를 확장하고, 유전자 알고리즘(GA, Genetic Algorithm)을 이용해 유사 혹은 유사최적의 입력변수집합과 파라미터를 탐색하는 새로운 융합모형을 제시한다. 제안모형의 유용성을 검증하기 위하여 H은행의 비외감 중공업 기업 데이터를 이용하여 실험을 수행하였으며, 비교모형으로는 로짓분석, 판별분석, 의사결정나무, 사례기반추론, 인공신경망, SVM을 선정하였다. 실험결과, 제안모형이 모든 비교모형들에 비해 우수한 예측력을 보이는 것으로 나타났다. 본 연구는 우수한 예측 성능을 가진 다기법 융합 모형을 새롭게 제안하여, 부도예측 분야에 학술적, 실무적으로 기여할 수 있을 것으로 기대된다."
        },
        {
          "rank": 6,
          "score": 0.7222648859024048,
          "doc_id": "DIKO0013372384",
          "title": "앙상블 SVM을 이용한 기업 부도 예측 모형",
          "abstract": "Bankruptcy prediction has been an important topic in the accounting and finance field for a long time. Several data mining techniques have been used for bankruptcy prediction. However, there are many limits for application to real classification problem with a single model. This study proposes ensemble SVM (support vector machine) model which assembles different SVM models with each different kernel functions. Our ensemble model is made and evaluated by v-fold cross-validation approach. The top performing models are recruited into the ensemble. The classification is then carried out using the majority voting opinion of the ensemble. The performance of the ensemble SVM classifier is investigated in terms of accuracy, error rate, sensitivity, specificity, ROC curve, and AUC to compare with single SVM classifiers based on two financial ratios datasets and simulation datasets. The results confirmed the advantages of our method: It is being robust while providing good performance.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0013372384&target=NART&cn=DIKO0013372384",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "앙상블 SVM을 이용한 기업 부도 예측 모형 앙상블 SVM을 이용한 기업 부도 예측 모형 앙상블 SVM을 이용한 기업 부도 예측 모형 Bankruptcy prediction has been an important topic in the accounting and finance field for a long time. Several data mining techniques have been used for bankruptcy prediction. However, there are many limits for application to real classification problem with a single model. This study proposes ensemble SVM (support vector machine) model which assembles different SVM models with each different kernel functions. Our ensemble model is made and evaluated by v-fold cross-validation approach. The top performing models are recruited into the ensemble. The classification is then carried out using the majority voting opinion of the ensemble. The performance of the ensemble SVM classifier is investigated in terms of accuracy, error rate, sensitivity, specificity, ROC curve, and AUC to compare with single SVM classifiers based on two financial ratios datasets and simulation datasets. The results confirmed the advantages of our method: It is being robust while providing good performance."
        },
        {
          "rank": 7,
          "score": 0.721034049987793,
          "doc_id": "DIKO0009404537",
          "title": "Support vector machine을 이용한 기업부도예측",
          "abstract": "Predicting bankruptcy is one of the most important problems to parties such as bankers, managers, government policy makers, and investors. It provides information for interested parties to minimize their predictable losses from bankruptcy. There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. since the late 1980s, Artificial Intelligence (AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network (ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine (SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0009404537&target=NART&cn=DIKO0009404537",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Support vector machine을 이용한 기업부도예측 Support vector machine을 이용한 기업부도예측 Support vector machine을 이용한 기업부도예측 Predicting bankruptcy is one of the most important problems to parties such as bankers, managers, government policy makers, and investors. It provides information for interested parties to minimize their predictable losses from bankruptcy. There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. since the late 1980s, Artificial Intelligence (AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network (ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine (SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction."
        },
        {
          "rank": 8,
          "score": 0.7191249132156372,
          "doc_id": "JAKO200111920938436",
          "title": "퍼지신경망을 이용한 기업부도예측",
          "abstract": "본 연구에서는 퍼지신경망을 이용한 기업부실예측모형을 제안한다. 신경망은 탁월한 학습능력을 가진 것으로 알려져 있으나, 잡음이 심한 재무자료에 대해서는 종종 일관되지 못하고 기대에 미치지 못하는 예측성과를 보인다. 이는 연속형의 형태를 지닌 독립변수와 과다한 양의 원자료로부터 예측에 필요한 일정한 패턴을 찾기가 어렵기 때문이다. 이러한 문제점은 예측모형에서의 독립변수와 종속변수간의 인과관계를 신경망이 용이하게 찾아낼 수 있도록 독립변수의 형태를 변환함으로써 해결한 수 있다. 이러한 해결방법의 하나는 기존 신경망에 퍼지집합의 개념을 적용하여 신경망 학습에 사용될 자료를 퍼지화하고 이를 신경망에 학습시키는 것이다 입력자료를 퍼지화 함으로써 정보의 손실 없이도 신경망이 자료 내의 복잡한 관계를 용이하게 학습하는 것이 가능하다. 본 연구에서 제안된 퍼지신경망을 기업부도예측에 적용한 결과, 퍼지신경망이 기존의 신경망보다 우월한 예측성과를 나타내었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200111920938436&target=NART&cn=JAKO200111920938436",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "퍼지신경망을 이용한 기업부도예측 퍼지신경망을 이용한 기업부도예측 퍼지신경망을 이용한 기업부도예측 본 연구에서는 퍼지신경망을 이용한 기업부실예측모형을 제안한다. 신경망은 탁월한 학습능력을 가진 것으로 알려져 있으나, 잡음이 심한 재무자료에 대해서는 종종 일관되지 못하고 기대에 미치지 못하는 예측성과를 보인다. 이는 연속형의 형태를 지닌 독립변수와 과다한 양의 원자료로부터 예측에 필요한 일정한 패턴을 찾기가 어렵기 때문이다. 이러한 문제점은 예측모형에서의 독립변수와 종속변수간의 인과관계를 신경망이 용이하게 찾아낼 수 있도록 독립변수의 형태를 변환함으로써 해결한 수 있다. 이러한 해결방법의 하나는 기존 신경망에 퍼지집합의 개념을 적용하여 신경망 학습에 사용될 자료를 퍼지화하고 이를 신경망에 학습시키는 것이다 입력자료를 퍼지화 함으로써 정보의 손실 없이도 신경망이 자료 내의 복잡한 관계를 용이하게 학습하는 것이 가능하다. 본 연구에서 제안된 퍼지신경망을 기업부도예측에 적용한 결과, 퍼지신경망이 기존의 신경망보다 우월한 예측성과를 나타내었다."
        },
        {
          "rank": 9,
          "score": 0.7175508737564087,
          "doc_id": "ART001692415",
          "title": "SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상",
          "abstract": "기업 도산은 다양한 이해관계자에게 사회 경제적으로 큰 손실을 주게 된다. 따라서 기업 도산 및 부실화를 사전에 예측할 수 있다면 이에 대한 대비를 하거나 기업 부도 요인을 사전 제거함에 따라 기업 도산에 대한 손실을 최소화하는 것이 가능할 것이다. 기업의 도산을 예측하기 위한 다양한 통계적 모형 및 데이터 마이닝 모형이 제시되고 있다. 기업 도산 예측 모형의 주요 이슈 중 하나는 예측력을 높이는 것이다. 이 논문은 기업 도산예측에 주로 사용되며 예측성능이 비교적 높은 로짓모형, 의사결정나무모형, 신경망모형, SVM모형에 대해서 살펴본다. 표본기업은 코스닥기업 중 도산 및 정상기업으로 각각 49개 기업을 선정하였고, 설명변수로는 통계적으로 유의미한 9개의 재무변수를 이용하고, 5년간의 재무변수 자료를 사용하였다. SVM모형을 전처리기를 사용한 경우 다양한 분류모형의 도산예측 성능을 크게 높일 수 있음을 실험결과로 보이고 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART001692415&target=NART&cn=ART001692415",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상 SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상 SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상 기업 도산은 다양한 이해관계자에게 사회 경제적으로 큰 손실을 주게 된다. 따라서 기업 도산 및 부실화를 사전에 예측할 수 있다면 이에 대한 대비를 하거나 기업 부도 요인을 사전 제거함에 따라 기업 도산에 대한 손실을 최소화하는 것이 가능할 것이다. 기업의 도산을 예측하기 위한 다양한 통계적 모형 및 데이터 마이닝 모형이 제시되고 있다. 기업 도산 예측 모형의 주요 이슈 중 하나는 예측력을 높이는 것이다. 이 논문은 기업 도산예측에 주로 사용되며 예측성능이 비교적 높은 로짓모형, 의사결정나무모형, 신경망모형, SVM모형에 대해서 살펴본다. 표본기업은 코스닥기업 중 도산 및 정상기업으로 각각 49개 기업을 선정하였고, 설명변수로는 통계적으로 유의미한 9개의 재무변수를 이용하고, 5년간의 재무변수 자료를 사용하였다. SVM모형을 전처리기를 사용한 경우 다양한 분류모형의 도산예측 성능을 크게 높일 수 있음을 실험결과로 보이고 있다."
        },
        {
          "rank": 10,
          "score": 0.7126467227935791,
          "doc_id": "JAKO202404861562091",
          "title": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교",
          "abstract": "대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202404861562091&target=NART&cn=JAKO202404861562091",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다."
        },
        {
          "rank": 11,
          "score": 0.7120630741119385,
          "doc_id": "JAKO200516638000020",
          "title": "Support Vector Machine을 이용한 기업부도예측",
          "abstract": "There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. Since the late 1980s, Artificial Intelligence(AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network(ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine(SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200516638000020&target=NART&cn=JAKO200516638000020",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Support Vector Machine을 이용한 기업부도예측 Support Vector Machine을 이용한 기업부도예측 Support Vector Machine을 이용한 기업부도예측 There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. Since the late 1980s, Artificial Intelligence(AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network(ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine(SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction."
        },
        {
          "rank": 12,
          "score": 0.7107452154159546,
          "doc_id": "ATN0031726879",
          "title": "딥러닝 기반 부실기업 예측모형에 관한 연구",
          "abstract": "Predicting insolvent companies is a research topic that has been important in accounting and finance. Especially, due to the rapidly changing business environments and the recent COVID-19 pandemic, many domestic companies are facing financial adversity. Thus, the necessity of research on corporate insolvency is being emphasized. As a related research, there is a prediction of corporate bankruptcy, however, a bankrupt company is the company whose business activities have been suspended, and there is a limitation in which it is inappropriate to determine which companies show signs of bankruptcy among continuing companies. Therefore, marginal company, one of the categories of insolvent companies, is selected as the prediction target. Marginal companies are the firms that are operating income interest compensation ratio are less than 1 for three consecutive years, and are engaged in business activities but have not consistently secured adequate profits. In this study, deep learning techniques are used to predict them. It is one of the machine learning techniques that has recently attracted attention because of its excellence in various fields. Nonetheless, has not been applied in research to predict marginal companies. This study applies RNN and CNN among deep learning techniques using several financial ratios as independent variables. Their performance are compared with machine learning ensemble models that have been reported to have excellent predictive power in previous studies. As a result of analysis on corporate data from 2017 to 2019 as training and test data, deep learning models such as RNN-LSTM, RNN-GRU, and CNN are better in forecasting of marginal companies than the ensemble models in terms of Recall score. Therefore, the deep learning models are expected to become widely used in the prediction of marginal companies in the future.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0031726879&target=NART&cn=ATN0031726879",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 부실기업 예측모형에 관한 연구 딥러닝 기반 부실기업 예측모형에 관한 연구 딥러닝 기반 부실기업 예측모형에 관한 연구 Predicting insolvent companies is a research topic that has been important in accounting and finance. Especially, due to the rapidly changing business environments and the recent COVID-19 pandemic, many domestic companies are facing financial adversity. Thus, the necessity of research on corporate insolvency is being emphasized. As a related research, there is a prediction of corporate bankruptcy, however, a bankrupt company is the company whose business activities have been suspended, and there is a limitation in which it is inappropriate to determine which companies show signs of bankruptcy among continuing companies. Therefore, marginal company, one of the categories of insolvent companies, is selected as the prediction target. Marginal companies are the firms that are operating income interest compensation ratio are less than 1 for three consecutive years, and are engaged in business activities but have not consistently secured adequate profits. In this study, deep learning techniques are used to predict them. It is one of the machine learning techniques that has recently attracted attention because of its excellence in various fields. Nonetheless, has not been applied in research to predict marginal companies. This study applies RNN and CNN among deep learning techniques using several financial ratios as independent variables. Their performance are compared with machine learning ensemble models that have been reported to have excellent predictive power in previous studies. As a result of analysis on corporate data from 2017 to 2019 as training and test data, deep learning models such as RNN-LSTM, RNN-GRU, and CNN are better in forecasting of marginal companies than the ensemble models in terms of Recall score. Therefore, the deep learning models are expected to become widely used in the prediction of marginal companies in the future."
        },
        {
          "rank": 13,
          "score": 0.7064449787139893,
          "doc_id": "JAKO202305062334676",
          "title": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측",
          "abstract": "본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202305062334676&target=NART&cn=JAKO202305062334676",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다."
        },
        {
          "rank": 14,
          "score": 0.7033728361129761,
          "doc_id": "JAKO201810866003990",
          "title": "딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증",
          "abstract": "본 연구는 경제적으로 국내에 큰 영향을 주었던 글로벌 금융위기를 기반으로 총 10년의 연간 기업데이터를 이용한다. 먼저 시대 변화 흐름에 일관성있는 부도 모형을 구축하는 것을 목표로 금융위기 이전(2000~2006년)의 데이터를 학습한다. 이후 매개 변수 튜닝을 통해 금융위기 기간이 포함(2007~2008년)된 유효성 검증 데이터가 학습데이터의 결과와 비슷한 양상을 보이고, 우수한 예측력을 가지도록 조정한다. 이후 학습 및 유효성 검증 데이터를 통합(2000~2008년)하여 유효성 검증 때와 같은 매개변수를 적용하여 모형을 재구축하고, 결과적으로 최종 학습된 모형을 기반으로 시험 데이터(2009년) 결과를 바탕으로 딥러닝 시계열 알고리즘 기반의 기업부도예측 모형이 유용함을 검증한다. 부도에 대한 정의는 Lee(2015) 연구와 동일하게 기업의 상장폐지 사유들 중 실적이 부진했던 경우를 부도로 선정한다. 독립변수의 경우, 기존 선행연구에서 이용되었던 재무비율 변수를 비롯한 기타 재무정보를 포함한다. 이후 최적의 변수군을 선별하는 방식으로 다변량 판별분석, 로짓 모형, 그리고 Lasso 회귀분석 모형을 이용한다. 기업부도예측 모형 방법론으로는 Altman(1968)이 제시했던 다중판별분석 모형, Ohlson(1980)이 제시한 로짓모형, 그리고 비시계열 기계학습 기반 부도예측모형과 딥러닝 시계열 알고리즘을 이용한다. 기업 데이터의 경우, '비선형적인 변수들', 변수들의 '다중 공선성 문제', 그리고 '데이터 수 부족'이란 한계점이 존재한다. 이에 로짓 모형은 '비선형성'을, Lasso 회귀분석 모형은 '다중 공선성 문제'를 해결하고, 가변적인 데이터 생성 방식을 이용하는 딥러닝 시계열 알고리즘을 접목함으로서 데이터 수가 부족한 점을 보완하여 연구를 진행한다. 현 정부를 비롯한 해외 정부에서는 4차 산업혁명을 통해 국가 및 사회의 시스템, 일상생활 전반을 아우르기 위해 힘쓰고 있다. 즉, 현재는 다양한 산업에 이르러 빅데이터를 이용한 딥러닝 연구가 활발히 진행되고 있지만, 금융 산업을 위한 연구분야는 아직도 미비하다. 따라서 이 연구는 기업 부도에 관하여 딥러닝 시계열 알고리즘 분석을 진행한 초기 논문으로서, 금융 데이터와 딥러닝 시계열 알고리즘을 접목한 연구를 시작하는 비 전공자에게 비교분석 자료로 쓰이기를 바란다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201810866003990&target=NART&cn=JAKO201810866003990",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증 딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증 딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증 본 연구는 경제적으로 국내에 큰 영향을 주었던 글로벌 금융위기를 기반으로 총 10년의 연간 기업데이터를 이용한다. 먼저 시대 변화 흐름에 일관성있는 부도 모형을 구축하는 것을 목표로 금융위기 이전(2000~2006년)의 데이터를 학습한다. 이후 매개 변수 튜닝을 통해 금융위기 기간이 포함(2007~2008년)된 유효성 검증 데이터가 학습데이터의 결과와 비슷한 양상을 보이고, 우수한 예측력을 가지도록 조정한다. 이후 학습 및 유효성 검증 데이터를 통합(2000~2008년)하여 유효성 검증 때와 같은 매개변수를 적용하여 모형을 재구축하고, 결과적으로 최종 학습된 모형을 기반으로 시험 데이터(2009년) 결과를 바탕으로 딥러닝 시계열 알고리즘 기반의 기업부도예측 모형이 유용함을 검증한다. 부도에 대한 정의는 Lee(2015) 연구와 동일하게 기업의 상장폐지 사유들 중 실적이 부진했던 경우를 부도로 선정한다. 독립변수의 경우, 기존 선행연구에서 이용되었던 재무비율 변수를 비롯한 기타 재무정보를 포함한다. 이후 최적의 변수군을 선별하는 방식으로 다변량 판별분석, 로짓 모형, 그리고 Lasso 회귀분석 모형을 이용한다. 기업부도예측 모형 방법론으로는 Altman(1968)이 제시했던 다중판별분석 모형, Ohlson(1980)이 제시한 로짓모형, 그리고 비시계열 기계학습 기반 부도예측모형과 딥러닝 시계열 알고리즘을 이용한다. 기업 데이터의 경우, '비선형적인 변수들', 변수들의 '다중 공선성 문제', 그리고 '데이터 수 부족'이란 한계점이 존재한다. 이에 로짓 모형은 '비선형성'을, Lasso 회귀분석 모형은 '다중 공선성 문제'를 해결하고, 가변적인 데이터 생성 방식을 이용하는 딥러닝 시계열 알고리즘을 접목함으로서 데이터 수가 부족한 점을 보완하여 연구를 진행한다. 현 정부를 비롯한 해외 정부에서는 4차 산업혁명을 통해 국가 및 사회의 시스템, 일상생활 전반을 아우르기 위해 힘쓰고 있다. 즉, 현재는 다양한 산업에 이르러 빅데이터를 이용한 딥러닝 연구가 활발히 진행되고 있지만, 금융 산업을 위한 연구분야는 아직도 미비하다. 따라서 이 연구는 기업 부도에 관하여 딥러닝 시계열 알고리즘 분석을 진행한 초기 논문으로서, 금융 데이터와 딥러닝 시계열 알고리즘을 접목한 연구를 시작하는 비 전공자에게 비교분석 자료로 쓰이기를 바란다."
        },
        {
          "rank": 15,
          "score": 0.7022004723548889,
          "doc_id": "JAKO201208438434752",
          "title": "부도 예측을 위한 앙상블 분류기 개발",
          "abstract": "분류기의 앙상블 학습은 여러 개의 서로 다른 분류기들의 조합을 통해 만들어진다. 앙상블 학습은 기계학습 분야에서 많은 관심을 끌고 있는 중요한 연구주제이며 대부분의 경우에 있어서 앙상블 모형은 개별 기저 분류기보다 더 좋은 성과를 내는 것으로 알려져 있다. 본 연구는 부도 예측 모형의 성능개선에 관한 연구이다. 이를 위해 본 연구에서는 단일 모형으로 그 우수성을 인정받고 있는 SVM을 기저 분류기로 사용하는 앙상블 모형에 대해 고찰하였다. SVM 모형의 성능 개선을 위해 bagging과 random subspace 모형을 부도 예측 문제에 적용해 보았으며 bagging 모형과 random subspace 모형의 성과 개선을 위해 bagging과 random subspace의 통합 모형을 제안하였다. 제안한 모형의 성과를 검증하기 위해 실제 기업의 부도 예측 데이터를 사용하여 실험하였고, 실험 결과 본 연구에서 제안한 새로운 형태의 통합 모형이 가장 좋은 성과를 보임을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201208438434752&target=NART&cn=JAKO201208438434752",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "부도 예측을 위한 앙상블 분류기 개발 부도 예측을 위한 앙상블 분류기 개발 부도 예측을 위한 앙상블 분류기 개발 분류기의 앙상블 학습은 여러 개의 서로 다른 분류기들의 조합을 통해 만들어진다. 앙상블 학습은 기계학습 분야에서 많은 관심을 끌고 있는 중요한 연구주제이며 대부분의 경우에 있어서 앙상블 모형은 개별 기저 분류기보다 더 좋은 성과를 내는 것으로 알려져 있다. 본 연구는 부도 예측 모형의 성능개선에 관한 연구이다. 이를 위해 본 연구에서는 단일 모형으로 그 우수성을 인정받고 있는 SVM을 기저 분류기로 사용하는 앙상블 모형에 대해 고찰하였다. SVM 모형의 성능 개선을 위해 bagging과 random subspace 모형을 부도 예측 문제에 적용해 보았으며 bagging 모형과 random subspace 모형의 성과 개선을 위해 bagging과 random subspace의 통합 모형을 제안하였다. 제안한 모형의 성과를 검증하기 위해 실제 기업의 부도 예측 데이터를 사용하여 실험하였고, 실험 결과 본 연구에서 제안한 새로운 형태의 통합 모형이 가장 좋은 성과를 보임을 알 수 있었다."
        },
        {
          "rank": 16,
          "score": 0.6993558406829834,
          "doc_id": "ATN0052773847",
          "title": "머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구",
          "abstract": "본 논문은 상장기업의 재무 및 거시경제 데이터를 활용하여 기업부도를 예측하는 통계적 모형과 다양한 머신러닝 기법의 성능을 비교하고, 불균형 데이터 문제를 완화하기 위한 오버샘플링 기법의 효과를 분석하였다. 실증 분석에는 로지스틱 회귀, 랜덤 포레스트, XGBoost(extreme gradient boosting), 심층신경망 모형을 적용하였으며, 오버샘플링 기법인 SMOTE(synthetic minority over-sampling technique) 및 ADASYN(adapti-ve synthetic sampling)을 사용하였다. 분석 결과, XGBoost는 원자료뿐 아니라 오버샘플링을 적용한 경우 모두에서 가장 우수하고 균형 있는 예측 성능을 보였다. 반면, 로지스틱 회귀는 높은 재현율을 나타냈으나, 낮은 정밀도로 인해 실무적 활용에는 한계가 있었다. 이러한 결과는 불균형 데이터 환경에서 오버샘플링 기법과 XGBoost와 같은 머신러닝 모형을 결합하여 사용하는 것이 기업부도 예측에 있어 보다 효과적이고 실용적인 접근법이 될 수 있음을 시사한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0052773847&target=NART&cn=ATN0052773847",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구 머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구 머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구 본 논문은 상장기업의 재무 및 거시경제 데이터를 활용하여 기업부도를 예측하는 통계적 모형과 다양한 머신러닝 기법의 성능을 비교하고, 불균형 데이터 문제를 완화하기 위한 오버샘플링 기법의 효과를 분석하였다. 실증 분석에는 로지스틱 회귀, 랜덤 포레스트, XGBoost(extreme gradient boosting), 심층신경망 모형을 적용하였으며, 오버샘플링 기법인 SMOTE(synthetic minority over-sampling technique) 및 ADASYN(adapti-ve synthetic sampling)을 사용하였다. 분석 결과, XGBoost는 원자료뿐 아니라 오버샘플링을 적용한 경우 모두에서 가장 우수하고 균형 있는 예측 성능을 보였다. 반면, 로지스틱 회귀는 높은 재현율을 나타냈으나, 낮은 정밀도로 인해 실무적 활용에는 한계가 있었다. 이러한 결과는 불균형 데이터 환경에서 오버샘플링 기법과 XGBoost와 같은 머신러닝 모형을 결합하여 사용하는 것이 기업부도 예측에 있어 보다 효과적이고 실용적인 접근법이 될 수 있음을 시사한다."
        },
        {
          "rank": 17,
          "score": 0.6989777088165283,
          "doc_id": "DIKO0015731994",
          "title": "기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로",
          "abstract": "본 연구에서는 기업의 부도를 사전에 예측하기 위한 모형을 연구하였고 정량 정보인 재무 데이터와 비정형 정보인 뉴스 콘텐츠의 감성분석 정보를 변수로 선정하였다. 재무 정보는 회계와 재무분야의 문헌에서 잘 알려지고 오랜 기간을 통하여 검증된 3개의 재무모델 변수(Altman, 1968; Beaver, 1968; Horrigan, 1966)와 기업의 경영상태를 종합적으로 분석하는 방법인 기업경영분석 지표(한국은행)를 결합하여 총 3개년치의 재무변수를 선정하였다. &amp;#xD; &amp;#xD; 기업의 부도 징후를 나타내는 유의미한 재무적 요인을 도출하기 위해 t-test와 logistic regression방법으로 통계적 검증 작업을 진행하였고, 연구 결과 총자산 이익잉여금률, 총자산 이익률, 매출액 운전자본 비율, 자본 매출액 비율, 차입금 의존도가 유의미한 재무 변수로 확인되었다. &amp;#xD; &amp;#xD; 비정형 정보인 뉴스 콘텐츠가 기업 부도를 예측하는데 얼마나 효과적인지 검증하기 위해 재무 변수에 뉴스 감성 분석 점수를 추가하여 모델링을 적용하였다. 부도 기업인 경우 부도 직전 6개월치 뉴스를, 정상 기업인 경우 2019. 7 ~ 12월까지의 6개월치 뉴스 기사를 크롤링 하였고 실제 기업 뉴스와 상관없는 기사들은 정제 작업 등의 전처리를 진행하였다. 정제가 완료된 텍스트에서 명사를 추출하여 말뭉치 기반의 감성사전을 구축하고 뉴스의 수집 기간별 감성점수를 변수로 추가하였다. &amp;#xD; &amp;#xD; 기업 뉴스의 감성점수를 추가한 결과 재무 데이터만을 사용했을 때보다 훨씬 더 좋은 성능이 나타남을 알 수 있었다. 민감도 기준(실제 부도기업을 부도기업으로 예측)으로 가장 성능이 우수한 SVM(Support Vector Machine)의 경우 재무 변수만 사용했을 때 87.50%였는데 뉴스 감성점수를 추가했을 때 93.75%로 약 6% 정도의 성능 향상이 있었다. 그리고 뉴스 수집기간은 3,4개월치를 적용 했을 때 민감도의 성능이 가장 좋은 것으로 확인되었다. &amp;#xD; &amp;#xD; 금융기관에서는 전통적으로 부도 예측을 위해 재무 데이터를 주로 사용하고 있는데 해당 정보는 분기별로 업데이트가 되는 정보의 적시성에 문제가 있을 수 있다. 따라서 부도 예측시 본 연구에서 실증한 온라인 뉴스의 감성분석 정보인 비정형 데이터를 함께 사용한다면 효과적인 여신 의사결정 지원 체계를 수립하는데 많은 도움이 될 것으로 판단된다. &amp;#xD; &amp;#xD;",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015731994&target=NART&cn=DIKO0015731994",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로 기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로 기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로 본 연구에서는 기업의 부도를 사전에 예측하기 위한 모형을 연구하였고 정량 정보인 재무 데이터와 비정형 정보인 뉴스 콘텐츠의 감성분석 정보를 변수로 선정하였다. 재무 정보는 회계와 재무분야의 문헌에서 잘 알려지고 오랜 기간을 통하여 검증된 3개의 재무모델 변수(Altman, 1968; Beaver, 1968; Horrigan, 1966)와 기업의 경영상태를 종합적으로 분석하는 방법인 기업경영분석 지표(한국은행)를 결합하여 총 3개년치의 재무변수를 선정하였다. &amp;#xD; &amp;#xD; 기업의 부도 징후를 나타내는 유의미한 재무적 요인을 도출하기 위해 t-test와 logistic regression방법으로 통계적 검증 작업을 진행하였고, 연구 결과 총자산 이익잉여금률, 총자산 이익률, 매출액 운전자본 비율, 자본 매출액 비율, 차입금 의존도가 유의미한 재무 변수로 확인되었다. &amp;#xD; &amp;#xD; 비정형 정보인 뉴스 콘텐츠가 기업 부도를 예측하는데 얼마나 효과적인지 검증하기 위해 재무 변수에 뉴스 감성 분석 점수를 추가하여 모델링을 적용하였다. 부도 기업인 경우 부도 직전 6개월치 뉴스를, 정상 기업인 경우 2019. 7 ~ 12월까지의 6개월치 뉴스 기사를 크롤링 하였고 실제 기업 뉴스와 상관없는 기사들은 정제 작업 등의 전처리를 진행하였다. 정제가 완료된 텍스트에서 명사를 추출하여 말뭉치 기반의 감성사전을 구축하고 뉴스의 수집 기간별 감성점수를 변수로 추가하였다. &amp;#xD; &amp;#xD; 기업 뉴스의 감성점수를 추가한 결과 재무 데이터만을 사용했을 때보다 훨씬 더 좋은 성능이 나타남을 알 수 있었다. 민감도 기준(실제 부도기업을 부도기업으로 예측)으로 가장 성능이 우수한 SVM(Support Vector Machine)의 경우 재무 변수만 사용했을 때 87.50%였는데 뉴스 감성점수를 추가했을 때 93.75%로 약 6% 정도의 성능 향상이 있었다. 그리고 뉴스 수집기간은 3,4개월치를 적용 했을 때 민감도의 성능이 가장 좋은 것으로 확인되었다. &amp;#xD; &amp;#xD; 금융기관에서는 전통적으로 부도 예측을 위해 재무 데이터를 주로 사용하고 있는데 해당 정보는 분기별로 업데이트가 되는 정보의 적시성에 문제가 있을 수 있다. 따라서 부도 예측시 본 연구에서 실증한 온라인 뉴스의 감성분석 정보인 비정형 데이터를 함께 사용한다면 효과적인 여신 의사결정 지원 체계를 수립하는데 많은 도움이 될 것으로 판단된다. &amp;#xD; &amp;#xD;"
        },
        {
          "rank": 18,
          "score": 0.6954096555709839,
          "doc_id": "JAKO202312473958811",
          "title": "작물 생산량 예측을 위한 심층강화학습 성능 분석",
          "abstract": "최근 딥러닝 기술을 활용하여 작물 생산량 예측 연구가 많이 진행되고 있다. 딥러닝 알고리즘은 입력 데이터 세트와 작물 예측 결과에 대한 선형 맵을 구성하는데 어려움이 있다. 또한, 알고리즘 구현은 획득한 속성의 비율에 긍정적으로 의존한다. 심층강화학습을 작물 생산량 예측 응용에 적용한다면 이러한 한계점을 보완할 수 있다. 본 논문은 작물 생산량 예측을 개선하기 위해 DQN, Double DQN 및 Dueling DQN 의 성능을 분석한다. DQN 알고리즘은 과대 평가 문제가 제기되지만, Double DQN은 과대 평가를 줄이고 더 나은 결과를 얻을 수 있다. 본 논문에서 제안된 모델은 거짓 판정을 줄이고 예측 정확도를 높이는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202312473958811&target=NART&cn=JAKO202312473958811",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "작물 생산량 예측을 위한 심층강화학습 성능 분석 작물 생산량 예측을 위한 심층강화학습 성능 분석 작물 생산량 예측을 위한 심층강화학습 성능 분석 최근 딥러닝 기술을 활용하여 작물 생산량 예측 연구가 많이 진행되고 있다. 딥러닝 알고리즘은 입력 데이터 세트와 작물 예측 결과에 대한 선형 맵을 구성하는데 어려움이 있다. 또한, 알고리즘 구현은 획득한 속성의 비율에 긍정적으로 의존한다. 심층강화학습을 작물 생산량 예측 응용에 적용한다면 이러한 한계점을 보완할 수 있다. 본 논문은 작물 생산량 예측을 개선하기 위해 DQN, Double DQN 및 Dueling DQN 의 성능을 분석한다. DQN 알고리즘은 과대 평가 문제가 제기되지만, Double DQN은 과대 평가를 줄이고 더 나은 결과를 얻을 수 있다. 본 논문에서 제안된 모델은 거짓 판정을 줄이고 예측 정확도를 높이는 것으로 나타났다."
        },
        {
          "rank": 19,
          "score": 0.6932364702224731,
          "doc_id": "JAKO201828138444462",
          "title": "효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용",
          "abstract": "분류 문제에서 특정 범주의 빈도가 다른 범주에 비해 과도하게 높은 경우, 왜곡된 기계 학습을 유발할 수 있는 데이터 불균형(imbalanced data) 문제가 발생한다. 기업부도 예측 문제도 그 중 하나인데, 일반적으로 금융기관과 거래하는 기업들의 부도율은 대단히 낮아서, 부도 사례보다 정상 사례의 빈도가 월등히 높은 데이터 불균형 문제가 발생하고 있다. 이러한 데이터 불균형 문제를 해결하기 위해서는 적절한 표본추출 기법이 적용될 필요가 있으며, 지금껏 소수 범주 데이터를 복원 추출함으로써 다수 범주 데이터와 비율을 맞추어 데이터 불균형을 해결하는 오버 샘플링(oversampling) 기법이 주로 활용되어 왔다. 그러나 전통적인 오버 샘플링은 과적합화(overfitting)가 발생할 위험이 높아질 수 있는 단점이 있다. 이러한 배경에서 본 연구는 효과적인 기업부도 예측 모형 학습을 위한 표본추출 기법으로 2014년에 Menardi와 Torelli가 제안한 ROSE(random over sampling examples) 기법을 제안한다. ROSE 기법은 학습에 사용될 사례를 반복적으로 새롭게 합성하여 생성(synthetic generation)하는 기법으로, 과적합화 문제를 회피하면서도 분류 예측 정확도 개선에 도움을 줄 수 있다. 이에 본 연구에서는 ROSE 기법을 가장 성능이 우수한 이분류기로 알려진 SVM(support vector machine)과 결합하여 국내 한 대형 은행의 기업부도 예측에 적용해 보고, 다른 표본추출 기법들과의 비교연구를 수행하였다. 실험 결과, ROSE 기법이 다른 기법에 비해 통계적으로 유의한 수준으로 SVM의 예측정확도 개선에 기여할 수 있음을 확인하였다. 이러한 본 연구의 결과는 부도예측 외에 다른 사회과학 분야 예측문제의 데이터 불균형 문제 해결에도 ROSE가 우수한 대안이 될 수 있다는 사실을 시사한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201828138444462&target=NART&cn=JAKO201828138444462",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용 효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용 효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용 분류 문제에서 특정 범주의 빈도가 다른 범주에 비해 과도하게 높은 경우, 왜곡된 기계 학습을 유발할 수 있는 데이터 불균형(imbalanced data) 문제가 발생한다. 기업부도 예측 문제도 그 중 하나인데, 일반적으로 금융기관과 거래하는 기업들의 부도율은 대단히 낮아서, 부도 사례보다 정상 사례의 빈도가 월등히 높은 데이터 불균형 문제가 발생하고 있다. 이러한 데이터 불균형 문제를 해결하기 위해서는 적절한 표본추출 기법이 적용될 필요가 있으며, 지금껏 소수 범주 데이터를 복원 추출함으로써 다수 범주 데이터와 비율을 맞추어 데이터 불균형을 해결하는 오버 샘플링(oversampling) 기법이 주로 활용되어 왔다. 그러나 전통적인 오버 샘플링은 과적합화(overfitting)가 발생할 위험이 높아질 수 있는 단점이 있다. 이러한 배경에서 본 연구는 효과적인 기업부도 예측 모형 학습을 위한 표본추출 기법으로 2014년에 Menardi와 Torelli가 제안한 ROSE(random over sampling examples) 기법을 제안한다. ROSE 기법은 학습에 사용될 사례를 반복적으로 새롭게 합성하여 생성(synthetic generation)하는 기법으로, 과적합화 문제를 회피하면서도 분류 예측 정확도 개선에 도움을 줄 수 있다. 이에 본 연구에서는 ROSE 기법을 가장 성능이 우수한 이분류기로 알려진 SVM(support vector machine)과 결합하여 국내 한 대형 은행의 기업부도 예측에 적용해 보고, 다른 표본추출 기법들과의 비교연구를 수행하였다. 실험 결과, ROSE 기법이 다른 기법에 비해 통계적으로 유의한 수준으로 SVM의 예측정확도 개선에 기여할 수 있음을 확인하였다. 이러한 본 연구의 결과는 부도예측 외에 다른 사회과학 분야 예측문제의 데이터 불균형 문제 해결에도 ROSE가 우수한 대안이 될 수 있다는 사실을 시사한다."
        },
        {
          "rank": 20,
          "score": 0.6898510456085205,
          "doc_id": "DIKO0015893049",
          "title": "도메인 적대적 신경망을 이용한 종단 간 억양음성인식",
          "abstract": "최근 딥러닝(Deep learning) 기술의 발전은 음성인식 성능 향상에 크게 기여하였다. 이러한 발전에도 불구하고 소음, 감정, 억양 등이 섞인 특정 발화에 대해서는 좋은 성능을 보이지 못하고 있다. 이 가운데 억양이 섞인 발화는 표준 발화와 비교했을 때 언어학적인 차이가 존재하는데, 이러한 차이가 억양이 섞인 발화를 인식하기 어렵게 만든다. 따라서 본 연구에서는 억양이 섞인 발화와 표준 발화 사이에 존재하는 특성의 차이를 줄이고자 도메인 적대적 신경망(Domain Adversarial Neural Network) 기법을 사용하였다. 또한, 종단 간(End-to-end) 기법을 사용하여 음성인식의 과정을 간소화하였다.&amp;#xD; 오래전부터 억양음성인식의 성능을 높이기 위한 연구는 활발히 진행되어 왔다. 2010년대 초반까지는 가우시안 혼합 모델(Gaussian Mixture Model) 기반의 최대 사후 확률(Maximum A Posteriori), 최대 우도 선형 회귀(Maximum Likelihood Linear Regression) 적응 기법이 주로 사용되었다. 하지만 딥러닝 기술이 발전하고 신경망 기반의 모델들이 주목 받기 시작하면서 가우시안 혼합 모델보다는 신경망 모델에 적합한 기법들이 사용되었다. 최근 몇 년간은 음성인식 모델에 억양에 대한 정보를 직접 삽입하는 accent embedding 기법이 많이 사용되었다. Accent embedding 기법은 억양음성인식의 성능을 향상시켰지만 몇 가지 문제점을 가지고 있다. 첫째, 억양을 분류하여 accent embedding 특징들을 만들어내는 모델을 독립적으로 만들어 훈련시켜야 하며, 해당 모델의 억양 분류 정확도가 음성인식 모델의 성능에 큰 영향을 미치기 때문에 모델을 정교하게 만들어야 하는 부담감이 있다. &amp;#xD; 둘째, 억양 분류 모델의 결과를 음성인식 모델의 추가적인 입력 특징(Input feature)으로 사용하기 때문에 음성인식 모델의 매개변수를 증가시키며 계산량 또한 증가한다는 문제가 있다. 따라서 본 연구에서는 추가적인 입력 특징이 필요하지 않고 음성인식에서 기본적으로 사용되는 특징인 주파수 정보(스펙트로그램)만을 이용하여 학습이 가능한 도메인 적대적 신경망을 기법을 제안하였다. &amp;#xD; 도메인 적대적 신경망은 소스 도메인(Source domain) 데이터와 타겟 도메인(Target domain) 데이터가 적대적으로 학습이 되면서 두 도메인 간의 분포 차이를 줄이는 것을 목적으로 한다. 본 연구의 목표인 억양음성인식에서는 표준 발화를 소스 도메인으로, 억양이 섞인 발화를 타겟 도메인으로 정하였다. 도메인 적대적 신경망은 특징 추출기(Feature extractor), 도메인 분류기(Domain classifier), 레이블 예측기(Label predictor) 총 3개의 부분망(sub-network)으로 구성된다. 각각의 부분망은 서로 다른 역할을 수행하기 때문에 신경망의 특성을 고려하여 만들어야 한다. 따라서 본 연구에서는 신경망의 특성을 고려하여 특징 추출기에는 합성곱 신경망(Convolutional Neural Network)을, 도메인 분류기에는 심층 신경망(Deep Neural Network)을, 그리고 레이블 예측기에는 양방향 게이트 순환 유닛(Bidirectional Gated Recurrent Unit)을 이용하여 도메인 적대적 신경망을 구성하였다. 또한, 레이블을 예측할 때 종단 간 기법을 활용하여 입력 데이터를 사전 분할하지 않고, 레이블 예측 이후의 후처리 작업을 없애면서 음성인식 과정을 간소화하였다.&amp;#xD; 본 연구에서 제안한 도메인 적대적 학습 기반의 억양음성인식 기법의 효과를 입증하기 위하여 Baseline 모델과 DANN 모델을 만들어 실험을 진행하였다. 실험 데이터로는 Mozilla의 Common Voice 코퍼스를 사용하였는데, Common Voice 코퍼스는 여러 언어에 대해 막대한 양의 검증된 음성파일을 오픈소스로 제공하기 때문에 음성인식 연구에서 많이 사용된다. 또한, Common Voice 코퍼스는 음성 녹음 파일과 함께 억양 정보도 같이 제공을 하기 때문에 억양음성인식 연구에 효율적으로 사용될 수 있다. &amp;#xD; Common Voice 코퍼스의 영어 데이터셋은 여러 억양의 음성파일들을 가지고 있는데, 본 연구에서는 미국 억양, 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양의 데이터를 실험에 사용하였으며, 미국 억양을 소스 도메인으로 나머지 네 개의 억양을 타겟 도메인으로 정하였다.&amp;#xD; 실험 결과 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양 모두에서 DANN 모델의 성능이 Baseline 모델보다 높은 성능을 보였다. 하지만 억양에 따라 성능 개선의 차이가 있었으며, 캐나다 억양에 비해 잉글랜드 억양과 인도 억양에서 성능이 눈에 띄게 향상되었다. 이 같은 결과는 잉글랜드 억양과 인도 억양이 소스 도메인으로 사용된 미국 억양 데이터와 언어학적으로 큰 차이가 존재하여 baseline 모델에서는 성능이 낮았으나, 도메인 적대적 학습을 통해 생성된 DANN 모델이 타겟 억양의 특성을 반영함으로써 성능이 크게 개선된 것으로 분석된다. 따라서 도메인 적대적 신경망은 소스 도메인과 타겟 도메인 사이의 분포의 차이를 줄임으로서 억양음성인식의 성능을 향상시킬 수 있음이 확인되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015893049&target=NART&cn=DIKO0015893049",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "도메인 적대적 신경망을 이용한 종단 간 억양음성인식 도메인 적대적 신경망을 이용한 종단 간 억양음성인식 도메인 적대적 신경망을 이용한 종단 간 억양음성인식 최근 딥러닝(Deep learning) 기술의 발전은 음성인식 성능 향상에 크게 기여하였다. 이러한 발전에도 불구하고 소음, 감정, 억양 등이 섞인 특정 발화에 대해서는 좋은 성능을 보이지 못하고 있다. 이 가운데 억양이 섞인 발화는 표준 발화와 비교했을 때 언어학적인 차이가 존재하는데, 이러한 차이가 억양이 섞인 발화를 인식하기 어렵게 만든다. 따라서 본 연구에서는 억양이 섞인 발화와 표준 발화 사이에 존재하는 특성의 차이를 줄이고자 도메인 적대적 신경망(Domain Adversarial Neural Network) 기법을 사용하였다. 또한, 종단 간(End-to-end) 기법을 사용하여 음성인식의 과정을 간소화하였다.&amp;#xD; 오래전부터 억양음성인식의 성능을 높이기 위한 연구는 활발히 진행되어 왔다. 2010년대 초반까지는 가우시안 혼합 모델(Gaussian Mixture Model) 기반의 최대 사후 확률(Maximum A Posteriori), 최대 우도 선형 회귀(Maximum Likelihood Linear Regression) 적응 기법이 주로 사용되었다. 하지만 딥러닝 기술이 발전하고 신경망 기반의 모델들이 주목 받기 시작하면서 가우시안 혼합 모델보다는 신경망 모델에 적합한 기법들이 사용되었다. 최근 몇 년간은 음성인식 모델에 억양에 대한 정보를 직접 삽입하는 accent embedding 기법이 많이 사용되었다. Accent embedding 기법은 억양음성인식의 성능을 향상시켰지만 몇 가지 문제점을 가지고 있다. 첫째, 억양을 분류하여 accent embedding 특징들을 만들어내는 모델을 독립적으로 만들어 훈련시켜야 하며, 해당 모델의 억양 분류 정확도가 음성인식 모델의 성능에 큰 영향을 미치기 때문에 모델을 정교하게 만들어야 하는 부담감이 있다. &amp;#xD; 둘째, 억양 분류 모델의 결과를 음성인식 모델의 추가적인 입력 특징(Input feature)으로 사용하기 때문에 음성인식 모델의 매개변수를 증가시키며 계산량 또한 증가한다는 문제가 있다. 따라서 본 연구에서는 추가적인 입력 특징이 필요하지 않고 음성인식에서 기본적으로 사용되는 특징인 주파수 정보(스펙트로그램)만을 이용하여 학습이 가능한 도메인 적대적 신경망을 기법을 제안하였다. &amp;#xD; 도메인 적대적 신경망은 소스 도메인(Source domain) 데이터와 타겟 도메인(Target domain) 데이터가 적대적으로 학습이 되면서 두 도메인 간의 분포 차이를 줄이는 것을 목적으로 한다. 본 연구의 목표인 억양음성인식에서는 표준 발화를 소스 도메인으로, 억양이 섞인 발화를 타겟 도메인으로 정하였다. 도메인 적대적 신경망은 특징 추출기(Feature extractor), 도메인 분류기(Domain classifier), 레이블 예측기(Label predictor) 총 3개의 부분망(sub-network)으로 구성된다. 각각의 부분망은 서로 다른 역할을 수행하기 때문에 신경망의 특성을 고려하여 만들어야 한다. 따라서 본 연구에서는 신경망의 특성을 고려하여 특징 추출기에는 합성곱 신경망(Convolutional Neural Network)을, 도메인 분류기에는 심층 신경망(Deep Neural Network)을, 그리고 레이블 예측기에는 양방향 게이트 순환 유닛(Bidirectional Gated Recurrent Unit)을 이용하여 도메인 적대적 신경망을 구성하였다. 또한, 레이블을 예측할 때 종단 간 기법을 활용하여 입력 데이터를 사전 분할하지 않고, 레이블 예측 이후의 후처리 작업을 없애면서 음성인식 과정을 간소화하였다.&amp;#xD; 본 연구에서 제안한 도메인 적대적 학습 기반의 억양음성인식 기법의 효과를 입증하기 위하여 Baseline 모델과 DANN 모델을 만들어 실험을 진행하였다. 실험 데이터로는 Mozilla의 Common Voice 코퍼스를 사용하였는데, Common Voice 코퍼스는 여러 언어에 대해 막대한 양의 검증된 음성파일을 오픈소스로 제공하기 때문에 음성인식 연구에서 많이 사용된다. 또한, Common Voice 코퍼스는 음성 녹음 파일과 함께 억양 정보도 같이 제공을 하기 때문에 억양음성인식 연구에 효율적으로 사용될 수 있다. &amp;#xD; Common Voice 코퍼스의 영어 데이터셋은 여러 억양의 음성파일들을 가지고 있는데, 본 연구에서는 미국 억양, 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양의 데이터를 실험에 사용하였으며, 미국 억양을 소스 도메인으로 나머지 네 개의 억양을 타겟 도메인으로 정하였다.&amp;#xD; 실험 결과 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양 모두에서 DANN 모델의 성능이 Baseline 모델보다 높은 성능을 보였다. 하지만 억양에 따라 성능 개선의 차이가 있었으며, 캐나다 억양에 비해 잉글랜드 억양과 인도 억양에서 성능이 눈에 띄게 향상되었다. 이 같은 결과는 잉글랜드 억양과 인도 억양이 소스 도메인으로 사용된 미국 억양 데이터와 언어학적으로 큰 차이가 존재하여 baseline 모델에서는 성능이 낮았으나, 도메인 적대적 학습을 통해 생성된 DANN 모델이 타겟 억양의 특성을 반영함으로써 성능이 크게 개선된 것으로 분석된다. 따라서 도메인 적대적 신경망은 소스 도메인과 타겟 도메인 사이의 분포의 차이를 줄임으로서 억양음성인식의 성능을 향상시킬 수 있음이 확인되었다."
        },
        {
          "rank": 21,
          "score": 0.6873433589935303,
          "doc_id": "ATN0035906971",
          "title": "딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구",
          "abstract": "In this research, we compare the explanatory power between linear regression model and deep-learning model when estimating stock returns. As predicted, the deep-learning model shows statistically significant improvement over linear regression model, although the improvement is not economically meaningful. We further investigate the effects of deep-learning model using different parameters and pre-processing. The results show that the predictive power of deep-learning model can be worse-off than that of linear model if it fails to select optimal parameters. Especially, it is important to choose adequate deep-learning parameters not to overfit the data, because the accounting data (which is at most quarterly) may not be sufficient enough for the deep model structure. Further, we show that the predictive power using researchers’ domain knowledge is sometimes better off than that relying simply on the deep-learning model. For instance, denomination with total assets brings better results than non-denomination. Another interesting finding is that winsorizing extreme values brings lower explanatory power when we use the deep-learning model. Such finding implies that, by removing extreme values, we may lose useful information in the parameter estimation. The results of this paper will help future research decide whether to utilize deep learning model or linear regression model",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0035906971&target=NART&cn=ATN0035906971",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 In this research, we compare the explanatory power between linear regression model and deep-learning model when estimating stock returns. As predicted, the deep-learning model shows statistically significant improvement over linear regression model, although the improvement is not economically meaningful. We further investigate the effects of deep-learning model using different parameters and pre-processing. The results show that the predictive power of deep-learning model can be worse-off than that of linear model if it fails to select optimal parameters. Especially, it is important to choose adequate deep-learning parameters not to overfit the data, because the accounting data (which is at most quarterly) may not be sufficient enough for the deep model structure. Further, we show that the predictive power using researchers’ domain knowledge is sometimes better off than that relying simply on the deep-learning model. For instance, denomination with total assets brings better results than non-denomination. Another interesting finding is that winsorizing extreme values brings lower explanatory power when we use the deep-learning model. Such finding implies that, by removing extreme values, we may lose useful information in the parameter estimation. The results of this paper will help future research decide whether to utilize deep learning model or linear regression model"
        },
        {
          "rank": 22,
          "score": 0.6872371435165405,
          "doc_id": "DIKO0015069923",
          "title": "딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템",
          "abstract": "데이터 예측 시스템들은 데이터를 예측하기 위해 특정 분야의 데이터를 컴퓨터가 분석하여 규칙을 찾아내고 데이터를 예측하였다. 이러한 방법은 과거 데이터를 분석한 결과로 사람이 규칙을 도출할 수 있어야 데이터를 예측하는 것이 가능하였다. 이에 반해 규칙을 도출할 수 없는 데이터들의 데이터를 예측하는 것은 사람의 능력으로는 한계가 있어 정확도가 낮아지는 문제점이 발생할 수 있다.&amp;#xD; 이를 해결하기 위해 컴퓨터를 활용하여 방대한 데이터를 데이터 예측 프로그램에 학습 데이터로 입력하고 결과로 데이터를 예측하였다. 이러한 방법론을 활용하기 위해서 고성능 컴퓨터로 딥 러닝(Deep Learning) 기술을 적용하여 데이터를 예측하고 있다. 해당 방법론이 활용되고 있는 분야로는 기상 데이터를 분석하여 날씨를 예측하는 날씨 분석과 스포츠 경기의 데이터를 예측하는 것이 대표적이다. &amp;#xD; 딥 러닝 기술은 프로그램이 데이터를 기반으로 학습을 진행하고 진행된 학습을 기반으로 데이터를 처리하는 것이다. 이는 과거에 사람이 직접 데이터를 분석하는 것보다 대규모 데이터를 분석하기에 적합하고 이로 인해 정확도가 올라가는 이점이 있다. 또한 목적에 따라 적합한 딥 러닝 모델을 적용하여 데이터를 예측할 경우 정확도의 기댓값이 높아지는 이점이 있다.&amp;#xD; 현재 딥 러닝 모델 중에서 데이터를 예측하기 위해 사용되는 모델은 신경망 구조를 기반으로 하는 DNN(Deep Neural Network) 모델과 RNN(Recurrent Neural Network) 모델이다. DNN 모델은 학습 데이터 내에서 규칙을 찾아내지 못하더라도 반복 학습을 통해 데이터 예측에 대한 정확도를 올릴 수 있고, RNN은 학습 과정 중에서 은닉층에서 적용될 가중치가 학습을 진행할 수록 변화하여 데이터를 예측하고 이로 인해 정확도를 올릴 수 있다. 이에 반해 DNN은 반복 학습의 횟수가 많아야 정확도가 높아지고 RNN은 가중치 변화의 횟수가 많아져야 정확도가 높아지기 때문에 결국 두 모델들은 학습의 반복이 많아져야 하는 문제점이 있다.&amp;#xD; 본 논문에서는 데이터 예측을 위해 딥 러닝 모델 기반 순차 데이터 예측 시스템을 제안한다. 제안하는 시스템에서 비정형 데이터를 순차 데이터로 정제하기 위해 전처리기를 구현하였다. 전처리기는 딥 러닝 모델에 학습 데이터를 입력하기 전에 데이터들을 정제하는 기능을 수행한다. 데이터는 ‘데이터 : 인덱스’ 구조로 이루어진 데이터 쌍이 되고 이러한 데이터 쌍들의 집합을 딥 러닝 모델에 입력하여 학습을 진행한다.&amp;#xD; 딥 러닝 모델은 DNN 모델, 기본 LSTM 모델, 상태유지 LSTM 모델을 활용하여 시스템을 각각 구축한다. 그리고 각 모델들의 설정 값을 변경하면서 정확도의 변화량을 분석한다. 또한 시퀀스의 길이를 변경해가며 실험을 진행하여 가장 정확도가 높은 데이터 셋과 시퀀스 길이의 비율을 제시한다.&amp;#xD; 딥 러닝 모듈 기반 시스템의 실험을 바탕으로 순차 데이터 예측에 가장 정확도가 높고 효율적인 딥 러닝 모듈을 선정하고 기존 시스템들과 비교 분석을 진행하여 제안하는 시스템의 우수성을 검증한다.&amp;#xD; 제안하는 시스템을 활용할 경우 학습 데이터가 적어도 높은 정확도를 요구하는 분야에서 기존 시스템들에 비해 효율성이 높을 것으로 사료된다.&amp;#xD;",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015069923&target=NART&cn=DIKO0015069923",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 데이터 예측 시스템들은 데이터를 예측하기 위해 특정 분야의 데이터를 컴퓨터가 분석하여 규칙을 찾아내고 데이터를 예측하였다. 이러한 방법은 과거 데이터를 분석한 결과로 사람이 규칙을 도출할 수 있어야 데이터를 예측하는 것이 가능하였다. 이에 반해 규칙을 도출할 수 없는 데이터들의 데이터를 예측하는 것은 사람의 능력으로는 한계가 있어 정확도가 낮아지는 문제점이 발생할 수 있다.&amp;#xD; 이를 해결하기 위해 컴퓨터를 활용하여 방대한 데이터를 데이터 예측 프로그램에 학습 데이터로 입력하고 결과로 데이터를 예측하였다. 이러한 방법론을 활용하기 위해서 고성능 컴퓨터로 딥 러닝(Deep Learning) 기술을 적용하여 데이터를 예측하고 있다. 해당 방법론이 활용되고 있는 분야로는 기상 데이터를 분석하여 날씨를 예측하는 날씨 분석과 스포츠 경기의 데이터를 예측하는 것이 대표적이다. &amp;#xD; 딥 러닝 기술은 프로그램이 데이터를 기반으로 학습을 진행하고 진행된 학습을 기반으로 데이터를 처리하는 것이다. 이는 과거에 사람이 직접 데이터를 분석하는 것보다 대규모 데이터를 분석하기에 적합하고 이로 인해 정확도가 올라가는 이점이 있다. 또한 목적에 따라 적합한 딥 러닝 모델을 적용하여 데이터를 예측할 경우 정확도의 기댓값이 높아지는 이점이 있다.&amp;#xD; 현재 딥 러닝 모델 중에서 데이터를 예측하기 위해 사용되는 모델은 신경망 구조를 기반으로 하는 DNN(Deep Neural Network) 모델과 RNN(Recurrent Neural Network) 모델이다. DNN 모델은 학습 데이터 내에서 규칙을 찾아내지 못하더라도 반복 학습을 통해 데이터 예측에 대한 정확도를 올릴 수 있고, RNN은 학습 과정 중에서 은닉층에서 적용될 가중치가 학습을 진행할 수록 변화하여 데이터를 예측하고 이로 인해 정확도를 올릴 수 있다. 이에 반해 DNN은 반복 학습의 횟수가 많아야 정확도가 높아지고 RNN은 가중치 변화의 횟수가 많아져야 정확도가 높아지기 때문에 결국 두 모델들은 학습의 반복이 많아져야 하는 문제점이 있다.&amp;#xD; 본 논문에서는 데이터 예측을 위해 딥 러닝 모델 기반 순차 데이터 예측 시스템을 제안한다. 제안하는 시스템에서 비정형 데이터를 순차 데이터로 정제하기 위해 전처리기를 구현하였다. 전처리기는 딥 러닝 모델에 학습 데이터를 입력하기 전에 데이터들을 정제하는 기능을 수행한다. 데이터는 ‘데이터 : 인덱스’ 구조로 이루어진 데이터 쌍이 되고 이러한 데이터 쌍들의 집합을 딥 러닝 모델에 입력하여 학습을 진행한다.&amp;#xD; 딥 러닝 모델은 DNN 모델, 기본 LSTM 모델, 상태유지 LSTM 모델을 활용하여 시스템을 각각 구축한다. 그리고 각 모델들의 설정 값을 변경하면서 정확도의 변화량을 분석한다. 또한 시퀀스의 길이를 변경해가며 실험을 진행하여 가장 정확도가 높은 데이터 셋과 시퀀스 길이의 비율을 제시한다.&amp;#xD; 딥 러닝 모듈 기반 시스템의 실험을 바탕으로 순차 데이터 예측에 가장 정확도가 높고 효율적인 딥 러닝 모듈을 선정하고 기존 시스템들과 비교 분석을 진행하여 제안하는 시스템의 우수성을 검증한다.&amp;#xD; 제안하는 시스템을 활용할 경우 학습 데이터가 적어도 높은 정확도를 요구하는 분야에서 기존 시스템들에 비해 효율성이 높을 것으로 사료된다.&amp;#xD;"
        },
        {
          "rank": 23,
          "score": 0.6849159002304077,
          "doc_id": "JAKO202320150299733",
          "title": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가",
          "abstract": "본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202320150299733&target=NART&cn=JAKO202320150299733",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다."
        },
        {
          "rank": 24,
          "score": 0.6822828054428101,
          "doc_id": "DIKO0014912357",
          "title": "딥러닝 기반 기술융합 예측 방법론",
          "abstract": "오늘날 기업들은 전략적인 관점에서 기술변화를 예측하고, 이를 활용한 기술전략 수립이 반드시 필요하다고 요구된다. 특히 기술융합 현상은 기술혁신을 주도하고 시장과 산업의 변화를 이끈 다고 할 수 있기 때문에, 기술융합을 정량적으로 측정하고 관측하기 위한 연구가 다수 이루어졌다. 선행 연구에서는 계량서지분석을 이용해 기술 구조를 분석하고, 기술네트워크를 통해 기술융합 현상을 분석하는데 그쳤다. 본 연구에서는 미래의 기술융합 구조를 예상할 수 있는 예측 방법론을 제시한다. 링크 예측은 현재 시점의 네트워크를 통해 미래 시점의 네트워크에 추가되거나 제거 될 링크를 예측하는 문제를 의미한다. 본 연구에서는 기술네트워크 형태로 표현 된 기술시스템에 링크 예측을 적용하여 미래 기술융합 관계에 대해 예측하는 것을 목적으로 한다. 특히 딥러닝 기법을 이용한 학습 기반 링크 예측을 수행한다는 특징이 있다. 기존에 링크 예측을 적용해 기술융합 예측을 시도한 연구가 일부 있었으나, 네트워크 내 이웃관계에 의한 토폴로지 유사도를 의사결정 척도로 이용했다는 점에서 한계에 머물렀다. 기술융합 예측이라는 도메인 속성이 고려되지 않았다는 점인데, 본 연구에서는 기존 링크 예측에서 보편적으로 사용되었던 이웃관계에 의한 네트워크 토폴로지 유사도와 인용관계에 의한 유사도를 함께 고려한 기술융합 예측모델을 제시한다. 또한 학습 기반 링크 예측에서는 기술네트워크에 대한 정보를 노드 쌍 조합 단위의 레코드를 갖는 정형화 된 데이터 구조로 변화해 이진분류 문제로 기술융합 예측 문제를 재정의 하였기 때문에, 다양한 교사학습 기반의 기계학습 알고리즘 적용이 가능하다. 본 연구에서는 분류 성능이 우수해 최근 주목받고 있는 딥러닝 기법의 DNN 알고리즘을 사용하여 학습 기반 링크 예측을 수행하고, SVM, 로지스틱회귀(logistic regression), 랜덤포레스트(random foreset)와 같은 보편적인 분류 알고리즘을 비교한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014912357&target=NART&cn=DIKO0014912357",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 기술융합 예측 방법론 딥러닝 기반 기술융합 예측 방법론 딥러닝 기반 기술융합 예측 방법론 오늘날 기업들은 전략적인 관점에서 기술변화를 예측하고, 이를 활용한 기술전략 수립이 반드시 필요하다고 요구된다. 특히 기술융합 현상은 기술혁신을 주도하고 시장과 산업의 변화를 이끈 다고 할 수 있기 때문에, 기술융합을 정량적으로 측정하고 관측하기 위한 연구가 다수 이루어졌다. 선행 연구에서는 계량서지분석을 이용해 기술 구조를 분석하고, 기술네트워크를 통해 기술융합 현상을 분석하는데 그쳤다. 본 연구에서는 미래의 기술융합 구조를 예상할 수 있는 예측 방법론을 제시한다. 링크 예측은 현재 시점의 네트워크를 통해 미래 시점의 네트워크에 추가되거나 제거 될 링크를 예측하는 문제를 의미한다. 본 연구에서는 기술네트워크 형태로 표현 된 기술시스템에 링크 예측을 적용하여 미래 기술융합 관계에 대해 예측하는 것을 목적으로 한다. 특히 딥러닝 기법을 이용한 학습 기반 링크 예측을 수행한다는 특징이 있다. 기존에 링크 예측을 적용해 기술융합 예측을 시도한 연구가 일부 있었으나, 네트워크 내 이웃관계에 의한 토폴로지 유사도를 의사결정 척도로 이용했다는 점에서 한계에 머물렀다. 기술융합 예측이라는 도메인 속성이 고려되지 않았다는 점인데, 본 연구에서는 기존 링크 예측에서 보편적으로 사용되었던 이웃관계에 의한 네트워크 토폴로지 유사도와 인용관계에 의한 유사도를 함께 고려한 기술융합 예측모델을 제시한다. 또한 학습 기반 링크 예측에서는 기술네트워크에 대한 정보를 노드 쌍 조합 단위의 레코드를 갖는 정형화 된 데이터 구조로 변화해 이진분류 문제로 기술융합 예측 문제를 재정의 하였기 때문에, 다양한 교사학습 기반의 기계학습 알고리즘 적용이 가능하다. 본 연구에서는 분류 성능이 우수해 최근 주목받고 있는 딥러닝 기법의 DNN 알고리즘을 사용하여 학습 기반 링크 예측을 수행하고, SVM, 로지스틱회귀(logistic regression), 랜덤포레스트(random foreset)와 같은 보편적인 분류 알고리즘을 비교한다."
        },
        {
          "rank": 25,
          "score": 0.6816314458847046,
          "doc_id": "DIKO0014861002",
          "title": "딥 러닝기반 고객평점 예측모델",
          "abstract": "인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014861002&target=NART&cn=DIKO0014861002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다."
        },
        {
          "rank": 26,
          "score": 0.6814261078834534,
          "doc_id": "ATN0050865232",
          "title": "XGBoost 머신러닝 기반 쉴드 TBM 지반침하 예측",
          "abstract": "본 연구에서는 도심지 쉴드 TBM (tunnel boring machine) 터널 시공 중 발생하는 지반침하를 예측하기 위한 XGBoost (eXtreme Gradient Boosting) 머신러닝 모델을 개발하고, 그 성능을 평가하였다. 기존 연구들에서 주로 터널 후방의 침하를 예측하는 연구가 많았던 반면, 본 연구에서는 실시간 쉴드 TBM 시공 데이터를 활용하여 터널의 후방 침하뿐 아니라 전방 침하에 대한 예측도 시도하였다. 이를 위해 이수가압식 쉴드 TBM으로 시공한 터널 현장 데이터를 제공받아 지반 조건, TBM 굴진자료 , 터널 기하 조건 등을 분석하고 17개의 머신러닝 모델 입력변수를 선정하였다. 선정된 17개의 입력변수에 대해 쉴드 TBM 본체를 기준으로 전방 예측 범위(세그먼트 25링 전방, CASE 1), 중앙부(TBM 본체 상부, CASE 2), 후방 예측 범위(세그먼트 25링 후방, CASE 3) 등 세 범위로 구분하고 각 범위에 대하여 입력변수와 침하량 간의 상관관계를 분석하였다. 그리고 각 CASE별로, 즉 터널 전방(CASE 1), 중앙(CASE 2), 후방(CASE 3) 위치에 대해서 XGBoost 머신러닝 알고리즘을 적용한 지반침하 예측 모델을 구축하고 베이지안 최적화와 5겹 교차 검증을 통해 하이퍼파라미터를 최적화하였다. 모델 평가결과, 후방 침하 예측 모델은 결정계수(R2)값이 0.82로 가장 높은 성능을 보인 반면, 전방 침하 예측 모델의 결정계수는 0.52로 상대적으로 낮은 성능을 나타내었다. 이러한 결과는 후방 침하 예측 정확도가 전방 예측보다 우수하고, 전방 예측의 경우 지반의 불확실성과 굴착 변수의 영향을 더 많이 받아 정확도가 낮아질 수 있음을 시사한다. 머신러닝 모델이 TBM 터널 시공 중 발생하는 지반침하, 특히 막장면 후방의 침하를 예측하는 데 효과적인 도구이나 아직 전방 침하의 예측 정확도를 높이기 위해서는 많은 추가 연구가 이루어져야 함을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0050865232&target=NART&cn=ATN0050865232",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "XGBoost 머신러닝 기반 쉴드 TBM 지반침하 예측 XGBoost 머신러닝 기반 쉴드 TBM 지반침하 예측 XGBoost 머신러닝 기반 쉴드 TBM 지반침하 예측 본 연구에서는 도심지 쉴드 TBM (tunnel boring machine) 터널 시공 중 발생하는 지반침하를 예측하기 위한 XGBoost (eXtreme Gradient Boosting) 머신러닝 모델을 개발하고, 그 성능을 평가하였다. 기존 연구들에서 주로 터널 후방의 침하를 예측하는 연구가 많았던 반면, 본 연구에서는 실시간 쉴드 TBM 시공 데이터를 활용하여 터널의 후방 침하뿐 아니라 전방 침하에 대한 예측도 시도하였다. 이를 위해 이수가압식 쉴드 TBM으로 시공한 터널 현장 데이터를 제공받아 지반 조건, TBM 굴진자료 , 터널 기하 조건 등을 분석하고 17개의 머신러닝 모델 입력변수를 선정하였다. 선정된 17개의 입력변수에 대해 쉴드 TBM 본체를 기준으로 전방 예측 범위(세그먼트 25링 전방, CASE 1), 중앙부(TBM 본체 상부, CASE 2), 후방 예측 범위(세그먼트 25링 후방, CASE 3) 등 세 범위로 구분하고 각 범위에 대하여 입력변수와 침하량 간의 상관관계를 분석하였다. 그리고 각 CASE별로, 즉 터널 전방(CASE 1), 중앙(CASE 2), 후방(CASE 3) 위치에 대해서 XGBoost 머신러닝 알고리즘을 적용한 지반침하 예측 모델을 구축하고 베이지안 최적화와 5겹 교차 검증을 통해 하이퍼파라미터를 최적화하였다. 모델 평가결과, 후방 침하 예측 모델은 결정계수(R2)값이 0.82로 가장 높은 성능을 보인 반면, 전방 침하 예측 모델의 결정계수는 0.52로 상대적으로 낮은 성능을 나타내었다. 이러한 결과는 후방 침하 예측 정확도가 전방 예측보다 우수하고, 전방 예측의 경우 지반의 불확실성과 굴착 변수의 영향을 더 많이 받아 정확도가 낮아질 수 있음을 시사한다. 머신러닝 모델이 TBM 터널 시공 중 발생하는 지반침하, 특히 막장면 후방의 침하를 예측하는 데 효과적인 도구이나 아직 전방 침하의 예측 정확도를 높이기 위해서는 많은 추가 연구가 이루어져야 함을 확인하였다."
        },
        {
          "rank": 27,
          "score": 0.6803015470504761,
          "doc_id": "JAKO201223052004277",
          "title": "회사채 신용등급 예측을 위한 SVM 앙상블학습",
          "abstract": "회사채 신용등급은 투자자의 입장에서는 수익률 결정의 중요한 요소이며 기업의 입장에서는 자본비용 및 기업 가치와 관련된 중요한 재무의사결정사항으로 정교한 신용등급 예측 모형의 개발은 재무 및 회계 분야에서 오랫동안 전통적인 연구 주제가 되어왔다. 그러나, 회사채 신용등급 예측 모형의 성과와 관련된 가장 중요한 문제는 등급별 데이터의 불균형 문제이다. 예측 문제에 있어서 데이터 불균형(Data imbalance) 은 사용되는 표본이 특정 범주에 편중되었을 때 나타난다. 데이터 불균형이 심화됨에 따라 범주 사이의 분류경계영역이 왜곡되므로 분류자의 학습성과가 저하되게 된다. 본 연구에서는 데이터 불균형 문제가 존재하는 다분류 문제를 효과적으로 해결하기 위한 다분류 기하평균 부스팅 기법 (Multiclass Geometric Mean-based Boosting MGM-Boost)을 제안하고자 한다. MGM-Boost 알고리즘은 부스팅 알고리즘에 기하평균 개념을 도입한 것으로 오분류된 표본에 대한 학습을 강화할 수 있으며 불균형 분포를 보이는 각 범주의 예측정확도를 동시에 고려한 학습이 가능하다는 장점이 있다. 회사채 신용등급 예측문제를 활용하여 MGM-Boost의 성과를 검증한 결과 SVM 및 AdaBoost 기법과 비교하여 통계적으로 유의적인 성과개선 효과를 보여주었으며 데이터 불균형 하에서도 벤치마킹 모형과 비교하여 견고한 학습성과를 나타냈다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201223052004277&target=NART&cn=JAKO201223052004277",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회사채 신용등급 예측을 위한 SVM 앙상블학습 회사채 신용등급 예측을 위한 SVM 앙상블학습 회사채 신용등급 예측을 위한 SVM 앙상블학습 회사채 신용등급은 투자자의 입장에서는 수익률 결정의 중요한 요소이며 기업의 입장에서는 자본비용 및 기업 가치와 관련된 중요한 재무의사결정사항으로 정교한 신용등급 예측 모형의 개발은 재무 및 회계 분야에서 오랫동안 전통적인 연구 주제가 되어왔다. 그러나, 회사채 신용등급 예측 모형의 성과와 관련된 가장 중요한 문제는 등급별 데이터의 불균형 문제이다. 예측 문제에 있어서 데이터 불균형(Data imbalance) 은 사용되는 표본이 특정 범주에 편중되었을 때 나타난다. 데이터 불균형이 심화됨에 따라 범주 사이의 분류경계영역이 왜곡되므로 분류자의 학습성과가 저하되게 된다. 본 연구에서는 데이터 불균형 문제가 존재하는 다분류 문제를 효과적으로 해결하기 위한 다분류 기하평균 부스팅 기법 (Multiclass Geometric Mean-based Boosting MGM-Boost)을 제안하고자 한다. MGM-Boost 알고리즘은 부스팅 알고리즘에 기하평균 개념을 도입한 것으로 오분류된 표본에 대한 학습을 강화할 수 있으며 불균형 분포를 보이는 각 범주의 예측정확도를 동시에 고려한 학습이 가능하다는 장점이 있다. 회사채 신용등급 예측문제를 활용하여 MGM-Boost의 성과를 검증한 결과 SVM 및 AdaBoost 기법과 비교하여 통계적으로 유의적인 성과개선 효과를 보여주었으며 데이터 불균형 하에서도 벤치마킹 모형과 비교하여 견고한 학습성과를 나타냈다."
        },
        {
          "rank": 28,
          "score": 0.6784291863441467,
          "doc_id": "ATN0050138201",
          "title": "머신러닝을 이용한 창업기업의 대출부도 예측",
          "abstract": "최근 중소기업 부도 예측 연구에 머신러닝(Machine Learning)을 활발하게 적용하고 있으나, 창 업기업 대출부도 예측에는 머신러닝을 활용한 연구는 드물다. 중소기업 부도 예측 연구의 미비점을 보완하기 위해 본 연구는 창업기업 대출부도 예측을 위한 머신러닝의 정확성을 평가하는 것을 목적 으로 한다. 이를 위해 국내 창업기업과 일반기업의 대출부도 예측에 가장 정확성이 높은 머신러닝은 각각 무엇이고, 그것은 통계적 모형과 어떤 성능적 차이가 있는지 비교하였다. 실증적 결과로부터 발견된 내용을 정리하면 다음과 같다.첫째, 창업기업의 장단기(1년∼3년) 대출부도 예측에서 정확성과 안정성이 우수한 머신러닝은 아다 부스트(AdaBoost)로 평가할 수 있고, 일반기업의 단기 예측에는 서포트벡터머신(Support Vector Machine)이 좋은 성능을 보여주었다. 둘째, 로지스틱 회귀모형(Logistic Regression) 등 통계적 모형의 창업기업 대출부도 예측 정확성은 머신러닝과 거의 대등한 수준이었다. 오히려 통계적 모형 보다 예측 성과가 미흡한 머신러닝이 존재한다. 셋째, 재무적 특성이 창업기업의 대출부도 예측에도 유의미한 설명력을 제공한다. 창업기업과 일반기업의 장단기 대출부도 예측에 부채상환능력과 유동 성 재무비율들이 유효하였다. 본 분석이 시사하는 바는 머신러닝이 중소기업 대출부도 예측의 정확 성을 확실히 개선하는 것은 분명하나 예측 변수의 설명가능성이 저하되는 단점이 있어 이용 목적에 따라 유용성이 달라진다. 본 논문은 창업기업의 대출부도 예측을 위해 시점간 적합성(Intertemporal Validation) 검증 방법을 통한 다수의 머신러닝과 재무적 특성의 유용성을 평가한 점에서 공헌이 있다",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0050138201&target=NART&cn=ATN0050138201",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝을 이용한 창업기업의 대출부도 예측 머신러닝을 이용한 창업기업의 대출부도 예측 머신러닝을 이용한 창업기업의 대출부도 예측 최근 중소기업 부도 예측 연구에 머신러닝(Machine Learning)을 활발하게 적용하고 있으나, 창 업기업 대출부도 예측에는 머신러닝을 활용한 연구는 드물다. 중소기업 부도 예측 연구의 미비점을 보완하기 위해 본 연구는 창업기업 대출부도 예측을 위한 머신러닝의 정확성을 평가하는 것을 목적 으로 한다. 이를 위해 국내 창업기업과 일반기업의 대출부도 예측에 가장 정확성이 높은 머신러닝은 각각 무엇이고, 그것은 통계적 모형과 어떤 성능적 차이가 있는지 비교하였다. 실증적 결과로부터 발견된 내용을 정리하면 다음과 같다.첫째, 창업기업의 장단기(1년∼3년) 대출부도 예측에서 정확성과 안정성이 우수한 머신러닝은 아다 부스트(AdaBoost)로 평가할 수 있고, 일반기업의 단기 예측에는 서포트벡터머신(Support Vector Machine)이 좋은 성능을 보여주었다. 둘째, 로지스틱 회귀모형(Logistic Regression) 등 통계적 모형의 창업기업 대출부도 예측 정확성은 머신러닝과 거의 대등한 수준이었다. 오히려 통계적 모형 보다 예측 성과가 미흡한 머신러닝이 존재한다. 셋째, 재무적 특성이 창업기업의 대출부도 예측에도 유의미한 설명력을 제공한다. 창업기업과 일반기업의 장단기 대출부도 예측에 부채상환능력과 유동 성 재무비율들이 유효하였다. 본 분석이 시사하는 바는 머신러닝이 중소기업 대출부도 예측의 정확 성을 확실히 개선하는 것은 분명하나 예측 변수의 설명가능성이 저하되는 단점이 있어 이용 목적에 따라 유용성이 달라진다. 본 논문은 창업기업의 대출부도 예측을 위해 시점간 적합성(Intertemporal Validation) 검증 방법을 통한 다수의 머신러닝과 재무적 특성의 유용성을 평가한 점에서 공헌이 있다"
        },
        {
          "rank": 29,
          "score": 0.6774362325668335,
          "doc_id": "JAKO202433861648179",
          "title": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교",
          "abstract": "본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202433861648179&target=NART&cn=JAKO202433861648179",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다."
        },
        {
          "rank": 30,
          "score": 0.6764342784881592,
          "doc_id": "JAKO202518361202534",
          "title": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측",
          "abstract": "본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202518361202534&target=NART&cn=JAKO202518361202534",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다."
        },
        {
          "rank": 31,
          "score": 0.6763963103294373,
          "doc_id": "JAKO202116954704821",
          "title": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측",
          "abstract": "본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116954704821&target=NART&cn=JAKO202116954704821",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다."
        },
        {
          "rank": 32,
          "score": 0.6744670271873474,
          "doc_id": "NART132071160",
          "title": "Integrating machine learning and deep learning for enhanced supplier risk prediction",
          "abstract": "<P>The importance of anticipating and preventing disruptions is underscored by the increased operational complexity and vulnerability caused by advancements in supply chain management (SCM). This has spurred interest in integrating machine learning (ML) and deep learning (DL) into supply chain risk management (SCRM). In this paper, we introduce a tailored method using ML and DL to improve SCRM by predicting supplier failures, thus boosting efficiency and resilience in SC operations. Our method involves five phases focused on classifying and predicting supplier failures in non-conforming deliveries. This involves forecasting failure quantities and estimating total disruption costs. Initially, data from an automotive company is selected, and appropriate potential features and algorithms are selected, performance metric aligns with case study objectives, facilitating method evaluation are used such as: Precision, recall, F1-score, and accuracy metrics assess classification models, while Mean Squared Error (MSE) is used for regression tasks. Finally, an experimental design optimizes models, assessing success rates of various algorithms and their parameters within the chosen feature space. Experimental results underscore the success of our methodology in model development. In the classification task, the Random Forest (RF) classifier achieved 86% accuracy. When combined with the Gradient Boosting classifier, the ensemble exhibited enhanced accuracy, highlighting the complementary strengths of both algorithms and their synergistic impact, surpassing the performance of RF, Support Vector Regression (SVR), k-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). Noteworthy is the performance in regression tasks, where Linear Regression, ANN, and RF Regressor displayed exceptionally low MSE compared to other models.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART132071160&target=NART&cn=NART132071160",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Integrating machine learning and deep learning for enhanced supplier risk prediction Integrating machine learning and deep learning for enhanced supplier risk prediction Integrating machine learning and deep learning for enhanced supplier risk prediction <P>The importance of anticipating and preventing disruptions is underscored by the increased operational complexity and vulnerability caused by advancements in supply chain management (SCM). This has spurred interest in integrating machine learning (ML) and deep learning (DL) into supply chain risk management (SCRM). In this paper, we introduce a tailored method using ML and DL to improve SCRM by predicting supplier failures, thus boosting efficiency and resilience in SC operations. Our method involves five phases focused on classifying and predicting supplier failures in non-conforming deliveries. This involves forecasting failure quantities and estimating total disruption costs. Initially, data from an automotive company is selected, and appropriate potential features and algorithms are selected, performance metric aligns with case study objectives, facilitating method evaluation are used such as: Precision, recall, F1-score, and accuracy metrics assess classification models, while Mean Squared Error (MSE) is used for regression tasks. Finally, an experimental design optimizes models, assessing success rates of various algorithms and their parameters within the chosen feature space. Experimental results underscore the success of our methodology in model development. In the classification task, the Random Forest (RF) classifier achieved 86% accuracy. When combined with the Gradient Boosting classifier, the ensemble exhibited enhanced accuracy, highlighting the complementary strengths of both algorithms and their synergistic impact, surpassing the performance of RF, Support Vector Regression (SVR), k-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). Noteworthy is the performance in regression tasks, where Linear Regression, ANN, and RF Regressor displayed exceptionally low MSE compared to other models.</P>"
        },
        {
          "rank": 33,
          "score": 0.6728672385215759,
          "doc_id": "NART118947969",
          "title": "Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction",
          "abstract": "<P>Recent growth in crop genomic and trait data have opened opportunities for the application of novel approaches to accelerate crop improvement. Machine learning and deep learning are at the forefront of prediction-based data analysis. However, few approaches for genotype to phenotype prediction compare machine learning with deep learning and further interpret the models that support the predictions. This study uses genome wide molecular markers and traits across 1110 soybean individuals to develop accurate prediction models. For 13/14 sets of predictions, XGBoost or random forest outperformed deep learning models in prediction performance. Top ranked SNPs by F-score were identified from XGBoost, and with further investigation found overlap with significantly associated loci identified from GWAS and previous literature. Feature importance rankings were used to reduce marker input by up to 90%, and subsequent models maintained or improved their prediction performance. These findings support interpretable machine learning as an approach for genomic based prediction of traits in soybean and other crops.</P><P><B>Supplementary Information</B></P><P>The online version contains supplementary material available at 10.1186/s12870-022-03559-z.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART118947969&target=NART&cn=NART118947969",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction <P>Recent growth in crop genomic and trait data have opened opportunities for the application of novel approaches to accelerate crop improvement. Machine learning and deep learning are at the forefront of prediction-based data analysis. However, few approaches for genotype to phenotype prediction compare machine learning with deep learning and further interpret the models that support the predictions. This study uses genome wide molecular markers and traits across 1110 soybean individuals to develop accurate prediction models. For 13/14 sets of predictions, XGBoost or random forest outperformed deep learning models in prediction performance. Top ranked SNPs by F-score were identified from XGBoost, and with further investigation found overlap with significantly associated loci identified from GWAS and previous literature. Feature importance rankings were used to reduce marker input by up to 90%, and subsequent models maintained or improved their prediction performance. These findings support interpretable machine learning as an approach for genomic based prediction of traits in soybean and other crops.</P><P><B>Supplementary Information</B></P><P>The online version contains supplementary material available at 10.1186/s12870-022-03559-z.</P>"
        },
        {
          "rank": 34,
          "score": 0.6728067994117737,
          "doc_id": "ATN0027106087",
          "title": "기계학습 방법을 이용한 기업부도의 예측",
          "abstract": "The analysis and management of business failure has been recognized to be important in the area of financial management in the evaluation of firms’ performance and the assessment of their viability. To this end, effective failure-prediction models are needed. This paper describes a new approach to prediction of business failure using the total margin algorithm which is a kind of support vector machine. It will be shown that the proposed method can evaluate the risk of failure better than existing methods through some real data.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0027106087&target=NART&cn=ATN0027106087",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "기계학습 방법을 이용한 기업부도의 예측 기계학습 방법을 이용한 기업부도의 예측 기계학습 방법을 이용한 기업부도의 예측 The analysis and management of business failure has been recognized to be important in the area of financial management in the evaluation of firms’ performance and the assessment of their viability. To this end, effective failure-prediction models are needed. This paper describes a new approach to prediction of business failure using the total margin algorithm which is a kind of support vector machine. It will be shown that the proposed method can evaluate the risk of failure better than existing methods through some real data."
        },
        {
          "rank": 35,
          "score": 0.6721909046173096,
          "doc_id": "ATN0052776138",
          "title": "머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구",
          "abstract": "첨단 무기체계의 도입에 따른 운영유지비 증가와 수리부속 조달환경의 악화로 인해, 정밀한 수요예측의 중요성이 더욱 강조되고 있다. 본 연구는 공군 수리부속의 수요가 소량이며 발생 간격이 불규칙한 특성으로 인해 예측이 어렵다는 점에 착안하여, 기존 통계기반 예측기법의 한계를 극복하고자 머신러닝 및 딥러닝 기반 예측모형을 적용하였다. 국방물자관리체계로 부터 수집한 약 37만 건의 수요 데이터를 유형별(Regular, Intermittent, Erratic, Lumpy)로 분류한 후, Random Forest, XG-Boost, LightGBM, LSTM, N-Beats 5가지 예측모델을 구축하고 성능을 비교하였다. 분석 결과, XG-Boost 모델이 가장 우수한 정확도(79.13%)를 기록하였으며, 그리드 서치를 통한 매개변수 최적화 결과, 품목 기준 최대 81.28%의 예측 정확도를 달성하였다. 본 연구를 통해 세부 품목별 분류 기준 정립, 최적 모델 적용 및 매개변수 튜닝 효율화 등을 통해 공군 수리부속 수요예측의 정확도를 실질적으로 향상시킬 수 있음을 실증적으로 확인하였으며, 이는 대규모 군수 데이터셋에 대한 정량적 분석과 실용적인 예측모형 적용을 통해 현장 활용 가능성이 높은 모델을 제시하였다는 점에서 기존 연구와 차별성을 지닌다. 본 연구의 결과는 향후 공군 및 국방 군수 시스템 전반의 운영 효율성 제고와 자원관리 혁신에 중요한 토대를 제공할 수 있을 것으로 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0052776138&target=NART&cn=ATN0052776138",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 첨단 무기체계의 도입에 따른 운영유지비 증가와 수리부속 조달환경의 악화로 인해, 정밀한 수요예측의 중요성이 더욱 강조되고 있다. 본 연구는 공군 수리부속의 수요가 소량이며 발생 간격이 불규칙한 특성으로 인해 예측이 어렵다는 점에 착안하여, 기존 통계기반 예측기법의 한계를 극복하고자 머신러닝 및 딥러닝 기반 예측모형을 적용하였다. 국방물자관리체계로 부터 수집한 약 37만 건의 수요 데이터를 유형별(Regular, Intermittent, Erratic, Lumpy)로 분류한 후, Random Forest, XG-Boost, LightGBM, LSTM, N-Beats 5가지 예측모델을 구축하고 성능을 비교하였다. 분석 결과, XG-Boost 모델이 가장 우수한 정확도(79.13%)를 기록하였으며, 그리드 서치를 통한 매개변수 최적화 결과, 품목 기준 최대 81.28%의 예측 정확도를 달성하였다. 본 연구를 통해 세부 품목별 분류 기준 정립, 최적 모델 적용 및 매개변수 튜닝 효율화 등을 통해 공군 수리부속 수요예측의 정확도를 실질적으로 향상시킬 수 있음을 실증적으로 확인하였으며, 이는 대규모 군수 데이터셋에 대한 정량적 분석과 실용적인 예측모형 적용을 통해 현장 활용 가능성이 높은 모델을 제시하였다는 점에서 기존 연구와 차별성을 지닌다. 본 연구의 결과는 향후 공군 및 국방 군수 시스템 전반의 운영 효율성 제고와 자원관리 혁신에 중요한 토대를 제공할 수 있을 것으로 기대한다."
        },
        {
          "rank": 36,
          "score": 0.6710610389709473,
          "doc_id": "JAKO201718054814596",
          "title": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석",
          "abstract": "딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201718054814596&target=NART&cn=JAKO201718054814596",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다."
        },
        {
          "rank": 37,
          "score": 0.67055743932724,
          "doc_id": "JAKO202213157607071",
          "title": "기업부도 예측 앙상블 모형의 최적화",
          "abstract": "본 연구에서는 범주 불균형 문제가 내재된 기업부도 예측 AdaBoost 앙상블 모형의 성과를 개선하기 위하여 GMOPTBoost 알고리즘을 제안한다. AdaBoost 알고리즘은 오분류 표본에 대하여 강건한 학습기회를 제공한다는 장점이 있지만, 산술평균 정확도에 기반하기 때문에 범주 불균형 문제를 효과적으로 해결하지 못한다는 한계점이 존재한다. GMOPTBoost는 가우시안 경사하강법(Gaussian gradient descent)을 적용하여 기하평균 정확도를 최적화하고 범주 불균형 문제를 효과적으로 해결할 수 있다는 장점이 있다. 본 연구에서는 첫째, 범주 불균형 문제가 예측 모형의 성과에 미치는 효과와 GMOPTBoost의 성과 개선 효과를 검증하기 위하여 5개의 범주 불균형 데이터를 구성하였으며, 둘째, 범주 균형 데이터에 대한 GMOPTBoost의 성과 개선 효과를 검증하기 위하여 데이터 샘플링 기법을 통하여 구성된 균형 데이터를 구성하였다. 30회의 교차타당성 분석의 주요 결과는 다음과 같다. 첫째, 범주 불균형 문제는 예측 성과에 부정적인 영향을 미친다. 둘째, GMOPTBoost는 불균형 데이터에 적용된 AdaBoost의 성과를 유의적으로 개선시키는 긍정적인 효과를 제공한다. 셋째, 데이터 샘플링 기법은 성과 개선에 긍정적인 영향을 미친다. 마지막으로 데이터 샘플링 기법을 적용한 범주 균형 데이터에서도 GMOPTBoost는 유의적인 성과 개선에 기여한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202213157607071&target=NART&cn=JAKO202213157607071",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "기업부도 예측 앙상블 모형의 최적화 기업부도 예측 앙상블 모형의 최적화 기업부도 예측 앙상블 모형의 최적화 본 연구에서는 범주 불균형 문제가 내재된 기업부도 예측 AdaBoost 앙상블 모형의 성과를 개선하기 위하여 GMOPTBoost 알고리즘을 제안한다. AdaBoost 알고리즘은 오분류 표본에 대하여 강건한 학습기회를 제공한다는 장점이 있지만, 산술평균 정확도에 기반하기 때문에 범주 불균형 문제를 효과적으로 해결하지 못한다는 한계점이 존재한다. GMOPTBoost는 가우시안 경사하강법(Gaussian gradient descent)을 적용하여 기하평균 정확도를 최적화하고 범주 불균형 문제를 효과적으로 해결할 수 있다는 장점이 있다. 본 연구에서는 첫째, 범주 불균형 문제가 예측 모형의 성과에 미치는 효과와 GMOPTBoost의 성과 개선 효과를 검증하기 위하여 5개의 범주 불균형 데이터를 구성하였으며, 둘째, 범주 균형 데이터에 대한 GMOPTBoost의 성과 개선 효과를 검증하기 위하여 데이터 샘플링 기법을 통하여 구성된 균형 데이터를 구성하였다. 30회의 교차타당성 분석의 주요 결과는 다음과 같다. 첫째, 범주 불균형 문제는 예측 성과에 부정적인 영향을 미친다. 둘째, GMOPTBoost는 불균형 데이터에 적용된 AdaBoost의 성과를 유의적으로 개선시키는 긍정적인 효과를 제공한다. 셋째, 데이터 샘플링 기법은 성과 개선에 긍정적인 영향을 미친다. 마지막으로 데이터 샘플링 기법을 적용한 범주 균형 데이터에서도 GMOPTBoost는 유의적인 성과 개선에 기여한다."
        },
        {
          "rank": 38,
          "score": 0.6699861288070679,
          "doc_id": "JAKO201726163356540",
          "title": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델",
          "abstract": "본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201726163356540&target=NART&cn=JAKO201726163356540",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다."
        },
        {
          "rank": 39,
          "score": 0.669974148273468,
          "doc_id": "ATN0048893828",
          "title": "머신러닝을 이용한 우리나라 환율 예측 연구",
          "abstract": "본 연구에서 환율의 예측 능력에 대하여 머신러닝 계열의 방법론과 비-머신러닝 계열의 방법론의 예측 능력을 상고 비교하고자 하였다. 데이터는 2001년부터 2018년을 학습의 기간으로 삼아 2019년을 테스트 하는 실험1과 2001년부터 2017년을 학습 기간으로 삼고 2018년과 2019년을 예측하는 실험2로 나누어 실험을 하였다. 실험1과 실험2 모두에서 머신러닝 계열의 예측이 비-머신러닝 계열의 예측보다 MSE측면에서 우수함을 보였다. 특히 두 실험 모두에서 다층퍼셉트론(MLP)이 매우 우수한 능력을 보였고, KNN을 시계열로 확장을 한 TSFKNN과 신경망을 시계열로 확장을 한 NNETAR 두 가지 모두 유사한 능력을 보였다. 전통적인 비-먼신러닝 계열에서는 충분히 데이터에 대한 특성이 파악이 되지 않아 낮은 수준의 예측 능력을 보이는 것으로 판단이 된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0048893828&target=NART&cn=ATN0048893828",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝을 이용한 우리나라 환율 예측 연구 머신러닝을 이용한 우리나라 환율 예측 연구 머신러닝을 이용한 우리나라 환율 예측 연구 본 연구에서 환율의 예측 능력에 대하여 머신러닝 계열의 방법론과 비-머신러닝 계열의 방법론의 예측 능력을 상고 비교하고자 하였다. 데이터는 2001년부터 2018년을 학습의 기간으로 삼아 2019년을 테스트 하는 실험1과 2001년부터 2017년을 학습 기간으로 삼고 2018년과 2019년을 예측하는 실험2로 나누어 실험을 하였다. 실험1과 실험2 모두에서 머신러닝 계열의 예측이 비-머신러닝 계열의 예측보다 MSE측면에서 우수함을 보였다. 특히 두 실험 모두에서 다층퍼셉트론(MLP)이 매우 우수한 능력을 보였고, KNN을 시계열로 확장을 한 TSFKNN과 신경망을 시계열로 확장을 한 NNETAR 두 가지 모두 유사한 능력을 보였다. 전통적인 비-먼신러닝 계열에서는 충분히 데이터에 대한 특성이 파악이 되지 않아 낮은 수준의 예측 능력을 보이는 것으로 판단이 된다."
        },
        {
          "rank": 40,
          "score": 0.669842004776001,
          "doc_id": "JAKO202520454002736",
          "title": "산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구",
          "abstract": "In industrial settings, inverters play a critical role in maintaining productivity and ensuring stable equipment operation. However, inverter failures can result in production downtime and increased maintenance costs. Traditional fault prediction methods based on physical models and expert experience often struggle to capture complex patterns and adapt to varying operational conditions. To address this, this study evaluates the performance of statistical, machine learning, and deep learning approaches for industrial inverter fault prediction, using operational data from a 90W-class inverter at an automotive parts manufacturer in Daegu, South Korea. The experimental results demonstrate that unsupervised anomaly detection models, particularly Autoencoder and SOM, achieved the highest accuracy. These findings suggest that models capable of detecting deviations from normal operating patterns are more effective for inverter fault prediction than conventional methods. In contrast, SVM and Logistic Regression exhibited limitations in handling time-series complexity. This study highlights the necessity of deploying real-time monitoring and predictive maintenance systems in industrial environments, with future research focusing on hyperparameter optimization and real-time data streaming validation.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202520454002736&target=NART&cn=JAKO202520454002736",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 In industrial settings, inverters play a critical role in maintaining productivity and ensuring stable equipment operation. However, inverter failures can result in production downtime and increased maintenance costs. Traditional fault prediction methods based on physical models and expert experience often struggle to capture complex patterns and adapt to varying operational conditions. To address this, this study evaluates the performance of statistical, machine learning, and deep learning approaches for industrial inverter fault prediction, using operational data from a 90W-class inverter at an automotive parts manufacturer in Daegu, South Korea. The experimental results demonstrate that unsupervised anomaly detection models, particularly Autoencoder and SOM, achieved the highest accuracy. These findings suggest that models capable of detecting deviations from normal operating patterns are more effective for inverter fault prediction than conventional methods. In contrast, SVM and Logistic Regression exhibited limitations in handling time-series complexity. This study highlights the necessity of deploying real-time monitoring and predictive maintenance systems in industrial environments, with future research focusing on hyperparameter optimization and real-time data streaming validation."
        },
        {
          "rank": 41,
          "score": 0.6698266267776489,
          "doc_id": "JAKO202108848920380",
          "title": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구",
          "abstract": "The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202108848920380&target=NART&cn=JAKO202108848920380",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity."
        },
        {
          "rank": 42,
          "score": 0.6682584285736084,
          "doc_id": "JAKO202012758284659",
          "title": "딥러닝을 활용한 다목적댐 유입량 예측",
          "abstract": "최근 데이터 예측 방법으로 인공신경망(Artificial Neural Network, ANN)분야에 대한 관심이 높아졌으며, 그 중 시계열 데이터 예측에 특화된 LSTM(Long Short-Term Memory)모형은 수문 시계열자료의 예측방법으로도 활용되고 있다. 본 연구에서는 구글에서 제공하는 딥러닝 오픈소스 라이브러리인 텐서플로우(TensorFlow)를 활용하여 LSTM모형을 구축하고 금강 상류에 위치한 용담다목적댐의 유입량을 예측하였다. 분석 자료로는 WAMIS에서 제공하는 용담댐의 2006년부터 2018년까지의 시간당 유입량 자료를 사용하였으며, 예측된 유입량과 관측 유입량의 비교를 통하여 평균제곱오차(RMSE), 평균절대오차(MAE), 용적오차(VE)를 계산하고 모형의 학습변수에 따른 정확도를 평가하였다. 분석결과, 모든 모형이 고유량에서의 정확도가 낮은 것으로 나타났으며, 이와 같은 문제를 해결하기 위하여 용담댐 유역의 시간당 강수량 자료를 추가 학습 자료로 활용하여 분석한 결과, 고유량에 대한 예측의 정확도가 높아지는 것을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202012758284659&target=NART&cn=JAKO202012758284659",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 활용한 다목적댐 유입량 예측 딥러닝을 활용한 다목적댐 유입량 예측 딥러닝을 활용한 다목적댐 유입량 예측 최근 데이터 예측 방법으로 인공신경망(Artificial Neural Network, ANN)분야에 대한 관심이 높아졌으며, 그 중 시계열 데이터 예측에 특화된 LSTM(Long Short-Term Memory)모형은 수문 시계열자료의 예측방법으로도 활용되고 있다. 본 연구에서는 구글에서 제공하는 딥러닝 오픈소스 라이브러리인 텐서플로우(TensorFlow)를 활용하여 LSTM모형을 구축하고 금강 상류에 위치한 용담다목적댐의 유입량을 예측하였다. 분석 자료로는 WAMIS에서 제공하는 용담댐의 2006년부터 2018년까지의 시간당 유입량 자료를 사용하였으며, 예측된 유입량과 관측 유입량의 비교를 통하여 평균제곱오차(RMSE), 평균절대오차(MAE), 용적오차(VE)를 계산하고 모형의 학습변수에 따른 정확도를 평가하였다. 분석결과, 모든 모형이 고유량에서의 정확도가 낮은 것으로 나타났으며, 이와 같은 문제를 해결하기 위하여 용담댐 유역의 시간당 강수량 자료를 추가 학습 자료로 활용하여 분석한 결과, 고유량에 대한 예측의 정확도가 높아지는 것을 알 수 있었다."
        },
        {
          "rank": 43,
          "score": 0.6670564413070679,
          "doc_id": "JAKO202121055483964",
          "title": "딥러닝을 통한 드론의 비정상 진동 예측",
          "abstract": "본 논문에서는 드론의 추락을 예방하기 위해 드론의 프로펠러와 연결된 모터로부터 진동 데이터를 수집하고 순환 신경망(recurrent neural network, RNN)과 long short term memory (LSTM)을 사용하여 드론의 비정상 진동을 예측하는 연구를 진행하였다. 드론의 비정상 진동 데이터를 수집하기 위해 드론의 프로펠러와 연결된 모터에 진동 센서를 부착하여 정상, 바(bar) 손상, 로터(rotor) 손상, 축 휨에 대한 진동 데이터를 수집하고 LSTM과 RNN을 통해 비정상 진동을 예측한 결과의 평균 제곱근 오차 (root mean square error, RMSE) 값을 비교분석 하였다. 시뮬레이션 비교 결과, RNN과 LSTM을 통해 예측한 결과 모두 비정상 진동 패턴을 매우 정확하게 예측하는 것을 확인하였으며 LSTM을 통해 예측한 진동이 RNN을 통해 예측한 진동보다 RMSE값이 평균 15.4% 낮은 것을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202121055483964&target=NART&cn=JAKO202121055483964",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 통한 드론의 비정상 진동 예측 딥러닝을 통한 드론의 비정상 진동 예측 딥러닝을 통한 드론의 비정상 진동 예측 본 논문에서는 드론의 추락을 예방하기 위해 드론의 프로펠러와 연결된 모터로부터 진동 데이터를 수집하고 순환 신경망(recurrent neural network, RNN)과 long short term memory (LSTM)을 사용하여 드론의 비정상 진동을 예측하는 연구를 진행하였다. 드론의 비정상 진동 데이터를 수집하기 위해 드론의 프로펠러와 연결된 모터에 진동 센서를 부착하여 정상, 바(bar) 손상, 로터(rotor) 손상, 축 휨에 대한 진동 데이터를 수집하고 LSTM과 RNN을 통해 비정상 진동을 예측한 결과의 평균 제곱근 오차 (root mean square error, RMSE) 값을 비교분석 하였다. 시뮬레이션 비교 결과, RNN과 LSTM을 통해 예측한 결과 모두 비정상 진동 패턴을 매우 정확하게 예측하는 것을 확인하였으며 LSTM을 통해 예측한 진동이 RNN을 통해 예측한 진동보다 RMSE값이 평균 15.4% 낮은 것을 확인하였다."
        },
        {
          "rank": 44,
          "score": 0.6656604409217834,
          "doc_id": "JAKO202108360626662",
          "title": "딥러닝을 이용한 외해 해양기상자료로부터의 항내파고 예측",
          "abstract": "본 연구에서는 항내 파고를 신속하고 비교적 정확하게 예측할 수 있는 딥러닝 모델을 구축하였다.다양한 머신러닝 기법들을 외해파랑의 항내로 전파 변형 특성을 감안하여 모델에 적용하였으며 스웰로 인해 하역중단 문제가 심각했던 포항신항을 모델적용 대상지로 선정하였다. 모델의 입력 자료는 외해의 파고, 주기, 파향 그리고 출력 및 예측 자료로는 항내 파고자료로 하여 모델을 학습시켰다. 이때 자료의 전처리 과정으로 항내&#x00B7;외 파랑 시계열자료의 상관성을 감안하여 파향 자료를 분리하는 방법을 적용하고 딥러닝 기법을 이용하여 모델을 학습하였다. 결과적으로 모델을 통해 예측한 값이 항내관측치의 파고 시계열자료를 잘 재현하였으며 모델의 안정성을 크게 향상시켰다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202108360626662&target=NART&cn=JAKO202108360626662",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 이용한 외해 해양기상자료로부터의 항내파고 예측 딥러닝을 이용한 외해 해양기상자료로부터의 항내파고 예측 딥러닝을 이용한 외해 해양기상자료로부터의 항내파고 예측 본 연구에서는 항내 파고를 신속하고 비교적 정확하게 예측할 수 있는 딥러닝 모델을 구축하였다.다양한 머신러닝 기법들을 외해파랑의 항내로 전파 변형 특성을 감안하여 모델에 적용하였으며 스웰로 인해 하역중단 문제가 심각했던 포항신항을 모델적용 대상지로 선정하였다. 모델의 입력 자료는 외해의 파고, 주기, 파향 그리고 출력 및 예측 자료로는 항내 파고자료로 하여 모델을 학습시켰다. 이때 자료의 전처리 과정으로 항내&#x00B7;외 파랑 시계열자료의 상관성을 감안하여 파향 자료를 분리하는 방법을 적용하고 딥러닝 기법을 이용하여 모델을 학습하였다. 결과적으로 모델을 통해 예측한 값이 항내관측치의 파고 시계열자료를 잘 재현하였으며 모델의 안정성을 크게 향상시켰다."
        },
        {
          "rank": 45,
          "score": 0.6652770042419434,
          "doc_id": "NART98545871",
          "title": "Review of bankruptcy prediction using machine learning and deep learning techniques",
          "abstract": "<P><B>Abstract</B></P>  <P>Bankruptcy prediction has long been a significant issue in finance and management science, which attracts the attention of researchers and practitioners. With the great development of modern information technology, it has evolved into using machine learning or deep learning algorithms to do the prediction, from the initial analysis of financial statements. In this paper, we will review the machine learning or deep learning models used in bankruptcy prediction, including the classical machine learning models such as Multivariant Discriminant Analysis (MDA), Logistic Regression (LR), Ensemble method, Neural Networks (NN) and Support Vector Machines (SVM), and major deep learning methods such as Deep Belief Network (DBN) and Convolutional Neural Network (CNN). In each model, the specific process of experiment and characteristics will be summarized through analyzing some typical articles. Finally, possible innovative changes of bankruptcy prediction and its future trends will be discussed.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART98545871&target=NART&cn=NART98545871",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Review of bankruptcy prediction using machine learning and deep learning techniques Review of bankruptcy prediction using machine learning and deep learning techniques Review of bankruptcy prediction using machine learning and deep learning techniques <P><B>Abstract</B></P>  <P>Bankruptcy prediction has long been a significant issue in finance and management science, which attracts the attention of researchers and practitioners. With the great development of modern information technology, it has evolved into using machine learning or deep learning algorithms to do the prediction, from the initial analysis of financial statements. In this paper, we will review the machine learning or deep learning models used in bankruptcy prediction, including the classical machine learning models such as Multivariant Discriminant Analysis (MDA), Logistic Regression (LR), Ensemble method, Neural Networks (NN) and Support Vector Machines (SVM), and major deep learning methods such as Deep Belief Network (DBN) and Convolutional Neural Network (CNN). In each model, the specific process of experiment and characteristics will be summarized through analyzing some typical articles. Finally, possible innovative changes of bankruptcy prediction and its future trends will be discussed.</P>"
        },
        {
          "rank": 46,
          "score": 0.6639634370803833,
          "doc_id": "ATN0037496660",
          "title": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교",
          "abstract": "Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037496660&target=NART&cn=ATN0037496660",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model."
        },
        {
          "rank": 47,
          "score": 0.6627801656723022,
          "doc_id": "JAKO202029462558904",
          "title": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식",
          "abstract": "특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202029462558904&target=NART&cn=JAKO202029462558904",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다."
        },
        {
          "rank": 48,
          "score": 0.6626695990562439,
          "doc_id": "JAKO199911921383665",
          "title": "회귀신경망을 이용한 음성인식에 관한 연구",
          "abstract": "본 논문은 회귀신경망을 이용한 음성인식에 관한 연구이다. 예측형 신경망으로 음절단위로 모델링한 후 미지의 입력음성에 대하여 예측오차가 최소가 되는 모델을 인식결과로 한다. 이를 위해서 예측형으로 구성된 신경망에 음성의 시변성을 신경망 내부에 흡수시키기 위해서 회귀구조의 동적인 신경망인 회귀예측신경망을 구성하고 Elman과 Jordan이 제안한 회귀구조에 따라 인식성능을 서로 비교하였다. 음성DB는 ETRI의 샘돌이 음성 데이터를 사용하였다. 그리고, 신경망의 최적모델을 구하기 위하여 예측차수와 은닉층 유니트 수의 변화에 따른 인식률의 변화와 문맥층에서 자기회귀계수를 두어 이전의 값들이 문맥층에서 누적되도록 하였을 경우에 대한 인식률의 변화를 비교하였다. 실험결과, 최적의 예측차수, 은닉층 유니트수, 자기회귀계수는 신경망의 구조에 따라 차이가 나타났으며, 전반적으로 Jordan망이 Elman망보다 인식률이 높았으며, 자기회귀계수에 대한 영향은 신경망의 구조와 계수값에 따라 불규칙하게 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO199911921383665&target=NART&cn=JAKO199911921383665",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회귀신경망을 이용한 음성인식에 관한 연구 회귀신경망을 이용한 음성인식에 관한 연구 회귀신경망을 이용한 음성인식에 관한 연구 본 논문은 회귀신경망을 이용한 음성인식에 관한 연구이다. 예측형 신경망으로 음절단위로 모델링한 후 미지의 입력음성에 대하여 예측오차가 최소가 되는 모델을 인식결과로 한다. 이를 위해서 예측형으로 구성된 신경망에 음성의 시변성을 신경망 내부에 흡수시키기 위해서 회귀구조의 동적인 신경망인 회귀예측신경망을 구성하고 Elman과 Jordan이 제안한 회귀구조에 따라 인식성능을 서로 비교하였다. 음성DB는 ETRI의 샘돌이 음성 데이터를 사용하였다. 그리고, 신경망의 최적모델을 구하기 위하여 예측차수와 은닉층 유니트 수의 변화에 따른 인식률의 변화와 문맥층에서 자기회귀계수를 두어 이전의 값들이 문맥층에서 누적되도록 하였을 경우에 대한 인식률의 변화를 비교하였다. 실험결과, 최적의 예측차수, 은닉층 유니트수, 자기회귀계수는 신경망의 구조에 따라 차이가 나타났으며, 전반적으로 Jordan망이 Elman망보다 인식률이 높았으며, 자기회귀계수에 대한 영향은 신경망의 구조와 계수값에 따라 불규칙하게 나타났다."
        },
        {
          "rank": 49,
          "score": 0.6623343229293823,
          "doc_id": "JAKO201722163438451",
          "title": "딥러닝과 통계 모델을 이용한 T-커머스 매출 예측",
          "abstract": "T-커머스는 양방향 디지털 TV를 기반으로 양방향 데이터방송 기술을 활용하여 상거래를 하는 기술융합형 서비스이다. 채널 번호와 판매상품이 제한된 환경에서 T-커머스의 매출을 극대화 하기 위해서는 각 제품의 시간대별 경쟁력을 고려하여 매출이 최대화 되도록 프로그램을 편성해야 한다. 이를 위해, 본 논문에서는 딥러닝을 이용해 T-커머스에서 각 상품을 각 시간대에 편성하였을 때의 매출을 예측하는 방법을 제안한다. 제안하는 방법은 심층신경망을 이용해 판매 상품과 시간대, 주차, 휴일 여부, 그리고 날씨를 입력 받아 실제 방송으로 편성했을 때 기대되는 매출을 예측한다. 그리고, 통계적 모델과 SVD(Singular Value Decomposition)를 적용하여 판매 데이터의 편중 및 희박성 문제를 완화한다. 실제 T-커머스 운영자인 (주)더블유쇼핑의 판매 기록 데이터에 대하여 실험하였을 때 실제 매출과 예측치의 차이가 0.12의 NMAE(Normalized Mean Absolute Error)를 보여 제안하는 알고리즘이 효과적으로 동작함을 확인하였다. 제안된 시스템은 (주)더블유쇼핑의 T-커머스 시스템 적용되어 방송 편성에 활용되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201722163438451&target=NART&cn=JAKO201722163438451",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝과 통계 모델을 이용한 T-커머스 매출 예측 딥러닝과 통계 모델을 이용한 T-커머스 매출 예측 딥러닝과 통계 모델을 이용한 T-커머스 매출 예측 T-커머스는 양방향 디지털 TV를 기반으로 양방향 데이터방송 기술을 활용하여 상거래를 하는 기술융합형 서비스이다. 채널 번호와 판매상품이 제한된 환경에서 T-커머스의 매출을 극대화 하기 위해서는 각 제품의 시간대별 경쟁력을 고려하여 매출이 최대화 되도록 프로그램을 편성해야 한다. 이를 위해, 본 논문에서는 딥러닝을 이용해 T-커머스에서 각 상품을 각 시간대에 편성하였을 때의 매출을 예측하는 방법을 제안한다. 제안하는 방법은 심층신경망을 이용해 판매 상품과 시간대, 주차, 휴일 여부, 그리고 날씨를 입력 받아 실제 방송으로 편성했을 때 기대되는 매출을 예측한다. 그리고, 통계적 모델과 SVD(Singular Value Decomposition)를 적용하여 판매 데이터의 편중 및 희박성 문제를 완화한다. 실제 T-커머스 운영자인 (주)더블유쇼핑의 판매 기록 데이터에 대하여 실험하였을 때 실제 매출과 예측치의 차이가 0.12의 NMAE(Normalized Mean Absolute Error)를 보여 제안하는 알고리즘이 효과적으로 동작함을 확인하였다. 제안된 시스템은 (주)더블유쇼핑의 T-커머스 시스템 적용되어 방송 편성에 활용되었다."
        },
        {
          "rank": 50,
          "score": 0.6608390808105469,
          "doc_id": "NART123583722",
          "title": "Sediment load prediction in Johor river: deep learning versus machine learning models",
          "abstract": "<P><B>Abstract</B><P>Sediment transport is a normal phenomenon in rivers and streams, contributing significantly to ecosystem production and preservation by replenishing vital nutrients and preserving aquatic life&rsquo;s natural habitats. Thus, sediment transport prediction through modeling is crucial for predicting flood events, tracking coastal erosion, planning for water supplies, and managing irrigation. The predictability of process-driven models may encounter various restrictions throughout the validation process. Given that data-driven models work on the assumption that the underlying physical process is not requisite, this opens up the avenue for AI-based model as alternative modeling. However, AI-based models, such as ANN and SVM, face problems, such as long-term dependency, which require alternative dynamic procedures. Since their performance as universal function approximation depends on their compatibility with the nature of the problem itself, this study investigated several distinct AI-based models, such as long short-term memory (LSTM), artificial neural network (ANN), and support vector machine (SVM), in predicting sediment transport in the Johor river. The collected historical daily sediment transport data from January 1, 2008, to December 01, 2018, through autocorrelation function, were used as input for the model. The statistical results showed that, despite their ability (deep learning and machine learning) to provide sediment predictions based on historical input datasets, machine learning, such as ANN, might be more prone to overfitting or being trapped in a local optimum than deep learning, evidenced by the worse in all metrics score. With RMSE = 11.395, MAE = 18.094, and <I>R</I>2 = 0.914, LSTM outperformed other models in the comparison.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART123583722&target=NART&cn=NART123583722",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Sediment load prediction in Johor river: deep learning versus machine learning models Sediment load prediction in Johor river: deep learning versus machine learning models Sediment load prediction in Johor river: deep learning versus machine learning models <P><B>Abstract</B><P>Sediment transport is a normal phenomenon in rivers and streams, contributing significantly to ecosystem production and preservation by replenishing vital nutrients and preserving aquatic life&rsquo;s natural habitats. Thus, sediment transport prediction through modeling is crucial for predicting flood events, tracking coastal erosion, planning for water supplies, and managing irrigation. The predictability of process-driven models may encounter various restrictions throughout the validation process. Given that data-driven models work on the assumption that the underlying physical process is not requisite, this opens up the avenue for AI-based model as alternative modeling. However, AI-based models, such as ANN and SVM, face problems, such as long-term dependency, which require alternative dynamic procedures. Since their performance as universal function approximation depends on their compatibility with the nature of the problem itself, this study investigated several distinct AI-based models, such as long short-term memory (LSTM), artificial neural network (ANN), and support vector machine (SVM), in predicting sediment transport in the Johor river. The collected historical daily sediment transport data from January 1, 2008, to December 01, 2018, through autocorrelation function, were used as input for the model. The statistical results showed that, despite their ability (deep learning and machine learning) to provide sediment predictions based on historical input datasets, machine learning, such as ANN, might be more prone to overfitting or being trapped in a local optimum than deep learning, evidenced by the worse in all metrics score. With RMSE = 11.395, MAE = 18.094, and <I>R</I>2 = 0.914, LSTM outperformed other models in the comparison.</P></P>"
        }
      ]
    },
    {
      "query": "기업부도 예측에서 DBN 기반 딥 러닝과 기존 SVM 방법 간의 전반적인 성능 차이는 무엇인가요?",
      "query_meta": {
        "type": "single_hop",
        "index": 0
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8409717082977295,
          "doc_id": "DIKO0014169472",
          "title": "딥러닝 알고리즘에 기반한 기업부도 예측",
          "abstract": "기업의 부도는 국가경제에 막대한 손실을 입히며, 해당기업의 이해관계자들 모두에게 경제적 손실을 초래하고 사회적 부를 감소시킨다. 따라서 기업의 부도를 좀 더 정확하게 예측하는 것은 사회적·경제적 측면에서 매우 중요한 연구라 할 수 있다. &amp;#xD; 이에 최근 이미지 인식, 음성 인식, 자연어 처리 등 여러 분야에서 우수한 예측력을 보여주고 있는 딥러닝(Deep Learning)을 기업부도예측에 이용하고자 하며, 본 논문에서는 기업부도예측 방법으로 여러 딥러닝 알고리즘 중 DBN(Deep Belief Network)을 제안한다. 기존에 사용되던 분석기법 대비 우수성을 확인하기 위해 최근까지 기업부도예측에서 연구되고 있는 SVM(Support Vector Machine)과 비교하고자 하였으며, 1999년부터 2015년 사이에 국내 코스닥·코스피에 상장된 비금융업의 기업데이터를 이용하였다. 건실기업의 수는 1669개, 부도기업의 수는 495개이며, 한국은행의 기업경영분석에서 소개된 재무비율 변수를 이용하여 분석을 진행하였다. 분석결과 DBN이 SVM보다 여러 평가척도에서 더 좋은 성능을 보였다. 특히 시험데이터에 대해 부도기업을 부도기업으로 예측하는 민감도에서 5%이상의 더 뛰어난 성능을 보였으며, 이에 기업부도예측분야에 딥러닝의 적용가능성을 확인해 볼 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014169472&target=NART&cn=DIKO0014169472",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 알고리즘에 기반한 기업부도 예측 딥러닝 알고리즘에 기반한 기업부도 예측 딥러닝 알고리즘에 기반한 기업부도 예측 기업의 부도는 국가경제에 막대한 손실을 입히며, 해당기업의 이해관계자들 모두에게 경제적 손실을 초래하고 사회적 부를 감소시킨다. 따라서 기업의 부도를 좀 더 정확하게 예측하는 것은 사회적·경제적 측면에서 매우 중요한 연구라 할 수 있다. &amp;#xD; 이에 최근 이미지 인식, 음성 인식, 자연어 처리 등 여러 분야에서 우수한 예측력을 보여주고 있는 딥러닝(Deep Learning)을 기업부도예측에 이용하고자 하며, 본 논문에서는 기업부도예측 방법으로 여러 딥러닝 알고리즘 중 DBN(Deep Belief Network)을 제안한다. 기존에 사용되던 분석기법 대비 우수성을 확인하기 위해 최근까지 기업부도예측에서 연구되고 있는 SVM(Support Vector Machine)과 비교하고자 하였으며, 1999년부터 2015년 사이에 국내 코스닥·코스피에 상장된 비금융업의 기업데이터를 이용하였다. 건실기업의 수는 1669개, 부도기업의 수는 495개이며, 한국은행의 기업경영분석에서 소개된 재무비율 변수를 이용하여 분석을 진행하였다. 분석결과 DBN이 SVM보다 여러 평가척도에서 더 좋은 성능을 보였다. 특히 시험데이터에 대해 부도기업을 부도기업으로 예측하는 민감도에서 5%이상의 더 뛰어난 성능을 보였으며, 이에 기업부도예측분야에 딥러닝의 적용가능성을 확인해 볼 수 있었다."
        },
        {
          "rank": 2,
          "score": 0.7495540380477905,
          "doc_id": "JAKO201614137727823",
          "title": "딥러닝 기법을 이용한 내일강수 예측",
          "abstract": "정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로 기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본 논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도 사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의 AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를 사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로 사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의 척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201614137727823&target=NART&cn=JAKO201614137727823",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기법을 이용한 내일강수 예측 딥러닝 기법을 이용한 내일강수 예측 딥러닝 기법을 이용한 내일강수 예측 정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로 기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본 논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도 사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의 AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를 사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로 사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의 척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다."
        },
        {
          "rank": 3,
          "score": 0.726906418800354,
          "doc_id": "JAKO201336161064414",
          "title": "앙상블 SVM 모형을 이용한 기업 부도 예측",
          "abstract": "기업의 부도를 예측하는 것은 회계나 재무 분야에서 중요한 연구주제이다. 지금까지 기업 부도예측을 위해 여러 가지 데이터마이닝 기법들이 적용되었으나 주로 단일 모형을 사용함으로서 복잡한 분류 문제에의 적용에 한계를 갖고 있었다. 본 논문에서는 최근에 각광받고 있는 SVM (support vector machine) 모형들을 결합한 앙상블 SVM 모형 (ensemble SVM model)을 부도예측에 사용하고자 한다. 제안된 앙상블 모형은 v-조각 교차 타당성 (v-fold cross-validation)에 의해 얻어진 여러 가지 모형 중에서 성능이 좋은 상위 k개의 단일 모형으로 구성하고 과반수 투표 방식 (majority voting)을 사용하여 미지의 클래스를 분류한다. 본 논문에서 제안된 앙상블 SVM 모형의 성능을 평가하기 위해 실제 기업의 재무비율 자료와 모의실험자료를 가지고 실험하였고, 실험결과 제안된 앙상블 모형이 여러 가지 평가척도 하에서 단일 SVM 모형들보다 좋은 성능을 보임을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201336161064414&target=NART&cn=JAKO201336161064414",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "앙상블 SVM 모형을 이용한 기업 부도 예측 앙상블 SVM 모형을 이용한 기업 부도 예측 앙상블 SVM 모형을 이용한 기업 부도 예측 기업의 부도를 예측하는 것은 회계나 재무 분야에서 중요한 연구주제이다. 지금까지 기업 부도예측을 위해 여러 가지 데이터마이닝 기법들이 적용되었으나 주로 단일 모형을 사용함으로서 복잡한 분류 문제에의 적용에 한계를 갖고 있었다. 본 논문에서는 최근에 각광받고 있는 SVM (support vector machine) 모형들을 결합한 앙상블 SVM 모형 (ensemble SVM model)을 부도예측에 사용하고자 한다. 제안된 앙상블 모형은 v-조각 교차 타당성 (v-fold cross-validation)에 의해 얻어진 여러 가지 모형 중에서 성능이 좋은 상위 k개의 단일 모형으로 구성하고 과반수 투표 방식 (majority voting)을 사용하여 미지의 클래스를 분류한다. 본 논문에서 제안된 앙상블 SVM 모형의 성능을 평가하기 위해 실제 기업의 재무비율 자료와 모의실험자료를 가지고 실험하였고, 실험결과 제안된 앙상블 모형이 여러 가지 평가척도 하에서 단일 SVM 모형들보다 좋은 성능을 보임을 알 수 있었다."
        },
        {
          "rank": 4,
          "score": 0.7201164960861206,
          "doc_id": "JAKO201403359939237",
          "title": "개선된 배깅 앙상블을 활용한 기업부도예측",
          "abstract": "기업의 부도 예측은 재무 및 회계 분야에서 매우 중요한 연구 주제이다. 기업의 부도로 인해 발생하는 비용이 매우 크기 때문에 부도 예측의 정확성은 금융기관으로서는 매우 중요한 일이다. 최근에는 여러 개의 모형을 결합하는 앙상블 모형을 부도 예측에 적용해 보려는 연구가 큰 관심을 끌고 있다. 앙상블 모형은 개별 모형보다 더 좋은 성과를 내기 위해 여러 개의 분류기를 결합하는 것이다. 이와 같은 앙상블 분류기는 분류기의 일반화 성능을 개선하는 데 매우 유용한 것으로 알려져 있다. 본 논문은 부도 예측 모형의 성과 개선에 관한 연구이다. 이를 위해 사례 선택(Instance Selection)을 활용한 배깅(Bagging) 모형을 제안하였다. 사례 선택은 원 데이터에서 가장 대표성 있고 관련성 높은 데이터를 선택하고 예측 모형에 악영향을 줄 수 있는 불필요한 데이터를 제거하는 것으로 이를 통해 예측 성과 개선도 기대할 수 있다. 배깅은 학습데이터에 변화를 줌으로써 기저 분류기들을 다양화시키는 앙상블 기법으로 단순하면서도 성과가 매우 좋은 것으로 알려져 있다. 사례 선택과 배깅은 각각 모형의 성과를 개선시킬 수 있는 잠재력이 있지만 이들 두 기법의 결합에 관한 연구는 아직까지 없는 것이 현실이다. 본 연구에서는 부도 예측 모형의 성과를 개선하기 위해 사례 선택과 배깅을 연결하는 새로운 모형을 제안하였다. 최적의 사례 선택을 위해 유전자 알고리즘이 사용되었으며, 이를 통해 최적의 사례 선택 조합을 찾고 이 결과를 배깅 앙상블 모형에 전달하여 새로운 형태의 배깅 앙상블 모형을 구성하게 된다. 본 연구에서 제안한 새로운 앙상블 모형의 성과를 검증하기 위해 ROC 커브, AUC, 예측정확도 등과 같은 성과지표를 사용해 다양한 모형과 비교 분석해 보았다. 실제 기업데이터를 사용해 실험한 결과 본 논문에서 제안한 새로운 형태의 모형이 가장 좋은 성과를 보임을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201403359939237&target=NART&cn=JAKO201403359939237",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "개선된 배깅 앙상블을 활용한 기업부도예측 개선된 배깅 앙상블을 활용한 기업부도예측 개선된 배깅 앙상블을 활용한 기업부도예측 기업의 부도 예측은 재무 및 회계 분야에서 매우 중요한 연구 주제이다. 기업의 부도로 인해 발생하는 비용이 매우 크기 때문에 부도 예측의 정확성은 금융기관으로서는 매우 중요한 일이다. 최근에는 여러 개의 모형을 결합하는 앙상블 모형을 부도 예측에 적용해 보려는 연구가 큰 관심을 끌고 있다. 앙상블 모형은 개별 모형보다 더 좋은 성과를 내기 위해 여러 개의 분류기를 결합하는 것이다. 이와 같은 앙상블 분류기는 분류기의 일반화 성능을 개선하는 데 매우 유용한 것으로 알려져 있다. 본 논문은 부도 예측 모형의 성과 개선에 관한 연구이다. 이를 위해 사례 선택(Instance Selection)을 활용한 배깅(Bagging) 모형을 제안하였다. 사례 선택은 원 데이터에서 가장 대표성 있고 관련성 높은 데이터를 선택하고 예측 모형에 악영향을 줄 수 있는 불필요한 데이터를 제거하는 것으로 이를 통해 예측 성과 개선도 기대할 수 있다. 배깅은 학습데이터에 변화를 줌으로써 기저 분류기들을 다양화시키는 앙상블 기법으로 단순하면서도 성과가 매우 좋은 것으로 알려져 있다. 사례 선택과 배깅은 각각 모형의 성과를 개선시킬 수 있는 잠재력이 있지만 이들 두 기법의 결합에 관한 연구는 아직까지 없는 것이 현실이다. 본 연구에서는 부도 예측 모형의 성과를 개선하기 위해 사례 선택과 배깅을 연결하는 새로운 모형을 제안하였다. 최적의 사례 선택을 위해 유전자 알고리즘이 사용되었으며, 이를 통해 최적의 사례 선택 조합을 찾고 이 결과를 배깅 앙상블 모형에 전달하여 새로운 형태의 배깅 앙상블 모형을 구성하게 된다. 본 연구에서 제안한 새로운 앙상블 모형의 성과를 검증하기 위해 ROC 커브, AUC, 예측정확도 등과 같은 성과지표를 사용해 다양한 모형과 비교 분석해 보았다. 실제 기업데이터를 사용해 실험한 결과 본 논문에서 제안한 새로운 형태의 모형이 가장 좋은 성과를 보임을 알 수 있었다."
        },
        {
          "rank": 5,
          "score": 0.7191941738128662,
          "doc_id": "DIKO0013372384",
          "title": "앙상블 SVM을 이용한 기업 부도 예측 모형",
          "abstract": "Bankruptcy prediction has been an important topic in the accounting and finance field for a long time. Several data mining techniques have been used for bankruptcy prediction. However, there are many limits for application to real classification problem with a single model. This study proposes ensemble SVM (support vector machine) model which assembles different SVM models with each different kernel functions. Our ensemble model is made and evaluated by v-fold cross-validation approach. The top performing models are recruited into the ensemble. The classification is then carried out using the majority voting opinion of the ensemble. The performance of the ensemble SVM classifier is investigated in terms of accuracy, error rate, sensitivity, specificity, ROC curve, and AUC to compare with single SVM classifiers based on two financial ratios datasets and simulation datasets. The results confirmed the advantages of our method: It is being robust while providing good performance.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0013372384&target=NART&cn=DIKO0013372384",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "앙상블 SVM을 이용한 기업 부도 예측 모형 앙상블 SVM을 이용한 기업 부도 예측 모형 앙상블 SVM을 이용한 기업 부도 예측 모형 Bankruptcy prediction has been an important topic in the accounting and finance field for a long time. Several data mining techniques have been used for bankruptcy prediction. However, there are many limits for application to real classification problem with a single model. This study proposes ensemble SVM (support vector machine) model which assembles different SVM models with each different kernel functions. Our ensemble model is made and evaluated by v-fold cross-validation approach. The top performing models are recruited into the ensemble. The classification is then carried out using the majority voting opinion of the ensemble. The performance of the ensemble SVM classifier is investigated in terms of accuracy, error rate, sensitivity, specificity, ROC curve, and AUC to compare with single SVM classifiers based on two financial ratios datasets and simulation datasets. The results confirmed the advantages of our method: It is being robust while providing good performance."
        },
        {
          "rank": 6,
          "score": 0.7122431397438049,
          "doc_id": "DIKO0009404537",
          "title": "Support vector machine을 이용한 기업부도예측",
          "abstract": "Predicting bankruptcy is one of the most important problems to parties such as bankers, managers, government policy makers, and investors. It provides information for interested parties to minimize their predictable losses from bankruptcy. There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. since the late 1980s, Artificial Intelligence (AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network (ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine (SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0009404537&target=NART&cn=DIKO0009404537",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Support vector machine을 이용한 기업부도예측 Support vector machine을 이용한 기업부도예측 Support vector machine을 이용한 기업부도예측 Predicting bankruptcy is one of the most important problems to parties such as bankers, managers, government policy makers, and investors. It provides information for interested parties to minimize their predictable losses from bankruptcy. There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. since the late 1980s, Artificial Intelligence (AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network (ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine (SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction."
        },
        {
          "rank": 7,
          "score": 0.7025823593139648,
          "doc_id": "JAKO201510534325002",
          "title": "퍼지이론과 SVM 결합을 통한 기업부도예측 최적화",
          "abstract": "기업부도예측은 재무 분야에 있어 중요한 연구주제 중 하나로 1960년대 이후부터 꾸준히 연구되어져 왔다. 국내의 경우, IMF 사태 이후 기업부도예측에 관한 중요성이 강조되고 있다. 이에 본 연구에서는 보다 정확한 기업부도예측을 위해 높은 예측력과 동시에 과적합화의 문제를 해결한다고 알려진 SVM(Support Vector Machine)을 기반으로 퍼지이론(fuzzy theory)을 활용해 입력변수를 확장하고, 유전자 알고리즘(GA, Genetic Algorithm)을 이용해 유사 혹은 유사최적의 입력변수집합과 파라미터를 탐색하는 새로운 융합모형을 제시한다. 제안모형의 유용성을 검증하기 위하여 H은행의 비외감 중공업 기업 데이터를 이용하여 실험을 수행하였으며, 비교모형으로는 로짓분석, 판별분석, 의사결정나무, 사례기반추론, 인공신경망, SVM을 선정하였다. 실험결과, 제안모형이 모든 비교모형들에 비해 우수한 예측력을 보이는 것으로 나타났다. 본 연구는 우수한 예측 성능을 가진 다기법 융합 모형을 새롭게 제안하여, 부도예측 분야에 학술적, 실무적으로 기여할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201510534325002&target=NART&cn=JAKO201510534325002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "퍼지이론과 SVM 결합을 통한 기업부도예측 최적화 퍼지이론과 SVM 결합을 통한 기업부도예측 최적화 퍼지이론과 SVM 결합을 통한 기업부도예측 최적화 기업부도예측은 재무 분야에 있어 중요한 연구주제 중 하나로 1960년대 이후부터 꾸준히 연구되어져 왔다. 국내의 경우, IMF 사태 이후 기업부도예측에 관한 중요성이 강조되고 있다. 이에 본 연구에서는 보다 정확한 기업부도예측을 위해 높은 예측력과 동시에 과적합화의 문제를 해결한다고 알려진 SVM(Support Vector Machine)을 기반으로 퍼지이론(fuzzy theory)을 활용해 입력변수를 확장하고, 유전자 알고리즘(GA, Genetic Algorithm)을 이용해 유사 혹은 유사최적의 입력변수집합과 파라미터를 탐색하는 새로운 융합모형을 제시한다. 제안모형의 유용성을 검증하기 위하여 H은행의 비외감 중공업 기업 데이터를 이용하여 실험을 수행하였으며, 비교모형으로는 로짓분석, 판별분석, 의사결정나무, 사례기반추론, 인공신경망, SVM을 선정하였다. 실험결과, 제안모형이 모든 비교모형들에 비해 우수한 예측력을 보이는 것으로 나타났다. 본 연구는 우수한 예측 성능을 가진 다기법 융합 모형을 새롭게 제안하여, 부도예측 분야에 학술적, 실무적으로 기여할 수 있을 것으로 기대된다."
        },
        {
          "rank": 8,
          "score": 0.7021004557609558,
          "doc_id": "JAKO200516638000020",
          "title": "Support Vector Machine을 이용한 기업부도예측",
          "abstract": "There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. Since the late 1980s, Artificial Intelligence(AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network(ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine(SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200516638000020&target=NART&cn=JAKO200516638000020",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Support Vector Machine을 이용한 기업부도예측 Support Vector Machine을 이용한 기업부도예측 Support Vector Machine을 이용한 기업부도예측 There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. Since the late 1980s, Artificial Intelligence(AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network(ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine(SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction."
        },
        {
          "rank": 9,
          "score": 0.7010439038276672,
          "doc_id": "ART001692415",
          "title": "SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상",
          "abstract": "기업 도산은 다양한 이해관계자에게 사회 경제적으로 큰 손실을 주게 된다. 따라서 기업 도산 및 부실화를 사전에 예측할 수 있다면 이에 대한 대비를 하거나 기업 부도 요인을 사전 제거함에 따라 기업 도산에 대한 손실을 최소화하는 것이 가능할 것이다. 기업의 도산을 예측하기 위한 다양한 통계적 모형 및 데이터 마이닝 모형이 제시되고 있다. 기업 도산 예측 모형의 주요 이슈 중 하나는 예측력을 높이는 것이다. 이 논문은 기업 도산예측에 주로 사용되며 예측성능이 비교적 높은 로짓모형, 의사결정나무모형, 신경망모형, SVM모형에 대해서 살펴본다. 표본기업은 코스닥기업 중 도산 및 정상기업으로 각각 49개 기업을 선정하였고, 설명변수로는 통계적으로 유의미한 9개의 재무변수를 이용하고, 5년간의 재무변수 자료를 사용하였다. SVM모형을 전처리기를 사용한 경우 다양한 분류모형의 도산예측 성능을 크게 높일 수 있음을 실험결과로 보이고 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART001692415&target=NART&cn=ART001692415",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상 SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상 SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상 기업 도산은 다양한 이해관계자에게 사회 경제적으로 큰 손실을 주게 된다. 따라서 기업 도산 및 부실화를 사전에 예측할 수 있다면 이에 대한 대비를 하거나 기업 부도 요인을 사전 제거함에 따라 기업 도산에 대한 손실을 최소화하는 것이 가능할 것이다. 기업의 도산을 예측하기 위한 다양한 통계적 모형 및 데이터 마이닝 모형이 제시되고 있다. 기업 도산 예측 모형의 주요 이슈 중 하나는 예측력을 높이는 것이다. 이 논문은 기업 도산예측에 주로 사용되며 예측성능이 비교적 높은 로짓모형, 의사결정나무모형, 신경망모형, SVM모형에 대해서 살펴본다. 표본기업은 코스닥기업 중 도산 및 정상기업으로 각각 49개 기업을 선정하였고, 설명변수로는 통계적으로 유의미한 9개의 재무변수를 이용하고, 5년간의 재무변수 자료를 사용하였다. SVM모형을 전처리기를 사용한 경우 다양한 분류모형의 도산예측 성능을 크게 높일 수 있음을 실험결과로 보이고 있다."
        },
        {
          "rank": 10,
          "score": 0.6983200907707214,
          "doc_id": "JAKO202433861648179",
          "title": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교",
          "abstract": "본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202433861648179&target=NART&cn=JAKO202433861648179",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다."
        },
        {
          "rank": 11,
          "score": 0.697577714920044,
          "doc_id": "ATN0031726879",
          "title": "딥러닝 기반 부실기업 예측모형에 관한 연구",
          "abstract": "Predicting insolvent companies is a research topic that has been important in accounting and finance. Especially, due to the rapidly changing business environments and the recent COVID-19 pandemic, many domestic companies are facing financial adversity. Thus, the necessity of research on corporate insolvency is being emphasized. As a related research, there is a prediction of corporate bankruptcy, however, a bankrupt company is the company whose business activities have been suspended, and there is a limitation in which it is inappropriate to determine which companies show signs of bankruptcy among continuing companies. Therefore, marginal company, one of the categories of insolvent companies, is selected as the prediction target. Marginal companies are the firms that are operating income interest compensation ratio are less than 1 for three consecutive years, and are engaged in business activities but have not consistently secured adequate profits. In this study, deep learning techniques are used to predict them. It is one of the machine learning techniques that has recently attracted attention because of its excellence in various fields. Nonetheless, has not been applied in research to predict marginal companies. This study applies RNN and CNN among deep learning techniques using several financial ratios as independent variables. Their performance are compared with machine learning ensemble models that have been reported to have excellent predictive power in previous studies. As a result of analysis on corporate data from 2017 to 2019 as training and test data, deep learning models such as RNN-LSTM, RNN-GRU, and CNN are better in forecasting of marginal companies than the ensemble models in terms of Recall score. Therefore, the deep learning models are expected to become widely used in the prediction of marginal companies in the future.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0031726879&target=NART&cn=ATN0031726879",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 부실기업 예측모형에 관한 연구 딥러닝 기반 부실기업 예측모형에 관한 연구 딥러닝 기반 부실기업 예측모형에 관한 연구 Predicting insolvent companies is a research topic that has been important in accounting and finance. Especially, due to the rapidly changing business environments and the recent COVID-19 pandemic, many domestic companies are facing financial adversity. Thus, the necessity of research on corporate insolvency is being emphasized. As a related research, there is a prediction of corporate bankruptcy, however, a bankrupt company is the company whose business activities have been suspended, and there is a limitation in which it is inappropriate to determine which companies show signs of bankruptcy among continuing companies. Therefore, marginal company, one of the categories of insolvent companies, is selected as the prediction target. Marginal companies are the firms that are operating income interest compensation ratio are less than 1 for three consecutive years, and are engaged in business activities but have not consistently secured adequate profits. In this study, deep learning techniques are used to predict them. It is one of the machine learning techniques that has recently attracted attention because of its excellence in various fields. Nonetheless, has not been applied in research to predict marginal companies. This study applies RNN and CNN among deep learning techniques using several financial ratios as independent variables. Their performance are compared with machine learning ensemble models that have been reported to have excellent predictive power in previous studies. As a result of analysis on corporate data from 2017 to 2019 as training and test data, deep learning models such as RNN-LSTM, RNN-GRU, and CNN are better in forecasting of marginal companies than the ensemble models in terms of Recall score. Therefore, the deep learning models are expected to become widely used in the prediction of marginal companies in the future."
        },
        {
          "rank": 12,
          "score": 0.6941199898719788,
          "doc_id": "JAKO202404861562091",
          "title": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교",
          "abstract": "대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202404861562091&target=NART&cn=JAKO202404861562091",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다."
        },
        {
          "rank": 13,
          "score": 0.6912899017333984,
          "doc_id": "JAKO201718054814596",
          "title": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석",
          "abstract": "딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201718054814596&target=NART&cn=JAKO201718054814596",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다."
        },
        {
          "rank": 14,
          "score": 0.6905083656311035,
          "doc_id": "JAKO200111920938436",
          "title": "퍼지신경망을 이용한 기업부도예측",
          "abstract": "본 연구에서는 퍼지신경망을 이용한 기업부실예측모형을 제안한다. 신경망은 탁월한 학습능력을 가진 것으로 알려져 있으나, 잡음이 심한 재무자료에 대해서는 종종 일관되지 못하고 기대에 미치지 못하는 예측성과를 보인다. 이는 연속형의 형태를 지닌 독립변수와 과다한 양의 원자료로부터 예측에 필요한 일정한 패턴을 찾기가 어렵기 때문이다. 이러한 문제점은 예측모형에서의 독립변수와 종속변수간의 인과관계를 신경망이 용이하게 찾아낼 수 있도록 독립변수의 형태를 변환함으로써 해결한 수 있다. 이러한 해결방법의 하나는 기존 신경망에 퍼지집합의 개념을 적용하여 신경망 학습에 사용될 자료를 퍼지화하고 이를 신경망에 학습시키는 것이다 입력자료를 퍼지화 함으로써 정보의 손실 없이도 신경망이 자료 내의 복잡한 관계를 용이하게 학습하는 것이 가능하다. 본 연구에서 제안된 퍼지신경망을 기업부도예측에 적용한 결과, 퍼지신경망이 기존의 신경망보다 우월한 예측성과를 나타내었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200111920938436&target=NART&cn=JAKO200111920938436",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "퍼지신경망을 이용한 기업부도예측 퍼지신경망을 이용한 기업부도예측 퍼지신경망을 이용한 기업부도예측 본 연구에서는 퍼지신경망을 이용한 기업부실예측모형을 제안한다. 신경망은 탁월한 학습능력을 가진 것으로 알려져 있으나, 잡음이 심한 재무자료에 대해서는 종종 일관되지 못하고 기대에 미치지 못하는 예측성과를 보인다. 이는 연속형의 형태를 지닌 독립변수와 과다한 양의 원자료로부터 예측에 필요한 일정한 패턴을 찾기가 어렵기 때문이다. 이러한 문제점은 예측모형에서의 독립변수와 종속변수간의 인과관계를 신경망이 용이하게 찾아낼 수 있도록 독립변수의 형태를 변환함으로써 해결한 수 있다. 이러한 해결방법의 하나는 기존 신경망에 퍼지집합의 개념을 적용하여 신경망 학습에 사용될 자료를 퍼지화하고 이를 신경망에 학습시키는 것이다 입력자료를 퍼지화 함으로써 정보의 손실 없이도 신경망이 자료 내의 복잡한 관계를 용이하게 학습하는 것이 가능하다. 본 연구에서 제안된 퍼지신경망을 기업부도예측에 적용한 결과, 퍼지신경망이 기존의 신경망보다 우월한 예측성과를 나타내었다."
        },
        {
          "rank": 15,
          "score": 0.6879781484603882,
          "doc_id": "JAKO202305062334676",
          "title": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측",
          "abstract": "본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202305062334676&target=NART&cn=JAKO202305062334676",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다."
        },
        {
          "rank": 16,
          "score": 0.6872730255126953,
          "doc_id": "JAKO201208438434752",
          "title": "부도 예측을 위한 앙상블 분류기 개발",
          "abstract": "분류기의 앙상블 학습은 여러 개의 서로 다른 분류기들의 조합을 통해 만들어진다. 앙상블 학습은 기계학습 분야에서 많은 관심을 끌고 있는 중요한 연구주제이며 대부분의 경우에 있어서 앙상블 모형은 개별 기저 분류기보다 더 좋은 성과를 내는 것으로 알려져 있다. 본 연구는 부도 예측 모형의 성능개선에 관한 연구이다. 이를 위해 본 연구에서는 단일 모형으로 그 우수성을 인정받고 있는 SVM을 기저 분류기로 사용하는 앙상블 모형에 대해 고찰하였다. SVM 모형의 성능 개선을 위해 bagging과 random subspace 모형을 부도 예측 문제에 적용해 보았으며 bagging 모형과 random subspace 모형의 성과 개선을 위해 bagging과 random subspace의 통합 모형을 제안하였다. 제안한 모형의 성과를 검증하기 위해 실제 기업의 부도 예측 데이터를 사용하여 실험하였고, 실험 결과 본 연구에서 제안한 새로운 형태의 통합 모형이 가장 좋은 성과를 보임을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201208438434752&target=NART&cn=JAKO201208438434752",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "부도 예측을 위한 앙상블 분류기 개발 부도 예측을 위한 앙상블 분류기 개발 부도 예측을 위한 앙상블 분류기 개발 분류기의 앙상블 학습은 여러 개의 서로 다른 분류기들의 조합을 통해 만들어진다. 앙상블 학습은 기계학습 분야에서 많은 관심을 끌고 있는 중요한 연구주제이며 대부분의 경우에 있어서 앙상블 모형은 개별 기저 분류기보다 더 좋은 성과를 내는 것으로 알려져 있다. 본 연구는 부도 예측 모형의 성능개선에 관한 연구이다. 이를 위해 본 연구에서는 단일 모형으로 그 우수성을 인정받고 있는 SVM을 기저 분류기로 사용하는 앙상블 모형에 대해 고찰하였다. SVM 모형의 성능 개선을 위해 bagging과 random subspace 모형을 부도 예측 문제에 적용해 보았으며 bagging 모형과 random subspace 모형의 성과 개선을 위해 bagging과 random subspace의 통합 모형을 제안하였다. 제안한 모형의 성과를 검증하기 위해 실제 기업의 부도 예측 데이터를 사용하여 실험하였고, 실험 결과 본 연구에서 제안한 새로운 형태의 통합 모형이 가장 좋은 성과를 보임을 알 수 있었다."
        },
        {
          "rank": 17,
          "score": 0.6865813732147217,
          "doc_id": "JAKO201810866003990",
          "title": "딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증",
          "abstract": "본 연구는 경제적으로 국내에 큰 영향을 주었던 글로벌 금융위기를 기반으로 총 10년의 연간 기업데이터를 이용한다. 먼저 시대 변화 흐름에 일관성있는 부도 모형을 구축하는 것을 목표로 금융위기 이전(2000~2006년)의 데이터를 학습한다. 이후 매개 변수 튜닝을 통해 금융위기 기간이 포함(2007~2008년)된 유효성 검증 데이터가 학습데이터의 결과와 비슷한 양상을 보이고, 우수한 예측력을 가지도록 조정한다. 이후 학습 및 유효성 검증 데이터를 통합(2000~2008년)하여 유효성 검증 때와 같은 매개변수를 적용하여 모형을 재구축하고, 결과적으로 최종 학습된 모형을 기반으로 시험 데이터(2009년) 결과를 바탕으로 딥러닝 시계열 알고리즘 기반의 기업부도예측 모형이 유용함을 검증한다. 부도에 대한 정의는 Lee(2015) 연구와 동일하게 기업의 상장폐지 사유들 중 실적이 부진했던 경우를 부도로 선정한다. 독립변수의 경우, 기존 선행연구에서 이용되었던 재무비율 변수를 비롯한 기타 재무정보를 포함한다. 이후 최적의 변수군을 선별하는 방식으로 다변량 판별분석, 로짓 모형, 그리고 Lasso 회귀분석 모형을 이용한다. 기업부도예측 모형 방법론으로는 Altman(1968)이 제시했던 다중판별분석 모형, Ohlson(1980)이 제시한 로짓모형, 그리고 비시계열 기계학습 기반 부도예측모형과 딥러닝 시계열 알고리즘을 이용한다. 기업 데이터의 경우, '비선형적인 변수들', 변수들의 '다중 공선성 문제', 그리고 '데이터 수 부족'이란 한계점이 존재한다. 이에 로짓 모형은 '비선형성'을, Lasso 회귀분석 모형은 '다중 공선성 문제'를 해결하고, 가변적인 데이터 생성 방식을 이용하는 딥러닝 시계열 알고리즘을 접목함으로서 데이터 수가 부족한 점을 보완하여 연구를 진행한다. 현 정부를 비롯한 해외 정부에서는 4차 산업혁명을 통해 국가 및 사회의 시스템, 일상생활 전반을 아우르기 위해 힘쓰고 있다. 즉, 현재는 다양한 산업에 이르러 빅데이터를 이용한 딥러닝 연구가 활발히 진행되고 있지만, 금융 산업을 위한 연구분야는 아직도 미비하다. 따라서 이 연구는 기업 부도에 관하여 딥러닝 시계열 알고리즘 분석을 진행한 초기 논문으로서, 금융 데이터와 딥러닝 시계열 알고리즘을 접목한 연구를 시작하는 비 전공자에게 비교분석 자료로 쓰이기를 바란다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201810866003990&target=NART&cn=JAKO201810866003990",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증 딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증 딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증 본 연구는 경제적으로 국내에 큰 영향을 주었던 글로벌 금융위기를 기반으로 총 10년의 연간 기업데이터를 이용한다. 먼저 시대 변화 흐름에 일관성있는 부도 모형을 구축하는 것을 목표로 금융위기 이전(2000~2006년)의 데이터를 학습한다. 이후 매개 변수 튜닝을 통해 금융위기 기간이 포함(2007~2008년)된 유효성 검증 데이터가 학습데이터의 결과와 비슷한 양상을 보이고, 우수한 예측력을 가지도록 조정한다. 이후 학습 및 유효성 검증 데이터를 통합(2000~2008년)하여 유효성 검증 때와 같은 매개변수를 적용하여 모형을 재구축하고, 결과적으로 최종 학습된 모형을 기반으로 시험 데이터(2009년) 결과를 바탕으로 딥러닝 시계열 알고리즘 기반의 기업부도예측 모형이 유용함을 검증한다. 부도에 대한 정의는 Lee(2015) 연구와 동일하게 기업의 상장폐지 사유들 중 실적이 부진했던 경우를 부도로 선정한다. 독립변수의 경우, 기존 선행연구에서 이용되었던 재무비율 변수를 비롯한 기타 재무정보를 포함한다. 이후 최적의 변수군을 선별하는 방식으로 다변량 판별분석, 로짓 모형, 그리고 Lasso 회귀분석 모형을 이용한다. 기업부도예측 모형 방법론으로는 Altman(1968)이 제시했던 다중판별분석 모형, Ohlson(1980)이 제시한 로짓모형, 그리고 비시계열 기계학습 기반 부도예측모형과 딥러닝 시계열 알고리즘을 이용한다. 기업 데이터의 경우, '비선형적인 변수들', 변수들의 '다중 공선성 문제', 그리고 '데이터 수 부족'이란 한계점이 존재한다. 이에 로짓 모형은 '비선형성'을, Lasso 회귀분석 모형은 '다중 공선성 문제'를 해결하고, 가변적인 데이터 생성 방식을 이용하는 딥러닝 시계열 알고리즘을 접목함으로서 데이터 수가 부족한 점을 보완하여 연구를 진행한다. 현 정부를 비롯한 해외 정부에서는 4차 산업혁명을 통해 국가 및 사회의 시스템, 일상생활 전반을 아우르기 위해 힘쓰고 있다. 즉, 현재는 다양한 산업에 이르러 빅데이터를 이용한 딥러닝 연구가 활발히 진행되고 있지만, 금융 산업을 위한 연구분야는 아직도 미비하다. 따라서 이 연구는 기업 부도에 관하여 딥러닝 시계열 알고리즘 분석을 진행한 초기 논문으로서, 금융 데이터와 딥러닝 시계열 알고리즘을 접목한 연구를 시작하는 비 전공자에게 비교분석 자료로 쓰이기를 바란다."
        },
        {
          "rank": 18,
          "score": 0.6861857175827026,
          "doc_id": "JAKO202312473958811",
          "title": "작물 생산량 예측을 위한 심층강화학습 성능 분석",
          "abstract": "최근 딥러닝 기술을 활용하여 작물 생산량 예측 연구가 많이 진행되고 있다. 딥러닝 알고리즘은 입력 데이터 세트와 작물 예측 결과에 대한 선형 맵을 구성하는데 어려움이 있다. 또한, 알고리즘 구현은 획득한 속성의 비율에 긍정적으로 의존한다. 심층강화학습을 작물 생산량 예측 응용에 적용한다면 이러한 한계점을 보완할 수 있다. 본 논문은 작물 생산량 예측을 개선하기 위해 DQN, Double DQN 및 Dueling DQN 의 성능을 분석한다. DQN 알고리즘은 과대 평가 문제가 제기되지만, Double DQN은 과대 평가를 줄이고 더 나은 결과를 얻을 수 있다. 본 논문에서 제안된 모델은 거짓 판정을 줄이고 예측 정확도를 높이는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202312473958811&target=NART&cn=JAKO202312473958811",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "작물 생산량 예측을 위한 심층강화학습 성능 분석 작물 생산량 예측을 위한 심층강화학습 성능 분석 작물 생산량 예측을 위한 심층강화학습 성능 분석 최근 딥러닝 기술을 활용하여 작물 생산량 예측 연구가 많이 진행되고 있다. 딥러닝 알고리즘은 입력 데이터 세트와 작물 예측 결과에 대한 선형 맵을 구성하는데 어려움이 있다. 또한, 알고리즘 구현은 획득한 속성의 비율에 긍정적으로 의존한다. 심층강화학습을 작물 생산량 예측 응용에 적용한다면 이러한 한계점을 보완할 수 있다. 본 논문은 작물 생산량 예측을 개선하기 위해 DQN, Double DQN 및 Dueling DQN 의 성능을 분석한다. DQN 알고리즘은 과대 평가 문제가 제기되지만, Double DQN은 과대 평가를 줄이고 더 나은 결과를 얻을 수 있다. 본 논문에서 제안된 모델은 거짓 판정을 줄이고 예측 정확도를 높이는 것으로 나타났다."
        },
        {
          "rank": 19,
          "score": 0.6855576038360596,
          "doc_id": "DIKO0015893049",
          "title": "도메인 적대적 신경망을 이용한 종단 간 억양음성인식",
          "abstract": "최근 딥러닝(Deep learning) 기술의 발전은 음성인식 성능 향상에 크게 기여하였다. 이러한 발전에도 불구하고 소음, 감정, 억양 등이 섞인 특정 발화에 대해서는 좋은 성능을 보이지 못하고 있다. 이 가운데 억양이 섞인 발화는 표준 발화와 비교했을 때 언어학적인 차이가 존재하는데, 이러한 차이가 억양이 섞인 발화를 인식하기 어렵게 만든다. 따라서 본 연구에서는 억양이 섞인 발화와 표준 발화 사이에 존재하는 특성의 차이를 줄이고자 도메인 적대적 신경망(Domain Adversarial Neural Network) 기법을 사용하였다. 또한, 종단 간(End-to-end) 기법을 사용하여 음성인식의 과정을 간소화하였다.&amp;#xD; 오래전부터 억양음성인식의 성능을 높이기 위한 연구는 활발히 진행되어 왔다. 2010년대 초반까지는 가우시안 혼합 모델(Gaussian Mixture Model) 기반의 최대 사후 확률(Maximum A Posteriori), 최대 우도 선형 회귀(Maximum Likelihood Linear Regression) 적응 기법이 주로 사용되었다. 하지만 딥러닝 기술이 발전하고 신경망 기반의 모델들이 주목 받기 시작하면서 가우시안 혼합 모델보다는 신경망 모델에 적합한 기법들이 사용되었다. 최근 몇 년간은 음성인식 모델에 억양에 대한 정보를 직접 삽입하는 accent embedding 기법이 많이 사용되었다. Accent embedding 기법은 억양음성인식의 성능을 향상시켰지만 몇 가지 문제점을 가지고 있다. 첫째, 억양을 분류하여 accent embedding 특징들을 만들어내는 모델을 독립적으로 만들어 훈련시켜야 하며, 해당 모델의 억양 분류 정확도가 음성인식 모델의 성능에 큰 영향을 미치기 때문에 모델을 정교하게 만들어야 하는 부담감이 있다. &amp;#xD; 둘째, 억양 분류 모델의 결과를 음성인식 모델의 추가적인 입력 특징(Input feature)으로 사용하기 때문에 음성인식 모델의 매개변수를 증가시키며 계산량 또한 증가한다는 문제가 있다. 따라서 본 연구에서는 추가적인 입력 특징이 필요하지 않고 음성인식에서 기본적으로 사용되는 특징인 주파수 정보(스펙트로그램)만을 이용하여 학습이 가능한 도메인 적대적 신경망을 기법을 제안하였다. &amp;#xD; 도메인 적대적 신경망은 소스 도메인(Source domain) 데이터와 타겟 도메인(Target domain) 데이터가 적대적으로 학습이 되면서 두 도메인 간의 분포 차이를 줄이는 것을 목적으로 한다. 본 연구의 목표인 억양음성인식에서는 표준 발화를 소스 도메인으로, 억양이 섞인 발화를 타겟 도메인으로 정하였다. 도메인 적대적 신경망은 특징 추출기(Feature extractor), 도메인 분류기(Domain classifier), 레이블 예측기(Label predictor) 총 3개의 부분망(sub-network)으로 구성된다. 각각의 부분망은 서로 다른 역할을 수행하기 때문에 신경망의 특성을 고려하여 만들어야 한다. 따라서 본 연구에서는 신경망의 특성을 고려하여 특징 추출기에는 합성곱 신경망(Convolutional Neural Network)을, 도메인 분류기에는 심층 신경망(Deep Neural Network)을, 그리고 레이블 예측기에는 양방향 게이트 순환 유닛(Bidirectional Gated Recurrent Unit)을 이용하여 도메인 적대적 신경망을 구성하였다. 또한, 레이블을 예측할 때 종단 간 기법을 활용하여 입력 데이터를 사전 분할하지 않고, 레이블 예측 이후의 후처리 작업을 없애면서 음성인식 과정을 간소화하였다.&amp;#xD; 본 연구에서 제안한 도메인 적대적 학습 기반의 억양음성인식 기법의 효과를 입증하기 위하여 Baseline 모델과 DANN 모델을 만들어 실험을 진행하였다. 실험 데이터로는 Mozilla의 Common Voice 코퍼스를 사용하였는데, Common Voice 코퍼스는 여러 언어에 대해 막대한 양의 검증된 음성파일을 오픈소스로 제공하기 때문에 음성인식 연구에서 많이 사용된다. 또한, Common Voice 코퍼스는 음성 녹음 파일과 함께 억양 정보도 같이 제공을 하기 때문에 억양음성인식 연구에 효율적으로 사용될 수 있다. &amp;#xD; Common Voice 코퍼스의 영어 데이터셋은 여러 억양의 음성파일들을 가지고 있는데, 본 연구에서는 미국 억양, 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양의 데이터를 실험에 사용하였으며, 미국 억양을 소스 도메인으로 나머지 네 개의 억양을 타겟 도메인으로 정하였다.&amp;#xD; 실험 결과 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양 모두에서 DANN 모델의 성능이 Baseline 모델보다 높은 성능을 보였다. 하지만 억양에 따라 성능 개선의 차이가 있었으며, 캐나다 억양에 비해 잉글랜드 억양과 인도 억양에서 성능이 눈에 띄게 향상되었다. 이 같은 결과는 잉글랜드 억양과 인도 억양이 소스 도메인으로 사용된 미국 억양 데이터와 언어학적으로 큰 차이가 존재하여 baseline 모델에서는 성능이 낮았으나, 도메인 적대적 학습을 통해 생성된 DANN 모델이 타겟 억양의 특성을 반영함으로써 성능이 크게 개선된 것으로 분석된다. 따라서 도메인 적대적 신경망은 소스 도메인과 타겟 도메인 사이의 분포의 차이를 줄임으로서 억양음성인식의 성능을 향상시킬 수 있음이 확인되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015893049&target=NART&cn=DIKO0015893049",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "도메인 적대적 신경망을 이용한 종단 간 억양음성인식 도메인 적대적 신경망을 이용한 종단 간 억양음성인식 도메인 적대적 신경망을 이용한 종단 간 억양음성인식 최근 딥러닝(Deep learning) 기술의 발전은 음성인식 성능 향상에 크게 기여하였다. 이러한 발전에도 불구하고 소음, 감정, 억양 등이 섞인 특정 발화에 대해서는 좋은 성능을 보이지 못하고 있다. 이 가운데 억양이 섞인 발화는 표준 발화와 비교했을 때 언어학적인 차이가 존재하는데, 이러한 차이가 억양이 섞인 발화를 인식하기 어렵게 만든다. 따라서 본 연구에서는 억양이 섞인 발화와 표준 발화 사이에 존재하는 특성의 차이를 줄이고자 도메인 적대적 신경망(Domain Adversarial Neural Network) 기법을 사용하였다. 또한, 종단 간(End-to-end) 기법을 사용하여 음성인식의 과정을 간소화하였다.&amp;#xD; 오래전부터 억양음성인식의 성능을 높이기 위한 연구는 활발히 진행되어 왔다. 2010년대 초반까지는 가우시안 혼합 모델(Gaussian Mixture Model) 기반의 최대 사후 확률(Maximum A Posteriori), 최대 우도 선형 회귀(Maximum Likelihood Linear Regression) 적응 기법이 주로 사용되었다. 하지만 딥러닝 기술이 발전하고 신경망 기반의 모델들이 주목 받기 시작하면서 가우시안 혼합 모델보다는 신경망 모델에 적합한 기법들이 사용되었다. 최근 몇 년간은 음성인식 모델에 억양에 대한 정보를 직접 삽입하는 accent embedding 기법이 많이 사용되었다. Accent embedding 기법은 억양음성인식의 성능을 향상시켰지만 몇 가지 문제점을 가지고 있다. 첫째, 억양을 분류하여 accent embedding 특징들을 만들어내는 모델을 독립적으로 만들어 훈련시켜야 하며, 해당 모델의 억양 분류 정확도가 음성인식 모델의 성능에 큰 영향을 미치기 때문에 모델을 정교하게 만들어야 하는 부담감이 있다. &amp;#xD; 둘째, 억양 분류 모델의 결과를 음성인식 모델의 추가적인 입력 특징(Input feature)으로 사용하기 때문에 음성인식 모델의 매개변수를 증가시키며 계산량 또한 증가한다는 문제가 있다. 따라서 본 연구에서는 추가적인 입력 특징이 필요하지 않고 음성인식에서 기본적으로 사용되는 특징인 주파수 정보(스펙트로그램)만을 이용하여 학습이 가능한 도메인 적대적 신경망을 기법을 제안하였다. &amp;#xD; 도메인 적대적 신경망은 소스 도메인(Source domain) 데이터와 타겟 도메인(Target domain) 데이터가 적대적으로 학습이 되면서 두 도메인 간의 분포 차이를 줄이는 것을 목적으로 한다. 본 연구의 목표인 억양음성인식에서는 표준 발화를 소스 도메인으로, 억양이 섞인 발화를 타겟 도메인으로 정하였다. 도메인 적대적 신경망은 특징 추출기(Feature extractor), 도메인 분류기(Domain classifier), 레이블 예측기(Label predictor) 총 3개의 부분망(sub-network)으로 구성된다. 각각의 부분망은 서로 다른 역할을 수행하기 때문에 신경망의 특성을 고려하여 만들어야 한다. 따라서 본 연구에서는 신경망의 특성을 고려하여 특징 추출기에는 합성곱 신경망(Convolutional Neural Network)을, 도메인 분류기에는 심층 신경망(Deep Neural Network)을, 그리고 레이블 예측기에는 양방향 게이트 순환 유닛(Bidirectional Gated Recurrent Unit)을 이용하여 도메인 적대적 신경망을 구성하였다. 또한, 레이블을 예측할 때 종단 간 기법을 활용하여 입력 데이터를 사전 분할하지 않고, 레이블 예측 이후의 후처리 작업을 없애면서 음성인식 과정을 간소화하였다.&amp;#xD; 본 연구에서 제안한 도메인 적대적 학습 기반의 억양음성인식 기법의 효과를 입증하기 위하여 Baseline 모델과 DANN 모델을 만들어 실험을 진행하였다. 실험 데이터로는 Mozilla의 Common Voice 코퍼스를 사용하였는데, Common Voice 코퍼스는 여러 언어에 대해 막대한 양의 검증된 음성파일을 오픈소스로 제공하기 때문에 음성인식 연구에서 많이 사용된다. 또한, Common Voice 코퍼스는 음성 녹음 파일과 함께 억양 정보도 같이 제공을 하기 때문에 억양음성인식 연구에 효율적으로 사용될 수 있다. &amp;#xD; Common Voice 코퍼스의 영어 데이터셋은 여러 억양의 음성파일들을 가지고 있는데, 본 연구에서는 미국 억양, 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양의 데이터를 실험에 사용하였으며, 미국 억양을 소스 도메인으로 나머지 네 개의 억양을 타겟 도메인으로 정하였다.&amp;#xD; 실험 결과 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양 모두에서 DANN 모델의 성능이 Baseline 모델보다 높은 성능을 보였다. 하지만 억양에 따라 성능 개선의 차이가 있었으며, 캐나다 억양에 비해 잉글랜드 억양과 인도 억양에서 성능이 눈에 띄게 향상되었다. 이 같은 결과는 잉글랜드 억양과 인도 억양이 소스 도메인으로 사용된 미국 억양 데이터와 언어학적으로 큰 차이가 존재하여 baseline 모델에서는 성능이 낮았으나, 도메인 적대적 학습을 통해 생성된 DANN 모델이 타겟 억양의 특성을 반영함으로써 성능이 크게 개선된 것으로 분석된다. 따라서 도메인 적대적 신경망은 소스 도메인과 타겟 도메인 사이의 분포의 차이를 줄임으로서 억양음성인식의 성능을 향상시킬 수 있음이 확인되었다."
        },
        {
          "rank": 20,
          "score": 0.681813657283783,
          "doc_id": "JAKO202320150299733",
          "title": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가",
          "abstract": "본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202320150299733&target=NART&cn=JAKO202320150299733",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다."
        },
        {
          "rank": 21,
          "score": 0.6801239252090454,
          "doc_id": "ATN0035906971",
          "title": "딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구",
          "abstract": "In this research, we compare the explanatory power between linear regression model and deep-learning model when estimating stock returns. As predicted, the deep-learning model shows statistically significant improvement over linear regression model, although the improvement is not economically meaningful. We further investigate the effects of deep-learning model using different parameters and pre-processing. The results show that the predictive power of deep-learning model can be worse-off than that of linear model if it fails to select optimal parameters. Especially, it is important to choose adequate deep-learning parameters not to overfit the data, because the accounting data (which is at most quarterly) may not be sufficient enough for the deep model structure. Further, we show that the predictive power using researchers’ domain knowledge is sometimes better off than that relying simply on the deep-learning model. For instance, denomination with total assets brings better results than non-denomination. Another interesting finding is that winsorizing extreme values brings lower explanatory power when we use the deep-learning model. Such finding implies that, by removing extreme values, we may lose useful information in the parameter estimation. The results of this paper will help future research decide whether to utilize deep learning model or linear regression model",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0035906971&target=NART&cn=ATN0035906971",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 In this research, we compare the explanatory power between linear regression model and deep-learning model when estimating stock returns. As predicted, the deep-learning model shows statistically significant improvement over linear regression model, although the improvement is not economically meaningful. We further investigate the effects of deep-learning model using different parameters and pre-processing. The results show that the predictive power of deep-learning model can be worse-off than that of linear model if it fails to select optimal parameters. Especially, it is important to choose adequate deep-learning parameters not to overfit the data, because the accounting data (which is at most quarterly) may not be sufficient enough for the deep model structure. Further, we show that the predictive power using researchers’ domain knowledge is sometimes better off than that relying simply on the deep-learning model. For instance, denomination with total assets brings better results than non-denomination. Another interesting finding is that winsorizing extreme values brings lower explanatory power when we use the deep-learning model. Such finding implies that, by removing extreme values, we may lose useful information in the parameter estimation. The results of this paper will help future research decide whether to utilize deep learning model or linear regression model"
        },
        {
          "rank": 22,
          "score": 0.6795012354850769,
          "doc_id": "ATN0052773847",
          "title": "머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구",
          "abstract": "본 논문은 상장기업의 재무 및 거시경제 데이터를 활용하여 기업부도를 예측하는 통계적 모형과 다양한 머신러닝 기법의 성능을 비교하고, 불균형 데이터 문제를 완화하기 위한 오버샘플링 기법의 효과를 분석하였다. 실증 분석에는 로지스틱 회귀, 랜덤 포레스트, XGBoost(extreme gradient boosting), 심층신경망 모형을 적용하였으며, 오버샘플링 기법인 SMOTE(synthetic minority over-sampling technique) 및 ADASYN(adapti-ve synthetic sampling)을 사용하였다. 분석 결과, XGBoost는 원자료뿐 아니라 오버샘플링을 적용한 경우 모두에서 가장 우수하고 균형 있는 예측 성능을 보였다. 반면, 로지스틱 회귀는 높은 재현율을 나타냈으나, 낮은 정밀도로 인해 실무적 활용에는 한계가 있었다. 이러한 결과는 불균형 데이터 환경에서 오버샘플링 기법과 XGBoost와 같은 머신러닝 모형을 결합하여 사용하는 것이 기업부도 예측에 있어 보다 효과적이고 실용적인 접근법이 될 수 있음을 시사한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0052773847&target=NART&cn=ATN0052773847",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구 머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구 머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구 본 논문은 상장기업의 재무 및 거시경제 데이터를 활용하여 기업부도를 예측하는 통계적 모형과 다양한 머신러닝 기법의 성능을 비교하고, 불균형 데이터 문제를 완화하기 위한 오버샘플링 기법의 효과를 분석하였다. 실증 분석에는 로지스틱 회귀, 랜덤 포레스트, XGBoost(extreme gradient boosting), 심층신경망 모형을 적용하였으며, 오버샘플링 기법인 SMOTE(synthetic minority over-sampling technique) 및 ADASYN(adapti-ve synthetic sampling)을 사용하였다. 분석 결과, XGBoost는 원자료뿐 아니라 오버샘플링을 적용한 경우 모두에서 가장 우수하고 균형 있는 예측 성능을 보였다. 반면, 로지스틱 회귀는 높은 재현율을 나타냈으나, 낮은 정밀도로 인해 실무적 활용에는 한계가 있었다. 이러한 결과는 불균형 데이터 환경에서 오버샘플링 기법과 XGBoost와 같은 머신러닝 모형을 결합하여 사용하는 것이 기업부도 예측에 있어 보다 효과적이고 실용적인 접근법이 될 수 있음을 시사한다."
        },
        {
          "rank": 23,
          "score": 0.6770213842391968,
          "doc_id": "DIKO0015069923",
          "title": "딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템",
          "abstract": "데이터 예측 시스템들은 데이터를 예측하기 위해 특정 분야의 데이터를 컴퓨터가 분석하여 규칙을 찾아내고 데이터를 예측하였다. 이러한 방법은 과거 데이터를 분석한 결과로 사람이 규칙을 도출할 수 있어야 데이터를 예측하는 것이 가능하였다. 이에 반해 규칙을 도출할 수 없는 데이터들의 데이터를 예측하는 것은 사람의 능력으로는 한계가 있어 정확도가 낮아지는 문제점이 발생할 수 있다.&amp;#xD; 이를 해결하기 위해 컴퓨터를 활용하여 방대한 데이터를 데이터 예측 프로그램에 학습 데이터로 입력하고 결과로 데이터를 예측하였다. 이러한 방법론을 활용하기 위해서 고성능 컴퓨터로 딥 러닝(Deep Learning) 기술을 적용하여 데이터를 예측하고 있다. 해당 방법론이 활용되고 있는 분야로는 기상 데이터를 분석하여 날씨를 예측하는 날씨 분석과 스포츠 경기의 데이터를 예측하는 것이 대표적이다. &amp;#xD; 딥 러닝 기술은 프로그램이 데이터를 기반으로 학습을 진행하고 진행된 학습을 기반으로 데이터를 처리하는 것이다. 이는 과거에 사람이 직접 데이터를 분석하는 것보다 대규모 데이터를 분석하기에 적합하고 이로 인해 정확도가 올라가는 이점이 있다. 또한 목적에 따라 적합한 딥 러닝 모델을 적용하여 데이터를 예측할 경우 정확도의 기댓값이 높아지는 이점이 있다.&amp;#xD; 현재 딥 러닝 모델 중에서 데이터를 예측하기 위해 사용되는 모델은 신경망 구조를 기반으로 하는 DNN(Deep Neural Network) 모델과 RNN(Recurrent Neural Network) 모델이다. DNN 모델은 학습 데이터 내에서 규칙을 찾아내지 못하더라도 반복 학습을 통해 데이터 예측에 대한 정확도를 올릴 수 있고, RNN은 학습 과정 중에서 은닉층에서 적용될 가중치가 학습을 진행할 수록 변화하여 데이터를 예측하고 이로 인해 정확도를 올릴 수 있다. 이에 반해 DNN은 반복 학습의 횟수가 많아야 정확도가 높아지고 RNN은 가중치 변화의 횟수가 많아져야 정확도가 높아지기 때문에 결국 두 모델들은 학습의 반복이 많아져야 하는 문제점이 있다.&amp;#xD; 본 논문에서는 데이터 예측을 위해 딥 러닝 모델 기반 순차 데이터 예측 시스템을 제안한다. 제안하는 시스템에서 비정형 데이터를 순차 데이터로 정제하기 위해 전처리기를 구현하였다. 전처리기는 딥 러닝 모델에 학습 데이터를 입력하기 전에 데이터들을 정제하는 기능을 수행한다. 데이터는 ‘데이터 : 인덱스’ 구조로 이루어진 데이터 쌍이 되고 이러한 데이터 쌍들의 집합을 딥 러닝 모델에 입력하여 학습을 진행한다.&amp;#xD; 딥 러닝 모델은 DNN 모델, 기본 LSTM 모델, 상태유지 LSTM 모델을 활용하여 시스템을 각각 구축한다. 그리고 각 모델들의 설정 값을 변경하면서 정확도의 변화량을 분석한다. 또한 시퀀스의 길이를 변경해가며 실험을 진행하여 가장 정확도가 높은 데이터 셋과 시퀀스 길이의 비율을 제시한다.&amp;#xD; 딥 러닝 모듈 기반 시스템의 실험을 바탕으로 순차 데이터 예측에 가장 정확도가 높고 효율적인 딥 러닝 모듈을 선정하고 기존 시스템들과 비교 분석을 진행하여 제안하는 시스템의 우수성을 검증한다.&amp;#xD; 제안하는 시스템을 활용할 경우 학습 데이터가 적어도 높은 정확도를 요구하는 분야에서 기존 시스템들에 비해 효율성이 높을 것으로 사료된다.&amp;#xD;",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015069923&target=NART&cn=DIKO0015069923",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 데이터 예측 시스템들은 데이터를 예측하기 위해 특정 분야의 데이터를 컴퓨터가 분석하여 규칙을 찾아내고 데이터를 예측하였다. 이러한 방법은 과거 데이터를 분석한 결과로 사람이 규칙을 도출할 수 있어야 데이터를 예측하는 것이 가능하였다. 이에 반해 규칙을 도출할 수 없는 데이터들의 데이터를 예측하는 것은 사람의 능력으로는 한계가 있어 정확도가 낮아지는 문제점이 발생할 수 있다.&amp;#xD; 이를 해결하기 위해 컴퓨터를 활용하여 방대한 데이터를 데이터 예측 프로그램에 학습 데이터로 입력하고 결과로 데이터를 예측하였다. 이러한 방법론을 활용하기 위해서 고성능 컴퓨터로 딥 러닝(Deep Learning) 기술을 적용하여 데이터를 예측하고 있다. 해당 방법론이 활용되고 있는 분야로는 기상 데이터를 분석하여 날씨를 예측하는 날씨 분석과 스포츠 경기의 데이터를 예측하는 것이 대표적이다. &amp;#xD; 딥 러닝 기술은 프로그램이 데이터를 기반으로 학습을 진행하고 진행된 학습을 기반으로 데이터를 처리하는 것이다. 이는 과거에 사람이 직접 데이터를 분석하는 것보다 대규모 데이터를 분석하기에 적합하고 이로 인해 정확도가 올라가는 이점이 있다. 또한 목적에 따라 적합한 딥 러닝 모델을 적용하여 데이터를 예측할 경우 정확도의 기댓값이 높아지는 이점이 있다.&amp;#xD; 현재 딥 러닝 모델 중에서 데이터를 예측하기 위해 사용되는 모델은 신경망 구조를 기반으로 하는 DNN(Deep Neural Network) 모델과 RNN(Recurrent Neural Network) 모델이다. DNN 모델은 학습 데이터 내에서 규칙을 찾아내지 못하더라도 반복 학습을 통해 데이터 예측에 대한 정확도를 올릴 수 있고, RNN은 학습 과정 중에서 은닉층에서 적용될 가중치가 학습을 진행할 수록 변화하여 데이터를 예측하고 이로 인해 정확도를 올릴 수 있다. 이에 반해 DNN은 반복 학습의 횟수가 많아야 정확도가 높아지고 RNN은 가중치 변화의 횟수가 많아져야 정확도가 높아지기 때문에 결국 두 모델들은 학습의 반복이 많아져야 하는 문제점이 있다.&amp;#xD; 본 논문에서는 데이터 예측을 위해 딥 러닝 모델 기반 순차 데이터 예측 시스템을 제안한다. 제안하는 시스템에서 비정형 데이터를 순차 데이터로 정제하기 위해 전처리기를 구현하였다. 전처리기는 딥 러닝 모델에 학습 데이터를 입력하기 전에 데이터들을 정제하는 기능을 수행한다. 데이터는 ‘데이터 : 인덱스’ 구조로 이루어진 데이터 쌍이 되고 이러한 데이터 쌍들의 집합을 딥 러닝 모델에 입력하여 학습을 진행한다.&amp;#xD; 딥 러닝 모델은 DNN 모델, 기본 LSTM 모델, 상태유지 LSTM 모델을 활용하여 시스템을 각각 구축한다. 그리고 각 모델들의 설정 값을 변경하면서 정확도의 변화량을 분석한다. 또한 시퀀스의 길이를 변경해가며 실험을 진행하여 가장 정확도가 높은 데이터 셋과 시퀀스 길이의 비율을 제시한다.&amp;#xD; 딥 러닝 모듈 기반 시스템의 실험을 바탕으로 순차 데이터 예측에 가장 정확도가 높고 효율적인 딥 러닝 모듈을 선정하고 기존 시스템들과 비교 분석을 진행하여 제안하는 시스템의 우수성을 검증한다.&amp;#xD; 제안하는 시스템을 활용할 경우 학습 데이터가 적어도 높은 정확도를 요구하는 분야에서 기존 시스템들에 비해 효율성이 높을 것으로 사료된다.&amp;#xD;"
        },
        {
          "rank": 24,
          "score": 0.6748830080032349,
          "doc_id": "JAKO201828138444462",
          "title": "효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용",
          "abstract": "분류 문제에서 특정 범주의 빈도가 다른 범주에 비해 과도하게 높은 경우, 왜곡된 기계 학습을 유발할 수 있는 데이터 불균형(imbalanced data) 문제가 발생한다. 기업부도 예측 문제도 그 중 하나인데, 일반적으로 금융기관과 거래하는 기업들의 부도율은 대단히 낮아서, 부도 사례보다 정상 사례의 빈도가 월등히 높은 데이터 불균형 문제가 발생하고 있다. 이러한 데이터 불균형 문제를 해결하기 위해서는 적절한 표본추출 기법이 적용될 필요가 있으며, 지금껏 소수 범주 데이터를 복원 추출함으로써 다수 범주 데이터와 비율을 맞추어 데이터 불균형을 해결하는 오버 샘플링(oversampling) 기법이 주로 활용되어 왔다. 그러나 전통적인 오버 샘플링은 과적합화(overfitting)가 발생할 위험이 높아질 수 있는 단점이 있다. 이러한 배경에서 본 연구는 효과적인 기업부도 예측 모형 학습을 위한 표본추출 기법으로 2014년에 Menardi와 Torelli가 제안한 ROSE(random over sampling examples) 기법을 제안한다. ROSE 기법은 학습에 사용될 사례를 반복적으로 새롭게 합성하여 생성(synthetic generation)하는 기법으로, 과적합화 문제를 회피하면서도 분류 예측 정확도 개선에 도움을 줄 수 있다. 이에 본 연구에서는 ROSE 기법을 가장 성능이 우수한 이분류기로 알려진 SVM(support vector machine)과 결합하여 국내 한 대형 은행의 기업부도 예측에 적용해 보고, 다른 표본추출 기법들과의 비교연구를 수행하였다. 실험 결과, ROSE 기법이 다른 기법에 비해 통계적으로 유의한 수준으로 SVM의 예측정확도 개선에 기여할 수 있음을 확인하였다. 이러한 본 연구의 결과는 부도예측 외에 다른 사회과학 분야 예측문제의 데이터 불균형 문제 해결에도 ROSE가 우수한 대안이 될 수 있다는 사실을 시사한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201828138444462&target=NART&cn=JAKO201828138444462",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용 효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용 효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용 분류 문제에서 특정 범주의 빈도가 다른 범주에 비해 과도하게 높은 경우, 왜곡된 기계 학습을 유발할 수 있는 데이터 불균형(imbalanced data) 문제가 발생한다. 기업부도 예측 문제도 그 중 하나인데, 일반적으로 금융기관과 거래하는 기업들의 부도율은 대단히 낮아서, 부도 사례보다 정상 사례의 빈도가 월등히 높은 데이터 불균형 문제가 발생하고 있다. 이러한 데이터 불균형 문제를 해결하기 위해서는 적절한 표본추출 기법이 적용될 필요가 있으며, 지금껏 소수 범주 데이터를 복원 추출함으로써 다수 범주 데이터와 비율을 맞추어 데이터 불균형을 해결하는 오버 샘플링(oversampling) 기법이 주로 활용되어 왔다. 그러나 전통적인 오버 샘플링은 과적합화(overfitting)가 발생할 위험이 높아질 수 있는 단점이 있다. 이러한 배경에서 본 연구는 효과적인 기업부도 예측 모형 학습을 위한 표본추출 기법으로 2014년에 Menardi와 Torelli가 제안한 ROSE(random over sampling examples) 기법을 제안한다. ROSE 기법은 학습에 사용될 사례를 반복적으로 새롭게 합성하여 생성(synthetic generation)하는 기법으로, 과적합화 문제를 회피하면서도 분류 예측 정확도 개선에 도움을 줄 수 있다. 이에 본 연구에서는 ROSE 기법을 가장 성능이 우수한 이분류기로 알려진 SVM(support vector machine)과 결합하여 국내 한 대형 은행의 기업부도 예측에 적용해 보고, 다른 표본추출 기법들과의 비교연구를 수행하였다. 실험 결과, ROSE 기법이 다른 기법에 비해 통계적으로 유의한 수준으로 SVM의 예측정확도 개선에 기여할 수 있음을 확인하였다. 이러한 본 연구의 결과는 부도예측 외에 다른 사회과학 분야 예측문제의 데이터 불균형 문제 해결에도 ROSE가 우수한 대안이 될 수 있다는 사실을 시사한다."
        },
        {
          "rank": 25,
          "score": 0.6747835278511047,
          "doc_id": "DIKO0014861002",
          "title": "딥 러닝기반 고객평점 예측모델",
          "abstract": "인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014861002&target=NART&cn=DIKO0014861002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다."
        },
        {
          "rank": 26,
          "score": 0.674226701259613,
          "doc_id": "DIKO0014912357",
          "title": "딥러닝 기반 기술융합 예측 방법론",
          "abstract": "오늘날 기업들은 전략적인 관점에서 기술변화를 예측하고, 이를 활용한 기술전략 수립이 반드시 필요하다고 요구된다. 특히 기술융합 현상은 기술혁신을 주도하고 시장과 산업의 변화를 이끈 다고 할 수 있기 때문에, 기술융합을 정량적으로 측정하고 관측하기 위한 연구가 다수 이루어졌다. 선행 연구에서는 계량서지분석을 이용해 기술 구조를 분석하고, 기술네트워크를 통해 기술융합 현상을 분석하는데 그쳤다. 본 연구에서는 미래의 기술융합 구조를 예상할 수 있는 예측 방법론을 제시한다. 링크 예측은 현재 시점의 네트워크를 통해 미래 시점의 네트워크에 추가되거나 제거 될 링크를 예측하는 문제를 의미한다. 본 연구에서는 기술네트워크 형태로 표현 된 기술시스템에 링크 예측을 적용하여 미래 기술융합 관계에 대해 예측하는 것을 목적으로 한다. 특히 딥러닝 기법을 이용한 학습 기반 링크 예측을 수행한다는 특징이 있다. 기존에 링크 예측을 적용해 기술융합 예측을 시도한 연구가 일부 있었으나, 네트워크 내 이웃관계에 의한 토폴로지 유사도를 의사결정 척도로 이용했다는 점에서 한계에 머물렀다. 기술융합 예측이라는 도메인 속성이 고려되지 않았다는 점인데, 본 연구에서는 기존 링크 예측에서 보편적으로 사용되었던 이웃관계에 의한 네트워크 토폴로지 유사도와 인용관계에 의한 유사도를 함께 고려한 기술융합 예측모델을 제시한다. 또한 학습 기반 링크 예측에서는 기술네트워크에 대한 정보를 노드 쌍 조합 단위의 레코드를 갖는 정형화 된 데이터 구조로 변화해 이진분류 문제로 기술융합 예측 문제를 재정의 하였기 때문에, 다양한 교사학습 기반의 기계학습 알고리즘 적용이 가능하다. 본 연구에서는 분류 성능이 우수해 최근 주목받고 있는 딥러닝 기법의 DNN 알고리즘을 사용하여 학습 기반 링크 예측을 수행하고, SVM, 로지스틱회귀(logistic regression), 랜덤포레스트(random foreset)와 같은 보편적인 분류 알고리즘을 비교한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014912357&target=NART&cn=DIKO0014912357",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 기술융합 예측 방법론 딥러닝 기반 기술융합 예측 방법론 딥러닝 기반 기술융합 예측 방법론 오늘날 기업들은 전략적인 관점에서 기술변화를 예측하고, 이를 활용한 기술전략 수립이 반드시 필요하다고 요구된다. 특히 기술융합 현상은 기술혁신을 주도하고 시장과 산업의 변화를 이끈 다고 할 수 있기 때문에, 기술융합을 정량적으로 측정하고 관측하기 위한 연구가 다수 이루어졌다. 선행 연구에서는 계량서지분석을 이용해 기술 구조를 분석하고, 기술네트워크를 통해 기술융합 현상을 분석하는데 그쳤다. 본 연구에서는 미래의 기술융합 구조를 예상할 수 있는 예측 방법론을 제시한다. 링크 예측은 현재 시점의 네트워크를 통해 미래 시점의 네트워크에 추가되거나 제거 될 링크를 예측하는 문제를 의미한다. 본 연구에서는 기술네트워크 형태로 표현 된 기술시스템에 링크 예측을 적용하여 미래 기술융합 관계에 대해 예측하는 것을 목적으로 한다. 특히 딥러닝 기법을 이용한 학습 기반 링크 예측을 수행한다는 특징이 있다. 기존에 링크 예측을 적용해 기술융합 예측을 시도한 연구가 일부 있었으나, 네트워크 내 이웃관계에 의한 토폴로지 유사도를 의사결정 척도로 이용했다는 점에서 한계에 머물렀다. 기술융합 예측이라는 도메인 속성이 고려되지 않았다는 점인데, 본 연구에서는 기존 링크 예측에서 보편적으로 사용되었던 이웃관계에 의한 네트워크 토폴로지 유사도와 인용관계에 의한 유사도를 함께 고려한 기술융합 예측모델을 제시한다. 또한 학습 기반 링크 예측에서는 기술네트워크에 대한 정보를 노드 쌍 조합 단위의 레코드를 갖는 정형화 된 데이터 구조로 변화해 이진분류 문제로 기술융합 예측 문제를 재정의 하였기 때문에, 다양한 교사학습 기반의 기계학습 알고리즘 적용이 가능하다. 본 연구에서는 분류 성능이 우수해 최근 주목받고 있는 딥러닝 기법의 DNN 알고리즘을 사용하여 학습 기반 링크 예측을 수행하고, SVM, 로지스틱회귀(logistic regression), 랜덤포레스트(random foreset)와 같은 보편적인 분류 알고리즘을 비교한다."
        },
        {
          "rank": 27,
          "score": 0.6735824346542358,
          "doc_id": "JAKO201223052004277",
          "title": "회사채 신용등급 예측을 위한 SVM 앙상블학습",
          "abstract": "회사채 신용등급은 투자자의 입장에서는 수익률 결정의 중요한 요소이며 기업의 입장에서는 자본비용 및 기업 가치와 관련된 중요한 재무의사결정사항으로 정교한 신용등급 예측 모형의 개발은 재무 및 회계 분야에서 오랫동안 전통적인 연구 주제가 되어왔다. 그러나, 회사채 신용등급 예측 모형의 성과와 관련된 가장 중요한 문제는 등급별 데이터의 불균형 문제이다. 예측 문제에 있어서 데이터 불균형(Data imbalance) 은 사용되는 표본이 특정 범주에 편중되었을 때 나타난다. 데이터 불균형이 심화됨에 따라 범주 사이의 분류경계영역이 왜곡되므로 분류자의 학습성과가 저하되게 된다. 본 연구에서는 데이터 불균형 문제가 존재하는 다분류 문제를 효과적으로 해결하기 위한 다분류 기하평균 부스팅 기법 (Multiclass Geometric Mean-based Boosting MGM-Boost)을 제안하고자 한다. MGM-Boost 알고리즘은 부스팅 알고리즘에 기하평균 개념을 도입한 것으로 오분류된 표본에 대한 학습을 강화할 수 있으며 불균형 분포를 보이는 각 범주의 예측정확도를 동시에 고려한 학습이 가능하다는 장점이 있다. 회사채 신용등급 예측문제를 활용하여 MGM-Boost의 성과를 검증한 결과 SVM 및 AdaBoost 기법과 비교하여 통계적으로 유의적인 성과개선 효과를 보여주었으며 데이터 불균형 하에서도 벤치마킹 모형과 비교하여 견고한 학습성과를 나타냈다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201223052004277&target=NART&cn=JAKO201223052004277",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회사채 신용등급 예측을 위한 SVM 앙상블학습 회사채 신용등급 예측을 위한 SVM 앙상블학습 회사채 신용등급 예측을 위한 SVM 앙상블학습 회사채 신용등급은 투자자의 입장에서는 수익률 결정의 중요한 요소이며 기업의 입장에서는 자본비용 및 기업 가치와 관련된 중요한 재무의사결정사항으로 정교한 신용등급 예측 모형의 개발은 재무 및 회계 분야에서 오랫동안 전통적인 연구 주제가 되어왔다. 그러나, 회사채 신용등급 예측 모형의 성과와 관련된 가장 중요한 문제는 등급별 데이터의 불균형 문제이다. 예측 문제에 있어서 데이터 불균형(Data imbalance) 은 사용되는 표본이 특정 범주에 편중되었을 때 나타난다. 데이터 불균형이 심화됨에 따라 범주 사이의 분류경계영역이 왜곡되므로 분류자의 학습성과가 저하되게 된다. 본 연구에서는 데이터 불균형 문제가 존재하는 다분류 문제를 효과적으로 해결하기 위한 다분류 기하평균 부스팅 기법 (Multiclass Geometric Mean-based Boosting MGM-Boost)을 제안하고자 한다. MGM-Boost 알고리즘은 부스팅 알고리즘에 기하평균 개념을 도입한 것으로 오분류된 표본에 대한 학습을 강화할 수 있으며 불균형 분포를 보이는 각 범주의 예측정확도를 동시에 고려한 학습이 가능하다는 장점이 있다. 회사채 신용등급 예측문제를 활용하여 MGM-Boost의 성과를 검증한 결과 SVM 및 AdaBoost 기법과 비교하여 통계적으로 유의적인 성과개선 효과를 보여주었으며 데이터 불균형 하에서도 벤치마킹 모형과 비교하여 견고한 학습성과를 나타냈다."
        },
        {
          "rank": 28,
          "score": 0.6729331016540527,
          "doc_id": "JAKO201813164518771",
          "title": "딥 러닝 및 서포트 벡터 머신기반 센서 고장 검출 기법",
          "abstract": "최근 산업현장에서 기계의 자동화가 크게 가속화됨에 따라 자동화 기계의 관리 및 유지보수에 대한 중요성이 갈수록 커지고 있다. 자동화 기계에 부착된 센서의 고장이 발생할 경우 기계가 오동작함으로써 공정라인 운용에 막대한 피해가 발생할 수 있다. 이를 막기 위해 센서의 상태를 모니터링하고 고장의 진단 및 분류를 하는 것이 필요하다. 본 논문에서는 센서에서 발생하는 대표적인 고장 유형인 erratic fault, drift fault, hard-over fault, spike fault, stuck fault를 기계학습 알고리즘인 SVM과 CNN을 적용하여 검출하고 분류하였다. SVM의 학습 및 테스트를 위해 데이터 샘플들로부터 시간영역 통계 특징들을 추출하고 최적의 특징을 찾기 위해 유전 알고리즘(genetic algorithm)을 적용하였다. Multi-class를 분류하기 위해 multi-layer SVM을 구성하여 센서 고장을 분류하였다. CNN에 대해서는 데이터 샘플들을 사용하여 학습시키고 성능을 높이기 위해 앙상블 기법을 적용하였다. 시뮬레이션 결과를 통해 유전 알고리즘에 의해 선별된 특징들을 사용한 SVM의 분류 결과는 모든 특징이 사용된 SVM 분류기 보다는 성능이 향상되었으나 전반적으로 CNN의 성능이 SVM보다 우수한 것을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201813164518771&target=NART&cn=JAKO201813164518771",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 및 서포트 벡터 머신기반 센서 고장 검출 기법 딥 러닝 및 서포트 벡터 머신기반 센서 고장 검출 기법 딥 러닝 및 서포트 벡터 머신기반 센서 고장 검출 기법 최근 산업현장에서 기계의 자동화가 크게 가속화됨에 따라 자동화 기계의 관리 및 유지보수에 대한 중요성이 갈수록 커지고 있다. 자동화 기계에 부착된 센서의 고장이 발생할 경우 기계가 오동작함으로써 공정라인 운용에 막대한 피해가 발생할 수 있다. 이를 막기 위해 센서의 상태를 모니터링하고 고장의 진단 및 분류를 하는 것이 필요하다. 본 논문에서는 센서에서 발생하는 대표적인 고장 유형인 erratic fault, drift fault, hard-over fault, spike fault, stuck fault를 기계학습 알고리즘인 SVM과 CNN을 적용하여 검출하고 분류하였다. SVM의 학습 및 테스트를 위해 데이터 샘플들로부터 시간영역 통계 특징들을 추출하고 최적의 특징을 찾기 위해 유전 알고리즘(genetic algorithm)을 적용하였다. Multi-class를 분류하기 위해 multi-layer SVM을 구성하여 센서 고장을 분류하였다. CNN에 대해서는 데이터 샘플들을 사용하여 학습시키고 성능을 높이기 위해 앙상블 기법을 적용하였다. 시뮬레이션 결과를 통해 유전 알고리즘에 의해 선별된 특징들을 사용한 SVM의 분류 결과는 모든 특징이 사용된 SVM 분류기 보다는 성능이 향상되었으나 전반적으로 CNN의 성능이 SVM보다 우수한 것을 확인할 수 있었다."
        },
        {
          "rank": 29,
          "score": 0.6718440651893616,
          "doc_id": "JAKO202009759220895",
          "title": "SVM과 딥러닝에서 불완전한 데이터를 처리하기 위한 알고리즘",
          "abstract": "본 논문은 불완전한 데이터를 처리하기 위해 2가지의 서로 다른 기법과 이를 학습하는 알고리즘을 소개한다. 첫째방법은 손실변수가 가질 수 있는 균등한 확률로 손실값을 할당하여 불완전한 데이터를 처리하고, SVM 알고리즘으로 이 데이터를 학습하는 것이다. 이 기법은 임의의 변수에 손실 값의 빈도가 높을수록 엔트로피가 높도록 하여 이 변수가 결정트리에서 선택되지 않도록 하는 것이다. 이 방법은 손실 변수에 남아있는 정보를 모두 무시하고 새로운 값을 할당한다는 특징이 있다. 이에 반해 새로운 방법은 손실 값을 제외하고 남아있는 정보로 엔트로피 확률을 구하고 이를 손실 변수의 추정 값으로 사용하는 것이다. 즉, 불완전한 학습데이터로부터 소실되지 않은 많은 정보들을 이용해 소실된 일부 정보를 복구하고 딥러닝을 이용해 학습한다. 이 2가지 방법은 학습데이터에서 차례로 변수 하나를 선택하고, 이 변수에 손실된 데이터의 비율을 달리하면서 서로 다른 측정값들의 결과들과 반복적으로 비교함으로써 성능을 측정한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202009759220895&target=NART&cn=JAKO202009759220895",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SVM과 딥러닝에서 불완전한 데이터를 처리하기 위한 알고리즘 SVM과 딥러닝에서 불완전한 데이터를 처리하기 위한 알고리즘 SVM과 딥러닝에서 불완전한 데이터를 처리하기 위한 알고리즘 본 논문은 불완전한 데이터를 처리하기 위해 2가지의 서로 다른 기법과 이를 학습하는 알고리즘을 소개한다. 첫째방법은 손실변수가 가질 수 있는 균등한 확률로 손실값을 할당하여 불완전한 데이터를 처리하고, SVM 알고리즘으로 이 데이터를 학습하는 것이다. 이 기법은 임의의 변수에 손실 값의 빈도가 높을수록 엔트로피가 높도록 하여 이 변수가 결정트리에서 선택되지 않도록 하는 것이다. 이 방법은 손실 변수에 남아있는 정보를 모두 무시하고 새로운 값을 할당한다는 특징이 있다. 이에 반해 새로운 방법은 손실 값을 제외하고 남아있는 정보로 엔트로피 확률을 구하고 이를 손실 변수의 추정 값으로 사용하는 것이다. 즉, 불완전한 학습데이터로부터 소실되지 않은 많은 정보들을 이용해 소실된 일부 정보를 복구하고 딥러닝을 이용해 학습한다. 이 2가지 방법은 학습데이터에서 차례로 변수 하나를 선택하고, 이 변수에 손실된 데이터의 비율을 달리하면서 서로 다른 측정값들의 결과들과 반복적으로 비교함으로써 성능을 측정한다."
        },
        {
          "rank": 30,
          "score": 0.6716092824935913,
          "doc_id": "JAKO201713056893580",
          "title": "딥 러닝 프레임워크의 비교 및 분석",
          "abstract": "딥 러닝은 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 딥 러닝은 세상을 이해하고 감지하는 인공지능을 개발하는데 가장 촉망받는 기술이 되고 있으며, 구글, 바이두, 페이스북 등이 가장 앞서서 개발을 하고 있다. 본 논문에서는 딥 러닝을 구현하는 딥 러닝 프레임워크의 종류에 대해 논의하고, 딥 러닝 프레임워크의 영상과 음성 인식 분야의 효율성에 대해 비교, 분석하고자 한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201713056893580&target=NART&cn=JAKO201713056893580",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 프레임워크의 비교 및 분석 딥 러닝 프레임워크의 비교 및 분석 딥 러닝 프레임워크의 비교 및 분석 딥 러닝은 사람이 가르치지 않아도 컴퓨터가 스스로 사람처럼 학습할 수 있는 인공지능 기술이다. 딥 러닝은 세상을 이해하고 감지하는 인공지능을 개발하는데 가장 촉망받는 기술이 되고 있으며, 구글, 바이두, 페이스북 등이 가장 앞서서 개발을 하고 있다. 본 논문에서는 딥 러닝을 구현하는 딥 러닝 프레임워크의 종류에 대해 논의하고, 딥 러닝 프레임워크의 영상과 음성 인식 분야의 효율성에 대해 비교, 분석하고자 한다."
        },
        {
          "rank": 31,
          "score": 0.6714013814926147,
          "doc_id": "NART118947969",
          "title": "Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction",
          "abstract": "<P>Recent growth in crop genomic and trait data have opened opportunities for the application of novel approaches to accelerate crop improvement. Machine learning and deep learning are at the forefront of prediction-based data analysis. However, few approaches for genotype to phenotype prediction compare machine learning with deep learning and further interpret the models that support the predictions. This study uses genome wide molecular markers and traits across 1110 soybean individuals to develop accurate prediction models. For 13/14 sets of predictions, XGBoost or random forest outperformed deep learning models in prediction performance. Top ranked SNPs by F-score were identified from XGBoost, and with further investigation found overlap with significantly associated loci identified from GWAS and previous literature. Feature importance rankings were used to reduce marker input by up to 90%, and subsequent models maintained or improved their prediction performance. These findings support interpretable machine learning as an approach for genomic based prediction of traits in soybean and other crops.</P><P><B>Supplementary Information</B></P><P>The online version contains supplementary material available at 10.1186/s12870-022-03559-z.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART118947969&target=NART&cn=NART118947969",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction <P>Recent growth in crop genomic and trait data have opened opportunities for the application of novel approaches to accelerate crop improvement. Machine learning and deep learning are at the forefront of prediction-based data analysis. However, few approaches for genotype to phenotype prediction compare machine learning with deep learning and further interpret the models that support the predictions. This study uses genome wide molecular markers and traits across 1110 soybean individuals to develop accurate prediction models. For 13/14 sets of predictions, XGBoost or random forest outperformed deep learning models in prediction performance. Top ranked SNPs by F-score were identified from XGBoost, and with further investigation found overlap with significantly associated loci identified from GWAS and previous literature. Feature importance rankings were used to reduce marker input by up to 90%, and subsequent models maintained or improved their prediction performance. These findings support interpretable machine learning as an approach for genomic based prediction of traits in soybean and other crops.</P><P><B>Supplementary Information</B></P><P>The online version contains supplementary material available at 10.1186/s12870-022-03559-z.</P>"
        },
        {
          "rank": 32,
          "score": 0.6701555252075195,
          "doc_id": "ATN0048893828",
          "title": "머신러닝을 이용한 우리나라 환율 예측 연구",
          "abstract": "본 연구에서 환율의 예측 능력에 대하여 머신러닝 계열의 방법론과 비-머신러닝 계열의 방법론의 예측 능력을 상고 비교하고자 하였다. 데이터는 2001년부터 2018년을 학습의 기간으로 삼아 2019년을 테스트 하는 실험1과 2001년부터 2017년을 학습 기간으로 삼고 2018년과 2019년을 예측하는 실험2로 나누어 실험을 하였다. 실험1과 실험2 모두에서 머신러닝 계열의 예측이 비-머신러닝 계열의 예측보다 MSE측면에서 우수함을 보였다. 특히 두 실험 모두에서 다층퍼셉트론(MLP)이 매우 우수한 능력을 보였고, KNN을 시계열로 확장을 한 TSFKNN과 신경망을 시계열로 확장을 한 NNETAR 두 가지 모두 유사한 능력을 보였다. 전통적인 비-먼신러닝 계열에서는 충분히 데이터에 대한 특성이 파악이 되지 않아 낮은 수준의 예측 능력을 보이는 것으로 판단이 된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0048893828&target=NART&cn=ATN0048893828",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝을 이용한 우리나라 환율 예측 연구 머신러닝을 이용한 우리나라 환율 예측 연구 머신러닝을 이용한 우리나라 환율 예측 연구 본 연구에서 환율의 예측 능력에 대하여 머신러닝 계열의 방법론과 비-머신러닝 계열의 방법론의 예측 능력을 상고 비교하고자 하였다. 데이터는 2001년부터 2018년을 학습의 기간으로 삼아 2019년을 테스트 하는 실험1과 2001년부터 2017년을 학습 기간으로 삼고 2018년과 2019년을 예측하는 실험2로 나누어 실험을 하였다. 실험1과 실험2 모두에서 머신러닝 계열의 예측이 비-머신러닝 계열의 예측보다 MSE측면에서 우수함을 보였다. 특히 두 실험 모두에서 다층퍼셉트론(MLP)이 매우 우수한 능력을 보였고, KNN을 시계열로 확장을 한 TSFKNN과 신경망을 시계열로 확장을 한 NNETAR 두 가지 모두 유사한 능력을 보였다. 전통적인 비-먼신러닝 계열에서는 충분히 데이터에 대한 특성이 파악이 되지 않아 낮은 수준의 예측 능력을 보이는 것으로 판단이 된다."
        },
        {
          "rank": 33,
          "score": 0.6695945858955383,
          "doc_id": "JAKO202223540366088",
          "title": "이미지 학습을 위한 딥러닝 프레임워크 비교분석",
          "abstract": "딥러닝 프레임워크는 현재에도 계속해서 발전되어 가고 있으며, 다양한 프레임워크들이 존재한다. 딥러닝의 대표적인 프레임워크는 TensorFlow, PyTorch, Keras 등이 있다. 딥러님 프레임워크는 이미지 학습을 통해 이미지 분류에서의 최적화 모델을 이용한다. 본 논문에서는 딥러닝 이미지 인식 분야에서 가장 많이 사용하고 있는 TensorFlow와 PyTorch 프레임워크를 활용하여 이미지 학습을 진행하였으며, 이 과정에서 도출한 결과를 비교 분석하여 최적화된 프레임워크을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202223540366088&target=NART&cn=JAKO202223540366088",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "이미지 학습을 위한 딥러닝 프레임워크 비교분석 이미지 학습을 위한 딥러닝 프레임워크 비교분석 이미지 학습을 위한 딥러닝 프레임워크 비교분석 딥러닝 프레임워크는 현재에도 계속해서 발전되어 가고 있으며, 다양한 프레임워크들이 존재한다. 딥러닝의 대표적인 프레임워크는 TensorFlow, PyTorch, Keras 등이 있다. 딥러님 프레임워크는 이미지 학습을 통해 이미지 분류에서의 최적화 모델을 이용한다. 본 논문에서는 딥러닝 이미지 인식 분야에서 가장 많이 사용하고 있는 TensorFlow와 PyTorch 프레임워크를 활용하여 이미지 학습을 진행하였으며, 이 과정에서 도출한 결과를 비교 분석하여 최적화된 프레임워크을 알 수 있었다."
        },
        {
          "rank": 34,
          "score": 0.6681462526321411,
          "doc_id": "ATN0027106087",
          "title": "기계학습 방법을 이용한 기업부도의 예측",
          "abstract": "The analysis and management of business failure has been recognized to be important in the area of financial management in the evaluation of firms’ performance and the assessment of their viability. To this end, effective failure-prediction models are needed. This paper describes a new approach to prediction of business failure using the total margin algorithm which is a kind of support vector machine. It will be shown that the proposed method can evaluate the risk of failure better than existing methods through some real data.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0027106087&target=NART&cn=ATN0027106087",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "기계학습 방법을 이용한 기업부도의 예측 기계학습 방법을 이용한 기업부도의 예측 기계학습 방법을 이용한 기업부도의 예측 The analysis and management of business failure has been recognized to be important in the area of financial management in the evaluation of firms’ performance and the assessment of their viability. To this end, effective failure-prediction models are needed. This paper describes a new approach to prediction of business failure using the total margin algorithm which is a kind of support vector machine. It will be shown that the proposed method can evaluate the risk of failure better than existing methods through some real data."
        },
        {
          "rank": 35,
          "score": 0.667304515838623,
          "doc_id": "NART132071160",
          "title": "Integrating machine learning and deep learning for enhanced supplier risk prediction",
          "abstract": "<P>The importance of anticipating and preventing disruptions is underscored by the increased operational complexity and vulnerability caused by advancements in supply chain management (SCM). This has spurred interest in integrating machine learning (ML) and deep learning (DL) into supply chain risk management (SCRM). In this paper, we introduce a tailored method using ML and DL to improve SCRM by predicting supplier failures, thus boosting efficiency and resilience in SC operations. Our method involves five phases focused on classifying and predicting supplier failures in non-conforming deliveries. This involves forecasting failure quantities and estimating total disruption costs. Initially, data from an automotive company is selected, and appropriate potential features and algorithms are selected, performance metric aligns with case study objectives, facilitating method evaluation are used such as: Precision, recall, F1-score, and accuracy metrics assess classification models, while Mean Squared Error (MSE) is used for regression tasks. Finally, an experimental design optimizes models, assessing success rates of various algorithms and their parameters within the chosen feature space. Experimental results underscore the success of our methodology in model development. In the classification task, the Random Forest (RF) classifier achieved 86% accuracy. When combined with the Gradient Boosting classifier, the ensemble exhibited enhanced accuracy, highlighting the complementary strengths of both algorithms and their synergistic impact, surpassing the performance of RF, Support Vector Regression (SVR), k-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). Noteworthy is the performance in regression tasks, where Linear Regression, ANN, and RF Regressor displayed exceptionally low MSE compared to other models.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART132071160&target=NART&cn=NART132071160",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Integrating machine learning and deep learning for enhanced supplier risk prediction Integrating machine learning and deep learning for enhanced supplier risk prediction Integrating machine learning and deep learning for enhanced supplier risk prediction <P>The importance of anticipating and preventing disruptions is underscored by the increased operational complexity and vulnerability caused by advancements in supply chain management (SCM). This has spurred interest in integrating machine learning (ML) and deep learning (DL) into supply chain risk management (SCRM). In this paper, we introduce a tailored method using ML and DL to improve SCRM by predicting supplier failures, thus boosting efficiency and resilience in SC operations. Our method involves five phases focused on classifying and predicting supplier failures in non-conforming deliveries. This involves forecasting failure quantities and estimating total disruption costs. Initially, data from an automotive company is selected, and appropriate potential features and algorithms are selected, performance metric aligns with case study objectives, facilitating method evaluation are used such as: Precision, recall, F1-score, and accuracy metrics assess classification models, while Mean Squared Error (MSE) is used for regression tasks. Finally, an experimental design optimizes models, assessing success rates of various algorithms and their parameters within the chosen feature space. Experimental results underscore the success of our methodology in model development. In the classification task, the Random Forest (RF) classifier achieved 86% accuracy. When combined with the Gradient Boosting classifier, the ensemble exhibited enhanced accuracy, highlighting the complementary strengths of both algorithms and their synergistic impact, surpassing the performance of RF, Support Vector Regression (SVR), k-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). Noteworthy is the performance in regression tasks, where Linear Regression, ANN, and RF Regressor displayed exceptionally low MSE compared to other models.</P>"
        },
        {
          "rank": 36,
          "score": 0.6672812700271606,
          "doc_id": "NART32653959",
          "title": "Integrating support vector machines and neural networks",
          "abstract": "<P><B>Abstract</B></P><P>Support vector machines (SVMs) are a powerful technique developed in the last decade to effectively tackle classification and regression problems. In this paper we describe how support vector machines and artificial neural networks can be integrated in order to classify objects correctly. This technique has been successfully applied to the problem of determining the quality of tiles. Using an optical reader system, some features are automatically extracted, then a subset of the features is determined and the tiles are classified based on this subset.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART32653959&target=NART&cn=NART32653959",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Integrating support vector machines and neural networks Integrating support vector machines and neural networks Integrating support vector machines and neural networks <P><B>Abstract</B></P><P>Support vector machines (SVMs) are a powerful technique developed in the last decade to effectively tackle classification and regression problems. In this paper we describe how support vector machines and artificial neural networks can be integrated in order to classify objects correctly. This technique has been successfully applied to the problem of determining the quality of tiles. Using an optical reader system, some features are automatically extracted, then a subset of the features is determined and the tiles are classified based on this subset.</P>"
        },
        {
          "rank": 37,
          "score": 0.666962742805481,
          "doc_id": "DIKO0015889140",
          "title": "딥 러닝 프레임워크 성능 비교 및 개선 방안",
          "abstract": "현 시대는 4차 산업혁명이 대두되는 시대로 요소 기술들 중 인공지능의 중 요성은 아무리 강조하더라도 지나치지 않으며, 기업들 경쟁력의 척도라고 불 릴만큼 모든 산업에서 활용되고있다. 2016년 경 DeepMind 의 AlphaGo 와 이 세돌 선수의 경기로 국내에서는 처음으로 인공지능의 위력과 Deep Learning 이라는 단어가 대중들에게 알려지게 되었다.&amp;#xD; 특정 IT 산업이 발전하게 되면 해당 분야의 개발자들의 생산성과 접근성을 높이기 위해 Framework 들이 등장, 발전하게 된다. 통신기술과 스마트폰의 출현으로 WEB 붐이 이르렀을 때, Server-side 에서는 Spring, django, Ruby on Rails 등이 출현하였고, Client-side 에서는 Angular, React, jQuery 와 같이 다양한 Framework 들이 등장 발전하였다. 컴퓨터 성능의 발전과 다양 한 컴퓨팅 기술의 발전으로 현 시대는 인공지능 3차 붐으로 Machine Learning 과 Deep Learning 의 시대로 불리고있다.&amp;#xD; 이와 같이 Deep Learning 분야에서도 다양한 Framework 들이 개발되었다. 이런 다양한 Framework 제품들의 목적은 개발자들의 생산성을 향상시키기 위 해 내부 알고리즘이나 메커니즘을 Black Box 형식으로 감추고 High Level API 를 제공하기 때문에, 내부적인 구현 방식은 Framework 별로 다르다. 본 논문에서는 현 시대에 가장 많이 사용하는 대표적인 Framework 들을 선정한 다. 그리고 선정된 Framework 들을 이용하여 Convolutional Neural Network 알고리즘을 구현, 동일한 Training Data 를 이용하여 학습 Model 을 만들어 낸다. 그리고 동일한 Cloud 환경에서 각 Framework 별 학습을 수행하여 성 능을 비교한다. 성능 비교 환경은 총 3가지로 CPU, GPU 1 Core, Multi GPU Core 환경에서 각 Framework 별 성능 지표를 추출한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015889140&target=NART&cn=DIKO0015889140",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 프레임워크 성능 비교 및 개선 방안 딥 러닝 프레임워크 성능 비교 및 개선 방안 딥 러닝 프레임워크 성능 비교 및 개선 방안 현 시대는 4차 산업혁명이 대두되는 시대로 요소 기술들 중 인공지능의 중 요성은 아무리 강조하더라도 지나치지 않으며, 기업들 경쟁력의 척도라고 불 릴만큼 모든 산업에서 활용되고있다. 2016년 경 DeepMind 의 AlphaGo 와 이 세돌 선수의 경기로 국내에서는 처음으로 인공지능의 위력과 Deep Learning 이라는 단어가 대중들에게 알려지게 되었다.&amp;#xD; 특정 IT 산업이 발전하게 되면 해당 분야의 개발자들의 생산성과 접근성을 높이기 위해 Framework 들이 등장, 발전하게 된다. 통신기술과 스마트폰의 출현으로 WEB 붐이 이르렀을 때, Server-side 에서는 Spring, django, Ruby on Rails 등이 출현하였고, Client-side 에서는 Angular, React, jQuery 와 같이 다양한 Framework 들이 등장 발전하였다. 컴퓨터 성능의 발전과 다양 한 컴퓨팅 기술의 발전으로 현 시대는 인공지능 3차 붐으로 Machine Learning 과 Deep Learning 의 시대로 불리고있다.&amp;#xD; 이와 같이 Deep Learning 분야에서도 다양한 Framework 들이 개발되었다. 이런 다양한 Framework 제품들의 목적은 개발자들의 생산성을 향상시키기 위 해 내부 알고리즘이나 메커니즘을 Black Box 형식으로 감추고 High Level API 를 제공하기 때문에, 내부적인 구현 방식은 Framework 별로 다르다. 본 논문에서는 현 시대에 가장 많이 사용하는 대표적인 Framework 들을 선정한 다. 그리고 선정된 Framework 들을 이용하여 Convolutional Neural Network 알고리즘을 구현, 동일한 Training Data 를 이용하여 학습 Model 을 만들어 낸다. 그리고 동일한 Cloud 환경에서 각 Framework 별 학습을 수행하여 성 능을 비교한다. 성능 비교 환경은 총 3가지로 CPU, GPU 1 Core, Multi GPU Core 환경에서 각 Framework 별 성능 지표를 추출한다."
        },
        {
          "rank": 38,
          "score": 0.6664416790008545,
          "doc_id": "JAKO202013261023095",
          "title": "딥러닝을 위한 경사하강법 비교",
          "abstract": "본 논문에서는 신경망을 학습하는 데 가장 많이 사용되고 있는 경사하강법에 대해 분석하였다. 학습이란 손실함수가 최소값이 되도록 매개변수를 갱신하는 것이다. 손실함수는 실제값과 예측값의 차이를 수치화 해주는 함수이다. 경사하강법은 오차가 최소화되도록 매개변수를 갱신하는데 손실함수의 기울기를 사용하는 것으로 현재 최고의 딥러닝 학습알고리즘을 제공하는 라이브러리에서 사용되고 있다. 그러나 이 알고리즘들은 블랙박스형태로 제공되고 있어서 다양한 경사하강법들의 장단점을 파악하는 것이 쉽지 않다. 경사하강법에서 현재 대표적으로 사용되고 있는 확률적 경사하강법(Stochastic Gradient Descent method), 모멘텀법(Momentum method), AdaGrad법 그리고 Adadelta법의 특성에 대하여 분석하였다. 실험 데이터는 신경망을 검증하는 데 널리 사용되는 MNIST 데이터 셋을 사용하였다. 은닉층은 2개의 층으로 첫 번째 층은 500개 그리고 두 번째 층은 300개의 뉴런으로 구성하였다. 출력 층의 활성화함수는 소프트 맥스함수이고 나머지 입력 층과 은닉 층의 활성화함수는 ReLu함수를 사용하였다. 그리고 손실함수는 교차 엔트로피 오차를 사용하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202013261023095&target=NART&cn=JAKO202013261023095",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 위한 경사하강법 비교 딥러닝을 위한 경사하강법 비교 딥러닝을 위한 경사하강법 비교 본 논문에서는 신경망을 학습하는 데 가장 많이 사용되고 있는 경사하강법에 대해 분석하였다. 학습이란 손실함수가 최소값이 되도록 매개변수를 갱신하는 것이다. 손실함수는 실제값과 예측값의 차이를 수치화 해주는 함수이다. 경사하강법은 오차가 최소화되도록 매개변수를 갱신하는데 손실함수의 기울기를 사용하는 것으로 현재 최고의 딥러닝 학습알고리즘을 제공하는 라이브러리에서 사용되고 있다. 그러나 이 알고리즘들은 블랙박스형태로 제공되고 있어서 다양한 경사하강법들의 장단점을 파악하는 것이 쉽지 않다. 경사하강법에서 현재 대표적으로 사용되고 있는 확률적 경사하강법(Stochastic Gradient Descent method), 모멘텀법(Momentum method), AdaGrad법 그리고 Adadelta법의 특성에 대하여 분석하였다. 실험 데이터는 신경망을 검증하는 데 널리 사용되는 MNIST 데이터 셋을 사용하였다. 은닉층은 2개의 층으로 첫 번째 층은 500개 그리고 두 번째 층은 300개의 뉴런으로 구성하였다. 출력 층의 활성화함수는 소프트 맥스함수이고 나머지 입력 층과 은닉 층의 활성화함수는 ReLu함수를 사용하였다. 그리고 손실함수는 교차 엔트로피 오차를 사용하였다."
        },
        {
          "rank": 39,
          "score": 0.6660518646240234,
          "doc_id": "ATN0037496660",
          "title": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교",
          "abstract": "Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0037496660&target=NART&cn=ATN0037496660",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 수요 패턴 별 최적 머신러닝 수요예측 모델 성능 비교 Demand forecasting is a way to manage resources by forecasting demands for products, so it has direct impacts on corporate resources and budget management. Based on these reasons, research on improving forecasting performances of demand forecasting models. In this research, 4 demand patterns for items were analyzed to improve demand prediction performance, and the optimal model was proposed. The data used to compare the performance were the demand data from each quarter for maintenance items for a T-50 aircraft of Republic of Korea air force. First, the demand patterns for the items adopted average demand interval(ADI) and coefficient of variation(CV) and were categorized into smooth, lumpy, intermittent, and erratic items. In this research, to compare the performance of demand forecasting models derived from different algorithms, 5 types of machine learning algorithms and 2 types of deep learning algorithms were used to construct demand forecasting models. In machine learning algorithms, there are ensemble learning such as random forest regression, adaboost, extra trees regression, bagging, gradient boosting regression and deep learning algorithm such as long-short term memory(LSTM) and deep neural network(DNN). We can confirm that item accuracy is 0.61% and quantity accuracy is 0.09% better than that of consistent models when the demand forecast results are derived by selecting models suitable for four types according to demand patterns. We expect that efficient demand management by experts will be achieved if the application of the proposed model."
        },
        {
          "rank": 40,
          "score": 0.6655195355415344,
          "doc_id": "DIKO0015731994",
          "title": "기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로",
          "abstract": "본 연구에서는 기업의 부도를 사전에 예측하기 위한 모형을 연구하였고 정량 정보인 재무 데이터와 비정형 정보인 뉴스 콘텐츠의 감성분석 정보를 변수로 선정하였다. 재무 정보는 회계와 재무분야의 문헌에서 잘 알려지고 오랜 기간을 통하여 검증된 3개의 재무모델 변수(Altman, 1968; Beaver, 1968; Horrigan, 1966)와 기업의 경영상태를 종합적으로 분석하는 방법인 기업경영분석 지표(한국은행)를 결합하여 총 3개년치의 재무변수를 선정하였다. &amp;#xD; &amp;#xD; 기업의 부도 징후를 나타내는 유의미한 재무적 요인을 도출하기 위해 t-test와 logistic regression방법으로 통계적 검증 작업을 진행하였고, 연구 결과 총자산 이익잉여금률, 총자산 이익률, 매출액 운전자본 비율, 자본 매출액 비율, 차입금 의존도가 유의미한 재무 변수로 확인되었다. &amp;#xD; &amp;#xD; 비정형 정보인 뉴스 콘텐츠가 기업 부도를 예측하는데 얼마나 효과적인지 검증하기 위해 재무 변수에 뉴스 감성 분석 점수를 추가하여 모델링을 적용하였다. 부도 기업인 경우 부도 직전 6개월치 뉴스를, 정상 기업인 경우 2019. 7 ~ 12월까지의 6개월치 뉴스 기사를 크롤링 하였고 실제 기업 뉴스와 상관없는 기사들은 정제 작업 등의 전처리를 진행하였다. 정제가 완료된 텍스트에서 명사를 추출하여 말뭉치 기반의 감성사전을 구축하고 뉴스의 수집 기간별 감성점수를 변수로 추가하였다. &amp;#xD; &amp;#xD; 기업 뉴스의 감성점수를 추가한 결과 재무 데이터만을 사용했을 때보다 훨씬 더 좋은 성능이 나타남을 알 수 있었다. 민감도 기준(실제 부도기업을 부도기업으로 예측)으로 가장 성능이 우수한 SVM(Support Vector Machine)의 경우 재무 변수만 사용했을 때 87.50%였는데 뉴스 감성점수를 추가했을 때 93.75%로 약 6% 정도의 성능 향상이 있었다. 그리고 뉴스 수집기간은 3,4개월치를 적용 했을 때 민감도의 성능이 가장 좋은 것으로 확인되었다. &amp;#xD; &amp;#xD; 금융기관에서는 전통적으로 부도 예측을 위해 재무 데이터를 주로 사용하고 있는데 해당 정보는 분기별로 업데이트가 되는 정보의 적시성에 문제가 있을 수 있다. 따라서 부도 예측시 본 연구에서 실증한 온라인 뉴스의 감성분석 정보인 비정형 데이터를 함께 사용한다면 효과적인 여신 의사결정 지원 체계를 수립하는데 많은 도움이 될 것으로 판단된다. &amp;#xD; &amp;#xD;",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015731994&target=NART&cn=DIKO0015731994",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로 기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로 기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로 본 연구에서는 기업의 부도를 사전에 예측하기 위한 모형을 연구하였고 정량 정보인 재무 데이터와 비정형 정보인 뉴스 콘텐츠의 감성분석 정보를 변수로 선정하였다. 재무 정보는 회계와 재무분야의 문헌에서 잘 알려지고 오랜 기간을 통하여 검증된 3개의 재무모델 변수(Altman, 1968; Beaver, 1968; Horrigan, 1966)와 기업의 경영상태를 종합적으로 분석하는 방법인 기업경영분석 지표(한국은행)를 결합하여 총 3개년치의 재무변수를 선정하였다. &amp;#xD; &amp;#xD; 기업의 부도 징후를 나타내는 유의미한 재무적 요인을 도출하기 위해 t-test와 logistic regression방법으로 통계적 검증 작업을 진행하였고, 연구 결과 총자산 이익잉여금률, 총자산 이익률, 매출액 운전자본 비율, 자본 매출액 비율, 차입금 의존도가 유의미한 재무 변수로 확인되었다. &amp;#xD; &amp;#xD; 비정형 정보인 뉴스 콘텐츠가 기업 부도를 예측하는데 얼마나 효과적인지 검증하기 위해 재무 변수에 뉴스 감성 분석 점수를 추가하여 모델링을 적용하였다. 부도 기업인 경우 부도 직전 6개월치 뉴스를, 정상 기업인 경우 2019. 7 ~ 12월까지의 6개월치 뉴스 기사를 크롤링 하였고 실제 기업 뉴스와 상관없는 기사들은 정제 작업 등의 전처리를 진행하였다. 정제가 완료된 텍스트에서 명사를 추출하여 말뭉치 기반의 감성사전을 구축하고 뉴스의 수집 기간별 감성점수를 변수로 추가하였다. &amp;#xD; &amp;#xD; 기업 뉴스의 감성점수를 추가한 결과 재무 데이터만을 사용했을 때보다 훨씬 더 좋은 성능이 나타남을 알 수 있었다. 민감도 기준(실제 부도기업을 부도기업으로 예측)으로 가장 성능이 우수한 SVM(Support Vector Machine)의 경우 재무 변수만 사용했을 때 87.50%였는데 뉴스 감성점수를 추가했을 때 93.75%로 약 6% 정도의 성능 향상이 있었다. 그리고 뉴스 수집기간은 3,4개월치를 적용 했을 때 민감도의 성능이 가장 좋은 것으로 확인되었다. &amp;#xD; &amp;#xD; 금융기관에서는 전통적으로 부도 예측을 위해 재무 데이터를 주로 사용하고 있는데 해당 정보는 분기별로 업데이트가 되는 정보의 적시성에 문제가 있을 수 있다. 따라서 부도 예측시 본 연구에서 실증한 온라인 뉴스의 감성분석 정보인 비정형 데이터를 함께 사용한다면 효과적인 여신 의사결정 지원 체계를 수립하는데 많은 도움이 될 것으로 판단된다. &amp;#xD; &amp;#xD;"
        },
        {
          "rank": 41,
          "score": 0.6650872230529785,
          "doc_id": "ATN0038661375",
          "title": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험",
          "abstract": "Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0038661375&target=NART&cn=ATN0038661375",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 단백질 기능 예측 모델의 주요 딥러닝 모델 비교 실험 Proteins are the basic unit of all life activities, and understanding them is essential for studying life phenomena. Since the emergenceof the machine learning methodology using artificial neural networks, many researchers have tried to predict the function of proteinsusing only protein sequences. Many combinations of deep learning models have been reported to academia, but the methods are differentand there is no formal methodology, and they are tailored to different data, so there has never been a direct comparative analysis ofwhich algorithms are more suitable for handling protein data. In this paper, the single model performance of each algorithm was comparedand evaluated based on accuracy and speed by applying the same data to CNN, LSTM, and GRU models, which are the most frequentlyused representative algorithms in the convergence research field of predicting protein functions, and the final evaluation scale is presentedas Micro Precision, Recall, and F1-score. The combined models CNN-LSTM and CNN-GRU models also were evaluated in the same way.Through this study, it was confirmed that the performance of LSTM as a single model is good in simple classification problems, overlappingCNN was suitable as a single model in complex classification problems, and the CNN-LSTM was relatively better as a combination model."
        },
        {
          "rank": 42,
          "score": 0.6618877649307251,
          "doc_id": "JAKO202029462558904",
          "title": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식",
          "abstract": "특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202029462558904&target=NART&cn=JAKO202029462558904",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 심층신경망 기반의 음성인식을 위한 절충된 특징 정규화 방식 특징 정규화는 음성 특징 파라미터들의 통계적인 특성의 정규화를 통해 훈련 및 테스트 조건 사이의 환경 불일치의 영향을 감소시키는 방법으로서 기존의 Gaussian mixture model-hidden Markov model(GMM-HMM) 기반의 음성인식 시스템에서 우수한 성능개선을 입증한 바 있다. 하지만 심층신경망(deep neural network, DNN) 기반의 음성인식 시스템에서는 환경 불일치의 영향을 최소화 하는 것이 반드시 최고의 성능 개선으로 연결되지는 않는다. 본 논문에서는 이러한 현상의 원인을 과도한 특징 정규화로 인한 정보손실 때문이라 보고, 음향모델을 훈련 하는데 유용한 정보는 보존하면서 환경 불일치의 영향은 적절히 감소시켜 음성인식 성능을 최대화 하는 특징 정규화 방식이 있는 지 검토해보고자 한다. 이를 위해 평균 정규화(mean normalization, MN)와 평균 및 분산 정규화(mean and variance normalization, MVN)의 절충 방식인 평균 및 지수적 분산 정규화(mean and exponentiated variance normalization, MEVN)를 도입하여, 잡음 및 잔향 환경에서 분산에 대한 정규화의 정도에 따른 DNN 기반의 음성인식 시스템의 성능을 비교한다. 실험 결과, 성능 개선의 폭이 크지는 않으나 분산 정규화의 정도에 따라 MEVN이 MN과 MVN보다 성능이 우수함을 보여준다."
        },
        {
          "rank": 43,
          "score": 0.6612454652786255,
          "doc_id": "DIKO0014477767",
          "title": "머신러닝에 기반한 k-최근접 이웃과 서포트벡터머신 분류기 비교실험 및 평가",
          "abstract": "최근 들어 정보기술의 발전과 더불어 수많은 다양하고 형태의 데이터들이 기하급수적으로 방대하게 늘어나고 있어 많은 복잡한 데이터의 분석에 대한 필요성이 여러 분야에서 대두 되고 있다. 다량의 복잡한 데이터를 분석하고 원하는 결과로 나타내기위한 방법으로 머신러닝이 각광받고 있다. 머신러닝이란 어떠한 명시적인 프로그램 없이도 스스로 학습하는 능력을 갖춘 컴퓨터로 제공되는 데이터 인공지능 분야의 한 종류이며 새로운 데이터가 입력되면 스스로 학습하여 자신을 향상시키고 변화시키는 컴퓨터 프로그램을 바탕으로 동작한다. 머신러닝은 크게 지도학습과 비지도학습으로 분류할 수 있고, 지도학습은 분류와 회귀 등의 알고리즘들이 있고, 비지도학습은 군집화와 밀도추정 등의 알고리즘들이 있다. &amp;#xD; 본 논문에서는 분류기중에서도 많이 사용되어 비교되어지는 k-최근접 이웃(k-Nearest Neighbor: k-NN)과 서포트벡터머신(Support Vector Machine: SVM) 분류기(Classifier) 알고리즘 두 개를 실험하여 비교 평가하였다. 두 알고리즘을 실험하기 위해서 표본 데이터로 어린이집에서 보유한 영유아 건강검진 신체 데이터 중에 나이, 성별, 신장, 몸무게 정보만을 따로 분류 수집하여 이용하였다. 그중에 분류를 위한 수치정보는 신장과 몸무게를 이용하였고 데이터집합의 항목 값으로는 나이를 사용하였다. &amp;#xD; k-NN은 비모수적 밀도추정에 기반을 두고 가장 가까운 k개의 인접 데이터를 취하여 다수결을 통해 해당 데이터집합의 항목 값을 분류할 데이터의 항목 값으로 결정하는 방식이다. SVM은 기본적으로 이진선형분류에 사용되어지며 분류를 원하는 집합 사이의 가장 큰 폭의 경계에 위치한 서포트벡터 데이터를 기준으로 선형경계를 이용하여 두 집단을 분리시키는 알고리즘이다. SVM은 선형분리뿐 아니라 비선형분리에도 사용 할 수 있지만 주어진 데이터를 고차원으로 사상시키는 커널(Kernel) 방법을 사용해야한다. &amp;#xD; 원천수집데이터는 100개뿐이라서 본 실험에서 사용되어지는 500개, 1,000개 그리고 10,000개의 실험을 하기위해서는 데이터를 늘릴 필요가 있었다. 데이터개수를 늘리는 방법으로는 모든 각각의 데이터 임의의 반경 내에서 100의 배수만큼 무작위로 생성시키는 방법을 취하였다. &amp;#xD; k-NN의 알고리즘은 분류할 데이터가 입력 될 때마다 다시 모든 데이터를 학습하는 방식이고 SVM은 미리 학습하여 미리 분리할 경계를 찾고 분류할 대상인 데이터가 입력되면 학습만 별도로 수행이 가능한 알고리즘 방식이다. 비교실험은 두 분류기의 분류하는 소요시간과 분류 정확도를 실험데이터 크기별로 비교 실험 하였다. &amp;#xD; 본 논문을 통하여 두 분류기의 이론적인 내용을 심화 학습하고 분류기라는 공통적인 머신러닝 알고리즘을 각각 실험하여 비교 평가하였다. 비교 평가한 결과물은 다른 종류의 데이터를 분류하고자 할 때 적당한 분류기를 선택 할 수 있는 기준을 제시한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014477767&target=NART&cn=DIKO0014477767",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝에 기반한 k-최근접 이웃과 서포트벡터머신 분류기 비교실험 및 평가 머신러닝에 기반한 k-최근접 이웃과 서포트벡터머신 분류기 비교실험 및 평가 머신러닝에 기반한 k-최근접 이웃과 서포트벡터머신 분류기 비교실험 및 평가 최근 들어 정보기술의 발전과 더불어 수많은 다양하고 형태의 데이터들이 기하급수적으로 방대하게 늘어나고 있어 많은 복잡한 데이터의 분석에 대한 필요성이 여러 분야에서 대두 되고 있다. 다량의 복잡한 데이터를 분석하고 원하는 결과로 나타내기위한 방법으로 머신러닝이 각광받고 있다. 머신러닝이란 어떠한 명시적인 프로그램 없이도 스스로 학습하는 능력을 갖춘 컴퓨터로 제공되는 데이터 인공지능 분야의 한 종류이며 새로운 데이터가 입력되면 스스로 학습하여 자신을 향상시키고 변화시키는 컴퓨터 프로그램을 바탕으로 동작한다. 머신러닝은 크게 지도학습과 비지도학습으로 분류할 수 있고, 지도학습은 분류와 회귀 등의 알고리즘들이 있고, 비지도학습은 군집화와 밀도추정 등의 알고리즘들이 있다. &amp;#xD; 본 논문에서는 분류기중에서도 많이 사용되어 비교되어지는 k-최근접 이웃(k-Nearest Neighbor: k-NN)과 서포트벡터머신(Support Vector Machine: SVM) 분류기(Classifier) 알고리즘 두 개를 실험하여 비교 평가하였다. 두 알고리즘을 실험하기 위해서 표본 데이터로 어린이집에서 보유한 영유아 건강검진 신체 데이터 중에 나이, 성별, 신장, 몸무게 정보만을 따로 분류 수집하여 이용하였다. 그중에 분류를 위한 수치정보는 신장과 몸무게를 이용하였고 데이터집합의 항목 값으로는 나이를 사용하였다. &amp;#xD; k-NN은 비모수적 밀도추정에 기반을 두고 가장 가까운 k개의 인접 데이터를 취하여 다수결을 통해 해당 데이터집합의 항목 값을 분류할 데이터의 항목 값으로 결정하는 방식이다. SVM은 기본적으로 이진선형분류에 사용되어지며 분류를 원하는 집합 사이의 가장 큰 폭의 경계에 위치한 서포트벡터 데이터를 기준으로 선형경계를 이용하여 두 집단을 분리시키는 알고리즘이다. SVM은 선형분리뿐 아니라 비선형분리에도 사용 할 수 있지만 주어진 데이터를 고차원으로 사상시키는 커널(Kernel) 방법을 사용해야한다. &amp;#xD; 원천수집데이터는 100개뿐이라서 본 실험에서 사용되어지는 500개, 1,000개 그리고 10,000개의 실험을 하기위해서는 데이터를 늘릴 필요가 있었다. 데이터개수를 늘리는 방법으로는 모든 각각의 데이터 임의의 반경 내에서 100의 배수만큼 무작위로 생성시키는 방법을 취하였다. &amp;#xD; k-NN의 알고리즘은 분류할 데이터가 입력 될 때마다 다시 모든 데이터를 학습하는 방식이고 SVM은 미리 학습하여 미리 분리할 경계를 찾고 분류할 대상인 데이터가 입력되면 학습만 별도로 수행이 가능한 알고리즘 방식이다. 비교실험은 두 분류기의 분류하는 소요시간과 분류 정확도를 실험데이터 크기별로 비교 실험 하였다. &amp;#xD; 본 논문을 통하여 두 분류기의 이론적인 내용을 심화 학습하고 분류기라는 공통적인 머신러닝 알고리즘을 각각 실험하여 비교 평가하였다. 비교 평가한 결과물은 다른 종류의 데이터를 분류하고자 할 때 적당한 분류기를 선택 할 수 있는 기준을 제시한다."
        },
        {
          "rank": 44,
          "score": 0.6609405279159546,
          "doc_id": "NART98545871",
          "title": "Review of bankruptcy prediction using machine learning and deep learning techniques",
          "abstract": "<P><B>Abstract</B></P>  <P>Bankruptcy prediction has long been a significant issue in finance and management science, which attracts the attention of researchers and practitioners. With the great development of modern information technology, it has evolved into using machine learning or deep learning algorithms to do the prediction, from the initial analysis of financial statements. In this paper, we will review the machine learning or deep learning models used in bankruptcy prediction, including the classical machine learning models such as Multivariant Discriminant Analysis (MDA), Logistic Regression (LR), Ensemble method, Neural Networks (NN) and Support Vector Machines (SVM), and major deep learning methods such as Deep Belief Network (DBN) and Convolutional Neural Network (CNN). In each model, the specific process of experiment and characteristics will be summarized through analyzing some typical articles. Finally, possible innovative changes of bankruptcy prediction and its future trends will be discussed.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART98545871&target=NART&cn=NART98545871",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Review of bankruptcy prediction using machine learning and deep learning techniques Review of bankruptcy prediction using machine learning and deep learning techniques Review of bankruptcy prediction using machine learning and deep learning techniques <P><B>Abstract</B></P>  <P>Bankruptcy prediction has long been a significant issue in finance and management science, which attracts the attention of researchers and practitioners. With the great development of modern information technology, it has evolved into using machine learning or deep learning algorithms to do the prediction, from the initial analysis of financial statements. In this paper, we will review the machine learning or deep learning models used in bankruptcy prediction, including the classical machine learning models such as Multivariant Discriminant Analysis (MDA), Logistic Regression (LR), Ensemble method, Neural Networks (NN) and Support Vector Machines (SVM), and major deep learning methods such as Deep Belief Network (DBN) and Convolutional Neural Network (CNN). In each model, the specific process of experiment and characteristics will be summarized through analyzing some typical articles. Finally, possible innovative changes of bankruptcy prediction and its future trends will be discussed.</P>"
        },
        {
          "rank": 45,
          "score": 0.6605783104896545,
          "doc_id": "JAKO202108848920380",
          "title": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구",
          "abstract": "The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202108848920380&target=NART&cn=JAKO202108848920380",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity."
        },
        {
          "rank": 46,
          "score": 0.6603688597679138,
          "doc_id": "NPAP00072266",
          "title": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models",
          "abstract": "A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NPAP00072266&target=NART&cn=NPAP00072266",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models Phoneme recognition: neural networks vs. hidden Markov models vs. hidden Markov models A time-delay neural network (TDNN) for phoneme recognition is discussed. By the use of two hidden layers in addition to an input and output layer it is capable of representing complex nonlinear decision surfaces. Three important properties of the TDNNs have been observed. First, it was able to invent without human interference meaningful linguistic abstractions in time and frequency such as formant tracking and segmentation. Second, it has learned to form alternate representations linking different acoustic events with the same higher level concept. In this fashion it can implement trading relations between lower level acoustic events leading to robust recognition performance despite considerable variability in the input speech. Third, the network is translation-invariant and does not rely on precise alignment or segmentation of the input. The TDNNs performance is compared with the best of hidden Markov models (HMMs) on a speaker-dependent phoneme-recognition task. The TDNN achieved a recognition of 98.5% compared to 93.7% for the HMM, i.e., a fourfold reduction in error."
        },
        {
          "rank": 47,
          "score": 0.6603065729141235,
          "doc_id": "NART133898062",
          "title": "Hidden Markov Neural Networks",
          "abstract": "<P>We define an evolving in-time Bayesian neural network called a Hidden Markov Neural Network, which addresses the crucial challenge in time-series forecasting and continual learning: striking a balance between adapting to new data and appropriately forgetting outdated information. This is achieved by modelling the weights of a neural network as the hidden states of a Hidden Markov model, with the observed process defined by the available data. A filtering algorithm is employed to learn a variational approximation of the evolving-in-time posterior distribution over the weights. By leveraging a sequential variant of Bayes by Backprop, enriched with a stronger regularization technique called variational DropConnect, Hidden Markov Neural Networks achieve robust regularization and scalable inference. Experiments on MNIST, dynamic classification tasks, and next-frame forecasting in videos demonstrate that Hidden Markov Neural Networks provide strong predictive performance while enabling effective uncertainty quantification.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART133898062&target=NART&cn=NART133898062",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden Markov Neural Networks Hidden Markov Neural Networks Hidden Markov Neural Networks <P>We define an evolving in-time Bayesian neural network called a Hidden Markov Neural Network, which addresses the crucial challenge in time-series forecasting and continual learning: striking a balance between adapting to new data and appropriately forgetting outdated information. This is achieved by modelling the weights of a neural network as the hidden states of a Hidden Markov model, with the observed process defined by the available data. A filtering algorithm is employed to learn a variational approximation of the evolving-in-time posterior distribution over the weights. By leveraging a sequential variant of Bayes by Backprop, enriched with a stronger regularization technique called variational DropConnect, Hidden Markov Neural Networks achieve robust regularization and scalable inference. Experiments on MNIST, dynamic classification tasks, and next-frame forecasting in videos demonstrate that Hidden Markov Neural Networks provide strong predictive performance while enabling effective uncertainty quantification.</P>"
        },
        {
          "rank": 48,
          "score": 0.6598185300827026,
          "doc_id": "JAKO202520454002736",
          "title": "산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구",
          "abstract": "In industrial settings, inverters play a critical role in maintaining productivity and ensuring stable equipment operation. However, inverter failures can result in production downtime and increased maintenance costs. Traditional fault prediction methods based on physical models and expert experience often struggle to capture complex patterns and adapt to varying operational conditions. To address this, this study evaluates the performance of statistical, machine learning, and deep learning approaches for industrial inverter fault prediction, using operational data from a 90W-class inverter at an automotive parts manufacturer in Daegu, South Korea. The experimental results demonstrate that unsupervised anomaly detection models, particularly Autoencoder and SOM, achieved the highest accuracy. These findings suggest that models capable of detecting deviations from normal operating patterns are more effective for inverter fault prediction than conventional methods. In contrast, SVM and Logistic Regression exhibited limitations in handling time-series complexity. This study highlights the necessity of deploying real-time monitoring and predictive maintenance systems in industrial environments, with future research focusing on hyperparameter optimization and real-time data streaming validation.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202520454002736&target=NART&cn=JAKO202520454002736",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 산업용 인버터 고장예측을 위한 머신러닝 및 딥러닝 모델의 성능 평가 및 개선 연구 In industrial settings, inverters play a critical role in maintaining productivity and ensuring stable equipment operation. However, inverter failures can result in production downtime and increased maintenance costs. Traditional fault prediction methods based on physical models and expert experience often struggle to capture complex patterns and adapt to varying operational conditions. To address this, this study evaluates the performance of statistical, machine learning, and deep learning approaches for industrial inverter fault prediction, using operational data from a 90W-class inverter at an automotive parts manufacturer in Daegu, South Korea. The experimental results demonstrate that unsupervised anomaly detection models, particularly Autoencoder and SOM, achieved the highest accuracy. These findings suggest that models capable of detecting deviations from normal operating patterns are more effective for inverter fault prediction than conventional methods. In contrast, SVM and Logistic Regression exhibited limitations in handling time-series complexity. This study highlights the necessity of deploying real-time monitoring and predictive maintenance systems in industrial environments, with future research focusing on hyperparameter optimization and real-time data streaming validation."
        },
        {
          "rank": 49,
          "score": 0.6595978140830994,
          "doc_id": "JAKO200734515919504",
          "title": "지역 기반 분류기의 앙상블 학습",
          "abstract": "기계학습에서 분류기틀의 집합으로 구성된 앙상블 분류기는 단일 분류기에 비해 정확도가 높다는 것이 입증되어왔다. 본 논문에서는 새로운 앙상블 학습으로서 데이터의 지역 기반 분류기들의 앙상블 학습을 제시하여 기존의 앙상블 학습과의 비교를 통해 성능을 검증하고자 한다. 지역 기반 분류기의 앙상블 학습은 데이터의 분포가 지역에 따라 다르다는 점에 착안하여 학습 데이터를 분할하여 해당하는 지역에 기반을 둔 분류기들을 만들어 나간다. 이렇게 만들어진 분류기들로부터 지역에 따라 가중치를 둔 투표를 적용하여 앙상블 방법을 이끌어낸다. 본 논문에서 제시한 앙상블 분류기의 성능평가를 위해 단일 분류기와 기존의 앙상블 분류기인 배깅과 부스팅 등을 UCI Machine Learning Repository에 있는 11개의 데이터 셋으로 정확도 비교를 하였다. 그 결과 새로운 앙상블 방법이 기본 분류기로 나이브 베이즈와 SVM을 사용했을 때 다른 방법보다 좋은 성능을 보이는 것을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200734515919504&target=NART&cn=JAKO200734515919504",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "지역 기반 분류기의 앙상블 학습 지역 기반 분류기의 앙상블 학습 지역 기반 분류기의 앙상블 학습 기계학습에서 분류기틀의 집합으로 구성된 앙상블 분류기는 단일 분류기에 비해 정확도가 높다는 것이 입증되어왔다. 본 논문에서는 새로운 앙상블 학습으로서 데이터의 지역 기반 분류기들의 앙상블 학습을 제시하여 기존의 앙상블 학습과의 비교를 통해 성능을 검증하고자 한다. 지역 기반 분류기의 앙상블 학습은 데이터의 분포가 지역에 따라 다르다는 점에 착안하여 학습 데이터를 분할하여 해당하는 지역에 기반을 둔 분류기들을 만들어 나간다. 이렇게 만들어진 분류기들로부터 지역에 따라 가중치를 둔 투표를 적용하여 앙상블 방법을 이끌어낸다. 본 논문에서 제시한 앙상블 분류기의 성능평가를 위해 단일 분류기와 기존의 앙상블 분류기인 배깅과 부스팅 등을 UCI Machine Learning Repository에 있는 11개의 데이터 셋으로 정확도 비교를 하였다. 그 결과 새로운 앙상블 방법이 기본 분류기로 나이브 베이즈와 SVM을 사용했을 때 다른 방법보다 좋은 성능을 보이는 것을 알 수 있었다."
        },
        {
          "rank": 50,
          "score": 0.6593669056892395,
          "doc_id": "JAKO202116954704821",
          "title": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측",
          "abstract": "본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116954704821&target=NART&cn=JAKO202116954704821",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다."
        }
      ]
    },
    {
      "query": "DBN 기반 딥 러닝이 기존 SVM 방법에 비해 부도기업 예측 민감도 측면에서 어떤 향상 결과를 보였나요?",
      "query_meta": {
        "type": "single_hop",
        "index": 1
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8492860794067383,
          "doc_id": "DIKO0014169472",
          "title": "딥러닝 알고리즘에 기반한 기업부도 예측",
          "abstract": "기업의 부도는 국가경제에 막대한 손실을 입히며, 해당기업의 이해관계자들 모두에게 경제적 손실을 초래하고 사회적 부를 감소시킨다. 따라서 기업의 부도를 좀 더 정확하게 예측하는 것은 사회적·경제적 측면에서 매우 중요한 연구라 할 수 있다. &amp;#xD; 이에 최근 이미지 인식, 음성 인식, 자연어 처리 등 여러 분야에서 우수한 예측력을 보여주고 있는 딥러닝(Deep Learning)을 기업부도예측에 이용하고자 하며, 본 논문에서는 기업부도예측 방법으로 여러 딥러닝 알고리즘 중 DBN(Deep Belief Network)을 제안한다. 기존에 사용되던 분석기법 대비 우수성을 확인하기 위해 최근까지 기업부도예측에서 연구되고 있는 SVM(Support Vector Machine)과 비교하고자 하였으며, 1999년부터 2015년 사이에 국내 코스닥·코스피에 상장된 비금융업의 기업데이터를 이용하였다. 건실기업의 수는 1669개, 부도기업의 수는 495개이며, 한국은행의 기업경영분석에서 소개된 재무비율 변수를 이용하여 분석을 진행하였다. 분석결과 DBN이 SVM보다 여러 평가척도에서 더 좋은 성능을 보였다. 특히 시험데이터에 대해 부도기업을 부도기업으로 예측하는 민감도에서 5%이상의 더 뛰어난 성능을 보였으며, 이에 기업부도예측분야에 딥러닝의 적용가능성을 확인해 볼 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014169472&target=NART&cn=DIKO0014169472",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 알고리즘에 기반한 기업부도 예측 딥러닝 알고리즘에 기반한 기업부도 예측 딥러닝 알고리즘에 기반한 기업부도 예측 기업의 부도는 국가경제에 막대한 손실을 입히며, 해당기업의 이해관계자들 모두에게 경제적 손실을 초래하고 사회적 부를 감소시킨다. 따라서 기업의 부도를 좀 더 정확하게 예측하는 것은 사회적·경제적 측면에서 매우 중요한 연구라 할 수 있다. &amp;#xD; 이에 최근 이미지 인식, 음성 인식, 자연어 처리 등 여러 분야에서 우수한 예측력을 보여주고 있는 딥러닝(Deep Learning)을 기업부도예측에 이용하고자 하며, 본 논문에서는 기업부도예측 방법으로 여러 딥러닝 알고리즘 중 DBN(Deep Belief Network)을 제안한다. 기존에 사용되던 분석기법 대비 우수성을 확인하기 위해 최근까지 기업부도예측에서 연구되고 있는 SVM(Support Vector Machine)과 비교하고자 하였으며, 1999년부터 2015년 사이에 국내 코스닥·코스피에 상장된 비금융업의 기업데이터를 이용하였다. 건실기업의 수는 1669개, 부도기업의 수는 495개이며, 한국은행의 기업경영분석에서 소개된 재무비율 변수를 이용하여 분석을 진행하였다. 분석결과 DBN이 SVM보다 여러 평가척도에서 더 좋은 성능을 보였다. 특히 시험데이터에 대해 부도기업을 부도기업으로 예측하는 민감도에서 5%이상의 더 뛰어난 성능을 보였으며, 이에 기업부도예측분야에 딥러닝의 적용가능성을 확인해 볼 수 있었다."
        },
        {
          "rank": 2,
          "score": 0.7583019733428955,
          "doc_id": "JAKO201614137727823",
          "title": "딥러닝 기법을 이용한 내일강수 예측",
          "abstract": "정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로 기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본 논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도 사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의 AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를 사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로 사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의 척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201614137727823&target=NART&cn=JAKO201614137727823",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기법을 이용한 내일강수 예측 딥러닝 기법을 이용한 내일강수 예측 딥러닝 기법을 이용한 내일강수 예측 정확한 강수예측을 위해서는 예측인자 선정과 예측방법에 대한 선택이 매우 중요하다. 최근에는 강수예측 방법으로 기계학습 기법이 많이 사용되고 있으며, 그 중에서도 특히 인공신경망을 사용한 강수예측 방법은 좋은 성능을 보였다. 본 논문에서는 딥러닝 기법 중 하나인 DBN(deep belief network)를 이용한 새로운 강수예측 방법을 제안한다. DBN는 비지도 사전 학습을 통해 초기 가중치를 설정하여 기존 인공신경망의 문제점을 보완한다. 예측인자로는 기온, 전일-전주 강수일, 태양과 달 궤도 관련 자료를 선정하였다. 기온과 전일-전주 강수일은 서울에서의 1974년부터 2013년까지 총 40년간의 AWS(automatic weather system) 관측 자료를 사용하였고, 태양과 달의 궤도 관련 자료는 서울을 중심으로 계산한 결과를 사용하였다. 전체 기간에서 일부는 학습 자료로 사용하여 예측모델을 생성하였고, 나머지를 생성한 모델의 검증 자료로 사용하였다. 모델 검증 결과로 나온 예측값들은 확률값을 가지며 임계치를 이용하여 강수유무를 판별하였다. 강수 정확도의 척도로 양분예보기법 중 CSI(critical successive index)와 Bias(frequency bias)를 계산하였다. 이를 통해 DBN와 MLP(multilayer perceptron)의 성능을 비교한 결과 DBN의 강수 예측 정확도가 높았고, 수행속도 또한 2배 이상 빨랐다."
        },
        {
          "rank": 3,
          "score": 0.7281583547592163,
          "doc_id": "JAKO201403359939237",
          "title": "개선된 배깅 앙상블을 활용한 기업부도예측",
          "abstract": "기업의 부도 예측은 재무 및 회계 분야에서 매우 중요한 연구 주제이다. 기업의 부도로 인해 발생하는 비용이 매우 크기 때문에 부도 예측의 정확성은 금융기관으로서는 매우 중요한 일이다. 최근에는 여러 개의 모형을 결합하는 앙상블 모형을 부도 예측에 적용해 보려는 연구가 큰 관심을 끌고 있다. 앙상블 모형은 개별 모형보다 더 좋은 성과를 내기 위해 여러 개의 분류기를 결합하는 것이다. 이와 같은 앙상블 분류기는 분류기의 일반화 성능을 개선하는 데 매우 유용한 것으로 알려져 있다. 본 논문은 부도 예측 모형의 성과 개선에 관한 연구이다. 이를 위해 사례 선택(Instance Selection)을 활용한 배깅(Bagging) 모형을 제안하였다. 사례 선택은 원 데이터에서 가장 대표성 있고 관련성 높은 데이터를 선택하고 예측 모형에 악영향을 줄 수 있는 불필요한 데이터를 제거하는 것으로 이를 통해 예측 성과 개선도 기대할 수 있다. 배깅은 학습데이터에 변화를 줌으로써 기저 분류기들을 다양화시키는 앙상블 기법으로 단순하면서도 성과가 매우 좋은 것으로 알려져 있다. 사례 선택과 배깅은 각각 모형의 성과를 개선시킬 수 있는 잠재력이 있지만 이들 두 기법의 결합에 관한 연구는 아직까지 없는 것이 현실이다. 본 연구에서는 부도 예측 모형의 성과를 개선하기 위해 사례 선택과 배깅을 연결하는 새로운 모형을 제안하였다. 최적의 사례 선택을 위해 유전자 알고리즘이 사용되었으며, 이를 통해 최적의 사례 선택 조합을 찾고 이 결과를 배깅 앙상블 모형에 전달하여 새로운 형태의 배깅 앙상블 모형을 구성하게 된다. 본 연구에서 제안한 새로운 앙상블 모형의 성과를 검증하기 위해 ROC 커브, AUC, 예측정확도 등과 같은 성과지표를 사용해 다양한 모형과 비교 분석해 보았다. 실제 기업데이터를 사용해 실험한 결과 본 논문에서 제안한 새로운 형태의 모형이 가장 좋은 성과를 보임을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201403359939237&target=NART&cn=JAKO201403359939237",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "개선된 배깅 앙상블을 활용한 기업부도예측 개선된 배깅 앙상블을 활용한 기업부도예측 개선된 배깅 앙상블을 활용한 기업부도예측 기업의 부도 예측은 재무 및 회계 분야에서 매우 중요한 연구 주제이다. 기업의 부도로 인해 발생하는 비용이 매우 크기 때문에 부도 예측의 정확성은 금융기관으로서는 매우 중요한 일이다. 최근에는 여러 개의 모형을 결합하는 앙상블 모형을 부도 예측에 적용해 보려는 연구가 큰 관심을 끌고 있다. 앙상블 모형은 개별 모형보다 더 좋은 성과를 내기 위해 여러 개의 분류기를 결합하는 것이다. 이와 같은 앙상블 분류기는 분류기의 일반화 성능을 개선하는 데 매우 유용한 것으로 알려져 있다. 본 논문은 부도 예측 모형의 성과 개선에 관한 연구이다. 이를 위해 사례 선택(Instance Selection)을 활용한 배깅(Bagging) 모형을 제안하였다. 사례 선택은 원 데이터에서 가장 대표성 있고 관련성 높은 데이터를 선택하고 예측 모형에 악영향을 줄 수 있는 불필요한 데이터를 제거하는 것으로 이를 통해 예측 성과 개선도 기대할 수 있다. 배깅은 학습데이터에 변화를 줌으로써 기저 분류기들을 다양화시키는 앙상블 기법으로 단순하면서도 성과가 매우 좋은 것으로 알려져 있다. 사례 선택과 배깅은 각각 모형의 성과를 개선시킬 수 있는 잠재력이 있지만 이들 두 기법의 결합에 관한 연구는 아직까지 없는 것이 현실이다. 본 연구에서는 부도 예측 모형의 성과를 개선하기 위해 사례 선택과 배깅을 연결하는 새로운 모형을 제안하였다. 최적의 사례 선택을 위해 유전자 알고리즘이 사용되었으며, 이를 통해 최적의 사례 선택 조합을 찾고 이 결과를 배깅 앙상블 모형에 전달하여 새로운 형태의 배깅 앙상블 모형을 구성하게 된다. 본 연구에서 제안한 새로운 앙상블 모형의 성과를 검증하기 위해 ROC 커브, AUC, 예측정확도 등과 같은 성과지표를 사용해 다양한 모형과 비교 분석해 보았다. 실제 기업데이터를 사용해 실험한 결과 본 논문에서 제안한 새로운 형태의 모형이 가장 좋은 성과를 보임을 알 수 있었다."
        },
        {
          "rank": 4,
          "score": 0.7208594083786011,
          "doc_id": "JAKO201336161064414",
          "title": "앙상블 SVM 모형을 이용한 기업 부도 예측",
          "abstract": "기업의 부도를 예측하는 것은 회계나 재무 분야에서 중요한 연구주제이다. 지금까지 기업 부도예측을 위해 여러 가지 데이터마이닝 기법들이 적용되었으나 주로 단일 모형을 사용함으로서 복잡한 분류 문제에의 적용에 한계를 갖고 있었다. 본 논문에서는 최근에 각광받고 있는 SVM (support vector machine) 모형들을 결합한 앙상블 SVM 모형 (ensemble SVM model)을 부도예측에 사용하고자 한다. 제안된 앙상블 모형은 v-조각 교차 타당성 (v-fold cross-validation)에 의해 얻어진 여러 가지 모형 중에서 성능이 좋은 상위 k개의 단일 모형으로 구성하고 과반수 투표 방식 (majority voting)을 사용하여 미지의 클래스를 분류한다. 본 논문에서 제안된 앙상블 SVM 모형의 성능을 평가하기 위해 실제 기업의 재무비율 자료와 모의실험자료를 가지고 실험하였고, 실험결과 제안된 앙상블 모형이 여러 가지 평가척도 하에서 단일 SVM 모형들보다 좋은 성능을 보임을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201336161064414&target=NART&cn=JAKO201336161064414",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "앙상블 SVM 모형을 이용한 기업 부도 예측 앙상블 SVM 모형을 이용한 기업 부도 예측 앙상블 SVM 모형을 이용한 기업 부도 예측 기업의 부도를 예측하는 것은 회계나 재무 분야에서 중요한 연구주제이다. 지금까지 기업 부도예측을 위해 여러 가지 데이터마이닝 기법들이 적용되었으나 주로 단일 모형을 사용함으로서 복잡한 분류 문제에의 적용에 한계를 갖고 있었다. 본 논문에서는 최근에 각광받고 있는 SVM (support vector machine) 모형들을 결합한 앙상블 SVM 모형 (ensemble SVM model)을 부도예측에 사용하고자 한다. 제안된 앙상블 모형은 v-조각 교차 타당성 (v-fold cross-validation)에 의해 얻어진 여러 가지 모형 중에서 성능이 좋은 상위 k개의 단일 모형으로 구성하고 과반수 투표 방식 (majority voting)을 사용하여 미지의 클래스를 분류한다. 본 논문에서 제안된 앙상블 SVM 모형의 성능을 평가하기 위해 실제 기업의 재무비율 자료와 모의실험자료를 가지고 실험하였고, 실험결과 제안된 앙상블 모형이 여러 가지 평가척도 하에서 단일 SVM 모형들보다 좋은 성능을 보임을 알 수 있었다."
        },
        {
          "rank": 5,
          "score": 0.7118887901306152,
          "doc_id": "DIKO0015893049",
          "title": "도메인 적대적 신경망을 이용한 종단 간 억양음성인식",
          "abstract": "최근 딥러닝(Deep learning) 기술의 발전은 음성인식 성능 향상에 크게 기여하였다. 이러한 발전에도 불구하고 소음, 감정, 억양 등이 섞인 특정 발화에 대해서는 좋은 성능을 보이지 못하고 있다. 이 가운데 억양이 섞인 발화는 표준 발화와 비교했을 때 언어학적인 차이가 존재하는데, 이러한 차이가 억양이 섞인 발화를 인식하기 어렵게 만든다. 따라서 본 연구에서는 억양이 섞인 발화와 표준 발화 사이에 존재하는 특성의 차이를 줄이고자 도메인 적대적 신경망(Domain Adversarial Neural Network) 기법을 사용하였다. 또한, 종단 간(End-to-end) 기법을 사용하여 음성인식의 과정을 간소화하였다.&amp;#xD; 오래전부터 억양음성인식의 성능을 높이기 위한 연구는 활발히 진행되어 왔다. 2010년대 초반까지는 가우시안 혼합 모델(Gaussian Mixture Model) 기반의 최대 사후 확률(Maximum A Posteriori), 최대 우도 선형 회귀(Maximum Likelihood Linear Regression) 적응 기법이 주로 사용되었다. 하지만 딥러닝 기술이 발전하고 신경망 기반의 모델들이 주목 받기 시작하면서 가우시안 혼합 모델보다는 신경망 모델에 적합한 기법들이 사용되었다. 최근 몇 년간은 음성인식 모델에 억양에 대한 정보를 직접 삽입하는 accent embedding 기법이 많이 사용되었다. Accent embedding 기법은 억양음성인식의 성능을 향상시켰지만 몇 가지 문제점을 가지고 있다. 첫째, 억양을 분류하여 accent embedding 특징들을 만들어내는 모델을 독립적으로 만들어 훈련시켜야 하며, 해당 모델의 억양 분류 정확도가 음성인식 모델의 성능에 큰 영향을 미치기 때문에 모델을 정교하게 만들어야 하는 부담감이 있다. &amp;#xD; 둘째, 억양 분류 모델의 결과를 음성인식 모델의 추가적인 입력 특징(Input feature)으로 사용하기 때문에 음성인식 모델의 매개변수를 증가시키며 계산량 또한 증가한다는 문제가 있다. 따라서 본 연구에서는 추가적인 입력 특징이 필요하지 않고 음성인식에서 기본적으로 사용되는 특징인 주파수 정보(스펙트로그램)만을 이용하여 학습이 가능한 도메인 적대적 신경망을 기법을 제안하였다. &amp;#xD; 도메인 적대적 신경망은 소스 도메인(Source domain) 데이터와 타겟 도메인(Target domain) 데이터가 적대적으로 학습이 되면서 두 도메인 간의 분포 차이를 줄이는 것을 목적으로 한다. 본 연구의 목표인 억양음성인식에서는 표준 발화를 소스 도메인으로, 억양이 섞인 발화를 타겟 도메인으로 정하였다. 도메인 적대적 신경망은 특징 추출기(Feature extractor), 도메인 분류기(Domain classifier), 레이블 예측기(Label predictor) 총 3개의 부분망(sub-network)으로 구성된다. 각각의 부분망은 서로 다른 역할을 수행하기 때문에 신경망의 특성을 고려하여 만들어야 한다. 따라서 본 연구에서는 신경망의 특성을 고려하여 특징 추출기에는 합성곱 신경망(Convolutional Neural Network)을, 도메인 분류기에는 심층 신경망(Deep Neural Network)을, 그리고 레이블 예측기에는 양방향 게이트 순환 유닛(Bidirectional Gated Recurrent Unit)을 이용하여 도메인 적대적 신경망을 구성하였다. 또한, 레이블을 예측할 때 종단 간 기법을 활용하여 입력 데이터를 사전 분할하지 않고, 레이블 예측 이후의 후처리 작업을 없애면서 음성인식 과정을 간소화하였다.&amp;#xD; 본 연구에서 제안한 도메인 적대적 학습 기반의 억양음성인식 기법의 효과를 입증하기 위하여 Baseline 모델과 DANN 모델을 만들어 실험을 진행하였다. 실험 데이터로는 Mozilla의 Common Voice 코퍼스를 사용하였는데, Common Voice 코퍼스는 여러 언어에 대해 막대한 양의 검증된 음성파일을 오픈소스로 제공하기 때문에 음성인식 연구에서 많이 사용된다. 또한, Common Voice 코퍼스는 음성 녹음 파일과 함께 억양 정보도 같이 제공을 하기 때문에 억양음성인식 연구에 효율적으로 사용될 수 있다. &amp;#xD; Common Voice 코퍼스의 영어 데이터셋은 여러 억양의 음성파일들을 가지고 있는데, 본 연구에서는 미국 억양, 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양의 데이터를 실험에 사용하였으며, 미국 억양을 소스 도메인으로 나머지 네 개의 억양을 타겟 도메인으로 정하였다.&amp;#xD; 실험 결과 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양 모두에서 DANN 모델의 성능이 Baseline 모델보다 높은 성능을 보였다. 하지만 억양에 따라 성능 개선의 차이가 있었으며, 캐나다 억양에 비해 잉글랜드 억양과 인도 억양에서 성능이 눈에 띄게 향상되었다. 이 같은 결과는 잉글랜드 억양과 인도 억양이 소스 도메인으로 사용된 미국 억양 데이터와 언어학적으로 큰 차이가 존재하여 baseline 모델에서는 성능이 낮았으나, 도메인 적대적 학습을 통해 생성된 DANN 모델이 타겟 억양의 특성을 반영함으로써 성능이 크게 개선된 것으로 분석된다. 따라서 도메인 적대적 신경망은 소스 도메인과 타겟 도메인 사이의 분포의 차이를 줄임으로서 억양음성인식의 성능을 향상시킬 수 있음이 확인되었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015893049&target=NART&cn=DIKO0015893049",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "도메인 적대적 신경망을 이용한 종단 간 억양음성인식 도메인 적대적 신경망을 이용한 종단 간 억양음성인식 도메인 적대적 신경망을 이용한 종단 간 억양음성인식 최근 딥러닝(Deep learning) 기술의 발전은 음성인식 성능 향상에 크게 기여하였다. 이러한 발전에도 불구하고 소음, 감정, 억양 등이 섞인 특정 발화에 대해서는 좋은 성능을 보이지 못하고 있다. 이 가운데 억양이 섞인 발화는 표준 발화와 비교했을 때 언어학적인 차이가 존재하는데, 이러한 차이가 억양이 섞인 발화를 인식하기 어렵게 만든다. 따라서 본 연구에서는 억양이 섞인 발화와 표준 발화 사이에 존재하는 특성의 차이를 줄이고자 도메인 적대적 신경망(Domain Adversarial Neural Network) 기법을 사용하였다. 또한, 종단 간(End-to-end) 기법을 사용하여 음성인식의 과정을 간소화하였다.&amp;#xD; 오래전부터 억양음성인식의 성능을 높이기 위한 연구는 활발히 진행되어 왔다. 2010년대 초반까지는 가우시안 혼합 모델(Gaussian Mixture Model) 기반의 최대 사후 확률(Maximum A Posteriori), 최대 우도 선형 회귀(Maximum Likelihood Linear Regression) 적응 기법이 주로 사용되었다. 하지만 딥러닝 기술이 발전하고 신경망 기반의 모델들이 주목 받기 시작하면서 가우시안 혼합 모델보다는 신경망 모델에 적합한 기법들이 사용되었다. 최근 몇 년간은 음성인식 모델에 억양에 대한 정보를 직접 삽입하는 accent embedding 기법이 많이 사용되었다. Accent embedding 기법은 억양음성인식의 성능을 향상시켰지만 몇 가지 문제점을 가지고 있다. 첫째, 억양을 분류하여 accent embedding 특징들을 만들어내는 모델을 독립적으로 만들어 훈련시켜야 하며, 해당 모델의 억양 분류 정확도가 음성인식 모델의 성능에 큰 영향을 미치기 때문에 모델을 정교하게 만들어야 하는 부담감이 있다. &amp;#xD; 둘째, 억양 분류 모델의 결과를 음성인식 모델의 추가적인 입력 특징(Input feature)으로 사용하기 때문에 음성인식 모델의 매개변수를 증가시키며 계산량 또한 증가한다는 문제가 있다. 따라서 본 연구에서는 추가적인 입력 특징이 필요하지 않고 음성인식에서 기본적으로 사용되는 특징인 주파수 정보(스펙트로그램)만을 이용하여 학습이 가능한 도메인 적대적 신경망을 기법을 제안하였다. &amp;#xD; 도메인 적대적 신경망은 소스 도메인(Source domain) 데이터와 타겟 도메인(Target domain) 데이터가 적대적으로 학습이 되면서 두 도메인 간의 분포 차이를 줄이는 것을 목적으로 한다. 본 연구의 목표인 억양음성인식에서는 표준 발화를 소스 도메인으로, 억양이 섞인 발화를 타겟 도메인으로 정하였다. 도메인 적대적 신경망은 특징 추출기(Feature extractor), 도메인 분류기(Domain classifier), 레이블 예측기(Label predictor) 총 3개의 부분망(sub-network)으로 구성된다. 각각의 부분망은 서로 다른 역할을 수행하기 때문에 신경망의 특성을 고려하여 만들어야 한다. 따라서 본 연구에서는 신경망의 특성을 고려하여 특징 추출기에는 합성곱 신경망(Convolutional Neural Network)을, 도메인 분류기에는 심층 신경망(Deep Neural Network)을, 그리고 레이블 예측기에는 양방향 게이트 순환 유닛(Bidirectional Gated Recurrent Unit)을 이용하여 도메인 적대적 신경망을 구성하였다. 또한, 레이블을 예측할 때 종단 간 기법을 활용하여 입력 데이터를 사전 분할하지 않고, 레이블 예측 이후의 후처리 작업을 없애면서 음성인식 과정을 간소화하였다.&amp;#xD; 본 연구에서 제안한 도메인 적대적 학습 기반의 억양음성인식 기법의 효과를 입증하기 위하여 Baseline 모델과 DANN 모델을 만들어 실험을 진행하였다. 실험 데이터로는 Mozilla의 Common Voice 코퍼스를 사용하였는데, Common Voice 코퍼스는 여러 언어에 대해 막대한 양의 검증된 음성파일을 오픈소스로 제공하기 때문에 음성인식 연구에서 많이 사용된다. 또한, Common Voice 코퍼스는 음성 녹음 파일과 함께 억양 정보도 같이 제공을 하기 때문에 억양음성인식 연구에 효율적으로 사용될 수 있다. &amp;#xD; Common Voice 코퍼스의 영어 데이터셋은 여러 억양의 음성파일들을 가지고 있는데, 본 연구에서는 미국 억양, 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양의 데이터를 실험에 사용하였으며, 미국 억양을 소스 도메인으로 나머지 네 개의 억양을 타겟 도메인으로 정하였다.&amp;#xD; 실험 결과 호주 억양, 캐나다 억양, 잉글랜드 억양, 인도 억양 모두에서 DANN 모델의 성능이 Baseline 모델보다 높은 성능을 보였다. 하지만 억양에 따라 성능 개선의 차이가 있었으며, 캐나다 억양에 비해 잉글랜드 억양과 인도 억양에서 성능이 눈에 띄게 향상되었다. 이 같은 결과는 잉글랜드 억양과 인도 억양이 소스 도메인으로 사용된 미국 억양 데이터와 언어학적으로 큰 차이가 존재하여 baseline 모델에서는 성능이 낮았으나, 도메인 적대적 학습을 통해 생성된 DANN 모델이 타겟 억양의 특성을 반영함으로써 성능이 크게 개선된 것으로 분석된다. 따라서 도메인 적대적 신경망은 소스 도메인과 타겟 도메인 사이의 분포의 차이를 줄임으로서 억양음성인식의 성능을 향상시킬 수 있음이 확인되었다."
        },
        {
          "rank": 6,
          "score": 0.7095520496368408,
          "doc_id": "JAKO202305062334676",
          "title": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측",
          "abstract": "본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202305062334676&target=NART&cn=JAKO202305062334676",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 딥러닝 모델을 이용한 전자 입찰에서의 예정가격 예측 본 논문은 입찰사이트 전기넷과 OK EMS에서 입수한 입찰데이터로 DNBP(Deep learning Network to predict Budget Price) 모델을 통해 예정가격을 예측한다. 우리는 DNBP 모델을 활용하여 4개의 추첨예비가격을 예측을 하고, 이를 산술평균 한 뒤 예정가격 사정률을 계산하여, 실제 예정가격 사정률과 비교하여 모델의 성능을 평가한다. DNBP의 15개의 입력노드 중 일부 입력노드를 제거하여 모델을 학습시켰다. 예측 결과 예측 결과 입력노드가 6개(a, g, h, i, j, k) 일 때 DNBP의 RMSE가 0.75788% 로 가장 낮았다."
        },
        {
          "rank": 7,
          "score": 0.7084358930587769,
          "doc_id": "ART001692415",
          "title": "SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상",
          "abstract": "기업 도산은 다양한 이해관계자에게 사회 경제적으로 큰 손실을 주게 된다. 따라서 기업 도산 및 부실화를 사전에 예측할 수 있다면 이에 대한 대비를 하거나 기업 부도 요인을 사전 제거함에 따라 기업 도산에 대한 손실을 최소화하는 것이 가능할 것이다. 기업의 도산을 예측하기 위한 다양한 통계적 모형 및 데이터 마이닝 모형이 제시되고 있다. 기업 도산 예측 모형의 주요 이슈 중 하나는 예측력을 높이는 것이다. 이 논문은 기업 도산예측에 주로 사용되며 예측성능이 비교적 높은 로짓모형, 의사결정나무모형, 신경망모형, SVM모형에 대해서 살펴본다. 표본기업은 코스닥기업 중 도산 및 정상기업으로 각각 49개 기업을 선정하였고, 설명변수로는 통계적으로 유의미한 9개의 재무변수를 이용하고, 5년간의 재무변수 자료를 사용하였다. SVM모형을 전처리기를 사용한 경우 다양한 분류모형의 도산예측 성능을 크게 높일 수 있음을 실험결과로 보이고 있다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART001692415&target=NART&cn=ART001692415",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상 SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상 SVM 전처리기를 활용한 코스닥기업 도산예측 성능 향상 기업 도산은 다양한 이해관계자에게 사회 경제적으로 큰 손실을 주게 된다. 따라서 기업 도산 및 부실화를 사전에 예측할 수 있다면 이에 대한 대비를 하거나 기업 부도 요인을 사전 제거함에 따라 기업 도산에 대한 손실을 최소화하는 것이 가능할 것이다. 기업의 도산을 예측하기 위한 다양한 통계적 모형 및 데이터 마이닝 모형이 제시되고 있다. 기업 도산 예측 모형의 주요 이슈 중 하나는 예측력을 높이는 것이다. 이 논문은 기업 도산예측에 주로 사용되며 예측성능이 비교적 높은 로짓모형, 의사결정나무모형, 신경망모형, SVM모형에 대해서 살펴본다. 표본기업은 코스닥기업 중 도산 및 정상기업으로 각각 49개 기업을 선정하였고, 설명변수로는 통계적으로 유의미한 9개의 재무변수를 이용하고, 5년간의 재무변수 자료를 사용하였다. SVM모형을 전처리기를 사용한 경우 다양한 분류모형의 도산예측 성능을 크게 높일 수 있음을 실험결과로 보이고 있다."
        },
        {
          "rank": 8,
          "score": 0.7080678343772888,
          "doc_id": "JAKO201510534325002",
          "title": "퍼지이론과 SVM 결합을 통한 기업부도예측 최적화",
          "abstract": "기업부도예측은 재무 분야에 있어 중요한 연구주제 중 하나로 1960년대 이후부터 꾸준히 연구되어져 왔다. 국내의 경우, IMF 사태 이후 기업부도예측에 관한 중요성이 강조되고 있다. 이에 본 연구에서는 보다 정확한 기업부도예측을 위해 높은 예측력과 동시에 과적합화의 문제를 해결한다고 알려진 SVM(Support Vector Machine)을 기반으로 퍼지이론(fuzzy theory)을 활용해 입력변수를 확장하고, 유전자 알고리즘(GA, Genetic Algorithm)을 이용해 유사 혹은 유사최적의 입력변수집합과 파라미터를 탐색하는 새로운 융합모형을 제시한다. 제안모형의 유용성을 검증하기 위하여 H은행의 비외감 중공업 기업 데이터를 이용하여 실험을 수행하였으며, 비교모형으로는 로짓분석, 판별분석, 의사결정나무, 사례기반추론, 인공신경망, SVM을 선정하였다. 실험결과, 제안모형이 모든 비교모형들에 비해 우수한 예측력을 보이는 것으로 나타났다. 본 연구는 우수한 예측 성능을 가진 다기법 융합 모형을 새롭게 제안하여, 부도예측 분야에 학술적, 실무적으로 기여할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201510534325002&target=NART&cn=JAKO201510534325002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "퍼지이론과 SVM 결합을 통한 기업부도예측 최적화 퍼지이론과 SVM 결합을 통한 기업부도예측 최적화 퍼지이론과 SVM 결합을 통한 기업부도예측 최적화 기업부도예측은 재무 분야에 있어 중요한 연구주제 중 하나로 1960년대 이후부터 꾸준히 연구되어져 왔다. 국내의 경우, IMF 사태 이후 기업부도예측에 관한 중요성이 강조되고 있다. 이에 본 연구에서는 보다 정확한 기업부도예측을 위해 높은 예측력과 동시에 과적합화의 문제를 해결한다고 알려진 SVM(Support Vector Machine)을 기반으로 퍼지이론(fuzzy theory)을 활용해 입력변수를 확장하고, 유전자 알고리즘(GA, Genetic Algorithm)을 이용해 유사 혹은 유사최적의 입력변수집합과 파라미터를 탐색하는 새로운 융합모형을 제시한다. 제안모형의 유용성을 검증하기 위하여 H은행의 비외감 중공업 기업 데이터를 이용하여 실험을 수행하였으며, 비교모형으로는 로짓분석, 판별분석, 의사결정나무, 사례기반추론, 인공신경망, SVM을 선정하였다. 실험결과, 제안모형이 모든 비교모형들에 비해 우수한 예측력을 보이는 것으로 나타났다. 본 연구는 우수한 예측 성능을 가진 다기법 융합 모형을 새롭게 제안하여, 부도예측 분야에 학술적, 실무적으로 기여할 수 있을 것으로 기대된다."
        },
        {
          "rank": 9,
          "score": 0.7075458765029907,
          "doc_id": "DIKO0013372384",
          "title": "앙상블 SVM을 이용한 기업 부도 예측 모형",
          "abstract": "Bankruptcy prediction has been an important topic in the accounting and finance field for a long time. Several data mining techniques have been used for bankruptcy prediction. However, there are many limits for application to real classification problem with a single model. This study proposes ensemble SVM (support vector machine) model which assembles different SVM models with each different kernel functions. Our ensemble model is made and evaluated by v-fold cross-validation approach. The top performing models are recruited into the ensemble. The classification is then carried out using the majority voting opinion of the ensemble. The performance of the ensemble SVM classifier is investigated in terms of accuracy, error rate, sensitivity, specificity, ROC curve, and AUC to compare with single SVM classifiers based on two financial ratios datasets and simulation datasets. The results confirmed the advantages of our method: It is being robust while providing good performance.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0013372384&target=NART&cn=DIKO0013372384",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "앙상블 SVM을 이용한 기업 부도 예측 모형 앙상블 SVM을 이용한 기업 부도 예측 모형 앙상블 SVM을 이용한 기업 부도 예측 모형 Bankruptcy prediction has been an important topic in the accounting and finance field for a long time. Several data mining techniques have been used for bankruptcy prediction. However, there are many limits for application to real classification problem with a single model. This study proposes ensemble SVM (support vector machine) model which assembles different SVM models with each different kernel functions. Our ensemble model is made and evaluated by v-fold cross-validation approach. The top performing models are recruited into the ensemble. The classification is then carried out using the majority voting opinion of the ensemble. The performance of the ensemble SVM classifier is investigated in terms of accuracy, error rate, sensitivity, specificity, ROC curve, and AUC to compare with single SVM classifiers based on two financial ratios datasets and simulation datasets. The results confirmed the advantages of our method: It is being robust while providing good performance."
        },
        {
          "rank": 10,
          "score": 0.7070445418357849,
          "doc_id": "JAKO201208438434752",
          "title": "부도 예측을 위한 앙상블 분류기 개발",
          "abstract": "분류기의 앙상블 학습은 여러 개의 서로 다른 분류기들의 조합을 통해 만들어진다. 앙상블 학습은 기계학습 분야에서 많은 관심을 끌고 있는 중요한 연구주제이며 대부분의 경우에 있어서 앙상블 모형은 개별 기저 분류기보다 더 좋은 성과를 내는 것으로 알려져 있다. 본 연구는 부도 예측 모형의 성능개선에 관한 연구이다. 이를 위해 본 연구에서는 단일 모형으로 그 우수성을 인정받고 있는 SVM을 기저 분류기로 사용하는 앙상블 모형에 대해 고찰하였다. SVM 모형의 성능 개선을 위해 bagging과 random subspace 모형을 부도 예측 문제에 적용해 보았으며 bagging 모형과 random subspace 모형의 성과 개선을 위해 bagging과 random subspace의 통합 모형을 제안하였다. 제안한 모형의 성과를 검증하기 위해 실제 기업의 부도 예측 데이터를 사용하여 실험하였고, 실험 결과 본 연구에서 제안한 새로운 형태의 통합 모형이 가장 좋은 성과를 보임을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201208438434752&target=NART&cn=JAKO201208438434752",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "부도 예측을 위한 앙상블 분류기 개발 부도 예측을 위한 앙상블 분류기 개발 부도 예측을 위한 앙상블 분류기 개발 분류기의 앙상블 학습은 여러 개의 서로 다른 분류기들의 조합을 통해 만들어진다. 앙상블 학습은 기계학습 분야에서 많은 관심을 끌고 있는 중요한 연구주제이며 대부분의 경우에 있어서 앙상블 모형은 개별 기저 분류기보다 더 좋은 성과를 내는 것으로 알려져 있다. 본 연구는 부도 예측 모형의 성능개선에 관한 연구이다. 이를 위해 본 연구에서는 단일 모형으로 그 우수성을 인정받고 있는 SVM을 기저 분류기로 사용하는 앙상블 모형에 대해 고찰하였다. SVM 모형의 성능 개선을 위해 bagging과 random subspace 모형을 부도 예측 문제에 적용해 보았으며 bagging 모형과 random subspace 모형의 성과 개선을 위해 bagging과 random subspace의 통합 모형을 제안하였다. 제안한 모형의 성과를 검증하기 위해 실제 기업의 부도 예측 데이터를 사용하여 실험하였고, 실험 결과 본 연구에서 제안한 새로운 형태의 통합 모형이 가장 좋은 성과를 보임을 알 수 있었다."
        },
        {
          "rank": 11,
          "score": 0.7054404020309448,
          "doc_id": "ATN0031726879",
          "title": "딥러닝 기반 부실기업 예측모형에 관한 연구",
          "abstract": "Predicting insolvent companies is a research topic that has been important in accounting and finance. Especially, due to the rapidly changing business environments and the recent COVID-19 pandemic, many domestic companies are facing financial adversity. Thus, the necessity of research on corporate insolvency is being emphasized. As a related research, there is a prediction of corporate bankruptcy, however, a bankrupt company is the company whose business activities have been suspended, and there is a limitation in which it is inappropriate to determine which companies show signs of bankruptcy among continuing companies. Therefore, marginal company, one of the categories of insolvent companies, is selected as the prediction target. Marginal companies are the firms that are operating income interest compensation ratio are less than 1 for three consecutive years, and are engaged in business activities but have not consistently secured adequate profits. In this study, deep learning techniques are used to predict them. It is one of the machine learning techniques that has recently attracted attention because of its excellence in various fields. Nonetheless, has not been applied in research to predict marginal companies. This study applies RNN and CNN among deep learning techniques using several financial ratios as independent variables. Their performance are compared with machine learning ensemble models that have been reported to have excellent predictive power in previous studies. As a result of analysis on corporate data from 2017 to 2019 as training and test data, deep learning models such as RNN-LSTM, RNN-GRU, and CNN are better in forecasting of marginal companies than the ensemble models in terms of Recall score. Therefore, the deep learning models are expected to become widely used in the prediction of marginal companies in the future.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0031726879&target=NART&cn=ATN0031726879",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 부실기업 예측모형에 관한 연구 딥러닝 기반 부실기업 예측모형에 관한 연구 딥러닝 기반 부실기업 예측모형에 관한 연구 Predicting insolvent companies is a research topic that has been important in accounting and finance. Especially, due to the rapidly changing business environments and the recent COVID-19 pandemic, many domestic companies are facing financial adversity. Thus, the necessity of research on corporate insolvency is being emphasized. As a related research, there is a prediction of corporate bankruptcy, however, a bankrupt company is the company whose business activities have been suspended, and there is a limitation in which it is inappropriate to determine which companies show signs of bankruptcy among continuing companies. Therefore, marginal company, one of the categories of insolvent companies, is selected as the prediction target. Marginal companies are the firms that are operating income interest compensation ratio are less than 1 for three consecutive years, and are engaged in business activities but have not consistently secured adequate profits. In this study, deep learning techniques are used to predict them. It is one of the machine learning techniques that has recently attracted attention because of its excellence in various fields. Nonetheless, has not been applied in research to predict marginal companies. This study applies RNN and CNN among deep learning techniques using several financial ratios as independent variables. Their performance are compared with machine learning ensemble models that have been reported to have excellent predictive power in previous studies. As a result of analysis on corporate data from 2017 to 2019 as training and test data, deep learning models such as RNN-LSTM, RNN-GRU, and CNN are better in forecasting of marginal companies than the ensemble models in terms of Recall score. Therefore, the deep learning models are expected to become widely used in the prediction of marginal companies in the future."
        },
        {
          "rank": 12,
          "score": 0.7020764946937561,
          "doc_id": "ATN0050865232",
          "title": "XGBoost 머신러닝 기반 쉴드 TBM 지반침하 예측",
          "abstract": "본 연구에서는 도심지 쉴드 TBM (tunnel boring machine) 터널 시공 중 발생하는 지반침하를 예측하기 위한 XGBoost (eXtreme Gradient Boosting) 머신러닝 모델을 개발하고, 그 성능을 평가하였다. 기존 연구들에서 주로 터널 후방의 침하를 예측하는 연구가 많았던 반면, 본 연구에서는 실시간 쉴드 TBM 시공 데이터를 활용하여 터널의 후방 침하뿐 아니라 전방 침하에 대한 예측도 시도하였다. 이를 위해 이수가압식 쉴드 TBM으로 시공한 터널 현장 데이터를 제공받아 지반 조건, TBM 굴진자료 , 터널 기하 조건 등을 분석하고 17개의 머신러닝 모델 입력변수를 선정하였다. 선정된 17개의 입력변수에 대해 쉴드 TBM 본체를 기준으로 전방 예측 범위(세그먼트 25링 전방, CASE 1), 중앙부(TBM 본체 상부, CASE 2), 후방 예측 범위(세그먼트 25링 후방, CASE 3) 등 세 범위로 구분하고 각 범위에 대하여 입력변수와 침하량 간의 상관관계를 분석하였다. 그리고 각 CASE별로, 즉 터널 전방(CASE 1), 중앙(CASE 2), 후방(CASE 3) 위치에 대해서 XGBoost 머신러닝 알고리즘을 적용한 지반침하 예측 모델을 구축하고 베이지안 최적화와 5겹 교차 검증을 통해 하이퍼파라미터를 최적화하였다. 모델 평가결과, 후방 침하 예측 모델은 결정계수(R2)값이 0.82로 가장 높은 성능을 보인 반면, 전방 침하 예측 모델의 결정계수는 0.52로 상대적으로 낮은 성능을 나타내었다. 이러한 결과는 후방 침하 예측 정확도가 전방 예측보다 우수하고, 전방 예측의 경우 지반의 불확실성과 굴착 변수의 영향을 더 많이 받아 정확도가 낮아질 수 있음을 시사한다. 머신러닝 모델이 TBM 터널 시공 중 발생하는 지반침하, 특히 막장면 후방의 침하를 예측하는 데 효과적인 도구이나 아직 전방 침하의 예측 정확도를 높이기 위해서는 많은 추가 연구가 이루어져야 함을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0050865232&target=NART&cn=ATN0050865232",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "XGBoost 머신러닝 기반 쉴드 TBM 지반침하 예측 XGBoost 머신러닝 기반 쉴드 TBM 지반침하 예측 XGBoost 머신러닝 기반 쉴드 TBM 지반침하 예측 본 연구에서는 도심지 쉴드 TBM (tunnel boring machine) 터널 시공 중 발생하는 지반침하를 예측하기 위한 XGBoost (eXtreme Gradient Boosting) 머신러닝 모델을 개발하고, 그 성능을 평가하였다. 기존 연구들에서 주로 터널 후방의 침하를 예측하는 연구가 많았던 반면, 본 연구에서는 실시간 쉴드 TBM 시공 데이터를 활용하여 터널의 후방 침하뿐 아니라 전방 침하에 대한 예측도 시도하였다. 이를 위해 이수가압식 쉴드 TBM으로 시공한 터널 현장 데이터를 제공받아 지반 조건, TBM 굴진자료 , 터널 기하 조건 등을 분석하고 17개의 머신러닝 모델 입력변수를 선정하였다. 선정된 17개의 입력변수에 대해 쉴드 TBM 본체를 기준으로 전방 예측 범위(세그먼트 25링 전방, CASE 1), 중앙부(TBM 본체 상부, CASE 2), 후방 예측 범위(세그먼트 25링 후방, CASE 3) 등 세 범위로 구분하고 각 범위에 대하여 입력변수와 침하량 간의 상관관계를 분석하였다. 그리고 각 CASE별로, 즉 터널 전방(CASE 1), 중앙(CASE 2), 후방(CASE 3) 위치에 대해서 XGBoost 머신러닝 알고리즘을 적용한 지반침하 예측 모델을 구축하고 베이지안 최적화와 5겹 교차 검증을 통해 하이퍼파라미터를 최적화하였다. 모델 평가결과, 후방 침하 예측 모델은 결정계수(R2)값이 0.82로 가장 높은 성능을 보인 반면, 전방 침하 예측 모델의 결정계수는 0.52로 상대적으로 낮은 성능을 나타내었다. 이러한 결과는 후방 침하 예측 정확도가 전방 예측보다 우수하고, 전방 예측의 경우 지반의 불확실성과 굴착 변수의 영향을 더 많이 받아 정확도가 낮아질 수 있음을 시사한다. 머신러닝 모델이 TBM 터널 시공 중 발생하는 지반침하, 특히 막장면 후방의 침하를 예측하는 데 효과적인 도구이나 아직 전방 침하의 예측 정확도를 높이기 위해서는 많은 추가 연구가 이루어져야 함을 확인하였다."
        },
        {
          "rank": 13,
          "score": 0.7017478346824646,
          "doc_id": "JAKO202404861562091",
          "title": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교",
          "abstract": "대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202404861562091&target=NART&cn=JAKO202404861562091",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 연약지반 침하예측을 위한 딥러닝 및 계측기반 기법의 예측 정확도 비교 대심도 연약지반에 선행재하 공법을 적용하는 경우 재하토 제거 시점을 예측하고 잔류침하량을 최소화하기 위해 연약지반의 침하거동을 정밀히 예측하는 것이 중요하다. 국내에서는 일반적으로 계측기반 침하예측 기법을 적용하고 있으나, 장기간 계측 결과가 필요하고 분석구간에 따라 예측이 달라지는 한계가 있다. 기존 침하예측 기법들의 한계를 보완하기 위해 가중 비선형 회귀 쌍곡선법과 여러 딥러닝 기반 최신 기법 및 모델들이 제시되었으나, 기법들간의 비교&#x00B7;분석이 부족한 실정이다. 그러므로, 본 연구에서는 최근 제안된 딥러닝 모델들과 계측기반 침하예측 기법들의 정확도를 비교&#x00B7;분석하기 위해, 4개의 딥러닝 알고리즘(ANN, LSTM, GRU, Transformer)과 3개의 계측기반 침하예측 기법(쌍곡선법, Asaoka법, 가중 비선형 회귀 쌍곡선법)을 적용하여 학습 및 회귀 일수(60일-150일)에 따라 총 392개 조건에서 침하예측을 수행하였다. 분석 결과, 가중 비선형 회귀 쌍곡선법과 GRU 모델은 모든 조건에서 전반적으로 가장 높은 예측 정확도를 나타내었고 계측 데이터 사용 기간이 증가할수록 모든 기법의 예측 정확도가 향상되었다. 150일간의 데이터를 사용할 경우 모든 기법에서 3cm 이하의 오차를 달성하여 정확한 예측 결과를 제공하였다."
        },
        {
          "rank": 14,
          "score": 0.6971392035484314,
          "doc_id": "JAKO202320150299733",
          "title": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가",
          "abstract": "본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202320150299733&target=NART&cn=JAKO202320150299733",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 RapidEye 위성영상과 Semantic Segmentation 기반 딥러닝 모델을 이용한 토지피복분류의 정확도 평가 본 연구는 딥러닝 모델(deep learning model)을 활용하여 토지피복분류를 수행하였으며 입력 이미지의 크기, Stride 적용 등 데이터세트(dataset)의 조절을 통해 토지피복분류를 위한 최적의 딥러닝 모델 선정을 목적으로 하였다. 적용한 딥러닝 모델은 3종류로 Encoder-Decoder 구조를 가진 U-net과 DeeplabV3+, 두 가지 모델을 결합한 앙상블(Ensemble) 모델을 활용하였다. 데이터세트는 RapidEye 위성영상을 입력영상으로, 라벨(label) 이미지는 Intergovernmental Panel on Climate Change 토지이용의 6가지 범주에 따라 구축한 Raster 이미지를 참값으로 활용하였다. 딥러닝 모델의 정확도 향상을 위해 데이터세트의 질적 향상 문제에 대해 주목하였으며 딥러닝 모델(U-net, DeeplabV3+, Ensemble), 입력 이미지 크기(64 &#x00D7; 64 pixel, 256 &#x00D7; 256 pixel), Stride 적용(50%, 100%) 조합을 통해 12가지 토지피복도를 구축하였다. 라벨 이미지와 딥러닝 모델 기반의 토지피복도의 정합성 평가결과, U-net과 DeeplabV3+ 모델의 전체 정확도는 각각 최대 약 87.9%와 89.8%, kappa 계수는 모두 약 72% 이상으로 높은 정확도를 보였으며, 64 &#x00D7; 64 pixel 크기의 데이터세트를 활용한 U-net 모델의 정확도가 가장 높았다. 또한 딥러닝 모델에 앙상블 및 Stride를 적용한 결과, 최대 약 3% 정확도가 상승하였으며 Semantic Segmentation 기반 딥러닝 모델의 단점인 경계간의 불일치가 개선됨을 확인하였다."
        },
        {
          "rank": 15,
          "score": 0.697079062461853,
          "doc_id": "JAKO201810866003990",
          "title": "딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증",
          "abstract": "본 연구는 경제적으로 국내에 큰 영향을 주었던 글로벌 금융위기를 기반으로 총 10년의 연간 기업데이터를 이용한다. 먼저 시대 변화 흐름에 일관성있는 부도 모형을 구축하는 것을 목표로 금융위기 이전(2000~2006년)의 데이터를 학습한다. 이후 매개 변수 튜닝을 통해 금융위기 기간이 포함(2007~2008년)된 유효성 검증 데이터가 학습데이터의 결과와 비슷한 양상을 보이고, 우수한 예측력을 가지도록 조정한다. 이후 학습 및 유효성 검증 데이터를 통합(2000~2008년)하여 유효성 검증 때와 같은 매개변수를 적용하여 모형을 재구축하고, 결과적으로 최종 학습된 모형을 기반으로 시험 데이터(2009년) 결과를 바탕으로 딥러닝 시계열 알고리즘 기반의 기업부도예측 모형이 유용함을 검증한다. 부도에 대한 정의는 Lee(2015) 연구와 동일하게 기업의 상장폐지 사유들 중 실적이 부진했던 경우를 부도로 선정한다. 독립변수의 경우, 기존 선행연구에서 이용되었던 재무비율 변수를 비롯한 기타 재무정보를 포함한다. 이후 최적의 변수군을 선별하는 방식으로 다변량 판별분석, 로짓 모형, 그리고 Lasso 회귀분석 모형을 이용한다. 기업부도예측 모형 방법론으로는 Altman(1968)이 제시했던 다중판별분석 모형, Ohlson(1980)이 제시한 로짓모형, 그리고 비시계열 기계학습 기반 부도예측모형과 딥러닝 시계열 알고리즘을 이용한다. 기업 데이터의 경우, '비선형적인 변수들', 변수들의 '다중 공선성 문제', 그리고 '데이터 수 부족'이란 한계점이 존재한다. 이에 로짓 모형은 '비선형성'을, Lasso 회귀분석 모형은 '다중 공선성 문제'를 해결하고, 가변적인 데이터 생성 방식을 이용하는 딥러닝 시계열 알고리즘을 접목함으로서 데이터 수가 부족한 점을 보완하여 연구를 진행한다. 현 정부를 비롯한 해외 정부에서는 4차 산업혁명을 통해 국가 및 사회의 시스템, 일상생활 전반을 아우르기 위해 힘쓰고 있다. 즉, 현재는 다양한 산업에 이르러 빅데이터를 이용한 딥러닝 연구가 활발히 진행되고 있지만, 금융 산업을 위한 연구분야는 아직도 미비하다. 따라서 이 연구는 기업 부도에 관하여 딥러닝 시계열 알고리즘 분석을 진행한 초기 논문으로서, 금융 데이터와 딥러닝 시계열 알고리즘을 접목한 연구를 시작하는 비 전공자에게 비교분석 자료로 쓰이기를 바란다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201810866003990&target=NART&cn=JAKO201810866003990",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증 딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증 딥러닝 시계열 알고리즘 적용한 기업부도예측모형 유용성 검증 본 연구는 경제적으로 국내에 큰 영향을 주었던 글로벌 금융위기를 기반으로 총 10년의 연간 기업데이터를 이용한다. 먼저 시대 변화 흐름에 일관성있는 부도 모형을 구축하는 것을 목표로 금융위기 이전(2000~2006년)의 데이터를 학습한다. 이후 매개 변수 튜닝을 통해 금융위기 기간이 포함(2007~2008년)된 유효성 검증 데이터가 학습데이터의 결과와 비슷한 양상을 보이고, 우수한 예측력을 가지도록 조정한다. 이후 학습 및 유효성 검증 데이터를 통합(2000~2008년)하여 유효성 검증 때와 같은 매개변수를 적용하여 모형을 재구축하고, 결과적으로 최종 학습된 모형을 기반으로 시험 데이터(2009년) 결과를 바탕으로 딥러닝 시계열 알고리즘 기반의 기업부도예측 모형이 유용함을 검증한다. 부도에 대한 정의는 Lee(2015) 연구와 동일하게 기업의 상장폐지 사유들 중 실적이 부진했던 경우를 부도로 선정한다. 독립변수의 경우, 기존 선행연구에서 이용되었던 재무비율 변수를 비롯한 기타 재무정보를 포함한다. 이후 최적의 변수군을 선별하는 방식으로 다변량 판별분석, 로짓 모형, 그리고 Lasso 회귀분석 모형을 이용한다. 기업부도예측 모형 방법론으로는 Altman(1968)이 제시했던 다중판별분석 모형, Ohlson(1980)이 제시한 로짓모형, 그리고 비시계열 기계학습 기반 부도예측모형과 딥러닝 시계열 알고리즘을 이용한다. 기업 데이터의 경우, '비선형적인 변수들', 변수들의 '다중 공선성 문제', 그리고 '데이터 수 부족'이란 한계점이 존재한다. 이에 로짓 모형은 '비선형성'을, Lasso 회귀분석 모형은 '다중 공선성 문제'를 해결하고, 가변적인 데이터 생성 방식을 이용하는 딥러닝 시계열 알고리즘을 접목함으로서 데이터 수가 부족한 점을 보완하여 연구를 진행한다. 현 정부를 비롯한 해외 정부에서는 4차 산업혁명을 통해 국가 및 사회의 시스템, 일상생활 전반을 아우르기 위해 힘쓰고 있다. 즉, 현재는 다양한 산업에 이르러 빅데이터를 이용한 딥러닝 연구가 활발히 진행되고 있지만, 금융 산업을 위한 연구분야는 아직도 미비하다. 따라서 이 연구는 기업 부도에 관하여 딥러닝 시계열 알고리즘 분석을 진행한 초기 논문으로서, 금융 데이터와 딥러닝 시계열 알고리즘을 접목한 연구를 시작하는 비 전공자에게 비교분석 자료로 쓰이기를 바란다."
        },
        {
          "rank": 16,
          "score": 0.6959120035171509,
          "doc_id": "DIKO0009404537",
          "title": "Support vector machine을 이용한 기업부도예측",
          "abstract": "Predicting bankruptcy is one of the most important problems to parties such as bankers, managers, government policy makers, and investors. It provides information for interested parties to minimize their predictable losses from bankruptcy. There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. since the late 1980s, Artificial Intelligence (AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network (ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine (SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0009404537&target=NART&cn=DIKO0009404537",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Support vector machine을 이용한 기업부도예측 Support vector machine을 이용한 기업부도예측 Support vector machine을 이용한 기업부도예측 Predicting bankruptcy is one of the most important problems to parties such as bankers, managers, government policy makers, and investors. It provides information for interested parties to minimize their predictable losses from bankruptcy. There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. since the late 1980s, Artificial Intelligence (AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network (ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine (SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction."
        },
        {
          "rank": 17,
          "score": 0.6940200328826904,
          "doc_id": "JAKO200111920938436",
          "title": "퍼지신경망을 이용한 기업부도예측",
          "abstract": "본 연구에서는 퍼지신경망을 이용한 기업부실예측모형을 제안한다. 신경망은 탁월한 학습능력을 가진 것으로 알려져 있으나, 잡음이 심한 재무자료에 대해서는 종종 일관되지 못하고 기대에 미치지 못하는 예측성과를 보인다. 이는 연속형의 형태를 지닌 독립변수와 과다한 양의 원자료로부터 예측에 필요한 일정한 패턴을 찾기가 어렵기 때문이다. 이러한 문제점은 예측모형에서의 독립변수와 종속변수간의 인과관계를 신경망이 용이하게 찾아낼 수 있도록 독립변수의 형태를 변환함으로써 해결한 수 있다. 이러한 해결방법의 하나는 기존 신경망에 퍼지집합의 개념을 적용하여 신경망 학습에 사용될 자료를 퍼지화하고 이를 신경망에 학습시키는 것이다 입력자료를 퍼지화 함으로써 정보의 손실 없이도 신경망이 자료 내의 복잡한 관계를 용이하게 학습하는 것이 가능하다. 본 연구에서 제안된 퍼지신경망을 기업부도예측에 적용한 결과, 퍼지신경망이 기존의 신경망보다 우월한 예측성과를 나타내었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200111920938436&target=NART&cn=JAKO200111920938436",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "퍼지신경망을 이용한 기업부도예측 퍼지신경망을 이용한 기업부도예측 퍼지신경망을 이용한 기업부도예측 본 연구에서는 퍼지신경망을 이용한 기업부실예측모형을 제안한다. 신경망은 탁월한 학습능력을 가진 것으로 알려져 있으나, 잡음이 심한 재무자료에 대해서는 종종 일관되지 못하고 기대에 미치지 못하는 예측성과를 보인다. 이는 연속형의 형태를 지닌 독립변수와 과다한 양의 원자료로부터 예측에 필요한 일정한 패턴을 찾기가 어렵기 때문이다. 이러한 문제점은 예측모형에서의 독립변수와 종속변수간의 인과관계를 신경망이 용이하게 찾아낼 수 있도록 독립변수의 형태를 변환함으로써 해결한 수 있다. 이러한 해결방법의 하나는 기존 신경망에 퍼지집합의 개념을 적용하여 신경망 학습에 사용될 자료를 퍼지화하고 이를 신경망에 학습시키는 것이다 입력자료를 퍼지화 함으로써 정보의 손실 없이도 신경망이 자료 내의 복잡한 관계를 용이하게 학습하는 것이 가능하다. 본 연구에서 제안된 퍼지신경망을 기업부도예측에 적용한 결과, 퍼지신경망이 기존의 신경망보다 우월한 예측성과를 나타내었다."
        },
        {
          "rank": 18,
          "score": 0.6895889043807983,
          "doc_id": "DIKO0014861002",
          "title": "딥 러닝기반 고객평점 예측모델",
          "abstract": "인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014861002&target=NART&cn=DIKO0014861002",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 딥 러닝기반 고객평점 예측모델 인터넷의 발달과 휴대용 기기의 발달로 사용자들이 데이터를 생산하고, 공유하는 일들이 매우 자연스럽고 쉬운 일이 되었다. e-마켓플레스로 대변되는 온라인 쇼핑몰에서도 사용자들의 데이터 생산과 공유가 리뷰의 형식으로 활발하게 이루어지고 있다. 리뷰의 형식은 보통 정해진 형식이 없는 비 정형데이터인 텍스트와 제품에 대한 고객의 평점으로 이루어져있다. 이와 같이 형태로 적극적으로 공유된 정보들은 구매에 중요한 요소로 사용되고 있다. &amp;#xD; 본 논문에서는 이렇게 누적된 리뷰 데이터를 학습하여 고객의 평점을 예측하는 딥 러닝(Deep learning) 모델을 작성하고자 한다. 학습에 필요한 입력데이터 즉 고객의 특성에 관한 일반적인 정보는 쇼핑몰 내부에 있고, 개인 정보가 포함되어 있기 때문에 사용하기 어려운 문제점이 있다. 이를 극복하기 위해 리뷰 자체에서 고객의 특징(feature)을 추출하는 방법을 사용하였다. 비정형 리뷰 데이터에서 텍스트 마이닝 기법을 사용하여 정형화된 고객의 특징을 추출하였다.&amp;#xD; 실험 대상 제품은 11번가 쇼핑몰에서 하나의 화장품을 선정하였다. 최적의 딥 러닝 모델을 찾기 위하여 Drop-Out 및 Rectified Linear hidden Unite(ReLU)를 사용하며 결과를 평가하였다. 딥 러닝의 예측 결과는 고객 평점을 기반으로 하여 좋음, 보통, 나쁨 3가지를 출력 하도록 실험을 진행하였다. 실험을 통해 완성된 딥 러닝 모델이 출력하는 좋은, 보통, 나쁨 3가지 결과와 실제 고객이 입력 한 평점을 비교하였다. 실험 결과 90%의 정확도를 보였다."
        },
        {
          "rank": 19,
          "score": 0.6879907250404358,
          "doc_id": "JAKO200516638000020",
          "title": "Support Vector Machine을 이용한 기업부도예측",
          "abstract": "There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. Since the late 1980s, Artificial Intelligence(AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network(ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine(SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO200516638000020&target=NART&cn=JAKO200516638000020",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Support Vector Machine을 이용한 기업부도예측 Support Vector Machine을 이용한 기업부도예측 Support Vector Machine을 이용한 기업부도예측 There has been substantial research into the bankruptcy prediction. Many researchers used the statistical method in the problem until the early 1980s. Since the late 1980s, Artificial Intelligence(AI) has been employed in bankruptcy prediction. And many studies have shown that artificial neural network(ANN) achieved better performance than traditional statistical methods. However, despite ANN's superior performance, it has some problems such as overfitting and poor explanatory power. To overcome these limitations, this paper suggests a relatively new machine learning technique, support vector machine(SVM), to bankruptcy prediction. SVM is simple enough to be analyzed mathematically, and leads to high performances in practical applications. The objective of this paper is to examine the feasibility of SVM in bankruptcy prediction by comparing it with ANN, logistic regression, and multivariate discriminant analysis. The experimental results show that SVM provides a promising alternative to bankruptcy prediction."
        },
        {
          "rank": 20,
          "score": 0.6846319437026978,
          "doc_id": "DIKO0015069923",
          "title": "딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템",
          "abstract": "데이터 예측 시스템들은 데이터를 예측하기 위해 특정 분야의 데이터를 컴퓨터가 분석하여 규칙을 찾아내고 데이터를 예측하였다. 이러한 방법은 과거 데이터를 분석한 결과로 사람이 규칙을 도출할 수 있어야 데이터를 예측하는 것이 가능하였다. 이에 반해 규칙을 도출할 수 없는 데이터들의 데이터를 예측하는 것은 사람의 능력으로는 한계가 있어 정확도가 낮아지는 문제점이 발생할 수 있다.&amp;#xD; 이를 해결하기 위해 컴퓨터를 활용하여 방대한 데이터를 데이터 예측 프로그램에 학습 데이터로 입력하고 결과로 데이터를 예측하였다. 이러한 방법론을 활용하기 위해서 고성능 컴퓨터로 딥 러닝(Deep Learning) 기술을 적용하여 데이터를 예측하고 있다. 해당 방법론이 활용되고 있는 분야로는 기상 데이터를 분석하여 날씨를 예측하는 날씨 분석과 스포츠 경기의 데이터를 예측하는 것이 대표적이다. &amp;#xD; 딥 러닝 기술은 프로그램이 데이터를 기반으로 학습을 진행하고 진행된 학습을 기반으로 데이터를 처리하는 것이다. 이는 과거에 사람이 직접 데이터를 분석하는 것보다 대규모 데이터를 분석하기에 적합하고 이로 인해 정확도가 올라가는 이점이 있다. 또한 목적에 따라 적합한 딥 러닝 모델을 적용하여 데이터를 예측할 경우 정확도의 기댓값이 높아지는 이점이 있다.&amp;#xD; 현재 딥 러닝 모델 중에서 데이터를 예측하기 위해 사용되는 모델은 신경망 구조를 기반으로 하는 DNN(Deep Neural Network) 모델과 RNN(Recurrent Neural Network) 모델이다. DNN 모델은 학습 데이터 내에서 규칙을 찾아내지 못하더라도 반복 학습을 통해 데이터 예측에 대한 정확도를 올릴 수 있고, RNN은 학습 과정 중에서 은닉층에서 적용될 가중치가 학습을 진행할 수록 변화하여 데이터를 예측하고 이로 인해 정확도를 올릴 수 있다. 이에 반해 DNN은 반복 학습의 횟수가 많아야 정확도가 높아지고 RNN은 가중치 변화의 횟수가 많아져야 정확도가 높아지기 때문에 결국 두 모델들은 학습의 반복이 많아져야 하는 문제점이 있다.&amp;#xD; 본 논문에서는 데이터 예측을 위해 딥 러닝 모델 기반 순차 데이터 예측 시스템을 제안한다. 제안하는 시스템에서 비정형 데이터를 순차 데이터로 정제하기 위해 전처리기를 구현하였다. 전처리기는 딥 러닝 모델에 학습 데이터를 입력하기 전에 데이터들을 정제하는 기능을 수행한다. 데이터는 ‘데이터 : 인덱스’ 구조로 이루어진 데이터 쌍이 되고 이러한 데이터 쌍들의 집합을 딥 러닝 모델에 입력하여 학습을 진행한다.&amp;#xD; 딥 러닝 모델은 DNN 모델, 기본 LSTM 모델, 상태유지 LSTM 모델을 활용하여 시스템을 각각 구축한다. 그리고 각 모델들의 설정 값을 변경하면서 정확도의 변화량을 분석한다. 또한 시퀀스의 길이를 변경해가며 실험을 진행하여 가장 정확도가 높은 데이터 셋과 시퀀스 길이의 비율을 제시한다.&amp;#xD; 딥 러닝 모듈 기반 시스템의 실험을 바탕으로 순차 데이터 예측에 가장 정확도가 높고 효율적인 딥 러닝 모듈을 선정하고 기존 시스템들과 비교 분석을 진행하여 제안하는 시스템의 우수성을 검증한다.&amp;#xD; 제안하는 시스템을 활용할 경우 학습 데이터가 적어도 높은 정확도를 요구하는 분야에서 기존 시스템들에 비해 효율성이 높을 것으로 사료된다.&amp;#xD;",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015069923&target=NART&cn=DIKO0015069923",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 딥 러닝 모델 최적화 기반 순차 데이터 예측 시스템 데이터 예측 시스템들은 데이터를 예측하기 위해 특정 분야의 데이터를 컴퓨터가 분석하여 규칙을 찾아내고 데이터를 예측하였다. 이러한 방법은 과거 데이터를 분석한 결과로 사람이 규칙을 도출할 수 있어야 데이터를 예측하는 것이 가능하였다. 이에 반해 규칙을 도출할 수 없는 데이터들의 데이터를 예측하는 것은 사람의 능력으로는 한계가 있어 정확도가 낮아지는 문제점이 발생할 수 있다.&amp;#xD; 이를 해결하기 위해 컴퓨터를 활용하여 방대한 데이터를 데이터 예측 프로그램에 학습 데이터로 입력하고 결과로 데이터를 예측하였다. 이러한 방법론을 활용하기 위해서 고성능 컴퓨터로 딥 러닝(Deep Learning) 기술을 적용하여 데이터를 예측하고 있다. 해당 방법론이 활용되고 있는 분야로는 기상 데이터를 분석하여 날씨를 예측하는 날씨 분석과 스포츠 경기의 데이터를 예측하는 것이 대표적이다. &amp;#xD; 딥 러닝 기술은 프로그램이 데이터를 기반으로 학습을 진행하고 진행된 학습을 기반으로 데이터를 처리하는 것이다. 이는 과거에 사람이 직접 데이터를 분석하는 것보다 대규모 데이터를 분석하기에 적합하고 이로 인해 정확도가 올라가는 이점이 있다. 또한 목적에 따라 적합한 딥 러닝 모델을 적용하여 데이터를 예측할 경우 정확도의 기댓값이 높아지는 이점이 있다.&amp;#xD; 현재 딥 러닝 모델 중에서 데이터를 예측하기 위해 사용되는 모델은 신경망 구조를 기반으로 하는 DNN(Deep Neural Network) 모델과 RNN(Recurrent Neural Network) 모델이다. DNN 모델은 학습 데이터 내에서 규칙을 찾아내지 못하더라도 반복 학습을 통해 데이터 예측에 대한 정확도를 올릴 수 있고, RNN은 학습 과정 중에서 은닉층에서 적용될 가중치가 학습을 진행할 수록 변화하여 데이터를 예측하고 이로 인해 정확도를 올릴 수 있다. 이에 반해 DNN은 반복 학습의 횟수가 많아야 정확도가 높아지고 RNN은 가중치 변화의 횟수가 많아져야 정확도가 높아지기 때문에 결국 두 모델들은 학습의 반복이 많아져야 하는 문제점이 있다.&amp;#xD; 본 논문에서는 데이터 예측을 위해 딥 러닝 모델 기반 순차 데이터 예측 시스템을 제안한다. 제안하는 시스템에서 비정형 데이터를 순차 데이터로 정제하기 위해 전처리기를 구현하였다. 전처리기는 딥 러닝 모델에 학습 데이터를 입력하기 전에 데이터들을 정제하는 기능을 수행한다. 데이터는 ‘데이터 : 인덱스’ 구조로 이루어진 데이터 쌍이 되고 이러한 데이터 쌍들의 집합을 딥 러닝 모델에 입력하여 학습을 진행한다.&amp;#xD; 딥 러닝 모델은 DNN 모델, 기본 LSTM 모델, 상태유지 LSTM 모델을 활용하여 시스템을 각각 구축한다. 그리고 각 모델들의 설정 값을 변경하면서 정확도의 변화량을 분석한다. 또한 시퀀스의 길이를 변경해가며 실험을 진행하여 가장 정확도가 높은 데이터 셋과 시퀀스 길이의 비율을 제시한다.&amp;#xD; 딥 러닝 모듈 기반 시스템의 실험을 바탕으로 순차 데이터 예측에 가장 정확도가 높고 효율적인 딥 러닝 모듈을 선정하고 기존 시스템들과 비교 분석을 진행하여 제안하는 시스템의 우수성을 검증한다.&amp;#xD; 제안하는 시스템을 활용할 경우 학습 데이터가 적어도 높은 정확도를 요구하는 분야에서 기존 시스템들에 비해 효율성이 높을 것으로 사료된다.&amp;#xD;"
        },
        {
          "rank": 21,
          "score": 0.6844030618667603,
          "doc_id": "JAKO201828138444462",
          "title": "효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용",
          "abstract": "분류 문제에서 특정 범주의 빈도가 다른 범주에 비해 과도하게 높은 경우, 왜곡된 기계 학습을 유발할 수 있는 데이터 불균형(imbalanced data) 문제가 발생한다. 기업부도 예측 문제도 그 중 하나인데, 일반적으로 금융기관과 거래하는 기업들의 부도율은 대단히 낮아서, 부도 사례보다 정상 사례의 빈도가 월등히 높은 데이터 불균형 문제가 발생하고 있다. 이러한 데이터 불균형 문제를 해결하기 위해서는 적절한 표본추출 기법이 적용될 필요가 있으며, 지금껏 소수 범주 데이터를 복원 추출함으로써 다수 범주 데이터와 비율을 맞추어 데이터 불균형을 해결하는 오버 샘플링(oversampling) 기법이 주로 활용되어 왔다. 그러나 전통적인 오버 샘플링은 과적합화(overfitting)가 발생할 위험이 높아질 수 있는 단점이 있다. 이러한 배경에서 본 연구는 효과적인 기업부도 예측 모형 학습을 위한 표본추출 기법으로 2014년에 Menardi와 Torelli가 제안한 ROSE(random over sampling examples) 기법을 제안한다. ROSE 기법은 학습에 사용될 사례를 반복적으로 새롭게 합성하여 생성(synthetic generation)하는 기법으로, 과적합화 문제를 회피하면서도 분류 예측 정확도 개선에 도움을 줄 수 있다. 이에 본 연구에서는 ROSE 기법을 가장 성능이 우수한 이분류기로 알려진 SVM(support vector machine)과 결합하여 국내 한 대형 은행의 기업부도 예측에 적용해 보고, 다른 표본추출 기법들과의 비교연구를 수행하였다. 실험 결과, ROSE 기법이 다른 기법에 비해 통계적으로 유의한 수준으로 SVM의 예측정확도 개선에 기여할 수 있음을 확인하였다. 이러한 본 연구의 결과는 부도예측 외에 다른 사회과학 분야 예측문제의 데이터 불균형 문제 해결에도 ROSE가 우수한 대안이 될 수 있다는 사실을 시사한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201828138444462&target=NART&cn=JAKO201828138444462",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용 효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용 효과적인 기업부도 예측모형을 위한 ROSE 표본추출기법의 적용 분류 문제에서 특정 범주의 빈도가 다른 범주에 비해 과도하게 높은 경우, 왜곡된 기계 학습을 유발할 수 있는 데이터 불균형(imbalanced data) 문제가 발생한다. 기업부도 예측 문제도 그 중 하나인데, 일반적으로 금융기관과 거래하는 기업들의 부도율은 대단히 낮아서, 부도 사례보다 정상 사례의 빈도가 월등히 높은 데이터 불균형 문제가 발생하고 있다. 이러한 데이터 불균형 문제를 해결하기 위해서는 적절한 표본추출 기법이 적용될 필요가 있으며, 지금껏 소수 범주 데이터를 복원 추출함으로써 다수 범주 데이터와 비율을 맞추어 데이터 불균형을 해결하는 오버 샘플링(oversampling) 기법이 주로 활용되어 왔다. 그러나 전통적인 오버 샘플링은 과적합화(overfitting)가 발생할 위험이 높아질 수 있는 단점이 있다. 이러한 배경에서 본 연구는 효과적인 기업부도 예측 모형 학습을 위한 표본추출 기법으로 2014년에 Menardi와 Torelli가 제안한 ROSE(random over sampling examples) 기법을 제안한다. ROSE 기법은 학습에 사용될 사례를 반복적으로 새롭게 합성하여 생성(synthetic generation)하는 기법으로, 과적합화 문제를 회피하면서도 분류 예측 정확도 개선에 도움을 줄 수 있다. 이에 본 연구에서는 ROSE 기법을 가장 성능이 우수한 이분류기로 알려진 SVM(support vector machine)과 결합하여 국내 한 대형 은행의 기업부도 예측에 적용해 보고, 다른 표본추출 기법들과의 비교연구를 수행하였다. 실험 결과, ROSE 기법이 다른 기법에 비해 통계적으로 유의한 수준으로 SVM의 예측정확도 개선에 기여할 수 있음을 확인하였다. 이러한 본 연구의 결과는 부도예측 외에 다른 사회과학 분야 예측문제의 데이터 불균형 문제 해결에도 ROSE가 우수한 대안이 될 수 있다는 사실을 시사한다."
        },
        {
          "rank": 22,
          "score": 0.6829303503036499,
          "doc_id": "JAKO202108360626662",
          "title": "딥러닝을 이용한 외해 해양기상자료로부터의 항내파고 예측",
          "abstract": "본 연구에서는 항내 파고를 신속하고 비교적 정확하게 예측할 수 있는 딥러닝 모델을 구축하였다.다양한 머신러닝 기법들을 외해파랑의 항내로 전파 변형 특성을 감안하여 모델에 적용하였으며 스웰로 인해 하역중단 문제가 심각했던 포항신항을 모델적용 대상지로 선정하였다. 모델의 입력 자료는 외해의 파고, 주기, 파향 그리고 출력 및 예측 자료로는 항내 파고자료로 하여 모델을 학습시켰다. 이때 자료의 전처리 과정으로 항내&#x00B7;외 파랑 시계열자료의 상관성을 감안하여 파향 자료를 분리하는 방법을 적용하고 딥러닝 기법을 이용하여 모델을 학습하였다. 결과적으로 모델을 통해 예측한 값이 항내관측치의 파고 시계열자료를 잘 재현하였으며 모델의 안정성을 크게 향상시켰다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202108360626662&target=NART&cn=JAKO202108360626662",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 이용한 외해 해양기상자료로부터의 항내파고 예측 딥러닝을 이용한 외해 해양기상자료로부터의 항내파고 예측 딥러닝을 이용한 외해 해양기상자료로부터의 항내파고 예측 본 연구에서는 항내 파고를 신속하고 비교적 정확하게 예측할 수 있는 딥러닝 모델을 구축하였다.다양한 머신러닝 기법들을 외해파랑의 항내로 전파 변형 특성을 감안하여 모델에 적용하였으며 스웰로 인해 하역중단 문제가 심각했던 포항신항을 모델적용 대상지로 선정하였다. 모델의 입력 자료는 외해의 파고, 주기, 파향 그리고 출력 및 예측 자료로는 항내 파고자료로 하여 모델을 학습시켰다. 이때 자료의 전처리 과정으로 항내&#x00B7;외 파랑 시계열자료의 상관성을 감안하여 파향 자료를 분리하는 방법을 적용하고 딥러닝 기법을 이용하여 모델을 학습하였다. 결과적으로 모델을 통해 예측한 값이 항내관측치의 파고 시계열자료를 잘 재현하였으며 모델의 안정성을 크게 향상시켰다."
        },
        {
          "rank": 23,
          "score": 0.6828393936157227,
          "doc_id": "DIKO0015731994",
          "title": "기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로",
          "abstract": "본 연구에서는 기업의 부도를 사전에 예측하기 위한 모형을 연구하였고 정량 정보인 재무 데이터와 비정형 정보인 뉴스 콘텐츠의 감성분석 정보를 변수로 선정하였다. 재무 정보는 회계와 재무분야의 문헌에서 잘 알려지고 오랜 기간을 통하여 검증된 3개의 재무모델 변수(Altman, 1968; Beaver, 1968; Horrigan, 1966)와 기업의 경영상태를 종합적으로 분석하는 방법인 기업경영분석 지표(한국은행)를 결합하여 총 3개년치의 재무변수를 선정하였다. &amp;#xD; &amp;#xD; 기업의 부도 징후를 나타내는 유의미한 재무적 요인을 도출하기 위해 t-test와 logistic regression방법으로 통계적 검증 작업을 진행하였고, 연구 결과 총자산 이익잉여금률, 총자산 이익률, 매출액 운전자본 비율, 자본 매출액 비율, 차입금 의존도가 유의미한 재무 변수로 확인되었다. &amp;#xD; &amp;#xD; 비정형 정보인 뉴스 콘텐츠가 기업 부도를 예측하는데 얼마나 효과적인지 검증하기 위해 재무 변수에 뉴스 감성 분석 점수를 추가하여 모델링을 적용하였다. 부도 기업인 경우 부도 직전 6개월치 뉴스를, 정상 기업인 경우 2019. 7 ~ 12월까지의 6개월치 뉴스 기사를 크롤링 하였고 실제 기업 뉴스와 상관없는 기사들은 정제 작업 등의 전처리를 진행하였다. 정제가 완료된 텍스트에서 명사를 추출하여 말뭉치 기반의 감성사전을 구축하고 뉴스의 수집 기간별 감성점수를 변수로 추가하였다. &amp;#xD; &amp;#xD; 기업 뉴스의 감성점수를 추가한 결과 재무 데이터만을 사용했을 때보다 훨씬 더 좋은 성능이 나타남을 알 수 있었다. 민감도 기준(실제 부도기업을 부도기업으로 예측)으로 가장 성능이 우수한 SVM(Support Vector Machine)의 경우 재무 변수만 사용했을 때 87.50%였는데 뉴스 감성점수를 추가했을 때 93.75%로 약 6% 정도의 성능 향상이 있었다. 그리고 뉴스 수집기간은 3,4개월치를 적용 했을 때 민감도의 성능이 가장 좋은 것으로 확인되었다. &amp;#xD; &amp;#xD; 금융기관에서는 전통적으로 부도 예측을 위해 재무 데이터를 주로 사용하고 있는데 해당 정보는 분기별로 업데이트가 되는 정보의 적시성에 문제가 있을 수 있다. 따라서 부도 예측시 본 연구에서 실증한 온라인 뉴스의 감성분석 정보인 비정형 데이터를 함께 사용한다면 효과적인 여신 의사결정 지원 체계를 수립하는데 많은 도움이 될 것으로 판단된다. &amp;#xD; &amp;#xD;",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015731994&target=NART&cn=DIKO0015731994",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로 기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로 기업 부도 사전 예측 모형 연구 : 머신러닝 기법을 중심으로 본 연구에서는 기업의 부도를 사전에 예측하기 위한 모형을 연구하였고 정량 정보인 재무 데이터와 비정형 정보인 뉴스 콘텐츠의 감성분석 정보를 변수로 선정하였다. 재무 정보는 회계와 재무분야의 문헌에서 잘 알려지고 오랜 기간을 통하여 검증된 3개의 재무모델 변수(Altman, 1968; Beaver, 1968; Horrigan, 1966)와 기업의 경영상태를 종합적으로 분석하는 방법인 기업경영분석 지표(한국은행)를 결합하여 총 3개년치의 재무변수를 선정하였다. &amp;#xD; &amp;#xD; 기업의 부도 징후를 나타내는 유의미한 재무적 요인을 도출하기 위해 t-test와 logistic regression방법으로 통계적 검증 작업을 진행하였고, 연구 결과 총자산 이익잉여금률, 총자산 이익률, 매출액 운전자본 비율, 자본 매출액 비율, 차입금 의존도가 유의미한 재무 변수로 확인되었다. &amp;#xD; &amp;#xD; 비정형 정보인 뉴스 콘텐츠가 기업 부도를 예측하는데 얼마나 효과적인지 검증하기 위해 재무 변수에 뉴스 감성 분석 점수를 추가하여 모델링을 적용하였다. 부도 기업인 경우 부도 직전 6개월치 뉴스를, 정상 기업인 경우 2019. 7 ~ 12월까지의 6개월치 뉴스 기사를 크롤링 하였고 실제 기업 뉴스와 상관없는 기사들은 정제 작업 등의 전처리를 진행하였다. 정제가 완료된 텍스트에서 명사를 추출하여 말뭉치 기반의 감성사전을 구축하고 뉴스의 수집 기간별 감성점수를 변수로 추가하였다. &amp;#xD; &amp;#xD; 기업 뉴스의 감성점수를 추가한 결과 재무 데이터만을 사용했을 때보다 훨씬 더 좋은 성능이 나타남을 알 수 있었다. 민감도 기준(실제 부도기업을 부도기업으로 예측)으로 가장 성능이 우수한 SVM(Support Vector Machine)의 경우 재무 변수만 사용했을 때 87.50%였는데 뉴스 감성점수를 추가했을 때 93.75%로 약 6% 정도의 성능 향상이 있었다. 그리고 뉴스 수집기간은 3,4개월치를 적용 했을 때 민감도의 성능이 가장 좋은 것으로 확인되었다. &amp;#xD; &amp;#xD; 금융기관에서는 전통적으로 부도 예측을 위해 재무 데이터를 주로 사용하고 있는데 해당 정보는 분기별로 업데이트가 되는 정보의 적시성에 문제가 있을 수 있다. 따라서 부도 예측시 본 연구에서 실증한 온라인 뉴스의 감성분석 정보인 비정형 데이터를 함께 사용한다면 효과적인 여신 의사결정 지원 체계를 수립하는데 많은 도움이 될 것으로 판단된다. &amp;#xD; &amp;#xD;"
        },
        {
          "rank": 24,
          "score": 0.6815927028656006,
          "doc_id": "JAKO202312473958811",
          "title": "작물 생산량 예측을 위한 심층강화학습 성능 분석",
          "abstract": "최근 딥러닝 기술을 활용하여 작물 생산량 예측 연구가 많이 진행되고 있다. 딥러닝 알고리즘은 입력 데이터 세트와 작물 예측 결과에 대한 선형 맵을 구성하는데 어려움이 있다. 또한, 알고리즘 구현은 획득한 속성의 비율에 긍정적으로 의존한다. 심층강화학습을 작물 생산량 예측 응용에 적용한다면 이러한 한계점을 보완할 수 있다. 본 논문은 작물 생산량 예측을 개선하기 위해 DQN, Double DQN 및 Dueling DQN 의 성능을 분석한다. DQN 알고리즘은 과대 평가 문제가 제기되지만, Double DQN은 과대 평가를 줄이고 더 나은 결과를 얻을 수 있다. 본 논문에서 제안된 모델은 거짓 판정을 줄이고 예측 정확도를 높이는 것으로 나타났다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202312473958811&target=NART&cn=JAKO202312473958811",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "작물 생산량 예측을 위한 심층강화학습 성능 분석 작물 생산량 예측을 위한 심층강화학습 성능 분석 작물 생산량 예측을 위한 심층강화학습 성능 분석 최근 딥러닝 기술을 활용하여 작물 생산량 예측 연구가 많이 진행되고 있다. 딥러닝 알고리즘은 입력 데이터 세트와 작물 예측 결과에 대한 선형 맵을 구성하는데 어려움이 있다. 또한, 알고리즘 구현은 획득한 속성의 비율에 긍정적으로 의존한다. 심층강화학습을 작물 생산량 예측 응용에 적용한다면 이러한 한계점을 보완할 수 있다. 본 논문은 작물 생산량 예측을 개선하기 위해 DQN, Double DQN 및 Dueling DQN 의 성능을 분석한다. DQN 알고리즘은 과대 평가 문제가 제기되지만, Double DQN은 과대 평가를 줄이고 더 나은 결과를 얻을 수 있다. 본 논문에서 제안된 모델은 거짓 판정을 줄이고 예측 정확도를 높이는 것으로 나타났다."
        },
        {
          "rank": 25,
          "score": 0.680736780166626,
          "doc_id": "JAKO202012758284659",
          "title": "딥러닝을 활용한 다목적댐 유입량 예측",
          "abstract": "최근 데이터 예측 방법으로 인공신경망(Artificial Neural Network, ANN)분야에 대한 관심이 높아졌으며, 그 중 시계열 데이터 예측에 특화된 LSTM(Long Short-Term Memory)모형은 수문 시계열자료의 예측방법으로도 활용되고 있다. 본 연구에서는 구글에서 제공하는 딥러닝 오픈소스 라이브러리인 텐서플로우(TensorFlow)를 활용하여 LSTM모형을 구축하고 금강 상류에 위치한 용담다목적댐의 유입량을 예측하였다. 분석 자료로는 WAMIS에서 제공하는 용담댐의 2006년부터 2018년까지의 시간당 유입량 자료를 사용하였으며, 예측된 유입량과 관측 유입량의 비교를 통하여 평균제곱오차(RMSE), 평균절대오차(MAE), 용적오차(VE)를 계산하고 모형의 학습변수에 따른 정확도를 평가하였다. 분석결과, 모든 모형이 고유량에서의 정확도가 낮은 것으로 나타났으며, 이와 같은 문제를 해결하기 위하여 용담댐 유역의 시간당 강수량 자료를 추가 학습 자료로 활용하여 분석한 결과, 고유량에 대한 예측의 정확도가 높아지는 것을 알 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202012758284659&target=NART&cn=JAKO202012758284659",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 활용한 다목적댐 유입량 예측 딥러닝을 활용한 다목적댐 유입량 예측 딥러닝을 활용한 다목적댐 유입량 예측 최근 데이터 예측 방법으로 인공신경망(Artificial Neural Network, ANN)분야에 대한 관심이 높아졌으며, 그 중 시계열 데이터 예측에 특화된 LSTM(Long Short-Term Memory)모형은 수문 시계열자료의 예측방법으로도 활용되고 있다. 본 연구에서는 구글에서 제공하는 딥러닝 오픈소스 라이브러리인 텐서플로우(TensorFlow)를 활용하여 LSTM모형을 구축하고 금강 상류에 위치한 용담다목적댐의 유입량을 예측하였다. 분석 자료로는 WAMIS에서 제공하는 용담댐의 2006년부터 2018년까지의 시간당 유입량 자료를 사용하였으며, 예측된 유입량과 관측 유입량의 비교를 통하여 평균제곱오차(RMSE), 평균절대오차(MAE), 용적오차(VE)를 계산하고 모형의 학습변수에 따른 정확도를 평가하였다. 분석결과, 모든 모형이 고유량에서의 정확도가 낮은 것으로 나타났으며, 이와 같은 문제를 해결하기 위하여 용담댐 유역의 시간당 강수량 자료를 추가 학습 자료로 활용하여 분석한 결과, 고유량에 대한 예측의 정확도가 높아지는 것을 알 수 있었다."
        },
        {
          "rank": 26,
          "score": 0.6785551309585571,
          "doc_id": "JAKO202116954704821",
          "title": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측",
          "abstract": "본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202116954704821&target=NART&cn=JAKO202116954704821",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 시간 연속성을 고려한 딥러닝 기반 레이더 강우예측 본 연구에서는 시계열 순서의 의미가 희석될 수 있는 기존의 U-net 기반 딥러닝 강우예측 모델의 성능을 개선하고자 하였다. 이를 위해서 데이터의 연속성을 고려한 ConvLSTM2D U-Net 신경망 구조를 갖는 모델을 적용하고, RainNet 모델 및 외삽 기반의 이류모델을 이용하여 예측정확도 개선 정도를 평가하였다. 또한 신경망 기반 모델 학습과정에서의 불확실성을 개선하기 위해 단일 모델뿐만 아니라 10개의 앙상블 모델로 학습을 수행하였다. 학습된 신경망 강우예측모델은 현재를 기준으로 과거 30분 전까지의 연속된 4개의 자료를 이용하여 10분 선행 예측자료를 생성하는데 최적화되었다. 최적화된 딥러닝 강우예측모델을 이용하여 강우예측을 수행한 결과, ConvLSTM2D U-Net을 사용하였을 때 예측 오차의 크기가 가장 작고, 강우 이동 위치를 상대적으로 정확히 구현하였다. 특히, 앙상블 ConvLSTM2D U-Net이 타 예측모델에 비해 높은 CSI와 낮은 MAE를 보이며, 상대적으로 정확하게 강우를 예측하였으며, 좁은 오차범위로 안정적인 예측성능을 보여주었다. 다만, 특정 지점만을 대상으로 한 예측성능은 전체 강우 영역에 대한 예측성능에 비해 낮게 나타나, 상세한 영역의 강우예측에 대한 딥러닝 강우예측모델의 한계도 확인하였다. 본 연구를 통해 시간의 변화를 고려하기 위한 ConvLSTM2D U-Net 신경망 구조가 예측정확도를 높일 수 있었으나, 여전히 강한 강우영역이나 상세한 강우예측에는 공간 평활로 인한 합성곱 신경망 모델의 한계가 있음을 확인하였다."
        },
        {
          "rank": 27,
          "score": 0.6774506568908691,
          "doc_id": "JAKO202518361202534",
          "title": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측",
          "abstract": "본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202518361202534&target=NART&cn=JAKO202518361202534",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 PNC 딥러닝 모델을 이용한 미세먼지 납 농도 예측 본 연구는 수도권(서울)의 2017~2024년 납(Pb) 농도 및 기상 데이터를 활용하여 일 단위 납 농도를 예측하는 딥러닝 기반 모델을 비교 분석하였다. 입력 변수로는 8개의 기상 요소와 과거 3일간 납 농도 값을 활용하였다. CNN, LSTM, GRU, TCN, Transformer, PNC 모델을 적용한 결과, PNC 모델이 시험 데이터 기준 RMSE 17.34, MAE 10.45로 가장 우수한 성능을 보였다. 본 연구는 중금속 예측에 있어 데이터 기반 모델의 적용 가능성을 확인하였으며, 향후 지역 확장 및 고농도 대응 성능 개선에 대한 연구가 필요하다."
        },
        {
          "rank": 28,
          "score": 0.6741006970405579,
          "doc_id": "NART133898062",
          "title": "Hidden Markov Neural Networks",
          "abstract": "<P>We define an evolving in-time Bayesian neural network called a Hidden Markov Neural Network, which addresses the crucial challenge in time-series forecasting and continual learning: striking a balance between adapting to new data and appropriately forgetting outdated information. This is achieved by modelling the weights of a neural network as the hidden states of a Hidden Markov model, with the observed process defined by the available data. A filtering algorithm is employed to learn a variational approximation of the evolving-in-time posterior distribution over the weights. By leveraging a sequential variant of Bayes by Backprop, enriched with a stronger regularization technique called variational DropConnect, Hidden Markov Neural Networks achieve robust regularization and scalable inference. Experiments on MNIST, dynamic classification tasks, and next-frame forecasting in videos demonstrate that Hidden Markov Neural Networks provide strong predictive performance while enabling effective uncertainty quantification.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART133898062&target=NART&cn=NART133898062",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Hidden Markov Neural Networks Hidden Markov Neural Networks Hidden Markov Neural Networks <P>We define an evolving in-time Bayesian neural network called a Hidden Markov Neural Network, which addresses the crucial challenge in time-series forecasting and continual learning: striking a balance between adapting to new data and appropriately forgetting outdated information. This is achieved by modelling the weights of a neural network as the hidden states of a Hidden Markov model, with the observed process defined by the available data. A filtering algorithm is employed to learn a variational approximation of the evolving-in-time posterior distribution over the weights. By leveraging a sequential variant of Bayes by Backprop, enriched with a stronger regularization technique called variational DropConnect, Hidden Markov Neural Networks achieve robust regularization and scalable inference. Experiments on MNIST, dynamic classification tasks, and next-frame forecasting in videos demonstrate that Hidden Markov Neural Networks provide strong predictive performance while enabling effective uncertainty quantification.</P>"
        },
        {
          "rank": 29,
          "score": 0.6723120212554932,
          "doc_id": "ATN0052773847",
          "title": "머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구",
          "abstract": "본 논문은 상장기업의 재무 및 거시경제 데이터를 활용하여 기업부도를 예측하는 통계적 모형과 다양한 머신러닝 기법의 성능을 비교하고, 불균형 데이터 문제를 완화하기 위한 오버샘플링 기법의 효과를 분석하였다. 실증 분석에는 로지스틱 회귀, 랜덤 포레스트, XGBoost(extreme gradient boosting), 심층신경망 모형을 적용하였으며, 오버샘플링 기법인 SMOTE(synthetic minority over-sampling technique) 및 ADASYN(adapti-ve synthetic sampling)을 사용하였다. 분석 결과, XGBoost는 원자료뿐 아니라 오버샘플링을 적용한 경우 모두에서 가장 우수하고 균형 있는 예측 성능을 보였다. 반면, 로지스틱 회귀는 높은 재현율을 나타냈으나, 낮은 정밀도로 인해 실무적 활용에는 한계가 있었다. 이러한 결과는 불균형 데이터 환경에서 오버샘플링 기법과 XGBoost와 같은 머신러닝 모형을 결합하여 사용하는 것이 기업부도 예측에 있어 보다 효과적이고 실용적인 접근법이 될 수 있음을 시사한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0052773847&target=NART&cn=ATN0052773847",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구 머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구 머신러닝과 오버샘플링(oversampling)을 이용한 상장기업 부도예측 연구 본 논문은 상장기업의 재무 및 거시경제 데이터를 활용하여 기업부도를 예측하는 통계적 모형과 다양한 머신러닝 기법의 성능을 비교하고, 불균형 데이터 문제를 완화하기 위한 오버샘플링 기법의 효과를 분석하였다. 실증 분석에는 로지스틱 회귀, 랜덤 포레스트, XGBoost(extreme gradient boosting), 심층신경망 모형을 적용하였으며, 오버샘플링 기법인 SMOTE(synthetic minority over-sampling technique) 및 ADASYN(adapti-ve synthetic sampling)을 사용하였다. 분석 결과, XGBoost는 원자료뿐 아니라 오버샘플링을 적용한 경우 모두에서 가장 우수하고 균형 있는 예측 성능을 보였다. 반면, 로지스틱 회귀는 높은 재현율을 나타냈으나, 낮은 정밀도로 인해 실무적 활용에는 한계가 있었다. 이러한 결과는 불균형 데이터 환경에서 오버샘플링 기법과 XGBoost와 같은 머신러닝 모형을 결합하여 사용하는 것이 기업부도 예측에 있어 보다 효과적이고 실용적인 접근법이 될 수 있음을 시사한다."
        },
        {
          "rank": 30,
          "score": 0.671603262424469,
          "doc_id": "NART80772700",
          "title": "Noisy training for deep neural networks in speech recognition",
          "abstract": "<P><B>Abstract</B><P>Deep neural networks (DNNs) have gained remarkable success in speech recognition, partially attributed to the flexibility of DNN models in learning complex patterns of speech signals. This flexibility, however, may lead to serious over-fitting and hence miserable performance degradation in adverse acoustic conditions such as those with high ambient noises. We propose a noisy training approach to tackle this problem: by injecting moderate noises into the training data intentionally and randomly, more generalizable DNN models can be learned. This &lsquo;noise injection&rsquo; technique, although known to the neural computation community already, has not been studied with DNNs which involve a highly complex objective function. The experiments presented in this paper confirm that the noisy training approach works well for the DNN model and can provide substantial performance improvement for DNN-based speech recognition.</P></P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART80772700&target=NART&cn=NART80772700",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Noisy training for deep neural networks in speech recognition Noisy training for deep neural networks in speech recognition Noisy training for deep neural networks in speech recognition <P><B>Abstract</B><P>Deep neural networks (DNNs) have gained remarkable success in speech recognition, partially attributed to the flexibility of DNN models in learning complex patterns of speech signals. This flexibility, however, may lead to serious over-fitting and hence miserable performance degradation in adverse acoustic conditions such as those with high ambient noises. We propose a noisy training approach to tackle this problem: by injecting moderate noises into the training data intentionally and randomly, more generalizable DNN models can be learned. This &lsquo;noise injection&rsquo; technique, although known to the neural computation community already, has not been studied with DNNs which involve a highly complex objective function. The experiments presented in this paper confirm that the noisy training approach works well for the DNN model and can provide substantial performance improvement for DNN-based speech recognition.</P></P>"
        },
        {
          "rank": 31,
          "score": 0.6715477705001831,
          "doc_id": "JAKO202407064802797",
          "title": "딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가",
          "abstract": "급격한 기후 변화로 인한 어획량 감소와 양식 기술의 발전으로 양식 생산물 수요가 전세계적으로 계속해서 증가하고 있다. 그러나 이에 따른 무분별한 시설물 확장이 연안 생태계와 어족 자원 가격 책정에 악영향을 미치기 때문에, 주기적인 연안 환경 모니터링을 통한 양식시설물 관리가 필수적이다. 본 연구에서는 Sentinel-2 광학 영상과 다양한 딥러닝 기반 탐지 기법을 활용하여 경상남도의 패류 양식시설물 탐지 정확도를 분석하였다. DeepLabv3+, ResUNet++ 그리고 Attention U-Net 모델을 적용하였으며, 실험 결과 Attention U-Net 모델이 F1 score 0.8708, Intersection over Union 0.7708로 가장 우수한 탐지 성능을 보였다. 연구에서 제시한 탐지 방법론은 조류 및 부유 물질에 영향을 받는 양식시설물을 주기적으로 관측할 수 있고, 다양한 양식 품종에 적용할 수 있어 넓은 지역으로의 확장 가능성이 높다. 따라서 본 연구 방법을 통해 도출된 양식 시설물 정보는 향후 해양 공간 활용에 관한 정책 결정에 유용하게 활용할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202407064802797&target=NART&cn=JAKO202407064802797",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 딥러닝 기법을 이용한 연안 양식 시설 탐지의 정확도 평가 급격한 기후 변화로 인한 어획량 감소와 양식 기술의 발전으로 양식 생산물 수요가 전세계적으로 계속해서 증가하고 있다. 그러나 이에 따른 무분별한 시설물 확장이 연안 생태계와 어족 자원 가격 책정에 악영향을 미치기 때문에, 주기적인 연안 환경 모니터링을 통한 양식시설물 관리가 필수적이다. 본 연구에서는 Sentinel-2 광학 영상과 다양한 딥러닝 기반 탐지 기법을 활용하여 경상남도의 패류 양식시설물 탐지 정확도를 분석하였다. DeepLabv3+, ResUNet++ 그리고 Attention U-Net 모델을 적용하였으며, 실험 결과 Attention U-Net 모델이 F1 score 0.8708, Intersection over Union 0.7708로 가장 우수한 탐지 성능을 보였다. 연구에서 제시한 탐지 방법론은 조류 및 부유 물질에 영향을 받는 양식시설물을 주기적으로 관측할 수 있고, 다양한 양식 품종에 적용할 수 있어 넓은 지역으로의 확장 가능성이 높다. 따라서 본 연구 방법을 통해 도출된 양식 시설물 정보는 향후 해양 공간 활용에 관한 정책 결정에 유용하게 활용할 수 있을 것으로 기대된다."
        },
        {
          "rank": 32,
          "score": 0.6713460683822632,
          "doc_id": "JAKO201726163356540",
          "title": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델",
          "abstract": "본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201726163356540&target=NART&cn=JAKO201726163356540",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 특수일 분리와 예측요소 확장을 이용한 전력수요 예측 딥 러닝 모델 본 연구는 전력수요 패턴이 다른 평일과 특수일 데이터가 가지는 상관관계를 분석하여, 별도의 데이터 셋을 구축하고, 각 데이터 셋에 적합한 딥 러닝 네트워크를 이용하여, 전력수요예측 오차를 감소하는 방안을 제시하였다. 또한, 기본적인 전력수요 예측요소인 기상요소에 환경요소, 구분요소 등 다양한 예측요소를 추가하여 예측율을 향상하는 방안을 제시하였다. 전체데이터는 시계열 데이터 학습에 적합한 LSTM을 이용하여 전력수요예측을 하였으며, 특수일 데이터는 DNN을 이용하여 전력수요예측을 하였다. 실험결과 기상요소 이외의 예측요소 추가를 통해 예측율이 향상되었다. 전체 데이터 셋의 평균 RMSE는 LSTM이 0.2597이며, DNN이 0.5474로 LSTM이 우수한 예측율을 보였다. 특수일 데이터 셋의 평균 RMSE는 0.2201로 DNN이 LSTM보다 우수한 예측율을 보였다. 또한, 전체 데이터 셋의 LSTM의 MAPE는 2.74 %이며, 특수 일의 MAPE는 3.07 %를 나타냈다."
        },
        {
          "rank": 33,
          "score": 0.6706749200820923,
          "doc_id": "JAKO202433861648179",
          "title": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교",
          "abstract": "본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202433861648179&target=NART&cn=JAKO202433861648179",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 스켈레톤 데이터에 기반한 동작 분류: 고전적인 머신러닝과 딥러닝 모델 성능 비교 본 연구는 3D 스켈레톤 데이터를 활용하여 머신러닝 및 딥러닝 모델을 통해 동작 인식을 수행하고, 모델 간 분류 성능 차이를 비교 분석하였다. 데이터는 NTU RGB+D 데이터의 정면 촬영 데이터로 40명의 참가자가 수행한 60가지 동작을 분류하였다. 머신러닝 모델로는 선형판별분석(LDA), 다중 클래스 서포트 벡터 머신(SVM), 그리고 랜덤 포레스트(RF)가 있으며, 딥러닝 모델로는 RNN 기반의 HBRNN (hierarchical bidirectional RNN) 모델과 GCN 기반의 SGN (semantics-guided neural network) 모델을 적용하였다. 각 모델의 분류 성능을 평가하기 위해 40명의 참가자별로 교차 검증을 실시하였다. 분석 결과, 모델 간 성능 차이는 동작 유형에 크게 영향을 받았으며, 군집 분석을 통해 각 동작에 대한 분류 성능을 살펴본 결과, 인식이 비교적 쉬운 큰 동작에서는 머신러닝 모델과 딥러닝 모델 간의 성능 차이가 유의미하지 않았고, 비슷한 성능을 나타냈다. 반면, 손뼉치기나 손을 비비는 동작처럼 정면 촬영된 관절 좌표만으로 구별하기 어려운 동작의 경우, 딥러닝 모델이 머신러닝 모델보다 관절의 미세한 움직임을 인식하는 데 더 우수한 성능을 보였다."
        },
        {
          "rank": 34,
          "score": 0.6704009771347046,
          "doc_id": "ATN0035906971",
          "title": "딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구",
          "abstract": "In this research, we compare the explanatory power between linear regression model and deep-learning model when estimating stock returns. As predicted, the deep-learning model shows statistically significant improvement over linear regression model, although the improvement is not economically meaningful. We further investigate the effects of deep-learning model using different parameters and pre-processing. The results show that the predictive power of deep-learning model can be worse-off than that of linear model if it fails to select optimal parameters. Especially, it is important to choose adequate deep-learning parameters not to overfit the data, because the accounting data (which is at most quarterly) may not be sufficient enough for the deep model structure. Further, we show that the predictive power using researchers’ domain knowledge is sometimes better off than that relying simply on the deep-learning model. For instance, denomination with total assets brings better results than non-denomination. Another interesting finding is that winsorizing extreme values brings lower explanatory power when we use the deep-learning model. Such finding implies that, by removing extreme values, we may lose useful information in the parameter estimation. The results of this paper will help future research decide whether to utilize deep learning model or linear regression model",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0035906971&target=NART&cn=ATN0035906971",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 딥러닝 방법론을 사용한 주가예측에 대한 탐색적 연구 In this research, we compare the explanatory power between linear regression model and deep-learning model when estimating stock returns. As predicted, the deep-learning model shows statistically significant improvement over linear regression model, although the improvement is not economically meaningful. We further investigate the effects of deep-learning model using different parameters and pre-processing. The results show that the predictive power of deep-learning model can be worse-off than that of linear model if it fails to select optimal parameters. Especially, it is important to choose adequate deep-learning parameters not to overfit the data, because the accounting data (which is at most quarterly) may not be sufficient enough for the deep model structure. Further, we show that the predictive power using researchers’ domain knowledge is sometimes better off than that relying simply on the deep-learning model. For instance, denomination with total assets brings better results than non-denomination. Another interesting finding is that winsorizing extreme values brings lower explanatory power when we use the deep-learning model. Such finding implies that, by removing extreme values, we may lose useful information in the parameter estimation. The results of this paper will help future research decide whether to utilize deep learning model or linear regression model"
        },
        {
          "rank": 35,
          "score": 0.6702110767364502,
          "doc_id": "JAKO202121055483964",
          "title": "딥러닝을 통한 드론의 비정상 진동 예측",
          "abstract": "본 논문에서는 드론의 추락을 예방하기 위해 드론의 프로펠러와 연결된 모터로부터 진동 데이터를 수집하고 순환 신경망(recurrent neural network, RNN)과 long short term memory (LSTM)을 사용하여 드론의 비정상 진동을 예측하는 연구를 진행하였다. 드론의 비정상 진동 데이터를 수집하기 위해 드론의 프로펠러와 연결된 모터에 진동 센서를 부착하여 정상, 바(bar) 손상, 로터(rotor) 손상, 축 휨에 대한 진동 데이터를 수집하고 LSTM과 RNN을 통해 비정상 진동을 예측한 결과의 평균 제곱근 오차 (root mean square error, RMSE) 값을 비교분석 하였다. 시뮬레이션 비교 결과, RNN과 LSTM을 통해 예측한 결과 모두 비정상 진동 패턴을 매우 정확하게 예측하는 것을 확인하였으며 LSTM을 통해 예측한 진동이 RNN을 통해 예측한 진동보다 RMSE값이 평균 15.4% 낮은 것을 확인하였다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202121055483964&target=NART&cn=JAKO202121055483964",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝을 통한 드론의 비정상 진동 예측 딥러닝을 통한 드론의 비정상 진동 예측 딥러닝을 통한 드론의 비정상 진동 예측 본 논문에서는 드론의 추락을 예방하기 위해 드론의 프로펠러와 연결된 모터로부터 진동 데이터를 수집하고 순환 신경망(recurrent neural network, RNN)과 long short term memory (LSTM)을 사용하여 드론의 비정상 진동을 예측하는 연구를 진행하였다. 드론의 비정상 진동 데이터를 수집하기 위해 드론의 프로펠러와 연결된 모터에 진동 센서를 부착하여 정상, 바(bar) 손상, 로터(rotor) 손상, 축 휨에 대한 진동 데이터를 수집하고 LSTM과 RNN을 통해 비정상 진동을 예측한 결과의 평균 제곱근 오차 (root mean square error, RMSE) 값을 비교분석 하였다. 시뮬레이션 비교 결과, RNN과 LSTM을 통해 예측한 결과 모두 비정상 진동 패턴을 매우 정확하게 예측하는 것을 확인하였으며 LSTM을 통해 예측한 진동이 RNN을 통해 예측한 진동보다 RMSE값이 평균 15.4% 낮은 것을 확인하였다."
        },
        {
          "rank": 36,
          "score": 0.6686438322067261,
          "doc_id": "NART132071160",
          "title": "Integrating machine learning and deep learning for enhanced supplier risk prediction",
          "abstract": "<P>The importance of anticipating and preventing disruptions is underscored by the increased operational complexity and vulnerability caused by advancements in supply chain management (SCM). This has spurred interest in integrating machine learning (ML) and deep learning (DL) into supply chain risk management (SCRM). In this paper, we introduce a tailored method using ML and DL to improve SCRM by predicting supplier failures, thus boosting efficiency and resilience in SC operations. Our method involves five phases focused on classifying and predicting supplier failures in non-conforming deliveries. This involves forecasting failure quantities and estimating total disruption costs. Initially, data from an automotive company is selected, and appropriate potential features and algorithms are selected, performance metric aligns with case study objectives, facilitating method evaluation are used such as: Precision, recall, F1-score, and accuracy metrics assess classification models, while Mean Squared Error (MSE) is used for regression tasks. Finally, an experimental design optimizes models, assessing success rates of various algorithms and their parameters within the chosen feature space. Experimental results underscore the success of our methodology in model development. In the classification task, the Random Forest (RF) classifier achieved 86% accuracy. When combined with the Gradient Boosting classifier, the ensemble exhibited enhanced accuracy, highlighting the complementary strengths of both algorithms and their synergistic impact, surpassing the performance of RF, Support Vector Regression (SVR), k-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). Noteworthy is the performance in regression tasks, where Linear Regression, ANN, and RF Regressor displayed exceptionally low MSE compared to other models.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART132071160&target=NART&cn=NART132071160",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Integrating machine learning and deep learning for enhanced supplier risk prediction Integrating machine learning and deep learning for enhanced supplier risk prediction Integrating machine learning and deep learning for enhanced supplier risk prediction <P>The importance of anticipating and preventing disruptions is underscored by the increased operational complexity and vulnerability caused by advancements in supply chain management (SCM). This has spurred interest in integrating machine learning (ML) and deep learning (DL) into supply chain risk management (SCRM). In this paper, we introduce a tailored method using ML and DL to improve SCRM by predicting supplier failures, thus boosting efficiency and resilience in SC operations. Our method involves five phases focused on classifying and predicting supplier failures in non-conforming deliveries. This involves forecasting failure quantities and estimating total disruption costs. Initially, data from an automotive company is selected, and appropriate potential features and algorithms are selected, performance metric aligns with case study objectives, facilitating method evaluation are used such as: Precision, recall, F1-score, and accuracy metrics assess classification models, while Mean Squared Error (MSE) is used for regression tasks. Finally, an experimental design optimizes models, assessing success rates of various algorithms and their parameters within the chosen feature space. Experimental results underscore the success of our methodology in model development. In the classification task, the Random Forest (RF) classifier achieved 86% accuracy. When combined with the Gradient Boosting classifier, the ensemble exhibited enhanced accuracy, highlighting the complementary strengths of both algorithms and their synergistic impact, surpassing the performance of RF, Support Vector Regression (SVR), k-Nearest Neighbors (KNN), and Artificial Neural Network (ANN). Noteworthy is the performance in regression tasks, where Linear Regression, ANN, and RF Regressor displayed exceptionally low MSE compared to other models.</P>"
        },
        {
          "rank": 37,
          "score": 0.6684135794639587,
          "doc_id": "JAKO201718054814596",
          "title": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석",
          "abstract": "딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201718054814596&target=NART&cn=JAKO201718054814596",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 스파크 기반 딥 러닝 분산 프레임워크 성능 비교 분석 딥 러닝(Deep learning)은 기존 인공 신경망 내 계층 수를 증가시킴과 동시에 효과적인 학습 방법론을 제시함으로써 객체/음성 인식 및 자연어 처리 등 고수준 문제 해결에 있어 괄목할만한 성과를 보이고 있다. 그러나 학습에 필요한 시간과 리소스가 크다는 한계를 지니고 있어, 이를 줄이기 위한 연구가 활발히 진행되고 있다. 본 연구에서는 아파치 스파크 기반 클러스터 컴퓨팅 프레임워크 상에서 딥 러닝을 분산화하는 두 가지 툴(DeepSpark, SparkNet)의 성능을 학습 정확도와 속도 측면에서 측정하고 분석하였다. CIFAR-10/CIFAR-100 데이터를 사용한 실험에서 SparkNet은 학습 과정의 정확도 변동 폭이 적은 반면 DeepSpark는 학습 초기 정확도는 변동 폭이 크지만 점차 변동 폭이 줄어들면서 SparkNet 대비 약 15% 높은 정확도를 보였고, 조건에 따라 단일 머신보다도 높은 정확도로 보다 빠르게 수렴하는 양상을 확인할 수 있었다."
        },
        {
          "rank": 38,
          "score": 0.6677590608596802,
          "doc_id": "ATN0051728135",
          "title": "딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구",
          "abstract": "하천 수위 예측의 주요 목적 중 하나는 홍수예경보 발령을 위한 기준으로 활용하는 것이다. 본 연구에서는 딥러닝 기반의 하천 수위 예측 모델을 홍수예경보 측면에서 효과적으로 활용하기 위해 학습데이터를 최적화하고, 딥러닝 모델의 정확도 향상을 평가하기 위해 딥러닝 모델의 자동 설계 및 최적화를 지원하는 AutoKeras를 활용하여 인위적인 요인을 배제한 모델을 구축하였다. 한탄강 상류유역을 대상지역으로 선정하고, 3개의 수위관측소와 유역평균강우 데이터를 구축하였고, 구축된 데이터를 이용하여 수위 변화 여부와 관계없이 강우가 발생한 모든 학습 데이터 셋을 사용한 모델(Model 1)과 일정 수준 이상의 수위 상승 변화가 있는 학습데이터 셋을 사용한 딥러닝 모델(Model 2)을 개발하여 한탄강 상류 한탄대교의 수위 및 홍수 예측 성능을 평가하였다. 실시간 하천 홍수예측 결과, 시계열 수위 예측에서 Model 1이 더 많은 데이터를 활용함으로써 상관계수와 평균제곱근오차(RMSE)에서 다소 우수한 성능을 보였다. 반면, Model 2는 홍수 예측에서 재현율(recall), F1-score, 임계성공지수(CSI) 등의 지표에서 더 뛰어난 성과를 보였다. 본 결과는 학습데이터의 특성과 구성 방식이 딥러닝 모델의 예측 능력에 큰 영향을 미친다는 것을 보여주며, 홍수와 같은 특정 사건을 예측하려면 수위 상승과 같은 핵심 요인 위주의 데이터를 더 집중적으로 학습시킬 필요가 있음을 시사한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0051728135&target=NART&cn=ATN0051728135",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구 딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구 딥러닝 기반 실시간 하천 홍수 예측 정확도 개선을 위한 학습데이터 최적화 연구 하천 수위 예측의 주요 목적 중 하나는 홍수예경보 발령을 위한 기준으로 활용하는 것이다. 본 연구에서는 딥러닝 기반의 하천 수위 예측 모델을 홍수예경보 측면에서 효과적으로 활용하기 위해 학습데이터를 최적화하고, 딥러닝 모델의 정확도 향상을 평가하기 위해 딥러닝 모델의 자동 설계 및 최적화를 지원하는 AutoKeras를 활용하여 인위적인 요인을 배제한 모델을 구축하였다. 한탄강 상류유역을 대상지역으로 선정하고, 3개의 수위관측소와 유역평균강우 데이터를 구축하였고, 구축된 데이터를 이용하여 수위 변화 여부와 관계없이 강우가 발생한 모든 학습 데이터 셋을 사용한 모델(Model 1)과 일정 수준 이상의 수위 상승 변화가 있는 학습데이터 셋을 사용한 딥러닝 모델(Model 2)을 개발하여 한탄강 상류 한탄대교의 수위 및 홍수 예측 성능을 평가하였다. 실시간 하천 홍수예측 결과, 시계열 수위 예측에서 Model 1이 더 많은 데이터를 활용함으로써 상관계수와 평균제곱근오차(RMSE)에서 다소 우수한 성능을 보였다. 반면, Model 2는 홍수 예측에서 재현율(recall), F1-score, 임계성공지수(CSI) 등의 지표에서 더 뛰어난 성과를 보였다. 본 결과는 학습데이터의 특성과 구성 방식이 딥러닝 모델의 예측 능력에 큰 영향을 미친다는 것을 보여주며, 홍수와 같은 특정 사건을 예측하려면 수위 상승과 같은 핵심 요인 위주의 데이터를 더 집중적으로 학습시킬 필요가 있음을 시사한다."
        },
        {
          "rank": 39,
          "score": 0.6664327383041382,
          "doc_id": "NART118947969",
          "title": "Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction",
          "abstract": "<P>Recent growth in crop genomic and trait data have opened opportunities for the application of novel approaches to accelerate crop improvement. Machine learning and deep learning are at the forefront of prediction-based data analysis. However, few approaches for genotype to phenotype prediction compare machine learning with deep learning and further interpret the models that support the predictions. This study uses genome wide molecular markers and traits across 1110 soybean individuals to develop accurate prediction models. For 13/14 sets of predictions, XGBoost or random forest outperformed deep learning models in prediction performance. Top ranked SNPs by F-score were identified from XGBoost, and with further investigation found overlap with significantly associated loci identified from GWAS and previous literature. Feature importance rankings were used to reduce marker input by up to 90%, and subsequent models maintained or improved their prediction performance. These findings support interpretable machine learning as an approach for genomic based prediction of traits in soybean and other crops.</P><P><B>Supplementary Information</B></P><P>The online version contains supplementary material available at 10.1186/s12870-022-03559-z.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART118947969&target=NART&cn=NART118947969",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction Machine learning models outperform deep learning models, provide interpretation and facilitate feature selection for soybean trait prediction <P>Recent growth in crop genomic and trait data have opened opportunities for the application of novel approaches to accelerate crop improvement. Machine learning and deep learning are at the forefront of prediction-based data analysis. However, few approaches for genotype to phenotype prediction compare machine learning with deep learning and further interpret the models that support the predictions. This study uses genome wide molecular markers and traits across 1110 soybean individuals to develop accurate prediction models. For 13/14 sets of predictions, XGBoost or random forest outperformed deep learning models in prediction performance. Top ranked SNPs by F-score were identified from XGBoost, and with further investigation found overlap with significantly associated loci identified from GWAS and previous literature. Feature importance rankings were used to reduce marker input by up to 90%, and subsequent models maintained or improved their prediction performance. These findings support interpretable machine learning as an approach for genomic based prediction of traits in soybean and other crops.</P><P><B>Supplementary Information</B></P><P>The online version contains supplementary material available at 10.1186/s12870-022-03559-z.</P>"
        },
        {
          "rank": 40,
          "score": 0.6662718057632446,
          "doc_id": "DIKO0014912357",
          "title": "딥러닝 기반 기술융합 예측 방법론",
          "abstract": "오늘날 기업들은 전략적인 관점에서 기술변화를 예측하고, 이를 활용한 기술전략 수립이 반드시 필요하다고 요구된다. 특히 기술융합 현상은 기술혁신을 주도하고 시장과 산업의 변화를 이끈 다고 할 수 있기 때문에, 기술융합을 정량적으로 측정하고 관측하기 위한 연구가 다수 이루어졌다. 선행 연구에서는 계량서지분석을 이용해 기술 구조를 분석하고, 기술네트워크를 통해 기술융합 현상을 분석하는데 그쳤다. 본 연구에서는 미래의 기술융합 구조를 예상할 수 있는 예측 방법론을 제시한다. 링크 예측은 현재 시점의 네트워크를 통해 미래 시점의 네트워크에 추가되거나 제거 될 링크를 예측하는 문제를 의미한다. 본 연구에서는 기술네트워크 형태로 표현 된 기술시스템에 링크 예측을 적용하여 미래 기술융합 관계에 대해 예측하는 것을 목적으로 한다. 특히 딥러닝 기법을 이용한 학습 기반 링크 예측을 수행한다는 특징이 있다. 기존에 링크 예측을 적용해 기술융합 예측을 시도한 연구가 일부 있었으나, 네트워크 내 이웃관계에 의한 토폴로지 유사도를 의사결정 척도로 이용했다는 점에서 한계에 머물렀다. 기술융합 예측이라는 도메인 속성이 고려되지 않았다는 점인데, 본 연구에서는 기존 링크 예측에서 보편적으로 사용되었던 이웃관계에 의한 네트워크 토폴로지 유사도와 인용관계에 의한 유사도를 함께 고려한 기술융합 예측모델을 제시한다. 또한 학습 기반 링크 예측에서는 기술네트워크에 대한 정보를 노드 쌍 조합 단위의 레코드를 갖는 정형화 된 데이터 구조로 변화해 이진분류 문제로 기술융합 예측 문제를 재정의 하였기 때문에, 다양한 교사학습 기반의 기계학습 알고리즘 적용이 가능하다. 본 연구에서는 분류 성능이 우수해 최근 주목받고 있는 딥러닝 기법의 DNN 알고리즘을 사용하여 학습 기반 링크 예측을 수행하고, SVM, 로지스틱회귀(logistic regression), 랜덤포레스트(random foreset)와 같은 보편적인 분류 알고리즘을 비교한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0014912357&target=NART&cn=DIKO0014912357",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 기술융합 예측 방법론 딥러닝 기반 기술융합 예측 방법론 딥러닝 기반 기술융합 예측 방법론 오늘날 기업들은 전략적인 관점에서 기술변화를 예측하고, 이를 활용한 기술전략 수립이 반드시 필요하다고 요구된다. 특히 기술융합 현상은 기술혁신을 주도하고 시장과 산업의 변화를 이끈 다고 할 수 있기 때문에, 기술융합을 정량적으로 측정하고 관측하기 위한 연구가 다수 이루어졌다. 선행 연구에서는 계량서지분석을 이용해 기술 구조를 분석하고, 기술네트워크를 통해 기술융합 현상을 분석하는데 그쳤다. 본 연구에서는 미래의 기술융합 구조를 예상할 수 있는 예측 방법론을 제시한다. 링크 예측은 현재 시점의 네트워크를 통해 미래 시점의 네트워크에 추가되거나 제거 될 링크를 예측하는 문제를 의미한다. 본 연구에서는 기술네트워크 형태로 표현 된 기술시스템에 링크 예측을 적용하여 미래 기술융합 관계에 대해 예측하는 것을 목적으로 한다. 특히 딥러닝 기법을 이용한 학습 기반 링크 예측을 수행한다는 특징이 있다. 기존에 링크 예측을 적용해 기술융합 예측을 시도한 연구가 일부 있었으나, 네트워크 내 이웃관계에 의한 토폴로지 유사도를 의사결정 척도로 이용했다는 점에서 한계에 머물렀다. 기술융합 예측이라는 도메인 속성이 고려되지 않았다는 점인데, 본 연구에서는 기존 링크 예측에서 보편적으로 사용되었던 이웃관계에 의한 네트워크 토폴로지 유사도와 인용관계에 의한 유사도를 함께 고려한 기술융합 예측모델을 제시한다. 또한 학습 기반 링크 예측에서는 기술네트워크에 대한 정보를 노드 쌍 조합 단위의 레코드를 갖는 정형화 된 데이터 구조로 변화해 이진분류 문제로 기술융합 예측 문제를 재정의 하였기 때문에, 다양한 교사학습 기반의 기계학습 알고리즘 적용이 가능하다. 본 연구에서는 분류 성능이 우수해 최근 주목받고 있는 딥러닝 기법의 DNN 알고리즘을 사용하여 학습 기반 링크 예측을 수행하고, SVM, 로지스틱회귀(logistic regression), 랜덤포레스트(random foreset)와 같은 보편적인 분류 알고리즘을 비교한다."
        },
        {
          "rank": 41,
          "score": 0.6658015847206116,
          "doc_id": "ART002897789",
          "title": "Modified Artificial Neural Networks and Support Vector Regression to Predict Lateral Pressure Exerted by Fresh Concrete on Formwork",
          "abstract": "In this study, a modified Artificial Neural Network (ANN) and Support Vector Regression (SVR) with three different optimization algorithms (Genetic, Salp Swarm and Grasshopper) were used to establish an accurate and easy-to-use module to predict the lateral pressure exerted by fresh concrete on formwork based on three main inputs, namely mix proportions (cement content, w/c, coarse aggregates, fine aggregates and admixture agent), casting rate, and height of specimens. The data have been obtained from 30 previously piloted experimental studies (resulted 113 samples). Achieved results for the model including all the input data provide the most excellent prediction of the exerted lateral pressure. Additionally, having different magnitudes of powder volume, aggregate volume and fluid content in the mix exposes different rising and descending in the lateral pressure outcomes. The results indicate that each model has its own advantages and disadvantages; however, the root mean square error values of the SVR models are lower than that of the ANN model. Additionally, the proposed models have been validated and all of them can accurately predict the lateral pressure of fresh concrete on the panel of the formwork.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ART002897789&target=NART&cn=ART002897789",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Modified Artificial Neural Networks and Support Vector Regression to Predict Lateral Pressure Exerted by Fresh Concrete on Formwork Modified Artificial Neural Networks and Support Vector Regression to Predict Lateral Pressure Exerted by Fresh Concrete on Formwork Modified Artificial Neural Networks and Support Vector Regression to Predict Lateral Pressure Exerted by Fresh Concrete on Formwork In this study, a modified Artificial Neural Network (ANN) and Support Vector Regression (SVR) with three different optimization algorithms (Genetic, Salp Swarm and Grasshopper) were used to establish an accurate and easy-to-use module to predict the lateral pressure exerted by fresh concrete on formwork based on three main inputs, namely mix proportions (cement content, w/c, coarse aggregates, fine aggregates and admixture agent), casting rate, and height of specimens. The data have been obtained from 30 previously piloted experimental studies (resulted 113 samples). Achieved results for the model including all the input data provide the most excellent prediction of the exerted lateral pressure. Additionally, having different magnitudes of powder volume, aggregate volume and fluid content in the mix exposes different rising and descending in the lateral pressure outcomes. The results indicate that each model has its own advantages and disadvantages; however, the root mean square error values of the SVR models are lower than that of the ANN model. Additionally, the proposed models have been validated and all of them can accurately predict the lateral pressure of fresh concrete on the panel of the formwork."
        },
        {
          "rank": 42,
          "score": 0.6657768487930298,
          "doc_id": "JAKO202108848920380",
          "title": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구",
          "abstract": "The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202108848920380&target=NART&cn=JAKO202108848920380",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 딥러닝과 앙상블 머신러닝 모형의 하천 탁도 예측 특성 비교 연구 The increased turbidity in rivers during flood events has various effects on water environmental management, including drinking water supply systems. Thus, prediction of turbid water is essential for water environmental management. Recently, various advanced machine learning algorithms have been increasingly used in water environmental management. Ensemble machine learning algorithms such as random forest (RF) and gradient boosting decision tree (GBDT) are some of the most popular machine learning algorithms used for water environmental management, along with deep learning algorithms such as recurrent neural networks. In this study GBDT, an ensemble machine learning algorithm, and gated recurrent unit (GRU), a recurrent neural networks algorithm, are used for model development to predict turbidity in a river. The observation frequencies of input data used for the model were 2, 4, 8, 24, 48, 120 and 168 h. The root-mean-square error-observations standard deviation ratio (RSR) of GRU and GBDT ranges between 0.182~0.766 and 0.400~0.683, respectively. Both models show similar prediction accuracy with RSR of 0.682 for GRU and 0.683 for GBDT. The GRU shows better prediction accuracy when the observation frequency is relatively short (i.e., 2, 4, and 8 h) where GBDT shows better prediction accuracy when the observation frequency is relatively long (i.e. 48, 120, 160 h). The results suggest that the characteristics of input data should be considered to develop an appropriate model to predict turbidity."
        },
        {
          "rank": 43,
          "score": 0.6652792096138,
          "doc_id": "JAKO201813164518771",
          "title": "딥 러닝 및 서포트 벡터 머신기반 센서 고장 검출 기법",
          "abstract": "최근 산업현장에서 기계의 자동화가 크게 가속화됨에 따라 자동화 기계의 관리 및 유지보수에 대한 중요성이 갈수록 커지고 있다. 자동화 기계에 부착된 센서의 고장이 발생할 경우 기계가 오동작함으로써 공정라인 운용에 막대한 피해가 발생할 수 있다. 이를 막기 위해 센서의 상태를 모니터링하고 고장의 진단 및 분류를 하는 것이 필요하다. 본 논문에서는 센서에서 발생하는 대표적인 고장 유형인 erratic fault, drift fault, hard-over fault, spike fault, stuck fault를 기계학습 알고리즘인 SVM과 CNN을 적용하여 검출하고 분류하였다. SVM의 학습 및 테스트를 위해 데이터 샘플들로부터 시간영역 통계 특징들을 추출하고 최적의 특징을 찾기 위해 유전 알고리즘(genetic algorithm)을 적용하였다. Multi-class를 분류하기 위해 multi-layer SVM을 구성하여 센서 고장을 분류하였다. CNN에 대해서는 데이터 샘플들을 사용하여 학습시키고 성능을 높이기 위해 앙상블 기법을 적용하였다. 시뮬레이션 결과를 통해 유전 알고리즘에 의해 선별된 특징들을 사용한 SVM의 분류 결과는 모든 특징이 사용된 SVM 분류기 보다는 성능이 향상되었으나 전반적으로 CNN의 성능이 SVM보다 우수한 것을 확인할 수 있었다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201813164518771&target=NART&cn=JAKO201813164518771",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝 및 서포트 벡터 머신기반 센서 고장 검출 기법 딥 러닝 및 서포트 벡터 머신기반 센서 고장 검출 기법 딥 러닝 및 서포트 벡터 머신기반 센서 고장 검출 기법 최근 산업현장에서 기계의 자동화가 크게 가속화됨에 따라 자동화 기계의 관리 및 유지보수에 대한 중요성이 갈수록 커지고 있다. 자동화 기계에 부착된 센서의 고장이 발생할 경우 기계가 오동작함으로써 공정라인 운용에 막대한 피해가 발생할 수 있다. 이를 막기 위해 센서의 상태를 모니터링하고 고장의 진단 및 분류를 하는 것이 필요하다. 본 논문에서는 센서에서 발생하는 대표적인 고장 유형인 erratic fault, drift fault, hard-over fault, spike fault, stuck fault를 기계학습 알고리즘인 SVM과 CNN을 적용하여 검출하고 분류하였다. SVM의 학습 및 테스트를 위해 데이터 샘플들로부터 시간영역 통계 특징들을 추출하고 최적의 특징을 찾기 위해 유전 알고리즘(genetic algorithm)을 적용하였다. Multi-class를 분류하기 위해 multi-layer SVM을 구성하여 센서 고장을 분류하였다. CNN에 대해서는 데이터 샘플들을 사용하여 학습시키고 성능을 높이기 위해 앙상블 기법을 적용하였다. 시뮬레이션 결과를 통해 유전 알고리즘에 의해 선별된 특징들을 사용한 SVM의 분류 결과는 모든 특징이 사용된 SVM 분류기 보다는 성능이 향상되었으나 전반적으로 CNN의 성능이 SVM보다 우수한 것을 확인할 수 있었다."
        },
        {
          "rank": 44,
          "score": 0.6650307178497314,
          "doc_id": "JAKO201223052004277",
          "title": "회사채 신용등급 예측을 위한 SVM 앙상블학습",
          "abstract": "회사채 신용등급은 투자자의 입장에서는 수익률 결정의 중요한 요소이며 기업의 입장에서는 자본비용 및 기업 가치와 관련된 중요한 재무의사결정사항으로 정교한 신용등급 예측 모형의 개발은 재무 및 회계 분야에서 오랫동안 전통적인 연구 주제가 되어왔다. 그러나, 회사채 신용등급 예측 모형의 성과와 관련된 가장 중요한 문제는 등급별 데이터의 불균형 문제이다. 예측 문제에 있어서 데이터 불균형(Data imbalance) 은 사용되는 표본이 특정 범주에 편중되었을 때 나타난다. 데이터 불균형이 심화됨에 따라 범주 사이의 분류경계영역이 왜곡되므로 분류자의 학습성과가 저하되게 된다. 본 연구에서는 데이터 불균형 문제가 존재하는 다분류 문제를 효과적으로 해결하기 위한 다분류 기하평균 부스팅 기법 (Multiclass Geometric Mean-based Boosting MGM-Boost)을 제안하고자 한다. MGM-Boost 알고리즘은 부스팅 알고리즘에 기하평균 개념을 도입한 것으로 오분류된 표본에 대한 학습을 강화할 수 있으며 불균형 분포를 보이는 각 범주의 예측정확도를 동시에 고려한 학습이 가능하다는 장점이 있다. 회사채 신용등급 예측문제를 활용하여 MGM-Boost의 성과를 검증한 결과 SVM 및 AdaBoost 기법과 비교하여 통계적으로 유의적인 성과개선 효과를 보여주었으며 데이터 불균형 하에서도 벤치마킹 모형과 비교하여 견고한 학습성과를 나타냈다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201223052004277&target=NART&cn=JAKO201223052004277",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "회사채 신용등급 예측을 위한 SVM 앙상블학습 회사채 신용등급 예측을 위한 SVM 앙상블학습 회사채 신용등급 예측을 위한 SVM 앙상블학습 회사채 신용등급은 투자자의 입장에서는 수익률 결정의 중요한 요소이며 기업의 입장에서는 자본비용 및 기업 가치와 관련된 중요한 재무의사결정사항으로 정교한 신용등급 예측 모형의 개발은 재무 및 회계 분야에서 오랫동안 전통적인 연구 주제가 되어왔다. 그러나, 회사채 신용등급 예측 모형의 성과와 관련된 가장 중요한 문제는 등급별 데이터의 불균형 문제이다. 예측 문제에 있어서 데이터 불균형(Data imbalance) 은 사용되는 표본이 특정 범주에 편중되었을 때 나타난다. 데이터 불균형이 심화됨에 따라 범주 사이의 분류경계영역이 왜곡되므로 분류자의 학습성과가 저하되게 된다. 본 연구에서는 데이터 불균형 문제가 존재하는 다분류 문제를 효과적으로 해결하기 위한 다분류 기하평균 부스팅 기법 (Multiclass Geometric Mean-based Boosting MGM-Boost)을 제안하고자 한다. MGM-Boost 알고리즘은 부스팅 알고리즘에 기하평균 개념을 도입한 것으로 오분류된 표본에 대한 학습을 강화할 수 있으며 불균형 분포를 보이는 각 범주의 예측정확도를 동시에 고려한 학습이 가능하다는 장점이 있다. 회사채 신용등급 예측문제를 활용하여 MGM-Boost의 성과를 검증한 결과 SVM 및 AdaBoost 기법과 비교하여 통계적으로 유의적인 성과개선 효과를 보여주었으며 데이터 불균형 하에서도 벤치마킹 모형과 비교하여 견고한 학습성과를 나타냈다."
        },
        {
          "rank": 45,
          "score": 0.6644520163536072,
          "doc_id": "JAKO202201253148351",
          "title": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰",
          "abstract": "위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO202201253148351&target=NART&cn=JAKO202201253148351",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 딥러닝 기반 레이더 간섭 위상 언래핑 기술 고찰 위상 언래핑은 위성레이더 간섭기법의 필수적인 자료처리 절차다. 이에 따라 비 딥러닝 기반 언래핑 기법이 다수 개발되었으며 최근에는 딥러닝 기반 언래핑 기법이 제안되고 있다. 본 논문에서는 딥러닝 기반 위성레이더 언래핑 기법을 1) 언래핑된 위상의 예측 방법, 2) 위상 언래핑을 위한 딥러닝 모델의 구조 그리고 3) 학습데이터 제작 방법의 측면에서 최근 연구 동향을 소개하였다. 언래핑된 위상을 예측하는 방법은 모호 정수 분류방법, 위상 단절 구간 탐지 방법, 위상 예측 방법, 딥러닝과 전통적인 언래핑 기법의 연계 방법에 따라 다시 세분화하여 연구 동향을 나타냈다. 일반적으로 활용되는 딥러닝 모델 구조의 특징과 전체 위상 정보를 파악하기 위한 모델 최적화 방법에 대한 연구 사례를 소개하였다. 또한 학습데이터 제작 방법은 주로 위상 변이 제작과 노이즈 시뮬레이션 방법으로 구분하여 연구 동향을 정리하였으며 추후 발전 방향을 제시하였다. 본 논문이 추후 국내의 딥러닝 기반 위상 언래핑 연구의 발전 방향을 모색하는 데에 필요한 기반 자료로 활용되기를 기대한다."
        },
        {
          "rank": 46,
          "score": 0.6642708778381348,
          "doc_id": "JAKO201911338887557",
          "title": "잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법",
          "abstract": "본 논문에서는 잡음 환경에서 효과적인 음성 인식을 위하여 DNN(Deep Neural Network) 기반의 잡음 오염 함수 예측을 이용한 음향 모델 적응 기법을 제안한다. 깨끗한 음성과 잡음 정보를 입력으로 하고 오염된 음성에 대한 특징 벡터를 출력으로 하는 DNN을 학습하여 비선형 관계를 갖는 잡음 오염 함수를 예측한다. 예측된 잡음 오염 함수를 음향모델의 평균 벡터에 적용하여 잡음 환경에 적응된 음향 모델을 생성한다. Aurora 2.0 데이터를 이용한 음성 인식 성능 평가에서 본 논문에서 제안한 모델 적응 기법이 기존의 전처리, 모델 적응 기법에 비해 일치, 불일치 잡음 환경에서 모두 평균적으로 우수한 성능을 나타낸다. 특히 불일치 잡음 환경에서 평균 오류율이 15.87 %의 상대 향상률을 나타낸다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=JAKO201911338887557&target=NART&cn=JAKO201911338887557",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 잡음 환경 음성 인식을 위한 심층 신경망 기반의 잡음 오염 함수 예측을 통한 음향 모델 적응 기법 본 논문에서는 잡음 환경에서 효과적인 음성 인식을 위하여 DNN(Deep Neural Network) 기반의 잡음 오염 함수 예측을 이용한 음향 모델 적응 기법을 제안한다. 깨끗한 음성과 잡음 정보를 입력으로 하고 오염된 음성에 대한 특징 벡터를 출력으로 하는 DNN을 학습하여 비선형 관계를 갖는 잡음 오염 함수를 예측한다. 예측된 잡음 오염 함수를 음향모델의 평균 벡터에 적용하여 잡음 환경에 적응된 음향 모델을 생성한다. Aurora 2.0 데이터를 이용한 음성 인식 성능 평가에서 본 논문에서 제안한 모델 적응 기법이 기존의 전처리, 모델 적응 기법에 비해 일치, 불일치 잡음 환경에서 모두 평균적으로 우수한 성능을 나타낸다. 특히 불일치 잡음 환경에서 평균 오류율이 15.87 %의 상대 향상률을 나타낸다."
        },
        {
          "rank": 47,
          "score": 0.6639562845230103,
          "doc_id": "DIKO0015644673",
          "title": "M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지",
          "abstract": "해상 교통량의 증가로 인해 해상 선박관리의 필요성이 늘어남에 따라 선박을 탐지하기 위한 연구들이 꾸준히 수행되어왔다. 특히 위성레이더 영상은 시간과 기후에 영향을 받지 않고 촬영할 수 있다는 장점으로 인해 선박탐지를 위한 많은 연구에서 활용되어왔다. 최근에는 딥러닝 기법의 발전으로 인해 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구들이 꾸준히 수행되고 있다. 그런데 위성레이더 영상은 값의 분포범위가 매우 넓고, 많은 스펙클 노이즈가 존재한다. 이러한 요소들은 딥러닝 모델의 학습에 부정적인 영향을 끼칠 수 있으므로 전처리를 통해 해당 요소들을 저감해줄 필요가 있다. 본 연구에서는 전처리된 위성레이더 영상으로부터 딥러닝 선박탐지를 수행하고, 영상의 전처리가 딥러닝 선박탐지에 미치는 요소를 비교분석 하고자 한다.&amp;#xD; 본 연구를 위해 TerraSAR-X와 COSMO-SkyMed 위성레이더 영상을 이용했다. 영상을 딥러닝 학습에 이용하기 전에 먼저 총 세 가지 다른 방법으로 전처리를 수행했다. 첫 번째는 위성레이더 영상에서 강도 값만을 추출한 강도 영상을 생성하는 방법이다. 강도 영상은 값의 범위가 매우 넓을 뿐만 아니라 많은 스펙클 노이즈를 가지고 있다. 두 번째는 강도영상에서 값의 단위를 데시벨로 변환한 데시벨 영상을 생성하는 방법이다. 데시벨 영상은 강도영상과 마찬가지로 많은 스펙클 노이즈를 가지고 있으나 값의 범위가 줄어들어, 더 안정적인 학습을 할 수 있다. 세 번째는 본 연구에서 제안하는 위성레이더 전처리방법으로써, 강도차분과 거칠기영상을 생성하는 방법이다. 두 영상은 중간값 필터링을 이용해 스펙클 노이즈를 줄이고, 값의 분포 대역을 좁힘으로써 빠른 학습이 가능하다.&amp;#xD; 각 전처리된 위성레이더 영상을 이용해 딥러닝 학습을 하기 위해 본 연구에서는 M2Det 객체탐지 모델을 사용했다. 객체탐지 모델을 학습시킨 뒤 테스트 영상을 이용해 선박탐지를 수행했으며, 테스트 결과는 정밀도(Precision), 재현율(Recall)을 이용해 나타냈으며, 두 지수를 하나의 값으로 표현하기 위해 AP(Average Precision)와 F1 점수(F1-score)를 이용해 나타냈다. 각 영상의 정밀도, 재현율, AP, F1 점수는 강도 영상 93.18%, 91.11%, 89.78%, 92.13%, 데시벨 영상 94.16%, 94.16%, 92.34%, 94.16%, 강도차분과 거칠기 영상 97.40%, 94.94%, 95.55%, 96.15%로 계산되었다. 강도 영상을 이용한 경우 미탐지와 오탐지 선박이 많았으며, 전처리된 영상을 이용한 경우 강도 영상에 비해 미탐지와 오탐지 선박이 줄어든 것을 확인할 수 있었다. 데시벨 영상과 강도차분, 거칠기 영상의 결과를 비교했을 때, 두 영상의 오탐지율은 유사했다. 하지만 강도차분, 거칠기 영상을 이용했을 때 강도 영상에 비해 미탐지 선박의 비율이 4% 줄어든 것을 확인할 수 있었다. 이 결과를 통해 위성레이더 영상을 전처리함으로써 딥러닝 학습을 돕고 선박탐지 결과를 향상시킬 수 있다는 것을 알 수 있다. 본 연구결과는 향후 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구의 발전에 이바지할 수 있을 것으로 기대된다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0015644673&target=NART&cn=DIKO0015644673",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 M2Det 딥러닝 모델을 이용한 X밴드 SAR 영상으로부터 선박탐지 해상 교통량의 증가로 인해 해상 선박관리의 필요성이 늘어남에 따라 선박을 탐지하기 위한 연구들이 꾸준히 수행되어왔다. 특히 위성레이더 영상은 시간과 기후에 영향을 받지 않고 촬영할 수 있다는 장점으로 인해 선박탐지를 위한 많은 연구에서 활용되어왔다. 최근에는 딥러닝 기법의 발전으로 인해 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구들이 꾸준히 수행되고 있다. 그런데 위성레이더 영상은 값의 분포범위가 매우 넓고, 많은 스펙클 노이즈가 존재한다. 이러한 요소들은 딥러닝 모델의 학습에 부정적인 영향을 끼칠 수 있으므로 전처리를 통해 해당 요소들을 저감해줄 필요가 있다. 본 연구에서는 전처리된 위성레이더 영상으로부터 딥러닝 선박탐지를 수행하고, 영상의 전처리가 딥러닝 선박탐지에 미치는 요소를 비교분석 하고자 한다.&amp;#xD; 본 연구를 위해 TerraSAR-X와 COSMO-SkyMed 위성레이더 영상을 이용했다. 영상을 딥러닝 학습에 이용하기 전에 먼저 총 세 가지 다른 방법으로 전처리를 수행했다. 첫 번째는 위성레이더 영상에서 강도 값만을 추출한 강도 영상을 생성하는 방법이다. 강도 영상은 값의 범위가 매우 넓을 뿐만 아니라 많은 스펙클 노이즈를 가지고 있다. 두 번째는 강도영상에서 값의 단위를 데시벨로 변환한 데시벨 영상을 생성하는 방법이다. 데시벨 영상은 강도영상과 마찬가지로 많은 스펙클 노이즈를 가지고 있으나 값의 범위가 줄어들어, 더 안정적인 학습을 할 수 있다. 세 번째는 본 연구에서 제안하는 위성레이더 전처리방법으로써, 강도차분과 거칠기영상을 생성하는 방법이다. 두 영상은 중간값 필터링을 이용해 스펙클 노이즈를 줄이고, 값의 분포 대역을 좁힘으로써 빠른 학습이 가능하다.&amp;#xD; 각 전처리된 위성레이더 영상을 이용해 딥러닝 학습을 하기 위해 본 연구에서는 M2Det 객체탐지 모델을 사용했다. 객체탐지 모델을 학습시킨 뒤 테스트 영상을 이용해 선박탐지를 수행했으며, 테스트 결과는 정밀도(Precision), 재현율(Recall)을 이용해 나타냈으며, 두 지수를 하나의 값으로 표현하기 위해 AP(Average Precision)와 F1 점수(F1-score)를 이용해 나타냈다. 각 영상의 정밀도, 재현율, AP, F1 점수는 강도 영상 93.18%, 91.11%, 89.78%, 92.13%, 데시벨 영상 94.16%, 94.16%, 92.34%, 94.16%, 강도차분과 거칠기 영상 97.40%, 94.94%, 95.55%, 96.15%로 계산되었다. 강도 영상을 이용한 경우 미탐지와 오탐지 선박이 많았으며, 전처리된 영상을 이용한 경우 강도 영상에 비해 미탐지와 오탐지 선박이 줄어든 것을 확인할 수 있었다. 데시벨 영상과 강도차분, 거칠기 영상의 결과를 비교했을 때, 두 영상의 오탐지율은 유사했다. 하지만 강도차분, 거칠기 영상을 이용했을 때 강도 영상에 비해 미탐지 선박의 비율이 4% 줄어든 것을 확인할 수 있었다. 이 결과를 통해 위성레이더 영상을 전처리함으로써 딥러닝 학습을 돕고 선박탐지 결과를 향상시킬 수 있다는 것을 알 수 있다. 본 연구결과는 향후 딥러닝을 적용한 위성레이더 영상에서의 선박탐지 연구의 발전에 이바지할 수 있을 것으로 기대된다."
        },
        {
          "rank": 48,
          "score": 0.6632798910140991,
          "doc_id": "DIKO0013710110",
          "title": "딥 러닝을 이용한 DC 모터 제어",
          "abstract": "딥 러닝(deep learning)은 최근에 많이 알려지게 된 심층 인공신경망 알고리즘이다. 일반적인 인공신경망보다 은닉층의 개수와 뉴런의 개수를 확장시키고, 학습이 효율적으로 될 수 있게 알고리즘을 개선한 것이 가장 큰 특징이다. 이러한 특징을 활용하여 기존의 인공신경망으로 풀지 못했던 크고 복잡한 문제들을 해결할 수 있게 되었다. 음성인식, 손 글씨 인식, 얼굴 인식 등 복잡한 패턴인식과 분류에 관련된 다양한 분야에 대한 적용 연구가 활발히 진행되고 있다. 하지만 이러한 장점에도 불구하고, 아직까지 딥 러닝이 제어문제를 해결하기 위해 적용된 사례는 찾아보기 어렵다. 본 논문에서는 간단한 사례를 통해 딥 러닝의 제어문제에 대한 적용 가능성을 확인해 본다. 딥 러닝 알고리즘 중에서 가장 잘 알려진, 깊은 믿음 네트워크(deep belief network) 알고리즘을 사용하여 산업현장에서 가장 많이 사용되고 있는 PID 제어기를 모방하는 딥 러닝 제어기를 설계한다. DC 모터를 제어하는 시스템에서 PID 제어기에 들어오는 입력과 PID 제어기에서 나오는 출력값을 학습 데이터로 사용하여 딥 러닝으로 학습하는 방법을 사용한다. 시뮬레이션을 통해 제안한 딥 러닝 제어기와 PID 제어기를 비교하여 딥 러닝 알고리즘의 성능을 검증한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=DIKO0013710110&target=NART&cn=DIKO0013710110",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "딥 러닝을 이용한 DC 모터 제어 딥 러닝을 이용한 DC 모터 제어 딥 러닝을 이용한 DC 모터 제어 딥 러닝(deep learning)은 최근에 많이 알려지게 된 심층 인공신경망 알고리즘이다. 일반적인 인공신경망보다 은닉층의 개수와 뉴런의 개수를 확장시키고, 학습이 효율적으로 될 수 있게 알고리즘을 개선한 것이 가장 큰 특징이다. 이러한 특징을 활용하여 기존의 인공신경망으로 풀지 못했던 크고 복잡한 문제들을 해결할 수 있게 되었다. 음성인식, 손 글씨 인식, 얼굴 인식 등 복잡한 패턴인식과 분류에 관련된 다양한 분야에 대한 적용 연구가 활발히 진행되고 있다. 하지만 이러한 장점에도 불구하고, 아직까지 딥 러닝이 제어문제를 해결하기 위해 적용된 사례는 찾아보기 어렵다. 본 논문에서는 간단한 사례를 통해 딥 러닝의 제어문제에 대한 적용 가능성을 확인해 본다. 딥 러닝 알고리즘 중에서 가장 잘 알려진, 깊은 믿음 네트워크(deep belief network) 알고리즘을 사용하여 산업현장에서 가장 많이 사용되고 있는 PID 제어기를 모방하는 딥 러닝 제어기를 설계한다. DC 모터를 제어하는 시스템에서 PID 제어기에 들어오는 입력과 PID 제어기에서 나오는 출력값을 학습 데이터로 사용하여 딥 러닝으로 학습하는 방법을 사용한다. 시뮬레이션을 통해 제안한 딥 러닝 제어기와 PID 제어기를 비교하여 딥 러닝 알고리즘의 성능을 검증한다."
        },
        {
          "rank": 49,
          "score": 0.6630519032478333,
          "doc_id": "NART106334309",
          "title": "Cognitive radar antenna selection via deep learning",
          "abstract": "<P>Direction&#x2010;of&#x2010;arrival (DoA) estimation of targets improves with the number of elements employed by a phased array radar antenna. Since larger arrays have high associated cost, area and computational load, there is a recent interest in thinning the antenna arrays without loss of far&#x2010;field DoA accuracy. In this context, a cognitive radar may deploy a full array and then select an optimal subarray to transmit and receive the signals in response to changes in the target environment. Prior works have used optimisation and greedy search methods to pick the best subarrays cognitively. In this study, deep learning is leveraged to address the antenna selection problem. Specifically, they construct a convolutional neural network (CNN) as a multi&#x2010;class classification framework, where each class designates a different subarray. The proposed network determines a new array every time data is received by the radar, thereby making antenna selection a cognitive operation. Their numerical experiments show that the proposed CNN structure provides 22% better classification performance than a support vector machine and the resulting subarrays yield 72% more accurate DoA estimates than random array selections.</P>",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=NART106334309&target=NART&cn=NART106334309",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "Cognitive radar antenna selection via deep learning Cognitive radar antenna selection via deep learning Cognitive radar antenna selection via deep learning <P>Direction&#x2010;of&#x2010;arrival (DoA) estimation of targets improves with the number of elements employed by a phased array radar antenna. Since larger arrays have high associated cost, area and computational load, there is a recent interest in thinning the antenna arrays without loss of far&#x2010;field DoA accuracy. In this context, a cognitive radar may deploy a full array and then select an optimal subarray to transmit and receive the signals in response to changes in the target environment. Prior works have used optimisation and greedy search methods to pick the best subarrays cognitively. In this study, deep learning is leveraged to address the antenna selection problem. Specifically, they construct a convolutional neural network (CNN) as a multi&#x2010;class classification framework, where each class designates a different subarray. The proposed network determines a new array every time data is received by the radar, thereby making antenna selection a cognitive operation. Their numerical experiments show that the proposed CNN structure provides 22% better classification performance than a support vector machine and the resulting subarrays yield 72% more accurate DoA estimates than random array selections.</P>"
        },
        {
          "rank": 50,
          "score": 0.6625348925590515,
          "doc_id": "ATN0052776138",
          "title": "머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구",
          "abstract": "첨단 무기체계의 도입에 따른 운영유지비 증가와 수리부속 조달환경의 악화로 인해, 정밀한 수요예측의 중요성이 더욱 강조되고 있다. 본 연구는 공군 수리부속의 수요가 소량이며 발생 간격이 불규칙한 특성으로 인해 예측이 어렵다는 점에 착안하여, 기존 통계기반 예측기법의 한계를 극복하고자 머신러닝 및 딥러닝 기반 예측모형을 적용하였다. 국방물자관리체계로 부터 수집한 약 37만 건의 수요 데이터를 유형별(Regular, Intermittent, Erratic, Lumpy)로 분류한 후, Random Forest, XG-Boost, LightGBM, LSTM, N-Beats 5가지 예측모델을 구축하고 성능을 비교하였다. 분석 결과, XG-Boost 모델이 가장 우수한 정확도(79.13%)를 기록하였으며, 그리드 서치를 통한 매개변수 최적화 결과, 품목 기준 최대 81.28%의 예측 정확도를 달성하였다. 본 연구를 통해 세부 품목별 분류 기준 정립, 최적 모델 적용 및 매개변수 튜닝 효율화 등을 통해 공군 수리부속 수요예측의 정확도를 실질적으로 향상시킬 수 있음을 실증적으로 확인하였으며, 이는 대규모 군수 데이터셋에 대한 정량적 분석과 실용적인 예측모형 적용을 통해 현장 활용 가능성이 높은 모델을 제시하였다는 점에서 기존 연구와 차별성을 지닌다. 본 연구의 결과는 향후 공군 및 국방 군수 시스템 전반의 운영 효율성 제고와 자원관리 혁신에 중요한 토대를 제공할 수 있을 것으로 기대한다.",
          "source": "http://click.ndsl.kr/servlet/OpenAPIDetailView?keyValue=ATN0052776138&target=NART&cn=ATN0052776138",
          "author": "nan",
          "embedding_mode": "3*title+abstract",
          "embedding_text": "머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 머신러닝과 딥러닝을 활용한 공군 수리부속 예측 정확도 개선에 관한 연구 첨단 무기체계의 도입에 따른 운영유지비 증가와 수리부속 조달환경의 악화로 인해, 정밀한 수요예측의 중요성이 더욱 강조되고 있다. 본 연구는 공군 수리부속의 수요가 소량이며 발생 간격이 불규칙한 특성으로 인해 예측이 어렵다는 점에 착안하여, 기존 통계기반 예측기법의 한계를 극복하고자 머신러닝 및 딥러닝 기반 예측모형을 적용하였다. 국방물자관리체계로 부터 수집한 약 37만 건의 수요 데이터를 유형별(Regular, Intermittent, Erratic, Lumpy)로 분류한 후, Random Forest, XG-Boost, LightGBM, LSTM, N-Beats 5가지 예측모델을 구축하고 성능을 비교하였다. 분석 결과, XG-Boost 모델이 가장 우수한 정확도(79.13%)를 기록하였으며, 그리드 서치를 통한 매개변수 최적화 결과, 품목 기준 최대 81.28%의 예측 정확도를 달성하였다. 본 연구를 통해 세부 품목별 분류 기준 정립, 최적 모델 적용 및 매개변수 튜닝 효율화 등을 통해 공군 수리부속 수요예측의 정확도를 실질적으로 향상시킬 수 있음을 실증적으로 확인하였으며, 이는 대규모 군수 데이터셋에 대한 정량적 분석과 실용적인 예측모형 적용을 통해 현장 활용 가능성이 높은 모델을 제시하였다는 점에서 기존 연구와 차별성을 지닌다. 본 연구의 결과는 향후 공군 및 국방 군수 시스템 전반의 운영 효율성 제고와 자원관리 혁신에 중요한 토대를 제공할 수 있을 것으로 기대한다."
        }
      ]
    }
  ],
  "meta": {
    "model": "gemini-2.5-flash",
    "temperature": 0.2
  }
}