{
  "id": "row_000003",
  "model_name": "Alibaba-NLP/gte-multilingual-base",
  "timestamp_kst": "2025-09-08T23:08:28.149343+09:00",
  "trial_id": "080f8a97",
  "queries": [
    {
      "query": "What approach and results characterize the system that learns TurKontrol’s POMDP parameters from Mechanical Turk data to optimize iterative crowdsourced tasks?",
      "query_meta": {
        "type": "original"
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8086857199668884,
          "doc_id": "NART66567138",
          "title": "POMDP-based control of workflows for crowdsourcing",
          "abstract": "Crowdsourcing, outsourcing of tasks to a crowd of unknown people (''workers'') in an open call, is rapidly rising in popularity. It is already being heavily used by numerous employers (''requesters'') for solving a wide variety of tasks, such as audio transcription, content screening, and labeling training data for machine learning. However, quality control of such tasks continues to be a key challenge because of the high variability in worker quality. In this paper we show the value of decision-theoretic techniques for the problem of optimizing workflows used in crowdsourcing. In particular, we design AI agents that use Bayesian network learning and inference in combination with Partially-Observable Markov Decision Processes (POMDPs) for obtaining excellent cost-quality tradeoffs. We use these techniques for three distinct crowdsourcing scenarios: (1) control of voting to answer a binary-choice question, (2) control of an iterative improvement workflow, and (3) control of switching between alternate workflows for a task. In each scenario, we design a Bayes net model that relates worker competency, task difficulty and worker response quality. We also design a POMDP for each task, whose solution provides the dynamic control policy. We demonstrate the usefulness of our models and agents in live experiments on Amazon Mechanical Turk. We consistently achieve superior quality results than non-adaptive controllers, while incurring equal or less cost.",
          "doc_source": "POMDP-based control of workflows for crowdsourcing POMDP-based control of workflows for crowdsourcing POMDP-based control of workflows for crowdsourcing Crowdsourcing, outsourcing of tasks to a crowd of unknown people (''workers'') in an open call, is rapidly rising in popularity. It is already being heavily used by numerous employers (''requesters'') for solving a wide variety of tasks, such as audio transcription, content screening, and labeling training data for machine learning. However, quality control of such tasks continues to be a key challenge because of the high variability in worker quality. In this paper we show the value of decision-theoretic techniques for the problem of optimizing workflows used in crowdsourcing. In particular, we design AI agents that use Bayesian network learning and inference in combination with Partially-Observable Markov Decision Processes (POMDPs) for obtaining excellent cost-quality tradeoffs. We use these techniques for three distinct crowdsourcing scenarios: (1) control of voting to answer a binary-choice question, (2) control of an iterative improvement workflow, and (3) control of switching between alternate workflows for a task. In each scenario, we design a Bayes net model that relates worker competency, task difficulty and worker response quality. We also design a POMDP for each task, whose solution provides the dynamic control policy. We demonstrate the usefulness of our models and agents in live experiments on Amazon Mechanical Turk. We consistently achieve superior quality results than non-adaptive controllers, while incurring equal or less cost.",
          "author": "Dai, P.;Lin, C.H.;Mausam;Weld, D.S.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 2,
          "score": 0.7450080513954163,
          "doc_id": "NART69625850",
          "title": "Inside the Turk : Understanding Mechanical Turk as a Participant Pool",
          "abstract": "<P>Mechanical Turk (MTurk), an online labor market created by Amazon, has recently become popular among social scientists as a source of survey and experimental data. The workers who populate this market have been assessed on dimensions that are universally relevant to understanding whether, why, and when they should be recruited as research participants. We discuss the characteristics of MTurk as a participant pool for psychology and other social sciences, highlighting the traits of the MTurk samples, why people become MTurk workers and research participants, and how data quality on MTurk compares to that from other pools and depends on controllable and uncontrollable factors.</P>",
          "doc_source": "Inside the Turk : Understanding Mechanical Turk as a Participant Pool Inside the Turk : Understanding Mechanical Turk as a Participant Pool Inside the Turk : Understanding Mechanical Turk as a Participant Pool <P>Mechanical Turk (MTurk), an online labor market created by Amazon, has recently become popular among social scientists as a source of survey and experimental data. The workers who populate this market have been assessed on dimensions that are universally relevant to understanding whether, why, and when they should be recruited as research participants. We discuss the characteristics of MTurk as a participant pool for psychology and other social sciences, highlighting the traits of the MTurk samples, why people become MTurk workers and research participants, and how data quality on MTurk compares to that from other pools and depends on controllable and uncontrollable factors.</P>",
          "author": "Paolacci, Gabriele;Chandler, Jesse;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 3,
          "score": 0.7175914645195007,
          "doc_id": "NART77197564",
          "title": "Online recruitment and testing of infants with Mechanical Turk",
          "abstract": "Testing infants in the laboratory is expensive in time and money; consequently, many studies are underpowered, reducing their reproducibility. We investigated whether the online platform, Amazon Mechanical Turk (MTurk), could be used as a resource to more easily recruit and measure the behavior of infant populations. Using a looking time paradigm, with users' webcams we recorded how long infants aged 5 to 8months attended while viewing children's television programs. We found that infants (N=57) were more reliably engaged by some movies than by others and that the most engaging movies could maintain attention for approximately 70% of a 10- to 13-min period. We then identified the cinematic features within the movies. Faces, singing-and-rhyming, and camera zooms were found to increase infant attention. Together, we established that MTurk can be used as a rapid tool for effectively recruiting and testing infants.",
          "doc_source": "Online recruitment and testing of infants with Mechanical Turk Online recruitment and testing of infants with Mechanical Turk Online recruitment and testing of infants with Mechanical Turk Testing infants in the laboratory is expensive in time and money; consequently, many studies are underpowered, reducing their reproducibility. We investigated whether the online platform, Amazon Mechanical Turk (MTurk), could be used as a resource to more easily recruit and measure the behavior of infant populations. Using a looking time paradigm, with users' webcams we recorded how long infants aged 5 to 8months attended while viewing children's television programs. We found that infants (N=57) were more reliably engaged by some movies than by others and that the most engaging movies could maintain attention for approximately 70% of a 10- to 13-min period. We then identified the cinematic features within the movies. Faces, singing-and-rhyming, and camera zooms were found to increase infant attention. Together, we established that MTurk can be used as a rapid tool for effectively recruiting and testing infants.",
          "author": "Tran, M.;Cabral, L.;Patel, R.;Cusack, R.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 4,
          "score": 0.7130172252655029,
          "doc_id": "NART127620540",
          "title": "Mechanical Turk Versus Student Samples: Comparisons and Recommendations",
          "abstract": "<P>Mechanical Turk and other online crowdsourcing markets (OCMs) have become a go-to data source across scientific disciplines. In 2014 Steelman and colleagues investigated how Mechanical Turk data compared with student samples and consumer panels. They found the data to be comparable and reliable for academic research. In the nearly 10 years since its publication, the use of Mechanical Turk in research has grown substantially. To understand whether their results still hold, we conducted a partial replication to determine how Mechanical Turk workers continue to compare with students using UTAUT 2 as our theoretical model and virtual-reality headsets as the focal IT artifact. Our findings generally align with Steelman et al. (2014) and confirm that Mechanical Turk continues to offer a suitable alternative to student samples. This study reveals consistent results between the student and OCM samples, indicating the potential for interchangeability. The OCM samples are primarily male, while the student sample is majority female, following current US academic trends. All samples are significantly different in age, and only the US OCM and non-US OCM samples are similar in education. The path coefficients from the non-US OCM sample differ significantly from those from other OCM samples; the path coefficients derived from the student sample do not differ significantly from any OCM sample. While sample differences exist, as expected, many are addressable post hoc if anticipated and designed for during data collection. From our findings and the extant literature, we summarize recommendations for researchers and review teams.</P>",
          "doc_source": "Mechanical Turk Versus Student Samples: Comparisons and Recommendations Mechanical Turk Versus Student Samples: Comparisons and Recommendations Mechanical Turk Versus Student Samples: Comparisons and Recommendations <P>Mechanical Turk and other online crowdsourcing markets (OCMs) have become a go-to data source across scientific disciplines. In 2014 Steelman and colleagues investigated how Mechanical Turk data compared with student samples and consumer panels. They found the data to be comparable and reliable for academic research. In the nearly 10 years since its publication, the use of Mechanical Turk in research has grown substantially. To understand whether their results still hold, we conducted a partial replication to determine how Mechanical Turk workers continue to compare with students using UTAUT 2 as our theoretical model and virtual-reality headsets as the focal IT artifact. Our findings generally align with Steelman et al. (2014) and confirm that Mechanical Turk continues to offer a suitable alternative to student samples. This study reveals consistent results between the student and OCM samples, indicating the potential for interchangeability. The OCM samples are primarily male, while the student sample is majority female, following current US academic trends. All samples are significantly different in age, and only the US OCM and non-US OCM samples are similar in education. The path coefficients from the non-US OCM sample differ significantly from those from other OCM samples; the path coefficients derived from the student sample do not differ significantly from any OCM sample. While sample differences exist, as expected, many are addressable post hoc if anticipated and designed for during data collection. From our findings and the extant literature, we summarize recommendations for researchers and review teams.</P>",
          "author": "De Lurgio II, Stephen A.;Young, Amber;Steelman, Zachary R.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 5,
          "score": 0.7119377851486206,
          "doc_id": "NART75736850",
          "title": "Mechanical Turk upends social sciences",
          "abstract": "<P>In May, 23,000 people voluntarily took part in thousands of social science experiments without ever visiting a lab. All they did was log on to Amazon Mechanical Turk (MTurk), an online crowdsourcing service run by the Seattle, Washington&#x2013;based company better known for its massive internet-based retail business. Those research subjects completed 230,000 tasks on their computers in 3.3 million minutes&#x2014;more than 6 years of effort in total. The prodigious output demonstrates the popularity of an online platform that scientists had only begun to exploit 5 years ago. But the growing use of MTurk has raised concerns, as researchers discussed at the Association for Psychological Science meeting in Chicago, Illinois, last month. Some worry that they are becoming too dependent on a commercial platform. Others question whether the research volunteers are paid fairly and treated ethically. And looming over it all are questions about who these anonymous volunteers actually are, and concerns that they are less numerous and diverse than researchers hope.</P>",
          "doc_source": "Mechanical Turk upends social sciences Mechanical Turk upends social sciences Mechanical Turk upends social sciences <P>In May, 23,000 people voluntarily took part in thousands of social science experiments without ever visiting a lab. All they did was log on to Amazon Mechanical Turk (MTurk), an online crowdsourcing service run by the Seattle, Washington&#x2013;based company better known for its massive internet-based retail business. Those research subjects completed 230,000 tasks on their computers in 3.3 million minutes&#x2014;more than 6 years of effort in total. The prodigious output demonstrates the popularity of an online platform that scientists had only begun to exploit 5 years ago. But the growing use of MTurk has raised concerns, as researchers discussed at the Association for Psychological Science meeting in Chicago, Illinois, last month. Some worry that they are becoming too dependent on a commercial platform. Others question whether the research volunteers are paid fairly and treated ethically. And looming over it all are questions about who these anonymous volunteers actually are, and concerns that they are less numerous and diverse than researchers hope.</P>",
          "author": "Bohannon, John",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 6,
          "score": 0.7054260969161987,
          "doc_id": "NART117776930",
          "title": "Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk",
          "abstract": "<P><B>Background</B></P><P>Crowdsourcing services, such as Amazon Mechanical Turk (AMT), allow researchers to use the collective intelligence of a wide range of web users for labor-intensive tasks. As the manual verification of the quality of the collected results is difficult because of the large volume of data and the quick turnaround time of the process, many questions remain to be explored regarding the reliability of these resources for developing digital public health systems.</P><P><B>Objective</B></P><P>This study aims to explore and evaluate the application of crowdsourcing, generally, and AMT, specifically, for developing digital public health surveillance systems.</P><P><B>Methods</B></P><P>We collected 296,166 crowd-generated labels for 98,722 tweets, labeled by 610 AMT workers, to develop machine learning (ML) models for detecting behaviors related to physical activity, sedentary behavior, and sleep quality among Twitter users. To infer the ground truth labels and explore the quality of these labels, we studied 4 statistical consensus methods that are agnostic of task features and only focus on worker labeling behavior. Moreover, to model the meta-information associated with each labeling task and leverage the potential of context-sensitive data in the truth inference process, we developed 7 ML models, including traditional classifiers (offline and active), a deep learning&#x2013;based classification model, and a hybrid convolutional neural network model.</P><P><B>Results</B></P><P>Although most crowdsourcing-based studies in public health have often equated majority vote with quality, the results of our study using a truth set of 9000 manually labeled tweets showed that consensus-based inference models mask underlying uncertainty in data and overlook the importance of task meta-information. Our evaluations across 3 physical activity, sedentary behavior, and sleep quality data sets showed that truth inference is a context-sensitive process, and none of the methods studied in this paper were consistently superior to others in predicting the truth label. We also found that the performance of the ML models trained on crowd-labeled data was sensitive to the quality of these labels, and poor-quality labels led to incorrect assessment of these models. Finally, we have provided a set of practical recommendations to improve the quality and reliability of crowdsourced data.</P><P><B>Conclusions</B></P><P>Our findings indicate the importance of the quality of crowd-generated labels in developing ML models designed for decision-making purposes, such as public health surveillance decisions. A combination of inference models outlined and analyzed in this study could be used to quantitatively measure and improve the quality of crowd-generated labels for training ML models.</P>",
          "doc_source": "Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk <P><B>Background</B></P><P>Crowdsourcing services, such as Amazon Mechanical Turk (AMT), allow researchers to use the collective intelligence of a wide range of web users for labor-intensive tasks. As the manual verification of the quality of the collected results is difficult because of the large volume of data and the quick turnaround time of the process, many questions remain to be explored regarding the reliability of these resources for developing digital public health systems.</P><P><B>Objective</B></P><P>This study aims to explore and evaluate the application of crowdsourcing, generally, and AMT, specifically, for developing digital public health surveillance systems.</P><P><B>Methods</B></P><P>We collected 296,166 crowd-generated labels for 98,722 tweets, labeled by 610 AMT workers, to develop machine learning (ML) models for detecting behaviors related to physical activity, sedentary behavior, and sleep quality among Twitter users. To infer the ground truth labels and explore the quality of these labels, we studied 4 statistical consensus methods that are agnostic of task features and only focus on worker labeling behavior. Moreover, to model the meta-information associated with each labeling task and leverage the potential of context-sensitive data in the truth inference process, we developed 7 ML models, including traditional classifiers (offline and active), a deep learning&#x2013;based classification model, and a hybrid convolutional neural network model.</P><P><B>Results</B></P><P>Although most crowdsourcing-based studies in public health have often equated majority vote with quality, the results of our study using a truth set of 9000 manually labeled tweets showed that consensus-based inference models mask underlying uncertainty in data and overlook the importance of task meta-information. Our evaluations across 3 physical activity, sedentary behavior, and sleep quality data sets showed that truth inference is a context-sensitive process, and none of the methods studied in this paper were consistently superior to others in predicting the truth label. We also found that the performance of the ML models trained on crowd-labeled data was sensitive to the quality of these labels, and poor-quality labels led to incorrect assessment of these models. Finally, we have provided a set of practical recommendations to improve the quality and reliability of crowdsourced data.</P><P><B>Conclusions</B></P><P>Our findings indicate the importance of the quality of crowd-generated labels in developing ML models designed for decision-making purposes, such as public health surveillance decisions. A combination of inference models outlined and analyzed in this study could be used to quantitatively measure and improve the quality of crowd-generated labels for training ML models.</P>",
          "author": "Shakeri Hossein Abad, Zahra;Butler, Gregory P;Thompson, Wendy;Lee, Joon;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 7,
          "score": 0.6938424110412598,
          "doc_id": "NART92832666",
          "title": "Using Amazon Mechanical Turk for linguistic research",
          "abstract": "<P>Amazon?s Mechanical Turk service makes linguistic experimentation quick, easy, and inexpensive. However, researchers have not been certain about its reliability. In a series of experiments, this paper compares data collected via Mechanical Turk to those obtained using more traditional methods One set of experiments measured the predictability of words in sentences using the Cloze sentence completion task (Taylor, 1953). The correlation between traditional and Turk Cloze scores is high (rho=0.823) and both data sets perform similarly against alternative measures of contextual predictability. Five other experiments on the semantic relatedness of verbs and phrasal verbs (how much is ?lift? part of ?lift up?) manipulate the presence of the sentence context and the composition of the experimental list. The results indicate that Turk data correlate well between experiments and with data from traditional methods (rho up to 0.9), and they show high inter-rater consistency and agreement. We conclude that Mechanical Turk is a reliable source of data for complex linguistic tasks in heavy use by psycholinguists. The paper provides suggestions for best practices in data collection and scrubbing.</P>",
          "doc_source": "Using Amazon Mechanical Turk for linguistic research Using Amazon Mechanical Turk for linguistic research Using Amazon Mechanical Turk for linguistic research <P>Amazon?s Mechanical Turk service makes linguistic experimentation quick, easy, and inexpensive. However, researchers have not been certain about its reliability. In a series of experiments, this paper compares data collected via Mechanical Turk to those obtained using more traditional methods One set of experiments measured the predictability of words in sentences using the Cloze sentence completion task (Taylor, 1953). The correlation between traditional and Turk Cloze scores is high (rho=0.823) and both data sets perform similarly against alternative measures of contextual predictability. Five other experiments on the semantic relatedness of verbs and phrasal verbs (how much is ?lift? part of ?lift up?) manipulate the presence of the sentence context and the composition of the experimental list. The results indicate that Turk data correlate well between experiments and with data from traditional methods (rho up to 0.9), and they show high inter-rater consistency and agreement. We conclude that Mechanical Turk is a reliable source of data for complex linguistic tasks in heavy use by psycholinguists. The paper provides suggestions for best practices in data collection and scrubbing.</P>",
          "author": "Schnoebelen, Tyler;Kuperman, Victor;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 8,
          "score": 0.6931095719337463,
          "doc_id": "NART106764981",
          "title": "Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data",
          "abstract": "<P>Within the field of Linguistics, Amazon&rsquo;s Mechanical Turk, a crowdsourcing marketplace specializes in computer-based Human Intelligence Tasks, has been praised as a cost efficient source of data for English and other major languages. Spanish is a good candidate due to its presence within the US and beyond. Still, detailed information concerning the linguistic and demographic profile of Spanish-speaking &lsquo;Turkers&rsquo; is missing, thus making it difficult for researchers to evaluate whether the Mechanical Turk provides the right environment for their tasks. This paper addresses this gap in our knowledge by developing the first detailed study of the presence of Spanish-speaking workers, focusing on factors relevant for research planning, namely, (socio)linguistically relevant variables and information concerning work habits. The results show that this platform provides access to a fairly active participant pool of both L1 and L2Spanish speakers as well as bilinguals. A brief introduction to how Amazon&rsquo;s Mechanical Turk works and overview of Hispanic Linguistics projects that have so far used the Mechanical Turk successfully is included.</P>",
          "doc_source": "Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data <P>Within the field of Linguistics, Amazon&rsquo;s Mechanical Turk, a crowdsourcing marketplace specializes in computer-based Human Intelligence Tasks, has been praised as a cost efficient source of data for English and other major languages. Spanish is a good candidate due to its presence within the US and beyond. Still, detailed information concerning the linguistic and demographic profile of Spanish-speaking &lsquo;Turkers&rsquo; is missing, thus making it difficult for researchers to evaluate whether the Mechanical Turk provides the right environment for their tasks. This paper addresses this gap in our knowledge by developing the first detailed study of the presence of Spanish-speaking workers, focusing on factors relevant for research planning, namely, (socio)linguistically relevant variables and information concerning work habits. The results show that this platform provides access to a fairly active participant pool of both L1 and L2Spanish speakers as well as bilinguals. A brief introduction to how Amazon&rsquo;s Mechanical Turk works and overview of Hispanic Linguistics projects that have so far used the Mechanical Turk successfully is included.</P>",
          "author": "Ortega-Santos, Iv&aacute;n;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 9,
          "score": 0.6922529935836792,
          "doc_id": "NART70754548",
          "title": "Amazon Mechanical Turk and the commodification of labour",
          "abstract": "<P>Crowd employment platforms enable firms to source labour and expertise by leveraging Internet technology. Rather than offshoring jobs to low&#8208;cost geographies, functions once performed by internal employees can be outsourced to an undefined pool of digital labour using a virtual network. This enables firms to shift costs and offload risk as they access a flexible, scalable workforce that sits outside the traditional boundaries of labour laws and regulations. The micro&#8208;tasks of &lsquo;clickwork&rsquo; are tedious, repetitive and poorly paid, with remuneration often well below minimum wage. This article will present an analysis of one of the most popular crowdsourcing sites&mdash;Mechanical Turk&mdash;to illuminate how Amazon's platform enables an array of companies to access digital labour at low cost and without any of the associated social protection or moral obligation.</P>",
          "doc_source": "Amazon Mechanical Turk and the commodification of labour Amazon Mechanical Turk and the commodification of labour Amazon Mechanical Turk and the commodification of labour <P>Crowd employment platforms enable firms to source labour and expertise by leveraging Internet technology. Rather than offshoring jobs to low&#8208;cost geographies, functions once performed by internal employees can be outsourced to an undefined pool of digital labour using a virtual network. This enables firms to shift costs and offload risk as they access a flexible, scalable workforce that sits outside the traditional boundaries of labour laws and regulations. The micro&#8208;tasks of &lsquo;clickwork&rsquo; are tedious, repetitive and poorly paid, with remuneration often well below minimum wage. This article will present an analysis of one of the most popular crowdsourcing sites&mdash;Mechanical Turk&mdash;to illuminate how Amazon's platform enables an array of companies to access digital labour at low cost and without any of the associated social protection or moral obligation.</P>",
          "author": "Bergvall&#8208;K&aring;reborn, Birgitta;Howcroft, Debra;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 10,
          "score": 0.6895528435707092,
          "doc_id": "NART74131061",
          "title": "Lessons Learned from Crowdsourcing Complex Engineering Tasks",
          "abstract": "<P><B>Crowdsourcing</B></P><P>Crowdsourcing is the practice of obtaining needed ideas, services, or content by requesting contributions from a large group of people. Amazon Mechanical Turk is a web marketplace for crowdsourcing microtasks, such as answering surveys and image tagging. We explored the limits of crowdsourcing by using Mechanical Turk for a more complicated task: analysis and creation of wind simulations.</P><P><B>Harnessing Crowdworkers for Engineering</B></P><P>Our investigation examined the feasibility of using crowdsourcing for complex, highly technical tasks. This was done to determine if the benefits of crowdsourcing could be harnessed to accurately and effectively contribute to solving complex real world engineering problems. Of course, untrained crowds cannot be used as a mere substitute for trained expertise. Rather, we sought to understand how crowd workers can be used as a large pool of labor for a preliminary analysis of complex data.</P><P><B>Virtual Wind Tunnel</B></P><P>We compared the skill of the anonymous crowd workers from Amazon Mechanical Turk with that of civil engineering graduate students, making a first pass at analyzing wind simulation data. For the first phase, we posted analysis questions to Amazon crowd workers and to two groups of civil engineering graduate students. A second phase of our experiment instructed crowd workers and students to create simulations on our Virtual Wind Tunnel website to solve a more complex task.</P><P><B>Conclusions</B></P><P>With a sufficiently comprehensive tutorial and compensation similar to typical crowd-sourcing wages, we were able to enlist crowd workers to effectively complete longer, more complex tasks with competence comparable to that of graduate students with more comprehensive, expert-level knowledge. Furthermore, more complex tasks require increased communication with the workers. As tasks become more complex, the employment relationship begins to become more akin to outsourcing than crowdsourcing. Through this investigation, we were able to stretch and explore the limits of crowdsourcing as a tool for solving complex problems.</P>",
          "doc_source": "Lessons Learned from Crowdsourcing Complex Engineering Tasks Lessons Learned from Crowdsourcing Complex Engineering Tasks Lessons Learned from Crowdsourcing Complex Engineering Tasks <P><B>Crowdsourcing</B></P><P>Crowdsourcing is the practice of obtaining needed ideas, services, or content by requesting contributions from a large group of people. Amazon Mechanical Turk is a web marketplace for crowdsourcing microtasks, such as answering surveys and image tagging. We explored the limits of crowdsourcing by using Mechanical Turk for a more complicated task: analysis and creation of wind simulations.</P><P><B>Harnessing Crowdworkers for Engineering</B></P><P>Our investigation examined the feasibility of using crowdsourcing for complex, highly technical tasks. This was done to determine if the benefits of crowdsourcing could be harnessed to accurately and effectively contribute to solving complex real world engineering problems. Of course, untrained crowds cannot be used as a mere substitute for trained expertise. Rather, we sought to understand how crowd workers can be used as a large pool of labor for a preliminary analysis of complex data.</P><P><B>Virtual Wind Tunnel</B></P><P>We compared the skill of the anonymous crowd workers from Amazon Mechanical Turk with that of civil engineering graduate students, making a first pass at analyzing wind simulation data. For the first phase, we posted analysis questions to Amazon crowd workers and to two groups of civil engineering graduate students. A second phase of our experiment instructed crowd workers and students to create simulations on our Virtual Wind Tunnel website to solve a more complex task.</P><P><B>Conclusions</B></P><P>With a sufficiently comprehensive tutorial and compensation similar to typical crowd-sourcing wages, we were able to enlist crowd workers to effectively complete longer, more complex tasks with competence comparable to that of graduate students with more comprehensive, expert-level knowledge. Furthermore, more complex tasks require increased communication with the workers. As tasks become more complex, the employment relationship begins to become more akin to outsourcing than crowdsourcing. Through this investigation, we were able to stretch and explore the limits of crowdsourcing as a tool for solving complex problems.</P>",
          "author": "Staffelbach, Matthew;Sempolinski, Peter;Kijewski-Correa, Tracy;Thain, Douglas;Wei, Daniel;Kareem, Ahsan;Madey, Gregory;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 11,
          "score": 0.6873599290847778,
          "doc_id": "JAKO202411139606539",
          "title": "Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이",
          "abstract": "온라인 크라우드소싱 플랫폼인 Amazon Mechanical Turk(MTurk)은 뛰어난 과제 수행 기록을 가진 참가자들에게 마스터 등급을 부여한다. 그러나 MTurk의 마스터 참가자와 일반 참가자를 비교한 선행 연구들은 두 집단이 실제로 수행의 차이를 보이는가에 대해 일관되지 않은 결과를 보고했다. 또한 선행 연구들은 대부분 설문 조사 방식을 사용했으며 MTurk의 마스터와 일반 참가자의 인지 과제 수행 능력을 비교한 연구는 부족한 상황이다. 본 연구는 시각 기억 재인 과제를 사용하여 MTurk 마스터 및 일반 참가자와 오프라인에서 모집한 대학생 참가자 집단의 수행을 비교했다. 연구 결과, MTurk 마스터 참가자와 오프라인 참가자는 동일한 수준의 기억 수행을 보였다. 그러나 MTurk 일반 참가자의 기억 과제 수행은 마스터와 오프라인 참가자 집단의 결과와 차이를 보였다. 각 집단에서 기억 과제 정확률이 낮은 참가자를 제외한 후에도 동일한 결과가 나타났다. 이러한 결과는 온라인에서 참가자 집단을 적절히 선발하면 기존의 오프라인 실험 결과를 잘 재현할 수 있음을 보여준다. 동시에 본 연구의 결과는 온라인 크라우드소싱 플랫폼의 참가자 집단이 균일하지 않으며, 집단 선정 방식에 따라 연구의 결과가 다르게 나타날 수 있음을 시사한다.",
          "doc_source": "Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이 Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이 Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이 온라인 크라우드소싱 플랫폼인 Amazon Mechanical Turk(MTurk)은 뛰어난 과제 수행 기록을 가진 참가자들에게 마스터 등급을 부여한다. 그러나 MTurk의 마스터 참가자와 일반 참가자를 비교한 선행 연구들은 두 집단이 실제로 수행의 차이를 보이는가에 대해 일관되지 않은 결과를 보고했다. 또한 선행 연구들은 대부분 설문 조사 방식을 사용했으며 MTurk의 마스터와 일반 참가자의 인지 과제 수행 능력을 비교한 연구는 부족한 상황이다. 본 연구는 시각 기억 재인 과제를 사용하여 MTurk 마스터 및 일반 참가자와 오프라인에서 모집한 대학생 참가자 집단의 수행을 비교했다. 연구 결과, MTurk 마스터 참가자와 오프라인 참가자는 동일한 수준의 기억 수행을 보였다. 그러나 MTurk 일반 참가자의 기억 과제 수행은 마스터와 오프라인 참가자 집단의 결과와 차이를 보였다. 각 집단에서 기억 과제 정확률이 낮은 참가자를 제외한 후에도 동일한 결과가 나타났다. 이러한 결과는 온라인에서 참가자 집단을 적절히 선발하면 기존의 오프라인 실험 결과를 잘 재현할 수 있음을 보여준다. 동시에 본 연구의 결과는 온라인 크라우드소싱 플랫폼의 참가자 집단이 균일하지 않으며, 집단 선정 방식에 따라 연구의 결과가 다르게 나타날 수 있음을 시사한다.",
          "author": "정수근",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 12,
          "score": 0.6868324279785156,
          "doc_id": "NART123803221",
          "title": "Running experiments on Amazon Mechanical Turk",
          "abstract": "<P><B>Abstract</B><P>Although Mechanical Turk has recently become popular among social scientists as a source of experimental data, doubts may linger about the quality of data provided by subjects recruited from online labor markets. We address these potential concerns by presenting new demographic data about the Mechanical Turk subject population, reviewing the strengths of Mechanical Turk relative to other online and offline methods of recruiting subjects, and comparing the magnitude of effects obtained using Mechanical Turk and traditional subject pools. We further discuss some additional benefits such as the possibility of longitudinal, cross cultural and prescreening designs, and offer some advice on how to best manage a common subject pool.</P></P>",
          "doc_source": "Running experiments on Amazon Mechanical Turk Running experiments on Amazon Mechanical Turk Running experiments on Amazon Mechanical Turk <P><B>Abstract</B><P>Although Mechanical Turk has recently become popular among social scientists as a source of experimental data, doubts may linger about the quality of data provided by subjects recruited from online labor markets. We address these potential concerns by presenting new demographic data about the Mechanical Turk subject population, reviewing the strengths of Mechanical Turk relative to other online and offline methods of recruiting subjects, and comparing the magnitude of effects obtained using Mechanical Turk and traditional subject pools. We further discuss some additional benefits such as the possibility of longitudinal, cross cultural and prescreening designs, and offer some advice on how to best manage a common subject pool.</P></P>",
          "author": "Paolacci, Gabriele;Chandler, Jesse;Ipeirotis, Panagiotis G.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 13,
          "score": 0.6805344820022583,
          "doc_id": "NART88129314",
          "title": "Using Mechanical Turk to Study Clinical Populations",
          "abstract": "<P> Although participants with psychiatric symptoms, specific risk factors, or rare demographic characteristics can be difficult to identify and recruit for participation in research, participants with these characteristics are crucial for research in the social, behavioral, and clinical sciences. Online research in general and crowdsourcing software in particular may offer a solution. However, no research to date has examined the utility of crowdsourcing software for conducting research on psychopathology. In the current study, we examined the prevalence of several psychiatric disorders and related problems, as well as the reliability and validity of participant reports on these domains, among users of Amazon&rsquo;s Mechanical Turk. Findings suggest that crowdsourcing software offers several advantages for clinical research while providing insight into potential problems, such as misrepresentation, that researchers should address when collecting data online. </P>",
          "doc_source": "Using Mechanical Turk to Study Clinical Populations Using Mechanical Turk to Study Clinical Populations Using Mechanical Turk to Study Clinical Populations <P> Although participants with psychiatric symptoms, specific risk factors, or rare demographic characteristics can be difficult to identify and recruit for participation in research, participants with these characteristics are crucial for research in the social, behavioral, and clinical sciences. Online research in general and crowdsourcing software in particular may offer a solution. However, no research to date has examined the utility of crowdsourcing software for conducting research on psychopathology. In the current study, we examined the prevalence of several psychiatric disorders and related problems, as well as the reliability and validity of participant reports on these domains, among users of Amazon&rsquo;s Mechanical Turk. Findings suggest that crowdsourcing software offers several advantages for clinical research while providing insight into potential problems, such as misrepresentation, that researchers should address when collecting data online. </P>",
          "author": "Shapiro, Danielle N.;Chandler, Jesse;Mueller, Pam A.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 14,
          "score": 0.6795188784599304,
          "doc_id": "ATN0036732137",
          "title": "How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk",
          "abstract": "<jats:p>Since its very first appearance the concept of crowdsourcing has undergone major variations, coming to include highly heterogeneous phenomena such as Google’s data mining, exchanges on sharing economy platforms (e.g. Airbnb or eBay), contents production within creative communities online (e.g. Wikipedia) and much more. If one assumes a very broad perspective, it is eventually possible to extend the category of crowdsourcing to cover whatsoever phenomena involving the participation of the crowd online, as in fact has been done. On the contrary, I will argue that crowdsourcing – and in particular its microwork branch – represents the specific practice of extending outsourcing processes to a large, low-cost, scalable and flexible workforce, in order to generate greater added value for a supply chain. To develop this analysis, I will especially focus on the case of Amazon Mechanical Turk, and on how the operations carried out on this platform are primarily intended to manage the huge flow of information which spans across a supply chain. The practice of subcontracting to the crowd tasks previously carried out by employees or third-party suppliers highlights how crowdsourcing involves a reshaping of the supply chain, further extending it to a large network of individuals. Through crowdsourcing processes, companies are either able to replace or train AI, integrating human computation skills in algorithmic structures through simple, and oftentimes tedious, microtasks. In this context, processes of gamification are capable to put further downward pressure on already small piece-wages, as long as crowdworkers are rather willing to earn an even lower economic compensation, if it’s associated to challenging tasks; thus, to make a task more enjoyable through gamification could be an effective way to further reduce a supply chain’s expenditures in crowdsourcing, pushing forward labor exploitation practices structurally embedded in this phenomenon.</jats:p>",
          "doc_source": "How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk <jats:p>Since its very first appearance the concept of crowdsourcing has undergone major variations, coming to include highly heterogeneous phenomena such as Google’s data mining, exchanges on sharing economy platforms (e.g. Airbnb or eBay), contents production within creative communities online (e.g. Wikipedia) and much more. If one assumes a very broad perspective, it is eventually possible to extend the category of crowdsourcing to cover whatsoever phenomena involving the participation of the crowd online, as in fact has been done. On the contrary, I will argue that crowdsourcing – and in particular its microwork branch – represents the specific practice of extending outsourcing processes to a large, low-cost, scalable and flexible workforce, in order to generate greater added value for a supply chain. To develop this analysis, I will especially focus on the case of Amazon Mechanical Turk, and on how the operations carried out on this platform are primarily intended to manage the huge flow of information which spans across a supply chain. The practice of subcontracting to the crowd tasks previously carried out by employees or third-party suppliers highlights how crowdsourcing involves a reshaping of the supply chain, further extending it to a large network of individuals. Through crowdsourcing processes, companies are either able to replace or train AI, integrating human computation skills in algorithmic structures through simple, and oftentimes tedious, microtasks. In this context, processes of gamification are capable to put further downward pressure on already small piece-wages, as long as crowdworkers are rather willing to earn an even lower economic compensation, if it’s associated to challenging tasks; thus, to make a task more enjoyable through gamification could be an effective way to further reduce a supply chain’s expenditures in crowdsourcing, pushing forward labor exploitation practices structurally embedded in this phenomenon.</jats:p>",
          "author": "De Lellis Lorenzo",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 15,
          "score": 0.6753081679344177,
          "doc_id": "NART103944733",
          "title": "The Language Demographics of Amazon Mechanical Turk",
          "abstract": "<P> We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers&rsquo; self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems. </P>",
          "doc_source": "The Language Demographics of Amazon Mechanical Turk The Language Demographics of Amazon Mechanical Turk The Language Demographics of Amazon Mechanical Turk <P> We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers&rsquo; self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems. </P>",
          "author": "Pavlick, Ellie;Post, Matt;Irvine, Ann;Kachaev, Dmitry;Callison-Burch, Chris;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 16,
          "score": 0.6718164682388306,
          "doc_id": "NART134452383",
          "title": "&raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk",
          "abstract": "<P>This article centers Amazon Mechanical Turk (MTurk) workers to examine their alienation, as they complete monotonous and repetitive microtasks from behind their screens. Confronted with various &raquo;virtual assembly lines&laquo; that produce data across the globe, their labor can be further used for machine learning specifically and Artificial Intelligence more generally. Engaging with these workers and their labor is central to general contemporary and future technological developments bound to bring their own repercussions with them - including the growing and central role of algorithms in managing the world of work.</P>",
          "doc_source": "&raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk &raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk &raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk <P>This article centers Amazon Mechanical Turk (MTurk) workers to examine their alienation, as they complete monotonous and repetitive microtasks from behind their screens. Confronted with various &raquo;virtual assembly lines&laquo; that produce data across the globe, their labor can be further used for machine learning specifically and Artificial Intelligence more generally. Engaging with these workers and their labor is central to general contemporary and future technological developments bound to bring their own repercussions with them - including the growing and central role of algorithms in managing the world of work.</P>",
          "author": "Kassem, Sarrah;Wilpert (&Uuml;bersetzung), Chris W.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 17,
          "score": 0.6646542549133301,
          "doc_id": "NART87817032",
          "title": "Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative",
          "abstract": "<P>In this paper we evaluate how to effectively use the crowdsourcing service, Amazon's Mechanical Turk (MTurk), to content analyze textual data for use in psychological research. MTurk is a marketplace for discrete tasks completed by workers, typically for small amounts of money. MTurk has been used to aid psychological research in general, and content analysis in particular. In the current study, MTurk workers content analyzed personally-written textual data using coding categories previously developed and validated in psychological research. These codes were evaluated for reliability, accuracy, completion time, and cost. Results indicate that MTurk workers categorized textual data with comparable reliability and accuracy to both previously published studies and expert raters. Further, the coding tasks were performed quickly and cheaply. These data suggest that crowdsourced content analysis can help advance psychological research.</P>",
          "doc_source": "Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative <P>In this paper we evaluate how to effectively use the crowdsourcing service, Amazon's Mechanical Turk (MTurk), to content analyze textual data for use in psychological research. MTurk is a marketplace for discrete tasks completed by workers, typically for small amounts of money. MTurk has been used to aid psychological research in general, and content analysis in particular. In the current study, MTurk workers content analyzed personally-written textual data using coding categories previously developed and validated in psychological research. These codes were evaluated for reliability, accuracy, completion time, and cost. Results indicate that MTurk workers categorized textual data with comparable reliability and accuracy to both previously published studies and expert raters. Further, the coding tasks were performed quickly and cheaply. These data suggest that crowdsourced content analysis can help advance psychological research.</P>",
          "author": "Tosti-Kharas, Jennifer;Conley, Caryn;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 18,
          "score": 0.6636712551116943,
          "doc_id": "NART73517650",
          "title": "Extending the Applicability of POMDP Solutions to Robotic Tasks",
          "abstract": "<P>Partially observable Markov decision processes (POMDPs) are used in many robotic task classes from soccer to household chores. Determining an approximately optimal action policy for POMDPs is PSPACE-complete, and the exponential growth of computation time prohibits solving large tasks. This paper describes two techniques to extend the range of robotic tasks that can be solved using a POMDP. Our first technique reduces the motion constraints of a robot and, then, uses state-of-the-art robotic motion planning techniques to respect the true motion constraints at runtime. We then propose a novel task decomposition that can be applied to some indoor robotic tasks. This decomposition transforms a long time horizon task into a set of shorter tasks. We empirically demonstrate the performance gain provided by these two techniques through simulated execution in a variety of environments. Comparing a direct formulation of a POMDP to solving our proposed reductions, we conclude that the techniques proposed in this paper can provide significant enhancement to current POMDP solution techniques, extending the POMDP instances that can be solved to include large continuous-state robotic tasks.</P>",
          "doc_source": "Extending the Applicability of POMDP Solutions to Robotic Tasks Extending the Applicability of POMDP Solutions to Robotic Tasks Extending the Applicability of POMDP Solutions to Robotic Tasks <P>Partially observable Markov decision processes (POMDPs) are used in many robotic task classes from soccer to household chores. Determining an approximately optimal action policy for POMDPs is PSPACE-complete, and the exponential growth of computation time prohibits solving large tasks. This paper describes two techniques to extend the range of robotic tasks that can be solved using a POMDP. Our first technique reduces the motion constraints of a robot and, then, uses state-of-the-art robotic motion planning techniques to respect the true motion constraints at runtime. We then propose a novel task decomposition that can be applied to some indoor robotic tasks. This decomposition transforms a long time horizon task into a set of shorter tasks. We empirically demonstrate the performance gain provided by these two techniques through simulated execution in a variety of environments. Comparing a direct formulation of a POMDP to solving our proposed reductions, we conclude that the techniques proposed in this paper can provide significant enhancement to current POMDP solution techniques, extending the POMDP instances that can be solved to include large continuous-state robotic tasks.</P>",
          "author": "Grady, Devin K.;Moll, Mark;Kavraki, Lydia E.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 19,
          "score": 0.6591975092887878,
          "doc_id": "NPAP12463036",
          "title": "Exploring Crowd Consistency in a Mechanical Turk Survey",
          "abstract": "<P>Crowdsourcing can provide a platform for evaluating software engineering research. In this paper, we aim to explore characteristics of the worker population on Amazon's Mechanical Turk, a popular micro task crowdsourcing environment, and measure the percentage of workers who are potentially qualified to perform software- or computer science- related tasks. Through a baseline survey and two replications, we measure workers' answer consistency as well as the consistency of sample characteristics. In the end, we deployed 1,200 total surveys that were completed by 1,064 unique workers. Our results show that 24% of the study participants have a computer science or IT background and most people are payment driven when choosing tasks. The sample characteristics can vary significantly, even on large samples with 300 participants. Additionally, we often observed inconsistency in workers' answers for those who completed two surveys; approximately 30% answered at least one question inconsistently between the two survey submissions. This implies a need for replication and quality controls in crowdsourced experiments.</P>",
          "doc_source": "Exploring Crowd Consistency in a Mechanical Turk Survey Exploring Crowd Consistency in a Mechanical Turk Survey Exploring Crowd Consistency in a Mechanical Turk Survey <P>Crowdsourcing can provide a platform for evaluating software engineering research. In this paper, we aim to explore characteristics of the worker population on Amazon's Mechanical Turk, a popular micro task crowdsourcing environment, and measure the percentage of workers who are potentially qualified to perform software- or computer science- related tasks. Through a baseline survey and two replications, we measure workers' answer consistency as well as the consistency of sample characteristics. In the end, we deployed 1,200 total surveys that were completed by 1,064 unique workers. Our results show that 24% of the study participants have a computer science or IT background and most people are payment driven when choosing tasks. The sample characteristics can vary significantly, even on large samples with 300 participants. Additionally, we often observed inconsistency in workers' answers for those who completed two surveys; approximately 30% answered at least one question inconsistently between the two survey submissions. This implies a need for replication and quality controls in crowdsourced experiments.</P>",
          "author": "Sun, Peng;Stolee, Kathryn T.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 20,
          "score": 0.657005786895752,
          "doc_id": "NART109637313",
          "title": "Annotator Rationales for Labeling Tasks in Crowdsourcing",
          "abstract": "<P>When collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. To address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. We evaluate this approach on an information retrieval task in which human judges rate the relevance of Web pages for different search topics. Cost-benefit analysis over 10,000 judgments collected on Amazon&rsquo;s Mechanical Turk suggests a win-win. Firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. Secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. Consequently, we can realize the above benefits with minimal additional cost.</P>",
          "doc_source": "Annotator Rationales for Labeling Tasks in Crowdsourcing Annotator Rationales for Labeling Tasks in Crowdsourcing Annotator Rationales for Labeling Tasks in Crowdsourcing <P>When collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. To address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. We evaluate this approach on an information retrieval task in which human judges rate the relevance of Web pages for different search topics. Cost-benefit analysis over 10,000 judgments collected on Amazon&rsquo;s Mechanical Turk suggests a win-win. Firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. Secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. Consequently, we can realize the above benefits with minimal additional cost.</P>",
          "author": "Kutlu, Mucahid;McDonnell, Tyler;Lease, Matthew;Elsayed, Tamer;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 21,
          "score": 0.6517137885093689,
          "doc_id": "NART73604379",
          "title": "Conducting behavioral research on Amazon’s Mechanical Turk",
          "abstract": "<P>Amazon&#039;s Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.</P>",
          "doc_source": "Conducting behavioral research on Amazon’s Mechanical Turk Conducting behavioral research on Amazon’s Mechanical Turk Conducting behavioral research on Amazon’s Mechanical Turk <P>Amazon&#039;s Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.</P>",
          "author": "Mason, Winter;Suri, Siddharth;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 22,
          "score": 0.6504759788513184,
          "doc_id": "NART73267731",
          "title": "The (Non) Religion of Mechanical Turk Workers",
          "abstract": "<P>Social science researchers have increasingly come to utilize Amazon's Mechanical Turk (MTurk) to obtain adult, opt&#8208;in samples for use with experiments. Based on the demographic characteristics of MTurk samples, studies have provided some support for the representativeness of MTurk. Others have warranted caution based on demographic characteristics and comparisons of reliability. Yet, what is missing is an examination of the most glaring demographic difference in MTurk&mdash;religion. We compare five MTurk samples with a student convenience sample and the 2012 General Social Survey, finding that MTurk samples have a consistent bias toward nonreligion. MTurk surveys significantly overrepresent seculars and underrepresent Catholics and evangelical Protestants. We then compare the religiosity of religious identifiers across samples as well as relationships between religiosity and partisanship, finding many similarities and a few important differences from the general population.</P>",
          "doc_source": "The (Non) Religion of Mechanical Turk Workers The (Non) Religion of Mechanical Turk Workers The (Non) Religion of Mechanical Turk Workers <P>Social science researchers have increasingly come to utilize Amazon's Mechanical Turk (MTurk) to obtain adult, opt&#8208;in samples for use with experiments. Based on the demographic characteristics of MTurk samples, studies have provided some support for the representativeness of MTurk. Others have warranted caution based on demographic characteristics and comparisons of reliability. Yet, what is missing is an examination of the most glaring demographic difference in MTurk&mdash;religion. We compare five MTurk samples with a student convenience sample and the 2012 General Social Survey, finding that MTurk samples have a consistent bias toward nonreligion. MTurk surveys significantly overrepresent seculars and underrepresent Catholics and evangelical Protestants. We then compare the religiosity of religious identifiers across samples as well as relationships between religiosity and partisanship, finding many similarities and a few important differences from the general population.</P>",
          "author": "Lewis, Andrew R.;Djupe, Paul A.;Mockabee, Stephen T.;Su&#8208;Ya Wu, Joshua;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 23,
          "score": 0.6447882652282715,
          "doc_id": "NPAP13833015",
          "title": "Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk",
          "abstract": "nan",
          "doc_source": "Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk ",
          "author": "Uzor, Stephen;Jacques, Jason T.;Dudley, John J;Kristensson, Per Ola;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 24,
          "score": 0.6416093111038208,
          "doc_id": "NART105659807",
          "title": "Analyzing the Amazon Mechanical Turk marketplace",
          "abstract": "<P>An associate professor at New York Universitys Stern School of Business uncovers answers about who are the employers in paid crowdsourcing, what tasks they post, and how much they pay.</P>",
          "doc_source": "Analyzing the Amazon Mechanical Turk marketplace Analyzing the Amazon Mechanical Turk marketplace Analyzing the Amazon Mechanical Turk marketplace <P>An associate professor at New York Universitys Stern School of Business uncovers answers about who are the employers in paid crowdsourcing, what tasks they post, and how much they pay.</P>",
          "author": "Ipeirotis, Panagiotis G.",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 25,
          "score": 0.6390166282653809,
          "doc_id": "NART70960968",
          "title": "A reliability analysis of Mechanical Turk data",
          "abstract": "Amazon's Mechanical Turk (MTurk) provides researchers with access to a diverse set of people who can serve as research participants, making the process of data collection a streamlined and cost-effective one. While a small number of studies are often cited to support the use of this methodology, there remains a need for additional analyses of the quality of the research data. In the present study, MTurk-based responses for a personality scale were found to be significantly less reliable than scores previously reported for a community sample. While score reliability was not affected by the length of the survey or the payment rates, the presence of an item asking respondents to affirm that they were attentive and honest was associated with more reliable responses. Best practices for MTurk-based research and continuing research needs are addressed.",
          "doc_source": "A reliability analysis of Mechanical Turk data A reliability analysis of Mechanical Turk data A reliability analysis of Mechanical Turk data Amazon's Mechanical Turk (MTurk) provides researchers with access to a diverse set of people who can serve as research participants, making the process of data collection a streamlined and cost-effective one. While a small number of studies are often cited to support the use of this methodology, there remains a need for additional analyses of the quality of the research data. In the present study, MTurk-based responses for a personality scale were found to be significantly less reliable than scores previously reported for a community sample. While score reliability was not affected by the length of the survey or the payment rates, the presence of an item asking respondents to affirm that they were attentive and honest was associated with more reliable responses. Best practices for MTurk-based research and continuing research needs are addressed.",
          "author": "Rouse, S.V.",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 26,
          "score": 0.6346577405929565,
          "doc_id": "NPAP13914175",
          "title": "Quality management on Amazon Mechanical Turk",
          "abstract": "nan",
          "doc_source": "Quality management on Amazon Mechanical Turk Quality management on Amazon Mechanical Turk Quality management on Amazon Mechanical Turk ",
          "author": "Ipeirotis, Panagiotis G.;Provost, Foster;Wang, Jing;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 27,
          "score": 0.6315504908561707,
          "doc_id": "NART78755911",
          "title": "Using Mechanical Turk for research on cancer survivors",
          "abstract": "<P><B>Abstract</B></P><P><B>Objective</B></P><P>The successful recruitment and study of cancer survivors within psycho&#8208;oncology research can be challenging, time&#8208;consuming, and expensive, particularly for key subgroups such as young adult cancer survivors. Online crowdsourcing platforms offer a potential solution that has not yet been investigated with regard to cancer populations. The current study assessed the presence of cancer survivors on Amazon's Mechanical Turk (MTurk) and the feasibility of using MTurk as an efficient, cost&#8208;effective, and reliable psycho&#8208;oncology recruitment and research platform.</P><P><B>Methods</B></P><P>During a <4&#8208;month period, cancer survivors living in the United States were recruited on MTurk to complete two assessments, spaced 1 week apart, relating to psychosocial and cancer&#8208;related functioning. The reliability and validity of responses were investigated.</P><P><B>Results</B></P><P>Within a <4&#8208;month period, 464 self&#8208;identified cancer survivors on MTurk consented to and completed an online assessment. The vast majority (79.09%) provided reliable and valid study data according to multiple indices. The sample was highly diverse in terms of U.S. geography, socioeconomic status, and cancer type, and reflected a particularly strong presence of distressed and young adult cancer survivors (median age = 36 years). A majority of participants (58.19%) responded to a second survey sent one week later.</P><P><B>Conclusions</B></P><P>Online crowdsourcing represents a feasible, efficient, and cost&#8208;effective recruitment and research platform for cancer survivors, particularly for young adult cancer survivors and those with significant distress. We discuss remaining challenges and future recommendations. Copyright &copy; 2016 John Wiley &amp; Sons, Ltd.</P>",
          "doc_source": "Using Mechanical Turk for research on cancer survivors Using Mechanical Turk for research on cancer survivors Using Mechanical Turk for research on cancer survivors <P><B>Abstract</B></P><P><B>Objective</B></P><P>The successful recruitment and study of cancer survivors within psycho&#8208;oncology research can be challenging, time&#8208;consuming, and expensive, particularly for key subgroups such as young adult cancer survivors. Online crowdsourcing platforms offer a potential solution that has not yet been investigated with regard to cancer populations. The current study assessed the presence of cancer survivors on Amazon's Mechanical Turk (MTurk) and the feasibility of using MTurk as an efficient, cost&#8208;effective, and reliable psycho&#8208;oncology recruitment and research platform.</P><P><B>Methods</B></P><P>During a <4&#8208;month period, cancer survivors living in the United States were recruited on MTurk to complete two assessments, spaced 1 week apart, relating to psychosocial and cancer&#8208;related functioning. The reliability and validity of responses were investigated.</P><P><B>Results</B></P><P>Within a <4&#8208;month period, 464 self&#8208;identified cancer survivors on MTurk consented to and completed an online assessment. The vast majority (79.09%) provided reliable and valid study data according to multiple indices. The sample was highly diverse in terms of U.S. geography, socioeconomic status, and cancer type, and reflected a particularly strong presence of distressed and young adult cancer survivors (median age = 36 years). A majority of participants (58.19%) responded to a second survey sent one week later.</P><P><B>Conclusions</B></P><P>Online crowdsourcing represents a feasible, efficient, and cost&#8208;effective recruitment and research platform for cancer survivors, particularly for young adult cancer survivors and those with significant distress. We discuss remaining challenges and future recommendations. Copyright &copy; 2016 John Wiley &amp; Sons, Ltd.</P>",
          "author": "Arch, Joanna J.;Carr, Alaina L.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 28,
          "score": 0.6310226917266846,
          "doc_id": "NART69743055",
          "title": "Crowdsourcing content analysis for managerial research",
          "abstract": "<P><B>Purpose</B></P> <P> &ndash; The purpose of this paper is to evaluate the effectiveness of a novel method for performing content analysis in managerial research &ndash; crowdsourcing, a system where geographically distributed workers complete small, discrete tasks via the internet for a small amount of money. </P> <P><B>Design/methodology/approach</B></P> <P> &ndash; The authors examined whether workers from one popular crowdsourcing marketplace, Amazon's Mechanical Turk, could perform subjective content analytic tasks involving the application of inductively generated codes to unstructured, personally written textual passages. </P> <P><B>Findings</B></P> <P> &ndash; The findings suggest that anonymous, self-selected, non-expert crowdsourced workers were applied content codes efficiently and at low cost, and that their reliability and accuracy compared to that of trained researchers. </P> <P><B>Research limitations/implications</B></P> <P> &ndash; The authors provide recommendations for management researchers interested in using crowdsourcing most effectively for content analysis, including a discussion of the limitations and ethical issues involved in using this method. Future research could extend the findings by considering alternative data sources and coding schemes of interest to management researchers. </P> <P><B>Originality/value</B></P> <P> &ndash; Scholars have begun to explore whether crowdsourcing can assist in academic research; however, this is the first study to examine how crowdsourcing might facilitate content analysis. Crowdsourcing offers several advantages over existing content analytic approaches by combining the efficiency of computer-aided text analysis with the interpretive ability of traditional human coding.</P>",
          "doc_source": "Crowdsourcing content analysis for managerial research Crowdsourcing content analysis for managerial research Crowdsourcing content analysis for managerial research <P><B>Purpose</B></P> <P> &ndash; The purpose of this paper is to evaluate the effectiveness of a novel method for performing content analysis in managerial research &ndash; crowdsourcing, a system where geographically distributed workers complete small, discrete tasks via the internet for a small amount of money. </P> <P><B>Design/methodology/approach</B></P> <P> &ndash; The authors examined whether workers from one popular crowdsourcing marketplace, Amazon's Mechanical Turk, could perform subjective content analytic tasks involving the application of inductively generated codes to unstructured, personally written textual passages. </P> <P><B>Findings</B></P> <P> &ndash; The findings suggest that anonymous, self-selected, non-expert crowdsourced workers were applied content codes efficiently and at low cost, and that their reliability and accuracy compared to that of trained researchers. </P> <P><B>Research limitations/implications</B></P> <P> &ndash; The authors provide recommendations for management researchers interested in using crowdsourcing most effectively for content analysis, including a discussion of the limitations and ethical issues involved in using this method. Future research could extend the findings by considering alternative data sources and coding schemes of interest to management researchers. </P> <P><B>Originality/value</B></P> <P> &ndash; Scholars have begun to explore whether crowdsourcing can assist in academic research; however, this is the first study to examine how crowdsourcing might facilitate content analysis. Crowdsourcing offers several advantages over existing content analytic approaches by combining the efficiency of computer-aided text analysis with the interpretive ability of traditional human coding.</P>",
          "author": "Conley, Caryn;Tosti-Kharas, Jennifer;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 29,
          "score": 0.6280822157859802,
          "doc_id": "NART120020213",
          "title": "Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces",
          "abstract": "<P>Controlling the quality of tasks is a major challenge in crowdsourcing marketplaces. Most of the existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in the marketplaces have to monitor the tasks continuously to find such improper tasks; however, it is too expensive to manually investigate each task. In this paper, we present the reports of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We perform experiments using real task data from a commercial crowdsourcing marketplace and show that the classifier trained by the operator judgments achieves high accuracy in detecting improper tasks. In addition, to reduce the annotation costs of the operator and improve the classification accuracy, we consider the use of crowdsourcing for task annotation. We hire a group of crowdsourcing (non-expert) workers to monitor posted tasks, and incorporate their judgments into the training data of the classifier. By applying quality control techniques to handle the variability in worker reliability, our results show that the use of non-expert judgments by crowdsourcing workers in combination with expert judgments improves the accuracy of detecting improper crowdsourcing tasks.</P>",
          "doc_source": "Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces <P>Controlling the quality of tasks is a major challenge in crowdsourcing marketplaces. Most of the existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in the marketplaces have to monitor the tasks continuously to find such improper tasks; however, it is too expensive to manually investigate each task. In this paper, we present the reports of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We perform experiments using real task data from a commercial crowdsourcing marketplace and show that the classifier trained by the operator judgments achieves high accuracy in detecting improper tasks. In addition, to reduce the annotation costs of the operator and improve the classification accuracy, we consider the use of crowdsourcing for task annotation. We hire a group of crowdsourcing (non-expert) workers to monitor posted tasks, and incorporate their judgments into the training data of the classifier. By applying quality control techniques to handle the variability in worker reliability, our results show that the use of non-expert judgments by crowdsourcing workers in combination with expert judgments improves the accuracy of detecting improper crowdsourcing tasks.</P>",
          "author": "Baba, Yukino;Kashima, Hisashi;Kinoshita, Kei;Yamaguchi, Goushi;Akiyoshi, Yosuke;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 30,
          "score": 0.6138415336608887,
          "doc_id": "NART121336460",
          "title": "Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples",
          "abstract": "<P> Two of the most popular populations for convenience sampling used in the psychological sciences are college students and Mechanical Turk (MTurk) workers. College students represent a traditional type of convenience sample, whereas MTurk workers provide a more modern source of data. However, little research has examined how these populations differ from each other in salient characteristics. Additionally, no research to date has investigated how MTurk college students (a traditional sample collected using modern methods) compare to either population. The current study examined 1,248 participants comprising three samples: MTurk noncollege workers ( n = 533), MTurk college students ( n = 385), and traditional college students ( n = 330). We compared the samples on demographic characteristics, study completion time, attention, and individual difference variables (i.e., personality, social desirability, need for cognition, personal values, and social attitudes). We examined the individual difference variables in terms of mean responses, internal consistency estimates, and subscale intercorrelations. Results indicated the samples were distinct from each other in terms of all variables assessed; in addition, adding demographic characteristics as covariates to the analyses of individual difference variables did not effectively account for sample differences. We conclude that research using convenience samples should take these differences into account. </P>",
          "doc_source": "Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples <P> Two of the most popular populations for convenience sampling used in the psychological sciences are college students and Mechanical Turk (MTurk) workers. College students represent a traditional type of convenience sample, whereas MTurk workers provide a more modern source of data. However, little research has examined how these populations differ from each other in salient characteristics. Additionally, no research to date has investigated how MTurk college students (a traditional sample collected using modern methods) compare to either population. The current study examined 1,248 participants comprising three samples: MTurk noncollege workers ( n = 533), MTurk college students ( n = 385), and traditional college students ( n = 330). We compared the samples on demographic characteristics, study completion time, attention, and individual difference variables (i.e., personality, social desirability, need for cognition, personal values, and social attitudes). We examined the individual difference variables in terms of mean responses, internal consistency estimates, and subscale intercorrelations. Results indicated the samples were distinct from each other in terms of all variables assessed; in addition, adding demographic characteristics as covariates to the analyses of individual difference variables did not effectively account for sample differences. We conclude that research using convenience samples should take these differences into account. </P>",
          "author": "Weigold, Arne;Weigold, Ingrid K.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 31,
          "score": 0.6100351810455322,
          "doc_id": "NART73866218",
          "title": "The Southern Dative Presentative Meets Mechanical Turk",
          "abstract": "<P>This article introduces the southern dative presentative, an understudied construction that varies across speakers of American English. The authors discuss similarities and differences between this construction and the better-studied personal dative construction and compare the Southern dative presentative with similar constructions cross-linguistically. They then present the results of a nationwide acceptability judgment survey administered on Amazon Mechanical Turk. The results show that Southern dative presentatives are alive and well in Southern dialects of American English. In the process, they also illustrate the usefulness of Amazon Mechanical Turk (and similar crowdsourcing platforms) for the study of dialect variation in the domain of syntax.</P>",
          "doc_source": "The Southern Dative Presentative Meets Mechanical Turk The Southern Dative Presentative Meets Mechanical Turk The Southern Dative Presentative Meets Mechanical Turk <P>This article introduces the southern dative presentative, an understudied construction that varies across speakers of American English. The authors discuss similarities and differences between this construction and the better-studied personal dative construction and compare the Southern dative presentative with similar constructions cross-linguistically. They then present the results of a nationwide acceptability judgment survey administered on Amazon Mechanical Turk. The results show that Southern dative presentatives are alive and well in Southern dialects of American English. In the process, they also illustrate the usefulness of Amazon Mechanical Turk (and similar crowdsourcing platforms) for the study of dialect variation in the domain of syntax.</P>",
          "author": "Wood, Jim;Horn, Laurence;Zanuttini, Raffaella;Lindemann, Luke;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 32,
          "score": 0.6061146259307861,
          "doc_id": "NART77189173",
          "title": "Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks",
          "abstract": "<P>We present the design and evaluation of a method for estimating gaze locations during the analysis of static visualizations using crowdsourcing. Understanding gaze patterns is helpful for evaluating visualizations and user behaviors, but traditional eye-tracking studies require specialized hardware and local users. To avoid these constraints, we developed a method called Fauxvea, which crowdsources visualization tasks on the Web and estimates gaze fixations through cursor interactions without eye-tracking hardware. We ran experiments to evaluate how gaze estimates from our method compare with eye-tracking data. First, we evaluated crowdsourced estimates for three common types of information visualizations and basic visualization tasks using Amazon Mechanical Turk (MTurk). In another, we reproduced findings from a previous eye-tracking study on tree layouts using our method on MTurk. Results from these experiments show that fixation estimates using Fauxvea are qualitatively and quantitatively similar to eye tracking on the same stimulus-task pairs. These findings suggest that crowdsourcing visual analysis tasks with static information visualizations could be a viable alternative to traditional eye-tracking studies for visualization research and design.</P>",
          "doc_source": "Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks <P>We present the design and evaluation of a method for estimating gaze locations during the analysis of static visualizations using crowdsourcing. Understanding gaze patterns is helpful for evaluating visualizations and user behaviors, but traditional eye-tracking studies require specialized hardware and local users. To avoid these constraints, we developed a method called Fauxvea, which crowdsources visualization tasks on the Web and estimates gaze fixations through cursor interactions without eye-tracking hardware. We ran experiments to evaluate how gaze estimates from our method compare with eye-tracking data. First, we evaluated crowdsourced estimates for three common types of information visualizations and basic visualization tasks using Amazon Mechanical Turk (MTurk). In another, we reproduced findings from a previous eye-tracking study on tree layouts using our method on MTurk. Results from these experiments show that fixation estimates using Fauxvea are qualitatively and quantitatively similar to eye tracking on the same stimulus-task pairs. These findings suggest that crowdsourcing visual analysis tasks with static information visualizations could be a viable alternative to traditional eye-tracking studies for visualization research and design.</P>",
          "author": "Gomez, Steven R.;Jianu, Radu;Cabeen, Ryan;Guo, Hua;Laidlaw, David H.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 33,
          "score": 0.6015686988830566,
          "doc_id": "NART111084731",
          "title": "Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction",
          "abstract": "nan",
          "doc_source": "Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction ",
          "author": "Nam, Sangha;Lee, Minho;Heo, Cheolhoon;Choi, Key-Sun;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 34,
          "score": 0.5925757884979248,
          "doc_id": "NPAP13897603",
          "title": "TurKit : tools for iterative tasks on mechanical Turk",
          "abstract": "nan",
          "doc_source": "TurKit : tools for iterative tasks on mechanical Turk TurKit : tools for iterative tasks on mechanical Turk TurKit : tools for iterative tasks on mechanical Turk ",
          "author": "Little, Greg;Chilton, Lydia B.;Goldman, Max;Miller, Robert C.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 35,
          "score": 0.5922701954841614,
          "doc_id": "NPAP13663736",
          "title": "모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정",
          "abstract": "최근 모바일 기술이 실생활에 널리 활용하면서 점점 모바일 크라우드소싱 활용이 크게 기대되고 있다. 그래서 배달 인력이 아닌 일반인도 어플리케이션을 모바일 기기에 설치하면 배달 인력이 되어 작업을 수행할 수 있다. 본 연구에서는 일반인도 참여할 수 있는 모바일 크라우드소싱 기반 배달에서 딥러닝을 이용한 작업자 선정 기법을 소개한다. 그리고 실험을 통하여 합성곱 신경망(Convolutional Neural Network)을 적용한 본 기법이 효과적이라는 것을 보인다.",
          "doc_source": "모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정 모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정 모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정 최근 모바일 기술이 실생활에 널리 활용하면서 점점 모바일 크라우드소싱 활용이 크게 기대되고 있다. 그래서 배달 인력이 아닌 일반인도 어플리케이션을 모바일 기기에 설치하면 배달 인력이 되어 작업을 수행할 수 있다. 본 연구에서는 일반인도 참여할 수 있는 모바일 크라우드소싱 기반 배달에서 딥러닝을 이용한 작업자 선정 기법을 소개한다. 그리고 실험을 통하여 합성곱 신경망(Convolutional Neural Network)을 적용한 본 기법이 효과적이라는 것을 보인다.",
          "author": "이윤열;김응모;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 36,
          "score": 0.5909650325775146,
          "doc_id": "NART75765228",
          "title": "Using Amazon Mechanical Turk and other compensated crowdsourcing sites",
          "abstract": "Crowdsourcing is becoming recognized as a powerful tool that organizations can use in order to get work done, this by freelancers and non-employees. We conceptualize crowdsourcing as a subcategory of outsourcing, with compensated crowdsourcing representing situations in which individuals performing the work receive some sort of payment for accomplishing the organization's tasks. Herein, we discuss how sites that create a crowd, such as Amazon Mechanical Turk, can be powerful tools for business purposes. We highlight the general features of crowdsourcing sites, offering examples drawn from current crowdsourcing sites. We then examine the wide range of tasks that can be accomplished through crowdsourcing sites. Large online worker community websites and forums have been created around such crowdsourcing sites, and we describe the functions they generally play for crowdsourced workers. We also describe how these functions offer opportunities and challenges for organizations. We close by discussing major considerations organizations need to take into account when trying to harness the power of the crowd through compensated crowdsourcing sites.",
          "doc_source": "Using Amazon Mechanical Turk and other compensated crowdsourcing sites Using Amazon Mechanical Turk and other compensated crowdsourcing sites Using Amazon Mechanical Turk and other compensated crowdsourcing sites Crowdsourcing is becoming recognized as a powerful tool that organizations can use in order to get work done, this by freelancers and non-employees. We conceptualize crowdsourcing as a subcategory of outsourcing, with compensated crowdsourcing representing situations in which individuals performing the work receive some sort of payment for accomplishing the organization's tasks. Herein, we discuss how sites that create a crowd, such as Amazon Mechanical Turk, can be powerful tools for business purposes. We highlight the general features of crowdsourcing sites, offering examples drawn from current crowdsourcing sites. We then examine the wide range of tasks that can be accomplished through crowdsourcing sites. Large online worker community websites and forums have been created around such crowdsourcing sites, and we describe the functions they generally play for crowdsourced workers. We also describe how these functions offer opportunities and challenges for organizations. We close by discussing major considerations organizations need to take into account when trying to harness the power of the crowd through compensated crowdsourcing sites.",
          "author": "Schmidt, G.B.;Jettinghoff, W.M.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 37,
          "score": 0.5901029109954834,
          "doc_id": "DIKO0016395992",
          "title": "모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당",
          "abstract": "nan",
          "doc_source": "모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당 모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당 모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당 ",
          "author": "이윤열",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 38,
          "score": 0.5717378854751587,
          "doc_id": "JAKO201922663514847",
          "title": "인공신경망 기반 VRF 시스템 제어",
          "abstract": "This study aimed at developing control algorithms for operating a variable refrigerant flow (VRF) heating and cooling system with optimal system parameter set-points. Two artificial neural network (ANN) models, which were respectively designed to predict the heating energy cost and cooling energy amount for upcoming next control cycle, was developed and embedded into the control algorithms. Performance of the algorithms were tested using the computer simulation programs - EnergyPlus, BCVTB, MATLAB in an incorporative manner. The results revealed that the proposed control algorithms remarkably saved the heating energy cost by as much as 7.93% and cooling energy consumption by as much as 28.44%, compared to a conventional control strategy. These findings support that the ANN-based predictive control algorithms showed potential for cost- and energy-effectiveness of VRF heating and cooling systems.",
          "doc_source": "인공신경망 기반 VRF 시스템 제어 인공신경망 기반 VRF 시스템 제어 인공신경망 기반 VRF 시스템 제어 This study aimed at developing control algorithms for operating a variable refrigerant flow (VRF) heating and cooling system with optimal system parameter set-points. Two artificial neural network (ANN) models, which were respectively designed to predict the heating energy cost and cooling energy amount for upcoming next control cycle, was developed and embedded into the control algorithms. Performance of the algorithms were tested using the computer simulation programs - EnergyPlus, BCVTB, MATLAB in an incorporative manner. The results revealed that the proposed control algorithms remarkably saved the heating energy cost by as much as 7.93% and cooling energy consumption by as much as 28.44%, compared to a conventional control strategy. These findings support that the ANN-based predictive control algorithms showed potential for cost- and energy-effectiveness of VRF heating and cooling systems.",
          "author": "문진우",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 39,
          "score": 0.5700033903121948,
          "doc_id": "JAKO201416760764337",
          "title": "인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가",
          "abstract": "본 논문에서는 TBM 터널의 세그먼트 라이닝 설계 자동화 기술 개발의 일환으로 인공신경망기법을 이용한 세그먼트 라이닝 부재력 산정기법 개발에 관한 내용을 다루었다. 부재력 평가가 가능한 인공신경망을 개발하기 위해 먼저 다양한 설계조건을 도출하고 이에 대해 2-Ring Beam 모델을 이용한 유한요소해석을 수행하여 인공신경망 학습에 필요한 설계조건별 부재력에 관한 DB를 구축하였다. 구축된 DB를 활용하여 인공신경망의 최적화 과정을 통해 최대 부재력 및 분포도를 예측할 수 있는 인공신경망을 구축하였다. 검토 결과 구축된 인공신경망은 유한요소해석과 동일한 정밀도의 부재력 산정 기능을 확보하는 것으로 검토되었으며 따라서 TBM 세그먼트 라이닝 설계시 필요한 부재력 평가를 위한 효율적인 수단으로 활용될 수 있는 것으로 판단된다.",
          "doc_source": "인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가 인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가 인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가 본 논문에서는 TBM 터널의 세그먼트 라이닝 설계 자동화 기술 개발의 일환으로 인공신경망기법을 이용한 세그먼트 라이닝 부재력 산정기법 개발에 관한 내용을 다루었다. 부재력 평가가 가능한 인공신경망을 개발하기 위해 먼저 다양한 설계조건을 도출하고 이에 대해 2-Ring Beam 모델을 이용한 유한요소해석을 수행하여 인공신경망 학습에 필요한 설계조건별 부재력에 관한 DB를 구축하였다. 구축된 DB를 활용하여 인공신경망의 최적화 과정을 통해 최대 부재력 및 분포도를 예측할 수 있는 인공신경망을 구축하였다. 검토 결과 구축된 인공신경망은 유한요소해석과 동일한 정밀도의 부재력 산정 기능을 확보하는 것으로 검토되었으며 따라서 TBM 세그먼트 라이닝 설계시 필요한 부재력 평가를 위한 효율적인 수단으로 활용될 수 있는 것으로 판단된다.",
          "author": "유충식;최정혁;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 40,
          "score": 0.5690590739250183,
          "doc_id": "JAKO202113759910728",
          "title": "FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현",
          "abstract": "In this paper, we present the FlappyBird Competition System (FCS) implementation, a competition-based automated assessment system used in an entry-level artificial intelligence (AI) course at a university. The proposed system provides an evaluation method suitable for AI courses while taking advantage of automated assessment methods. Students are to design a neural network structure, train the weights, and tune hyperparameters using the given reinforcement learning code to improve the overall performance of game AI. Students participate using the resulting trained model during the competition, and the system automatically calculates the final score based on the ranking. The user evaluation conducted after the semester ends shows that our competition-based automated assessment system promotes active participation and inspires students to be interested and motivated to learn AI. Using FCS, the instructor significantly reduces the amount of time required for assessment.",
          "doc_source": "FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현 FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현 FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현 In this paper, we present the FlappyBird Competition System (FCS) implementation, a competition-based automated assessment system used in an entry-level artificial intelligence (AI) course at a university. The proposed system provides an evaluation method suitable for AI courses while taking advantage of automated assessment methods. Students are to design a neural network structure, train the weights, and tune hyperparameters using the given reinforcement learning code to improve the overall performance of game AI. Students participate using the resulting trained model during the competition, and the system automatically calculates the final score based on the ranking. The user evaluation conducted after the semester ends shows that our competition-based automated assessment system promotes active participation and inspires students to be interested and motivated to learn AI. Using FCS, the instructor significantly reduces the amount of time required for assessment.",
          "author": "손의성;김재경;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 41,
          "score": 0.5670425891876221,
          "doc_id": "NPAP13686745",
          "title": "Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk",
          "abstract": "nan",
          "doc_source": "Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk ",
          "author": "Calvo, Roc&iacute;o;Kane, Shaun K.;Hurst, Amy;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 42,
          "score": 0.5650862455368042,
          "doc_id": "DIKO0015787734",
          "title": "검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현",
          "abstract": "본 논문은 검사 공정 작업자 이미지 반복 처리 작업을 자동화 시스템으로 대체하고자 하는 연구를 진행하였다. 검사 장비의 자동화에 따른 작업 스피드가 향상되었지만, 그 이면에 자동화 적용이 어려워 작업자가 직접 해야 하는 이미지 반복 확인 작업의 과제가 남아 있다. 이를 해결하기 위해 딥러닝을 활용한 이미지 분류 자동화 시스템을 개발하는 것이 본고의 목적이다.&amp;#xD; &amp;#xD; 검사 공정 작업자 이미지 반복 작업의 문제점은 단순 반복 행위로 인한 작업자의 건강상 역기능, 즉 시력 저하와 손목 통증이 대표적이다. 또한 작업자의 개인적인 판단과 오류로 인한 품질적인 측면인 재현성과 반복성도 문제점 중 하나이다. 뿐만 아니라 검사기에 이미지가 증가되면 작업자의 업무 부하로 인한 실시간 처리도 지연된다.&amp;#xD; &amp;#xD; 작업자의 이미지 확인 작업의 주요 이유는 이물성 불량을 양품으로 전환하여 수율을 향상시키는 것과 불량의 세부 Trend를 파악하고자 하기 위함이다. 이런 문제들을 해소하고자 딥러닝에서 CNN(Convolution Neural Network) 합성곱 신경망을 활용하여 이미지의 특징을 추출하고 학습을 진행 후 파라미터를 컴퓨터 신경망이 기억하였다가 분류 이미지가 들어오면 기억된 신경망이 이미지를 14가지 유형으로 자동 분류 예측하는 시스템을 개발하였다. 이미지 분류에는 전처리 과정과 이미지 분류 그리고 후처리 과정을 거치게 된다. 개발된 자동화 시스템에 평가 검증의 비교 대상은 작업자의 결과물과 비교하게 된다. 평가품을 선정 후 작업자가 선 작업을 하고 이 후 원본 이미지를 자동화 시스템이 분류하여 작업자 대비 자동화 시스템의 성능인 정합도를 비교하여 취합하였다. 이런 과정을 반복하면서 Confusion Matrix 오차 행렬을 이용하여 세부적 분석을 통해 정합도 이상 유형을 찾고 추가적인 학습에 파라미터를 수정하여 최적화에 점점 다가가게 된다.&amp;#xD; &amp;#xD; 14가지 항목의 이미지 15,000개를 학습시킨 후 1일 1,900,000개의 이미지를 분류하는 자동화 시스템의 정합도는 작업자 대비 93.26%의 정합도를 확인하였고 유형별 정합도의 최대 100%에서 최소 80.87%의 유형별 정합도를 확인하였다. 치명적 오류 681 ppm 그리고 치명적이지 않은 오류 6.7%에 성능을 확인하였다. 작업자 대비 이미지 처리 속도는 약 10배 이상의 개선 효과를 확인하였으며 이로 인하여 작업자의 건강상의 역기능 부담을 감소시켰고 작업자의 이미지 분류 확인 작업의 일관성을 시스템화하였다.&amp;#xD; &amp;#xD; 또한 이미지 처리 속도 향상으로 실시간 처리에 좀 더 가까워졌다. 현재 자동화 시스템의 정합도 93.26%에 유형별 정합도 100% 가까운 유형의 항목은 자동화 시스템의 이미지 분류 후 후처리 과정에서 분류 결과를 확인하여 강제로 작업자에게 전송시키지 않는 상태이며 이로 인한 작업자는 기존 대비 약 80%의 업무 과중을 줄일 수 있다.&amp;#xD; 업무에 과중을 최소화 함으로써 작업자 인력 소인화에 도달하였다. &amp;#xD; &amp;#xD; 추가적으로 향후 분리 되어 있는 검사기와 자동화 시스템을 검사기 장비 내에 결합하여 불필요한 전송 및 Loss를 최소화 하여 실시간 처리 및 인력 무인화에 도움이 될 것이라 기대된다. 또한 GPU를 추가적으로 멀티로 사용하여 자동화 시스템의 처리 속도 또한 현재보다 더 향상될 것으로 기대된다.",
          "doc_source": "검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현 검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현 검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현 본 논문은 검사 공정 작업자 이미지 반복 처리 작업을 자동화 시스템으로 대체하고자 하는 연구를 진행하였다. 검사 장비의 자동화에 따른 작업 스피드가 향상되었지만, 그 이면에 자동화 적용이 어려워 작업자가 직접 해야 하는 이미지 반복 확인 작업의 과제가 남아 있다. 이를 해결하기 위해 딥러닝을 활용한 이미지 분류 자동화 시스템을 개발하는 것이 본고의 목적이다.&amp;#xD; &amp;#xD; 검사 공정 작업자 이미지 반복 작업의 문제점은 단순 반복 행위로 인한 작업자의 건강상 역기능, 즉 시력 저하와 손목 통증이 대표적이다. 또한 작업자의 개인적인 판단과 오류로 인한 품질적인 측면인 재현성과 반복성도 문제점 중 하나이다. 뿐만 아니라 검사기에 이미지가 증가되면 작업자의 업무 부하로 인한 실시간 처리도 지연된다.&amp;#xD; &amp;#xD; 작업자의 이미지 확인 작업의 주요 이유는 이물성 불량을 양품으로 전환하여 수율을 향상시키는 것과 불량의 세부 Trend를 파악하고자 하기 위함이다. 이런 문제들을 해소하고자 딥러닝에서 CNN(Convolution Neural Network) 합성곱 신경망을 활용하여 이미지의 특징을 추출하고 학습을 진행 후 파라미터를 컴퓨터 신경망이 기억하였다가 분류 이미지가 들어오면 기억된 신경망이 이미지를 14가지 유형으로 자동 분류 예측하는 시스템을 개발하였다. 이미지 분류에는 전처리 과정과 이미지 분류 그리고 후처리 과정을 거치게 된다. 개발된 자동화 시스템에 평가 검증의 비교 대상은 작업자의 결과물과 비교하게 된다. 평가품을 선정 후 작업자가 선 작업을 하고 이 후 원본 이미지를 자동화 시스템이 분류하여 작업자 대비 자동화 시스템의 성능인 정합도를 비교하여 취합하였다. 이런 과정을 반복하면서 Confusion Matrix 오차 행렬을 이용하여 세부적 분석을 통해 정합도 이상 유형을 찾고 추가적인 학습에 파라미터를 수정하여 최적화에 점점 다가가게 된다.&amp;#xD; &amp;#xD; 14가지 항목의 이미지 15,000개를 학습시킨 후 1일 1,900,000개의 이미지를 분류하는 자동화 시스템의 정합도는 작업자 대비 93.26%의 정합도를 확인하였고 유형별 정합도의 최대 100%에서 최소 80.87%의 유형별 정합도를 확인하였다. 치명적 오류 681 ppm 그리고 치명적이지 않은 오류 6.7%에 성능을 확인하였다. 작업자 대비 이미지 처리 속도는 약 10배 이상의 개선 효과를 확인하였으며 이로 인하여 작업자의 건강상의 역기능 부담을 감소시켰고 작업자의 이미지 분류 확인 작업의 일관성을 시스템화하였다.&amp;#xD; &amp;#xD; 또한 이미지 처리 속도 향상으로 실시간 처리에 좀 더 가까워졌다. 현재 자동화 시스템의 정합도 93.26%에 유형별 정합도 100% 가까운 유형의 항목은 자동화 시스템의 이미지 분류 후 후처리 과정에서 분류 결과를 확인하여 강제로 작업자에게 전송시키지 않는 상태이며 이로 인한 작업자는 기존 대비 약 80%의 업무 과중을 줄일 수 있다.&amp;#xD; 업무에 과중을 최소화 함으로써 작업자 인력 소인화에 도달하였다. &amp;#xD; &amp;#xD; 추가적으로 향후 분리 되어 있는 검사기와 자동화 시스템을 검사기 장비 내에 결합하여 불필요한 전송 및 Loss를 최소화 하여 실시간 처리 및 인력 무인화에 도움이 될 것이라 기대된다. 또한 GPU를 추가적으로 멀티로 사용하여 자동화 시스템의 처리 속도 또한 현재보다 더 향상될 것으로 기대된다.",
          "author": "김영규",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 43,
          "score": 0.5639605522155762,
          "doc_id": "DIKO0010672988",
          "title": "인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN)",
          "abstract": "Abstract Li Chenghua Department of Information and Communication Chebuk National University Text categorization is an important application of machine learning to the field of document information retrieval. This thesis described two kinds of neural networks for text categorization, multi-output perceptron learning (MOPL) and back propagation neural network (BPNN). BPNN has been widely used in classification and pattern recognition. However it has some generally acknowledged defects, usually these defects evolve from some morbidity neurons In this thesis I proposed a novel adaptive learning approach for text categorization using improved back propagation neural network. This algorithm can overcome some shortcomings in traditional back propagation neural network such as slow training speed and easy to get into local minimum. We compared the training time and performance and test the three methods on the standard Reuter-21578. The results show that the proposed algorithm is able to achieve high categorization effectiveness as measured by precision, recall and F-measure. 요약 문서분류는 정보검색에서 기계학습을 응용하는 중요한 분야이다. 본 논문에서는 다중출력 퍼셉트론 학습(Multi-Output Perceptron Learning:MOPL)과 백 프로퍼게이션 신경망(Back Propagation Neural Network:BPNN) 두 가지의 신경망 이론을 문서분류에 적용하였다. BPNN은 분류와 패턴인식에 많이 사용되고 있지만, 치명적인 신경을 포함하는 몇 가지 결점이 있다. 본 논문에서는 향상된 백 프로퍼게이션 신경망이론을 사용한 새로운 학습법을 제안할 것이다. 이 알고리즘은 기존의 백 프로퍼게이션 신경망의 느린 학습 속도와 쉽게 국소적인 제한치로 빠지는 문제를 개선할 수 있다. 로이터 자료(Reuter-21578)을 이용하여 세 가지 방법을 테스트하고, 학습시간과 성능을 비교하였다. 정확율, 재현율, 그리고 F-mesure를 통하여 본 논문에서 제안한 문서분류 알고리즘의 높은 성능을 확인할 수 있다.",
          "doc_source": "인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN) 인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN) 인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN) Abstract Li Chenghua Department of Information and Communication Chebuk National University Text categorization is an important application of machine learning to the field of document information retrieval. This thesis described two kinds of neural networks for text categorization, multi-output perceptron learning (MOPL) and back propagation neural network (BPNN). BPNN has been widely used in classification and pattern recognition. However it has some generally acknowledged defects, usually these defects evolve from some morbidity neurons In this thesis I proposed a novel adaptive learning approach for text categorization using improved back propagation neural network. This algorithm can overcome some shortcomings in traditional back propagation neural network such as slow training speed and easy to get into local minimum. We compared the training time and performance and test the three methods on the standard Reuter-21578. The results show that the proposed algorithm is able to achieve high categorization effectiveness as measured by precision, recall and F-measure. 요약 문서분류는 정보검색에서 기계학습을 응용하는 중요한 분야이다. 본 논문에서는 다중출력 퍼셉트론 학습(Multi-Output Perceptron Learning:MOPL)과 백 프로퍼게이션 신경망(Back Propagation Neural Network:BPNN) 두 가지의 신경망 이론을 문서분류에 적용하였다. BPNN은 분류와 패턴인식에 많이 사용되고 있지만, 치명적인 신경을 포함하는 몇 가지 결점이 있다. 본 논문에서는 향상된 백 프로퍼게이션 신경망이론을 사용한 새로운 학습법을 제안할 것이다. 이 알고리즘은 기존의 백 프로퍼게이션 신경망의 느린 학습 속도와 쉽게 국소적인 제한치로 빠지는 문제를 개선할 수 있다. 로이터 자료(Reuter-21578)을 이용하여 세 가지 방법을 테스트하고, 학습시간과 성능을 비교하였다. 정확율, 재현율, 그리고 F-mesure를 통하여 본 논문에서 제안한 문서분류 알고리즘의 높은 성능을 확인할 수 있다.",
          "author": "리청화",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 44,
          "score": 0.560896635055542,
          "doc_id": "DIKO0016959108",
          "title": "AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구",
          "abstract": "본 연구는 교육부의 AI 디지털교과서 개발 방향 설정에 대한 초등 교사의 인식을 알아보기 위한 목적으로 실시되었다. 이를 위해 교육부의 AI 디지털교과서 개발 정책과 한국교육학술정보원의 AI 디지털교과서 개발 가이드라인을 바탕으로 약 40개의 문항이 담긴 설문지를 제작하여 배포하였다. 회수된 초등 교사 106명의 응답을 대응표본 t-검정, Borich 요구도, The Locus for Focus 모델을 통해 통계 처리하였으며 분석을 통해 얻은 결과는 다음과 같다.&amp;#xD; 첫째, 현재 서비스 되고 있는 디지털교과서와 발행사별 교수학습지원사이트의 이용 경험은 발행사별 교수학습지원사이트 이용 경험에 비해 굉장히 낮았으며 그 원인으로는 교수학습지원사이트에 비해 멀티미디어 콘텐츠의 부족, 사용법의 어려움과 기능의 불편함, 필요한 기능의 부재 등을 꼽았다.&amp;#xD; 둘째, AI 디지털교과서의 기본 기능인 통합 인증 기능, 통합 대시보드, 디지털교과서 책장, 학습데이터 허브 기능의 필요성을 묻는 질문에 대해 약 80% 이상의 교사가 필요하다고 응답하였다.&amp;#xD; 셋째, AI 기반 맞춤형 학습 지원 기능인 학습 진단 기능, 맞춤형 콘텐츠 제공 기능, 대시보드 기능, AI 튜터 기능, AI 보조교사 기능, 교사 재구성 기능의 필요성을 묻는 질문에 대한 응답을 전체 집단과 최종 학력 기준 집단으로 분석을 실시하였다. 대응표본 t-검정 결과는 모든 집단의 응답 결과에 대해 통계적으로 유의미한 차이(&amp;amp;lt; .001)가 있는 것으로 나타났다. Borich 요구도 및 The Locus for Focus 모델에 의한 분석 결과는 전체 집단의 경우 AI 보조교사 AI 튜터 기능에 대한 요구 수준이 가장 높았으며 The Locus for Focus 모델을 통한 분석 결과도 위의 두 기능 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력을 기준으로 집단을 나누어 분석한 결과는 최종 학력이 학사인 경우와 컴퓨터 관련 석사 과정 재학 중 또는 석사 학위 소지인 경우 AI 보조교사 기능과 AI 튜터 기능에 대한 요구도가 가장 높았으며 The Locus for Focus 모델 분석 결과 또한 두 기능이 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력이 그 외 석사 과정 재학 중 또는 석사 학위 소지인 경우 위의 두 집단과 마찬가지로 AI 튜터 기능과 AI 보조교사 기능이 요구도가 가장 높았으나 The Locus for Focus 모델 분석 결과의 경우 1사분면에 위치한 기능이 존재하지 않아 개발 우선 순위에 대한 논의가 필요한 것으로 나타났다.&amp;#xD; 넷째, 여섯 가지 AI 기반 맞춤형 학습 지원 기능에 대한 추가 수요 분석 결과를 실시하였다. 학습 진단 기능에 대한 추가 수요는 학생 참여 정도 진단 서비스, 협업 정도 진단 서비스, 정서 분석 서비스 등으로 나타났다. 맞춤형 콘텐츠 제공 기능에 대한 추가 수요는 이전 학년 과정 추천 기능, 맞춤형 콘텐츠 교사 제시 기능, 문항 난이도 조절 기능, 학습 경로 설정 기능 등으로 나타났다. 대시보드 기능에 대한 추가 수요는 오답 노트 작성 기능, 학습전략 추천 기능, 토론 및 채팅 기능 등으로 나타났다. AI 튜터 기능에 대한 추가 수요는 추가 학습 자료 제공 기능, 힌트 제공 기능 등으로 나타났다. AI 보조교사 기능에 대한 추가 수요는 문항 자동 채점 기능 및 결과 제공 기능, 교과학습발달상황 및 행동발상황 작성 지원 기능, 학생 수준별 문항 자동 구성 및 출제 기능, 수행평가 결과 NEIS 전송 기능 등으로 나타났다. 교사 재구성 기능에 대한 추가 수요는 프로젝트 학습을 위한 교과서 간 내용 통합 및 순서 조정 기능, 발행사별 교과서 내용 비교 및 끌어오기 기능 등으로 나타났다. 그 밖의 기능에 대한 수요는 에듀테크 사이트 연결 기능, 게임을 통한 성취도 확인 기능, 과제에 따른 보상 기능, 학교 간 학습 내용 공유 기능, 자료 저장소 기능, 출결 확인 기능, 커뮤니티 기능 등으로 나타났다. &amp;#xD; 본 연구는 AI 디지털교과서 개발과 관련한 교육부 정책의 방향을 확인하고 한국교육학술정보원이 발간한 AI 디지털교과서 개발 가이드라인의 의미를 해석하는 데 활용될 수 있다. 또한 AI 디지털교과서 개발 방향 설정에 대한 교사의 인식 및 수요를 확인하는 자료로 사용될 수 있으며 이를 바탕으로 AI 디지털교과서 개발 관련자들이 교수학습지원시스템으로서의 AI 디지털교과서를 개발하는 데 도움이 될 것으로 기대한다.",
          "doc_source": "AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구 AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구 AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구 본 연구는 교육부의 AI 디지털교과서 개발 방향 설정에 대한 초등 교사의 인식을 알아보기 위한 목적으로 실시되었다. 이를 위해 교육부의 AI 디지털교과서 개발 정책과 한국교육학술정보원의 AI 디지털교과서 개발 가이드라인을 바탕으로 약 40개의 문항이 담긴 설문지를 제작하여 배포하였다. 회수된 초등 교사 106명의 응답을 대응표본 t-검정, Borich 요구도, The Locus for Focus 모델을 통해 통계 처리하였으며 분석을 통해 얻은 결과는 다음과 같다.&amp;#xD; 첫째, 현재 서비스 되고 있는 디지털교과서와 발행사별 교수학습지원사이트의 이용 경험은 발행사별 교수학습지원사이트 이용 경험에 비해 굉장히 낮았으며 그 원인으로는 교수학습지원사이트에 비해 멀티미디어 콘텐츠의 부족, 사용법의 어려움과 기능의 불편함, 필요한 기능의 부재 등을 꼽았다.&amp;#xD; 둘째, AI 디지털교과서의 기본 기능인 통합 인증 기능, 통합 대시보드, 디지털교과서 책장, 학습데이터 허브 기능의 필요성을 묻는 질문에 대해 약 80% 이상의 교사가 필요하다고 응답하였다.&amp;#xD; 셋째, AI 기반 맞춤형 학습 지원 기능인 학습 진단 기능, 맞춤형 콘텐츠 제공 기능, 대시보드 기능, AI 튜터 기능, AI 보조교사 기능, 교사 재구성 기능의 필요성을 묻는 질문에 대한 응답을 전체 집단과 최종 학력 기준 집단으로 분석을 실시하였다. 대응표본 t-검정 결과는 모든 집단의 응답 결과에 대해 통계적으로 유의미한 차이(&amp;amp;lt; .001)가 있는 것으로 나타났다. Borich 요구도 및 The Locus for Focus 모델에 의한 분석 결과는 전체 집단의 경우 AI 보조교사 AI 튜터 기능에 대한 요구 수준이 가장 높았으며 The Locus for Focus 모델을 통한 분석 결과도 위의 두 기능 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력을 기준으로 집단을 나누어 분석한 결과는 최종 학력이 학사인 경우와 컴퓨터 관련 석사 과정 재학 중 또는 석사 학위 소지인 경우 AI 보조교사 기능과 AI 튜터 기능에 대한 요구도가 가장 높았으며 The Locus for Focus 모델 분석 결과 또한 두 기능이 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력이 그 외 석사 과정 재학 중 또는 석사 학위 소지인 경우 위의 두 집단과 마찬가지로 AI 튜터 기능과 AI 보조교사 기능이 요구도가 가장 높았으나 The Locus for Focus 모델 분석 결과의 경우 1사분면에 위치한 기능이 존재하지 않아 개발 우선 순위에 대한 논의가 필요한 것으로 나타났다.&amp;#xD; 넷째, 여섯 가지 AI 기반 맞춤형 학습 지원 기능에 대한 추가 수요 분석 결과를 실시하였다. 학습 진단 기능에 대한 추가 수요는 학생 참여 정도 진단 서비스, 협업 정도 진단 서비스, 정서 분석 서비스 등으로 나타났다. 맞춤형 콘텐츠 제공 기능에 대한 추가 수요는 이전 학년 과정 추천 기능, 맞춤형 콘텐츠 교사 제시 기능, 문항 난이도 조절 기능, 학습 경로 설정 기능 등으로 나타났다. 대시보드 기능에 대한 추가 수요는 오답 노트 작성 기능, 학습전략 추천 기능, 토론 및 채팅 기능 등으로 나타났다. AI 튜터 기능에 대한 추가 수요는 추가 학습 자료 제공 기능, 힌트 제공 기능 등으로 나타났다. AI 보조교사 기능에 대한 추가 수요는 문항 자동 채점 기능 및 결과 제공 기능, 교과학습발달상황 및 행동발상황 작성 지원 기능, 학생 수준별 문항 자동 구성 및 출제 기능, 수행평가 결과 NEIS 전송 기능 등으로 나타났다. 교사 재구성 기능에 대한 추가 수요는 프로젝트 학습을 위한 교과서 간 내용 통합 및 순서 조정 기능, 발행사별 교과서 내용 비교 및 끌어오기 기능 등으로 나타났다. 그 밖의 기능에 대한 수요는 에듀테크 사이트 연결 기능, 게임을 통한 성취도 확인 기능, 과제에 따른 보상 기능, 학교 간 학습 내용 공유 기능, 자료 저장소 기능, 출결 확인 기능, 커뮤니티 기능 등으로 나타났다. &amp;#xD; 본 연구는 AI 디지털교과서 개발과 관련한 교육부 정책의 방향을 확인하고 한국교육학술정보원이 발간한 AI 디지털교과서 개발 가이드라인의 의미를 해석하는 데 활용될 수 있다. 또한 AI 디지털교과서 개발 방향 설정에 대한 교사의 인식 및 수요를 확인하는 자료로 사용될 수 있으며 이를 바탕으로 AI 디지털교과서 개발 관련자들이 교수학습지원시스템으로서의 AI 디지털교과서를 개발하는 데 도움이 될 것으로 기대한다.",
          "author": "이승현",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 45,
          "score": 0.5599043369293213,
          "doc_id": "ATN0027086510",
          "title": "인공신경망과 강건설계를 이용한 기계적 RMR 분류",
          "abstract": "Rock mass rating (RMR) is a relatively simple method for classifying rock mass with the naked eye; however, it becomes inconvenient when the number of survey sections is large. In this study, we developed a learning model and a prediction model using an artificial neural network (ANN) to classify RMR mechanically. Using robust design, 3125 big data were optimized into 25 learning data. The test results after learning through the two methods were exactly the same. Through robust design, the learning data obtained by reducing the total number of cases to less than 1% had the same learning effect as the whole data, which means that the effort and cost of acquiring the learning data can be greatly reduced. For the perfect prediction of the RMR classification system, we tuned the primary predictions within a given range of rating levels. As a result, we implemented an RMR classification ANN system that perfectly predicts the RMR of 3125 big data using 25 learning data through robust design.",
          "doc_source": "인공신경망과 강건설계를 이용한 기계적 RMR 분류 인공신경망과 강건설계를 이용한 기계적 RMR 분류 인공신경망과 강건설계를 이용한 기계적 RMR 분류 Rock mass rating (RMR) is a relatively simple method for classifying rock mass with the naked eye; however, it becomes inconvenient when the number of survey sections is large. In this study, we developed a learning model and a prediction model using an artificial neural network (ANN) to classify RMR mechanically. Using robust design, 3125 big data were optimized into 25 learning data. The test results after learning through the two methods were exactly the same. Through robust design, the learning data obtained by reducing the total number of cases to less than 1% had the same learning effect as the whole data, which means that the effort and cost of acquiring the learning data can be greatly reduced. For the perfect prediction of the RMR classification system, we tuned the primary predictions within a given range of rating levels. As a result, we implemented an RMR classification ANN system that perfectly predicts the RMR of 3125 big data using 25 learning data through robust design.",
          "author": "장명환;하태욱;최기훈;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 46,
          "score": 0.5593733191490173,
          "doc_id": "ATN0047358973",
          "title": "딥러닝 언어 모델과 인공신경망 기계 번역을 활용한 담화 조응 현상과 한정 명사구 연구",
          "abstract": "In this preliminary study, we investigate the phenomena of discourse anaphora and definite descriptions within the framework of the so-called “donkey sentence.” Unlike English, Korean allows for the expression of donkey anaphora using either the pronoun kukes ‘it’ or definite noun phrases (bare NP or ku+NP). Employing neural machine translations and deep learning models, we examine the appropriateness of these two types of donkey sentences in Korean through the following procedure: Firstly, utilizing ChatGPT, we generate 60 sentences with donkey structures containing both pronouns and definite noun phrases. Secondly, we employ Google Translation and Papago to translate these sentences. Thirdly, we use KR-BERT to evaluate the acceptability of the translations. Finally, we conduct a statistical analysis based on the obtained acceptability scores. The results reveal that definite noun phrases are a more natural expression than pronouns in Korean donkey sentences. This novel finding suggests that the E-type approach would provide a better theoretical account than DRT (Discourse Representation Theory).",
          "doc_source": "딥러닝 언어 모델과 인공신경망 기계 번역을 활용한 담화 조응 현상과 한정 명사구 연구 딥러닝 언어 모델과 인공신경망 기계 번역을 활용한 담화 조응 현상과 한정 명사구 연구 딥러닝 언어 모델과 인공신경망 기계 번역을 활용한 담화 조응 현상과 한정 명사구 연구 In this preliminary study, we investigate the phenomena of discourse anaphora and definite descriptions within the framework of the so-called “donkey sentence.” Unlike English, Korean allows for the expression of donkey anaphora using either the pronoun kukes ‘it’ or definite noun phrases (bare NP or ku+NP). Employing neural machine translations and deep learning models, we examine the appropriateness of these two types of donkey sentences in Korean through the following procedure: Firstly, utilizing ChatGPT, we generate 60 sentences with donkey structures containing both pronouns and definite noun phrases. Secondly, we employ Google Translation and Papago to translate these sentences. Thirdly, we use KR-BERT to evaluate the acceptability of the translations. Finally, we conduct a statistical analysis based on the obtained acceptability scores. The results reveal that definite noun phrases are a more natural expression than pronouns in Korean donkey sentences. This novel finding suggests that the E-type approach would provide a better theoretical account than DRT (Discourse Representation Theory).",
          "author": "강아름;이용훈;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 47,
          "score": 0.5588761568069458,
          "doc_id": "JAKO201017337344335",
          "title": "인공신경망모델을 이용한 교량의 상태평가",
          "abstract": "대부분의 선진국에서 교량의 유지보수 및 보강(Maintenance Repair & Rehabilitation-MR&R)으로 인한 비용은 해마다 증가하고 있다. 전산화된 교량유지관리 및 의사결정시스템(Bridge Management System-BMS)은 가능한 최저의 생애주기비용(Life Cycle Cost - LCC)에 최적의 안정성를 확보하기 위해 개발되었다. 본 논문에서는 제한된 현존하는 교량진단기록을 이용하여 현존하지 않는 과거의 교량상태등급 데이타를 생성하기 위해 Backward Prediction Model(BPM)이라 불리는 인공신경망(Artificial Neural Network-ANN)에 기초한 예측모델을 제시한다. 제안된 BPM은 한정된 교량 정기점검기록으로부터 현존하는 교량진단기록과 연관성을 확립하기 위해 교통량과 인구, 그리고 기후 등과 같은 비구조적 요소를 이용하며, 제한된 교량진단기록과 비구조적 요소 사이에 맺어진 연관성을 통해 현존하지 않는 과거의 교량상태등급 데이타를 생성할 수 있다. BPM의 신뢰도를 측정하기 위하여 Maryland DOT로 부터 얻어진 National Bridge Inventory(NBI)와 BMS 교량진단자료를 이용하였다. 이중 NBI자료를 이용한 Backward comparison 에 있어서 실제 NBI기록과 BPM으로 생성된 교량상태등급과의 차이(상판: 6.68%, 상부구조부: 6.61%, 하부구조부: 7.52%)는 BPM으로 생성된 결과의 높은 신뢰도를 보여준다. 이 연구의 결과는 제한된 정기점검 기록으로 야기되는 BMS의 장기 교량손상 예측에 관련된 사용상의 문제를 최소화하고 전반적인 BMS 결과의 신뢰도를 높이는데 기여 할 수 있다.",
          "doc_source": "인공신경망모델을 이용한 교량의 상태평가 인공신경망모델을 이용한 교량의 상태평가 인공신경망모델을 이용한 교량의 상태평가 대부분의 선진국에서 교량의 유지보수 및 보강(Maintenance Repair & Rehabilitation-MR&R)으로 인한 비용은 해마다 증가하고 있다. 전산화된 교량유지관리 및 의사결정시스템(Bridge Management System-BMS)은 가능한 최저의 생애주기비용(Life Cycle Cost - LCC)에 최적의 안정성를 확보하기 위해 개발되었다. 본 논문에서는 제한된 현존하는 교량진단기록을 이용하여 현존하지 않는 과거의 교량상태등급 데이타를 생성하기 위해 Backward Prediction Model(BPM)이라 불리는 인공신경망(Artificial Neural Network-ANN)에 기초한 예측모델을 제시한다. 제안된 BPM은 한정된 교량 정기점검기록으로부터 현존하는 교량진단기록과 연관성을 확립하기 위해 교통량과 인구, 그리고 기후 등과 같은 비구조적 요소를 이용하며, 제한된 교량진단기록과 비구조적 요소 사이에 맺어진 연관성을 통해 현존하지 않는 과거의 교량상태등급 데이타를 생성할 수 있다. BPM의 신뢰도를 측정하기 위하여 Maryland DOT로 부터 얻어진 National Bridge Inventory(NBI)와 BMS 교량진단자료를 이용하였다. 이중 NBI자료를 이용한 Backward comparison 에 있어서 실제 NBI기록과 BPM으로 생성된 교량상태등급과의 차이(상판: 6.68%, 상부구조부: 6.61%, 하부구조부: 7.52%)는 BPM으로 생성된 결과의 높은 신뢰도를 보여준다. 이 연구의 결과는 제한된 정기점검 기록으로 야기되는 BMS의 장기 교량손상 예측에 관련된 사용상의 문제를 최소화하고 전반적인 BMS 결과의 신뢰도를 높이는데 기여 할 수 있다.",
          "author": "오순택;이동준;이재호;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 48,
          "score": 0.5585173964500427,
          "doc_id": "NPAP13430839",
          "title": "AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion",
          "abstract": "<P>Recent advances of AI applications in various of industries have led to remarkable performance and efficiency. Driven by the great success of datasets and experience sharing, people are exploring more precious datasets with diverse features and longer time range. The promising reasoning information of well-curated student grade datasets is expected to assist young students to find the best of themselves and then improve their learning outcome and study experience. Through data and experience sharing, young students can have a better understanding of their learning condition and possible learning outcomes. Existing course selection systems in Taiwan which offer limited basic enrolling functions fail to provide performance prediction and course arrangement guidance based on their own learning condition. Students now selecting courses with unawareness of their expecting performance. A personalized guide for students on course selection is crucial for how they structure professional knowledge and arrange study schedule. In this paper, we first analyzed what factors can be used on defining learning curve, and discovered the difference between students with different properties and background. Second, we developed a recommendation system based on great amount of grade datasets of past students, and the system can give students suggestions on how to assign their credits based on their own learning curve and students that had similar learning curve. The result of our research demonstrates the feasibility of a new approach on applying big data and AI technology on learning analysis and course selection.</P>",
          "doc_source": "AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion <P>Recent advances of AI applications in various of industries have led to remarkable performance and efficiency. Driven by the great success of datasets and experience sharing, people are exploring more precious datasets with diverse features and longer time range. The promising reasoning information of well-curated student grade datasets is expected to assist young students to find the best of themselves and then improve their learning outcome and study experience. Through data and experience sharing, young students can have a better understanding of their learning condition and possible learning outcomes. Existing course selection systems in Taiwan which offer limited basic enrolling functions fail to provide performance prediction and course arrangement guidance based on their own learning condition. Students now selecting courses with unawareness of their expecting performance. A personalized guide for students on course selection is crucial for how they structure professional knowledge and arrange study schedule. In this paper, we first analyzed what factors can be used on defining learning curve, and discovered the difference between students with different properties and background. Second, we developed a recommendation system based on great amount of grade datasets of past students, and the system can give students suggestions on how to assign their credits based on their own learning curve and students that had similar learning curve. The result of our research demonstrates the feasibility of a new approach on applying big data and AI technology on learning analysis and course selection.</P>",
          "author": "Wu, Yu Hsuan;Wu, Eric Hsiaokuang;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 49,
          "score": 0.5531696081161499,
          "doc_id": "JAKO200606141735001",
          "title": "이동 에이전트를 이용한 병렬 인공신경망 시뮬레이터",
          "abstract": "이 논문은 이동 에이전트 시스템에 기반을 둔 가상의 병렬분산 컴퓨팅 환경에서 병렬로 수행되는 다층 인공신경망 시뮬레이터를 구현하는 것을 목적으로 한다. 다층 신경망은 학습세션, 학습데이터, 계층, 노드, 가중치 수준에서 병렬화가 이루어진다. 이 논문에서는 네트워크의 통신량이 상대적으로 적은 학습세션 및 학습데이터 수준의 병렬화가 가능한 신경망 시뮬레이터를 개발하고 평가하였다. 평가결과, 학습세션 병렬화와 학습데이터 병렬화 성능분석에서 약 3.3배의 학습 수행 성능 향상을 확인할 수 있었다. 가상의 병렬 컴퓨터에서 신경망을 병렬로 구현하여 기존의 전용병렬컴퓨터에서 수행한 신경망의 병렬처리와 비슷한 성능을 발휘한다는 점에서 이 논문의 의의가 크다고 할 수 있다. 따라서 가상의 병렬 컴퓨터를 이용하여 신경망을 개발하는데 있어서, 비교적 시간이 많이 소요되는 학습시간을 줄임으로서 신경망 개발에 상당한 도움을 줄 수 있다고 본다.",
          "doc_source": "이동 에이전트를 이용한 병렬 인공신경망 시뮬레이터 이동 에이전트를 이용한 병렬 인공신경망 시뮬레이터 이동 에이전트를 이용한 병렬 인공신경망 시뮬레이터 이 논문은 이동 에이전트 시스템에 기반을 둔 가상의 병렬분산 컴퓨팅 환경에서 병렬로 수행되는 다층 인공신경망 시뮬레이터를 구현하는 것을 목적으로 한다. 다층 신경망은 학습세션, 학습데이터, 계층, 노드, 가중치 수준에서 병렬화가 이루어진다. 이 논문에서는 네트워크의 통신량이 상대적으로 적은 학습세션 및 학습데이터 수준의 병렬화가 가능한 신경망 시뮬레이터를 개발하고 평가하였다. 평가결과, 학습세션 병렬화와 학습데이터 병렬화 성능분석에서 약 3.3배의 학습 수행 성능 향상을 확인할 수 있었다. 가상의 병렬 컴퓨터에서 신경망을 병렬로 구현하여 기존의 전용병렬컴퓨터에서 수행한 신경망의 병렬처리와 비슷한 성능을 발휘한다는 점에서 이 논문의 의의가 크다고 할 수 있다. 따라서 가상의 병렬 컴퓨터를 이용하여 신경망을 개발하는데 있어서, 비교적 시간이 많이 소요되는 학습시간을 줄임으로서 신경망 개발에 상당한 도움을 줄 수 있다고 본다.",
          "author": "조용만;강태원;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 50,
          "score": 0.5518124103546143,
          "doc_id": "JAKO202510643203882",
          "title": "AI 디지털교과서 활용 역량 강화를 위한 교사 연수 모델 개발",
          "abstract": "The advancement of artificial intelligence (AI) in education has led to the emergence of AI Digital Textbooks (AIDT), transforming teacher-student interactions by supporting personalized, self-directed, and collaborative learning. To effectively utilize AIDT, teachers require enhanced AI and digital literacy competencies. However, existing training programs lack a structured approach to integrating AI-based pedagogical strategies. This study develops a teacher training model that strengthens AIDT utilization by incorporating the TPACK framework, Cognitive Load Theory, and Learning Analytics. Additionally, the Forschendes Lernen (FL) approach is applied to foster inquiry-based learning, enabling teachers to explore and implement AI-driven teaching methods. The proposed model emphasizes hands-on AI teaching strategies, adaptive lesson design, and AI-assisted feedback mechanisms, overcoming the limitations of traditional training programs. This research provides a structured framework for AIDT-based teacher professional development and offers insights into AI integration in education. Future research should focus on empirical validation of the model's effectiveness and continuous refinement to align with emerging AI technologies.",
          "doc_source": "AI 디지털교과서 활용 역량 강화를 위한 교사 연수 모델 개발 AI 디지털교과서 활용 역량 강화를 위한 교사 연수 모델 개발 AI 디지털교과서 활용 역량 강화를 위한 교사 연수 모델 개발 The advancement of artificial intelligence (AI) in education has led to the emergence of AI Digital Textbooks (AIDT), transforming teacher-student interactions by supporting personalized, self-directed, and collaborative learning. To effectively utilize AIDT, teachers require enhanced AI and digital literacy competencies. However, existing training programs lack a structured approach to integrating AI-based pedagogical strategies. This study develops a teacher training model that strengthens AIDT utilization by incorporating the TPACK framework, Cognitive Load Theory, and Learning Analytics. Additionally, the Forschendes Lernen (FL) approach is applied to foster inquiry-based learning, enabling teachers to explore and implement AI-driven teaching methods. The proposed model emphasizes hands-on AI teaching strategies, adaptive lesson design, and AI-assisted feedback mechanisms, overcoming the limitations of traditional training programs. This research provides a structured framework for AIDT-based teacher professional development and offers insights into AI integration in education. Future research should focus on empirical validation of the model's effectiveness and continuous refinement to align with emerging AI technologies.",
          "author": "고호경;안서현;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        }
      ]
    },
    {
      "query": "What approach does the system use to learn TurKontrol’s POMDP parameters from Mechanical Turk data to optimize iterative crowdsourced tasks?",
      "query_meta": {
        "type": "single_hop",
        "index": 0
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.7991548776626587,
          "doc_id": "NART66567138",
          "title": "POMDP-based control of workflows for crowdsourcing",
          "abstract": "Crowdsourcing, outsourcing of tasks to a crowd of unknown people (''workers'') in an open call, is rapidly rising in popularity. It is already being heavily used by numerous employers (''requesters'') for solving a wide variety of tasks, such as audio transcription, content screening, and labeling training data for machine learning. However, quality control of such tasks continues to be a key challenge because of the high variability in worker quality. In this paper we show the value of decision-theoretic techniques for the problem of optimizing workflows used in crowdsourcing. In particular, we design AI agents that use Bayesian network learning and inference in combination with Partially-Observable Markov Decision Processes (POMDPs) for obtaining excellent cost-quality tradeoffs. We use these techniques for three distinct crowdsourcing scenarios: (1) control of voting to answer a binary-choice question, (2) control of an iterative improvement workflow, and (3) control of switching between alternate workflows for a task. In each scenario, we design a Bayes net model that relates worker competency, task difficulty and worker response quality. We also design a POMDP for each task, whose solution provides the dynamic control policy. We demonstrate the usefulness of our models and agents in live experiments on Amazon Mechanical Turk. We consistently achieve superior quality results than non-adaptive controllers, while incurring equal or less cost.",
          "doc_source": "POMDP-based control of workflows for crowdsourcing POMDP-based control of workflows for crowdsourcing POMDP-based control of workflows for crowdsourcing Crowdsourcing, outsourcing of tasks to a crowd of unknown people (''workers'') in an open call, is rapidly rising in popularity. It is already being heavily used by numerous employers (''requesters'') for solving a wide variety of tasks, such as audio transcription, content screening, and labeling training data for machine learning. However, quality control of such tasks continues to be a key challenge because of the high variability in worker quality. In this paper we show the value of decision-theoretic techniques for the problem of optimizing workflows used in crowdsourcing. In particular, we design AI agents that use Bayesian network learning and inference in combination with Partially-Observable Markov Decision Processes (POMDPs) for obtaining excellent cost-quality tradeoffs. We use these techniques for three distinct crowdsourcing scenarios: (1) control of voting to answer a binary-choice question, (2) control of an iterative improvement workflow, and (3) control of switching between alternate workflows for a task. In each scenario, we design a Bayes net model that relates worker competency, task difficulty and worker response quality. We also design a POMDP for each task, whose solution provides the dynamic control policy. We demonstrate the usefulness of our models and agents in live experiments on Amazon Mechanical Turk. We consistently achieve superior quality results than non-adaptive controllers, while incurring equal or less cost.",
          "author": "Dai, P.;Lin, C.H.;Mausam;Weld, D.S.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 2,
          "score": 0.7442933320999146,
          "doc_id": "NART69625850",
          "title": "Inside the Turk : Understanding Mechanical Turk as a Participant Pool",
          "abstract": "<P>Mechanical Turk (MTurk), an online labor market created by Amazon, has recently become popular among social scientists as a source of survey and experimental data. The workers who populate this market have been assessed on dimensions that are universally relevant to understanding whether, why, and when they should be recruited as research participants. We discuss the characteristics of MTurk as a participant pool for psychology and other social sciences, highlighting the traits of the MTurk samples, why people become MTurk workers and research participants, and how data quality on MTurk compares to that from other pools and depends on controllable and uncontrollable factors.</P>",
          "doc_source": "Inside the Turk : Understanding Mechanical Turk as a Participant Pool Inside the Turk : Understanding Mechanical Turk as a Participant Pool Inside the Turk : Understanding Mechanical Turk as a Participant Pool <P>Mechanical Turk (MTurk), an online labor market created by Amazon, has recently become popular among social scientists as a source of survey and experimental data. The workers who populate this market have been assessed on dimensions that are universally relevant to understanding whether, why, and when they should be recruited as research participants. We discuss the characteristics of MTurk as a participant pool for psychology and other social sciences, highlighting the traits of the MTurk samples, why people become MTurk workers and research participants, and how data quality on MTurk compares to that from other pools and depends on controllable and uncontrollable factors.</P>",
          "author": "Paolacci, Gabriele;Chandler, Jesse;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 3,
          "score": 0.7119226455688477,
          "doc_id": "NART77197564",
          "title": "Online recruitment and testing of infants with Mechanical Turk",
          "abstract": "Testing infants in the laboratory is expensive in time and money; consequently, many studies are underpowered, reducing their reproducibility. We investigated whether the online platform, Amazon Mechanical Turk (MTurk), could be used as a resource to more easily recruit and measure the behavior of infant populations. Using a looking time paradigm, with users' webcams we recorded how long infants aged 5 to 8months attended while viewing children's television programs. We found that infants (N=57) were more reliably engaged by some movies than by others and that the most engaging movies could maintain attention for approximately 70% of a 10- to 13-min period. We then identified the cinematic features within the movies. Faces, singing-and-rhyming, and camera zooms were found to increase infant attention. Together, we established that MTurk can be used as a rapid tool for effectively recruiting and testing infants.",
          "doc_source": "Online recruitment and testing of infants with Mechanical Turk Online recruitment and testing of infants with Mechanical Turk Online recruitment and testing of infants with Mechanical Turk Testing infants in the laboratory is expensive in time and money; consequently, many studies are underpowered, reducing their reproducibility. We investigated whether the online platform, Amazon Mechanical Turk (MTurk), could be used as a resource to more easily recruit and measure the behavior of infant populations. Using a looking time paradigm, with users' webcams we recorded how long infants aged 5 to 8months attended while viewing children's television programs. We found that infants (N=57) were more reliably engaged by some movies than by others and that the most engaging movies could maintain attention for approximately 70% of a 10- to 13-min period. We then identified the cinematic features within the movies. Faces, singing-and-rhyming, and camera zooms were found to increase infant attention. Together, we established that MTurk can be used as a rapid tool for effectively recruiting and testing infants.",
          "author": "Tran, M.;Cabral, L.;Patel, R.;Cusack, R.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 4,
          "score": 0.7054991126060486,
          "doc_id": "NART127620540",
          "title": "Mechanical Turk Versus Student Samples: Comparisons and Recommendations",
          "abstract": "<P>Mechanical Turk and other online crowdsourcing markets (OCMs) have become a go-to data source across scientific disciplines. In 2014 Steelman and colleagues investigated how Mechanical Turk data compared with student samples and consumer panels. They found the data to be comparable and reliable for academic research. In the nearly 10 years since its publication, the use of Mechanical Turk in research has grown substantially. To understand whether their results still hold, we conducted a partial replication to determine how Mechanical Turk workers continue to compare with students using UTAUT 2 as our theoretical model and virtual-reality headsets as the focal IT artifact. Our findings generally align with Steelman et al. (2014) and confirm that Mechanical Turk continues to offer a suitable alternative to student samples. This study reveals consistent results between the student and OCM samples, indicating the potential for interchangeability. The OCM samples are primarily male, while the student sample is majority female, following current US academic trends. All samples are significantly different in age, and only the US OCM and non-US OCM samples are similar in education. The path coefficients from the non-US OCM sample differ significantly from those from other OCM samples; the path coefficients derived from the student sample do not differ significantly from any OCM sample. While sample differences exist, as expected, many are addressable post hoc if anticipated and designed for during data collection. From our findings and the extant literature, we summarize recommendations for researchers and review teams.</P>",
          "doc_source": "Mechanical Turk Versus Student Samples: Comparisons and Recommendations Mechanical Turk Versus Student Samples: Comparisons and Recommendations Mechanical Turk Versus Student Samples: Comparisons and Recommendations <P>Mechanical Turk and other online crowdsourcing markets (OCMs) have become a go-to data source across scientific disciplines. In 2014 Steelman and colleagues investigated how Mechanical Turk data compared with student samples and consumer panels. They found the data to be comparable and reliable for academic research. In the nearly 10 years since its publication, the use of Mechanical Turk in research has grown substantially. To understand whether their results still hold, we conducted a partial replication to determine how Mechanical Turk workers continue to compare with students using UTAUT 2 as our theoretical model and virtual-reality headsets as the focal IT artifact. Our findings generally align with Steelman et al. (2014) and confirm that Mechanical Turk continues to offer a suitable alternative to student samples. This study reveals consistent results between the student and OCM samples, indicating the potential for interchangeability. The OCM samples are primarily male, while the student sample is majority female, following current US academic trends. All samples are significantly different in age, and only the US OCM and non-US OCM samples are similar in education. The path coefficients from the non-US OCM sample differ significantly from those from other OCM samples; the path coefficients derived from the student sample do not differ significantly from any OCM sample. While sample differences exist, as expected, many are addressable post hoc if anticipated and designed for during data collection. From our findings and the extant literature, we summarize recommendations for researchers and review teams.</P>",
          "author": "De Lurgio II, Stephen A.;Young, Amber;Steelman, Zachary R.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 5,
          "score": 0.701129674911499,
          "doc_id": "NART75736850",
          "title": "Mechanical Turk upends social sciences",
          "abstract": "<P>In May, 23,000 people voluntarily took part in thousands of social science experiments without ever visiting a lab. All they did was log on to Amazon Mechanical Turk (MTurk), an online crowdsourcing service run by the Seattle, Washington&#x2013;based company better known for its massive internet-based retail business. Those research subjects completed 230,000 tasks on their computers in 3.3 million minutes&#x2014;more than 6 years of effort in total. The prodigious output demonstrates the popularity of an online platform that scientists had only begun to exploit 5 years ago. But the growing use of MTurk has raised concerns, as researchers discussed at the Association for Psychological Science meeting in Chicago, Illinois, last month. Some worry that they are becoming too dependent on a commercial platform. Others question whether the research volunteers are paid fairly and treated ethically. And looming over it all are questions about who these anonymous volunteers actually are, and concerns that they are less numerous and diverse than researchers hope.</P>",
          "doc_source": "Mechanical Turk upends social sciences Mechanical Turk upends social sciences Mechanical Turk upends social sciences <P>In May, 23,000 people voluntarily took part in thousands of social science experiments without ever visiting a lab. All they did was log on to Amazon Mechanical Turk (MTurk), an online crowdsourcing service run by the Seattle, Washington&#x2013;based company better known for its massive internet-based retail business. Those research subjects completed 230,000 tasks on their computers in 3.3 million minutes&#x2014;more than 6 years of effort in total. The prodigious output demonstrates the popularity of an online platform that scientists had only begun to exploit 5 years ago. But the growing use of MTurk has raised concerns, as researchers discussed at the Association for Psychological Science meeting in Chicago, Illinois, last month. Some worry that they are becoming too dependent on a commercial platform. Others question whether the research volunteers are paid fairly and treated ethically. And looming over it all are questions about who these anonymous volunteers actually are, and concerns that they are less numerous and diverse than researchers hope.</P>",
          "author": "Bohannon, John",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 6,
          "score": 0.6906577348709106,
          "doc_id": "NART117776930",
          "title": "Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk",
          "abstract": "<P><B>Background</B></P><P>Crowdsourcing services, such as Amazon Mechanical Turk (AMT), allow researchers to use the collective intelligence of a wide range of web users for labor-intensive tasks. As the manual verification of the quality of the collected results is difficult because of the large volume of data and the quick turnaround time of the process, many questions remain to be explored regarding the reliability of these resources for developing digital public health systems.</P><P><B>Objective</B></P><P>This study aims to explore and evaluate the application of crowdsourcing, generally, and AMT, specifically, for developing digital public health surveillance systems.</P><P><B>Methods</B></P><P>We collected 296,166 crowd-generated labels for 98,722 tweets, labeled by 610 AMT workers, to develop machine learning (ML) models for detecting behaviors related to physical activity, sedentary behavior, and sleep quality among Twitter users. To infer the ground truth labels and explore the quality of these labels, we studied 4 statistical consensus methods that are agnostic of task features and only focus on worker labeling behavior. Moreover, to model the meta-information associated with each labeling task and leverage the potential of context-sensitive data in the truth inference process, we developed 7 ML models, including traditional classifiers (offline and active), a deep learning&#x2013;based classification model, and a hybrid convolutional neural network model.</P><P><B>Results</B></P><P>Although most crowdsourcing-based studies in public health have often equated majority vote with quality, the results of our study using a truth set of 9000 manually labeled tweets showed that consensus-based inference models mask underlying uncertainty in data and overlook the importance of task meta-information. Our evaluations across 3 physical activity, sedentary behavior, and sleep quality data sets showed that truth inference is a context-sensitive process, and none of the methods studied in this paper were consistently superior to others in predicting the truth label. We also found that the performance of the ML models trained on crowd-labeled data was sensitive to the quality of these labels, and poor-quality labels led to incorrect assessment of these models. Finally, we have provided a set of practical recommendations to improve the quality and reliability of crowdsourced data.</P><P><B>Conclusions</B></P><P>Our findings indicate the importance of the quality of crowd-generated labels in developing ML models designed for decision-making purposes, such as public health surveillance decisions. A combination of inference models outlined and analyzed in this study could be used to quantitatively measure and improve the quality of crowd-generated labels for training ML models.</P>",
          "doc_source": "Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk <P><B>Background</B></P><P>Crowdsourcing services, such as Amazon Mechanical Turk (AMT), allow researchers to use the collective intelligence of a wide range of web users for labor-intensive tasks. As the manual verification of the quality of the collected results is difficult because of the large volume of data and the quick turnaround time of the process, many questions remain to be explored regarding the reliability of these resources for developing digital public health systems.</P><P><B>Objective</B></P><P>This study aims to explore and evaluate the application of crowdsourcing, generally, and AMT, specifically, for developing digital public health surveillance systems.</P><P><B>Methods</B></P><P>We collected 296,166 crowd-generated labels for 98,722 tweets, labeled by 610 AMT workers, to develop machine learning (ML) models for detecting behaviors related to physical activity, sedentary behavior, and sleep quality among Twitter users. To infer the ground truth labels and explore the quality of these labels, we studied 4 statistical consensus methods that are agnostic of task features and only focus on worker labeling behavior. Moreover, to model the meta-information associated with each labeling task and leverage the potential of context-sensitive data in the truth inference process, we developed 7 ML models, including traditional classifiers (offline and active), a deep learning&#x2013;based classification model, and a hybrid convolutional neural network model.</P><P><B>Results</B></P><P>Although most crowdsourcing-based studies in public health have often equated majority vote with quality, the results of our study using a truth set of 9000 manually labeled tweets showed that consensus-based inference models mask underlying uncertainty in data and overlook the importance of task meta-information. Our evaluations across 3 physical activity, sedentary behavior, and sleep quality data sets showed that truth inference is a context-sensitive process, and none of the methods studied in this paper were consistently superior to others in predicting the truth label. We also found that the performance of the ML models trained on crowd-labeled data was sensitive to the quality of these labels, and poor-quality labels led to incorrect assessment of these models. Finally, we have provided a set of practical recommendations to improve the quality and reliability of crowdsourced data.</P><P><B>Conclusions</B></P><P>Our findings indicate the importance of the quality of crowd-generated labels in developing ML models designed for decision-making purposes, such as public health surveillance decisions. A combination of inference models outlined and analyzed in this study could be used to quantitatively measure and improve the quality of crowd-generated labels for training ML models.</P>",
          "author": "Shakeri Hossein Abad, Zahra;Butler, Gregory P;Thompson, Wendy;Lee, Joon;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 7,
          "score": 0.690432608127594,
          "doc_id": "NART106764981",
          "title": "Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data",
          "abstract": "<P>Within the field of Linguistics, Amazon&rsquo;s Mechanical Turk, a crowdsourcing marketplace specializes in computer-based Human Intelligence Tasks, has been praised as a cost efficient source of data for English and other major languages. Spanish is a good candidate due to its presence within the US and beyond. Still, detailed information concerning the linguistic and demographic profile of Spanish-speaking &lsquo;Turkers&rsquo; is missing, thus making it difficult for researchers to evaluate whether the Mechanical Turk provides the right environment for their tasks. This paper addresses this gap in our knowledge by developing the first detailed study of the presence of Spanish-speaking workers, focusing on factors relevant for research planning, namely, (socio)linguistically relevant variables and information concerning work habits. The results show that this platform provides access to a fairly active participant pool of both L1 and L2Spanish speakers as well as bilinguals. A brief introduction to how Amazon&rsquo;s Mechanical Turk works and overview of Hispanic Linguistics projects that have so far used the Mechanical Turk successfully is included.</P>",
          "doc_source": "Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data <P>Within the field of Linguistics, Amazon&rsquo;s Mechanical Turk, a crowdsourcing marketplace specializes in computer-based Human Intelligence Tasks, has been praised as a cost efficient source of data for English and other major languages. Spanish is a good candidate due to its presence within the US and beyond. Still, detailed information concerning the linguistic and demographic profile of Spanish-speaking &lsquo;Turkers&rsquo; is missing, thus making it difficult for researchers to evaluate whether the Mechanical Turk provides the right environment for their tasks. This paper addresses this gap in our knowledge by developing the first detailed study of the presence of Spanish-speaking workers, focusing on factors relevant for research planning, namely, (socio)linguistically relevant variables and information concerning work habits. The results show that this platform provides access to a fairly active participant pool of both L1 and L2Spanish speakers as well as bilinguals. A brief introduction to how Amazon&rsquo;s Mechanical Turk works and overview of Hispanic Linguistics projects that have so far used the Mechanical Turk successfully is included.</P>",
          "author": "Ortega-Santos, Iv&aacute;n;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 8,
          "score": 0.6899299621582031,
          "doc_id": "NART70754548",
          "title": "Amazon Mechanical Turk and the commodification of labour",
          "abstract": "<P>Crowd employment platforms enable firms to source labour and expertise by leveraging Internet technology. Rather than offshoring jobs to low&#8208;cost geographies, functions once performed by internal employees can be outsourced to an undefined pool of digital labour using a virtual network. This enables firms to shift costs and offload risk as they access a flexible, scalable workforce that sits outside the traditional boundaries of labour laws and regulations. The micro&#8208;tasks of &lsquo;clickwork&rsquo; are tedious, repetitive and poorly paid, with remuneration often well below minimum wage. This article will present an analysis of one of the most popular crowdsourcing sites&mdash;Mechanical Turk&mdash;to illuminate how Amazon's platform enables an array of companies to access digital labour at low cost and without any of the associated social protection or moral obligation.</P>",
          "doc_source": "Amazon Mechanical Turk and the commodification of labour Amazon Mechanical Turk and the commodification of labour Amazon Mechanical Turk and the commodification of labour <P>Crowd employment platforms enable firms to source labour and expertise by leveraging Internet technology. Rather than offshoring jobs to low&#8208;cost geographies, functions once performed by internal employees can be outsourced to an undefined pool of digital labour using a virtual network. This enables firms to shift costs and offload risk as they access a flexible, scalable workforce that sits outside the traditional boundaries of labour laws and regulations. The micro&#8208;tasks of &lsquo;clickwork&rsquo; are tedious, repetitive and poorly paid, with remuneration often well below minimum wage. This article will present an analysis of one of the most popular crowdsourcing sites&mdash;Mechanical Turk&mdash;to illuminate how Amazon's platform enables an array of companies to access digital labour at low cost and without any of the associated social protection or moral obligation.</P>",
          "author": "Bergvall&#8208;K&aring;reborn, Birgitta;Howcroft, Debra;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 9,
          "score": 0.6803871393203735,
          "doc_id": "NART74131061",
          "title": "Lessons Learned from Crowdsourcing Complex Engineering Tasks",
          "abstract": "<P><B>Crowdsourcing</B></P><P>Crowdsourcing is the practice of obtaining needed ideas, services, or content by requesting contributions from a large group of people. Amazon Mechanical Turk is a web marketplace for crowdsourcing microtasks, such as answering surveys and image tagging. We explored the limits of crowdsourcing by using Mechanical Turk for a more complicated task: analysis and creation of wind simulations.</P><P><B>Harnessing Crowdworkers for Engineering</B></P><P>Our investigation examined the feasibility of using crowdsourcing for complex, highly technical tasks. This was done to determine if the benefits of crowdsourcing could be harnessed to accurately and effectively contribute to solving complex real world engineering problems. Of course, untrained crowds cannot be used as a mere substitute for trained expertise. Rather, we sought to understand how crowd workers can be used as a large pool of labor for a preliminary analysis of complex data.</P><P><B>Virtual Wind Tunnel</B></P><P>We compared the skill of the anonymous crowd workers from Amazon Mechanical Turk with that of civil engineering graduate students, making a first pass at analyzing wind simulation data. For the first phase, we posted analysis questions to Amazon crowd workers and to two groups of civil engineering graduate students. A second phase of our experiment instructed crowd workers and students to create simulations on our Virtual Wind Tunnel website to solve a more complex task.</P><P><B>Conclusions</B></P><P>With a sufficiently comprehensive tutorial and compensation similar to typical crowd-sourcing wages, we were able to enlist crowd workers to effectively complete longer, more complex tasks with competence comparable to that of graduate students with more comprehensive, expert-level knowledge. Furthermore, more complex tasks require increased communication with the workers. As tasks become more complex, the employment relationship begins to become more akin to outsourcing than crowdsourcing. Through this investigation, we were able to stretch and explore the limits of crowdsourcing as a tool for solving complex problems.</P>",
          "doc_source": "Lessons Learned from Crowdsourcing Complex Engineering Tasks Lessons Learned from Crowdsourcing Complex Engineering Tasks Lessons Learned from Crowdsourcing Complex Engineering Tasks <P><B>Crowdsourcing</B></P><P>Crowdsourcing is the practice of obtaining needed ideas, services, or content by requesting contributions from a large group of people. Amazon Mechanical Turk is a web marketplace for crowdsourcing microtasks, such as answering surveys and image tagging. We explored the limits of crowdsourcing by using Mechanical Turk for a more complicated task: analysis and creation of wind simulations.</P><P><B>Harnessing Crowdworkers for Engineering</B></P><P>Our investigation examined the feasibility of using crowdsourcing for complex, highly technical tasks. This was done to determine if the benefits of crowdsourcing could be harnessed to accurately and effectively contribute to solving complex real world engineering problems. Of course, untrained crowds cannot be used as a mere substitute for trained expertise. Rather, we sought to understand how crowd workers can be used as a large pool of labor for a preliminary analysis of complex data.</P><P><B>Virtual Wind Tunnel</B></P><P>We compared the skill of the anonymous crowd workers from Amazon Mechanical Turk with that of civil engineering graduate students, making a first pass at analyzing wind simulation data. For the first phase, we posted analysis questions to Amazon crowd workers and to two groups of civil engineering graduate students. A second phase of our experiment instructed crowd workers and students to create simulations on our Virtual Wind Tunnel website to solve a more complex task.</P><P><B>Conclusions</B></P><P>With a sufficiently comprehensive tutorial and compensation similar to typical crowd-sourcing wages, we were able to enlist crowd workers to effectively complete longer, more complex tasks with competence comparable to that of graduate students with more comprehensive, expert-level knowledge. Furthermore, more complex tasks require increased communication with the workers. As tasks become more complex, the employment relationship begins to become more akin to outsourcing than crowdsourcing. Through this investigation, we were able to stretch and explore the limits of crowdsourcing as a tool for solving complex problems.</P>",
          "author": "Staffelbach, Matthew;Sempolinski, Peter;Kijewski-Correa, Tracy;Thain, Douglas;Wei, Daniel;Kareem, Ahsan;Madey, Gregory;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 10,
          "score": 0.6797451972961426,
          "doc_id": "NART92832666",
          "title": "Using Amazon Mechanical Turk for linguistic research",
          "abstract": "<P>Amazon?s Mechanical Turk service makes linguistic experimentation quick, easy, and inexpensive. However, researchers have not been certain about its reliability. In a series of experiments, this paper compares data collected via Mechanical Turk to those obtained using more traditional methods One set of experiments measured the predictability of words in sentences using the Cloze sentence completion task (Taylor, 1953). The correlation between traditional and Turk Cloze scores is high (rho=0.823) and both data sets perform similarly against alternative measures of contextual predictability. Five other experiments on the semantic relatedness of verbs and phrasal verbs (how much is ?lift? part of ?lift up?) manipulate the presence of the sentence context and the composition of the experimental list. The results indicate that Turk data correlate well between experiments and with data from traditional methods (rho up to 0.9), and they show high inter-rater consistency and agreement. We conclude that Mechanical Turk is a reliable source of data for complex linguistic tasks in heavy use by psycholinguists. The paper provides suggestions for best practices in data collection and scrubbing.</P>",
          "doc_source": "Using Amazon Mechanical Turk for linguistic research Using Amazon Mechanical Turk for linguistic research Using Amazon Mechanical Turk for linguistic research <P>Amazon?s Mechanical Turk service makes linguistic experimentation quick, easy, and inexpensive. However, researchers have not been certain about its reliability. In a series of experiments, this paper compares data collected via Mechanical Turk to those obtained using more traditional methods One set of experiments measured the predictability of words in sentences using the Cloze sentence completion task (Taylor, 1953). The correlation between traditional and Turk Cloze scores is high (rho=0.823) and both data sets perform similarly against alternative measures of contextual predictability. Five other experiments on the semantic relatedness of verbs and phrasal verbs (how much is ?lift? part of ?lift up?) manipulate the presence of the sentence context and the composition of the experimental list. The results indicate that Turk data correlate well between experiments and with data from traditional methods (rho up to 0.9), and they show high inter-rater consistency and agreement. We conclude that Mechanical Turk is a reliable source of data for complex linguistic tasks in heavy use by psycholinguists. The paper provides suggestions for best practices in data collection and scrubbing.</P>",
          "author": "Schnoebelen, Tyler;Kuperman, Victor;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 11,
          "score": 0.6780396103858948,
          "doc_id": "JAKO202411139606539",
          "title": "Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이",
          "abstract": "온라인 크라우드소싱 플랫폼인 Amazon Mechanical Turk(MTurk)은 뛰어난 과제 수행 기록을 가진 참가자들에게 마스터 등급을 부여한다. 그러나 MTurk의 마스터 참가자와 일반 참가자를 비교한 선행 연구들은 두 집단이 실제로 수행의 차이를 보이는가에 대해 일관되지 않은 결과를 보고했다. 또한 선행 연구들은 대부분 설문 조사 방식을 사용했으며 MTurk의 마스터와 일반 참가자의 인지 과제 수행 능력을 비교한 연구는 부족한 상황이다. 본 연구는 시각 기억 재인 과제를 사용하여 MTurk 마스터 및 일반 참가자와 오프라인에서 모집한 대학생 참가자 집단의 수행을 비교했다. 연구 결과, MTurk 마스터 참가자와 오프라인 참가자는 동일한 수준의 기억 수행을 보였다. 그러나 MTurk 일반 참가자의 기억 과제 수행은 마스터와 오프라인 참가자 집단의 결과와 차이를 보였다. 각 집단에서 기억 과제 정확률이 낮은 참가자를 제외한 후에도 동일한 결과가 나타났다. 이러한 결과는 온라인에서 참가자 집단을 적절히 선발하면 기존의 오프라인 실험 결과를 잘 재현할 수 있음을 보여준다. 동시에 본 연구의 결과는 온라인 크라우드소싱 플랫폼의 참가자 집단이 균일하지 않으며, 집단 선정 방식에 따라 연구의 결과가 다르게 나타날 수 있음을 시사한다.",
          "doc_source": "Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이 Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이 Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이 온라인 크라우드소싱 플랫폼인 Amazon Mechanical Turk(MTurk)은 뛰어난 과제 수행 기록을 가진 참가자들에게 마스터 등급을 부여한다. 그러나 MTurk의 마스터 참가자와 일반 참가자를 비교한 선행 연구들은 두 집단이 실제로 수행의 차이를 보이는가에 대해 일관되지 않은 결과를 보고했다. 또한 선행 연구들은 대부분 설문 조사 방식을 사용했으며 MTurk의 마스터와 일반 참가자의 인지 과제 수행 능력을 비교한 연구는 부족한 상황이다. 본 연구는 시각 기억 재인 과제를 사용하여 MTurk 마스터 및 일반 참가자와 오프라인에서 모집한 대학생 참가자 집단의 수행을 비교했다. 연구 결과, MTurk 마스터 참가자와 오프라인 참가자는 동일한 수준의 기억 수행을 보였다. 그러나 MTurk 일반 참가자의 기억 과제 수행은 마스터와 오프라인 참가자 집단의 결과와 차이를 보였다. 각 집단에서 기억 과제 정확률이 낮은 참가자를 제외한 후에도 동일한 결과가 나타났다. 이러한 결과는 온라인에서 참가자 집단을 적절히 선발하면 기존의 오프라인 실험 결과를 잘 재현할 수 있음을 보여준다. 동시에 본 연구의 결과는 온라인 크라우드소싱 플랫폼의 참가자 집단이 균일하지 않으며, 집단 선정 방식에 따라 연구의 결과가 다르게 나타날 수 있음을 시사한다.",
          "author": "정수근",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 12,
          "score": 0.6768598556518555,
          "doc_id": "NART134452383",
          "title": "&raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk",
          "abstract": "<P>This article centers Amazon Mechanical Turk (MTurk) workers to examine their alienation, as they complete monotonous and repetitive microtasks from behind their screens. Confronted with various &raquo;virtual assembly lines&laquo; that produce data across the globe, their labor can be further used for machine learning specifically and Artificial Intelligence more generally. Engaging with these workers and their labor is central to general contemporary and future technological developments bound to bring their own repercussions with them - including the growing and central role of algorithms in managing the world of work.</P>",
          "doc_source": "&raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk &raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk &raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk <P>This article centers Amazon Mechanical Turk (MTurk) workers to examine their alienation, as they complete monotonous and repetitive microtasks from behind their screens. Confronted with various &raquo;virtual assembly lines&laquo; that produce data across the globe, their labor can be further used for machine learning specifically and Artificial Intelligence more generally. Engaging with these workers and their labor is central to general contemporary and future technological developments bound to bring their own repercussions with them - including the growing and central role of algorithms in managing the world of work.</P>",
          "author": "Kassem, Sarrah;Wilpert (&Uuml;bersetzung), Chris W.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 13,
          "score": 0.6761407852172852,
          "doc_id": "NART88129314",
          "title": "Using Mechanical Turk to Study Clinical Populations",
          "abstract": "<P> Although participants with psychiatric symptoms, specific risk factors, or rare demographic characteristics can be difficult to identify and recruit for participation in research, participants with these characteristics are crucial for research in the social, behavioral, and clinical sciences. Online research in general and crowdsourcing software in particular may offer a solution. However, no research to date has examined the utility of crowdsourcing software for conducting research on psychopathology. In the current study, we examined the prevalence of several psychiatric disorders and related problems, as well as the reliability and validity of participant reports on these domains, among users of Amazon&rsquo;s Mechanical Turk. Findings suggest that crowdsourcing software offers several advantages for clinical research while providing insight into potential problems, such as misrepresentation, that researchers should address when collecting data online. </P>",
          "doc_source": "Using Mechanical Turk to Study Clinical Populations Using Mechanical Turk to Study Clinical Populations Using Mechanical Turk to Study Clinical Populations <P> Although participants with psychiatric symptoms, specific risk factors, or rare demographic characteristics can be difficult to identify and recruit for participation in research, participants with these characteristics are crucial for research in the social, behavioral, and clinical sciences. Online research in general and crowdsourcing software in particular may offer a solution. However, no research to date has examined the utility of crowdsourcing software for conducting research on psychopathology. In the current study, we examined the prevalence of several psychiatric disorders and related problems, as well as the reliability and validity of participant reports on these domains, among users of Amazon&rsquo;s Mechanical Turk. Findings suggest that crowdsourcing software offers several advantages for clinical research while providing insight into potential problems, such as misrepresentation, that researchers should address when collecting data online. </P>",
          "author": "Shapiro, Danielle N.;Chandler, Jesse;Mueller, Pam A.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 14,
          "score": 0.6758310794830322,
          "doc_id": "ATN0036732137",
          "title": "How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk",
          "abstract": "<jats:p>Since its very first appearance the concept of crowdsourcing has undergone major variations, coming to include highly heterogeneous phenomena such as Google’s data mining, exchanges on sharing economy platforms (e.g. Airbnb or eBay), contents production within creative communities online (e.g. Wikipedia) and much more. If one assumes a very broad perspective, it is eventually possible to extend the category of crowdsourcing to cover whatsoever phenomena involving the participation of the crowd online, as in fact has been done. On the contrary, I will argue that crowdsourcing – and in particular its microwork branch – represents the specific practice of extending outsourcing processes to a large, low-cost, scalable and flexible workforce, in order to generate greater added value for a supply chain. To develop this analysis, I will especially focus on the case of Amazon Mechanical Turk, and on how the operations carried out on this platform are primarily intended to manage the huge flow of information which spans across a supply chain. The practice of subcontracting to the crowd tasks previously carried out by employees or third-party suppliers highlights how crowdsourcing involves a reshaping of the supply chain, further extending it to a large network of individuals. Through crowdsourcing processes, companies are either able to replace or train AI, integrating human computation skills in algorithmic structures through simple, and oftentimes tedious, microtasks. In this context, processes of gamification are capable to put further downward pressure on already small piece-wages, as long as crowdworkers are rather willing to earn an even lower economic compensation, if it’s associated to challenging tasks; thus, to make a task more enjoyable through gamification could be an effective way to further reduce a supply chain’s expenditures in crowdsourcing, pushing forward labor exploitation practices structurally embedded in this phenomenon.</jats:p>",
          "doc_source": "How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk <jats:p>Since its very first appearance the concept of crowdsourcing has undergone major variations, coming to include highly heterogeneous phenomena such as Google’s data mining, exchanges on sharing economy platforms (e.g. Airbnb or eBay), contents production within creative communities online (e.g. Wikipedia) and much more. If one assumes a very broad perspective, it is eventually possible to extend the category of crowdsourcing to cover whatsoever phenomena involving the participation of the crowd online, as in fact has been done. On the contrary, I will argue that crowdsourcing – and in particular its microwork branch – represents the specific practice of extending outsourcing processes to a large, low-cost, scalable and flexible workforce, in order to generate greater added value for a supply chain. To develop this analysis, I will especially focus on the case of Amazon Mechanical Turk, and on how the operations carried out on this platform are primarily intended to manage the huge flow of information which spans across a supply chain. The practice of subcontracting to the crowd tasks previously carried out by employees or third-party suppliers highlights how crowdsourcing involves a reshaping of the supply chain, further extending it to a large network of individuals. Through crowdsourcing processes, companies are either able to replace or train AI, integrating human computation skills in algorithmic structures through simple, and oftentimes tedious, microtasks. In this context, processes of gamification are capable to put further downward pressure on already small piece-wages, as long as crowdworkers are rather willing to earn an even lower economic compensation, if it’s associated to challenging tasks; thus, to make a task more enjoyable through gamification could be an effective way to further reduce a supply chain’s expenditures in crowdsourcing, pushing forward labor exploitation practices structurally embedded in this phenomenon.</jats:p>",
          "author": "De Lellis Lorenzo",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 15,
          "score": 0.6731349229812622,
          "doc_id": "NART123803221",
          "title": "Running experiments on Amazon Mechanical Turk",
          "abstract": "<P><B>Abstract</B><P>Although Mechanical Turk has recently become popular among social scientists as a source of experimental data, doubts may linger about the quality of data provided by subjects recruited from online labor markets. We address these potential concerns by presenting new demographic data about the Mechanical Turk subject population, reviewing the strengths of Mechanical Turk relative to other online and offline methods of recruiting subjects, and comparing the magnitude of effects obtained using Mechanical Turk and traditional subject pools. We further discuss some additional benefits such as the possibility of longitudinal, cross cultural and prescreening designs, and offer some advice on how to best manage a common subject pool.</P></P>",
          "doc_source": "Running experiments on Amazon Mechanical Turk Running experiments on Amazon Mechanical Turk Running experiments on Amazon Mechanical Turk <P><B>Abstract</B><P>Although Mechanical Turk has recently become popular among social scientists as a source of experimental data, doubts may linger about the quality of data provided by subjects recruited from online labor markets. We address these potential concerns by presenting new demographic data about the Mechanical Turk subject population, reviewing the strengths of Mechanical Turk relative to other online and offline methods of recruiting subjects, and comparing the magnitude of effects obtained using Mechanical Turk and traditional subject pools. We further discuss some additional benefits such as the possibility of longitudinal, cross cultural and prescreening designs, and offer some advice on how to best manage a common subject pool.</P></P>",
          "author": "Paolacci, Gabriele;Chandler, Jesse;Ipeirotis, Panagiotis G.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 16,
          "score": 0.666456401348114,
          "doc_id": "NART103944733",
          "title": "The Language Demographics of Amazon Mechanical Turk",
          "abstract": "<P> We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers&rsquo; self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems. </P>",
          "doc_source": "The Language Demographics of Amazon Mechanical Turk The Language Demographics of Amazon Mechanical Turk The Language Demographics of Amazon Mechanical Turk <P> We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers&rsquo; self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems. </P>",
          "author": "Pavlick, Ellie;Post, Matt;Irvine, Ann;Kachaev, Dmitry;Callison-Burch, Chris;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 17,
          "score": 0.6585756540298462,
          "doc_id": "NART73517650",
          "title": "Extending the Applicability of POMDP Solutions to Robotic Tasks",
          "abstract": "<P>Partially observable Markov decision processes (POMDPs) are used in many robotic task classes from soccer to household chores. Determining an approximately optimal action policy for POMDPs is PSPACE-complete, and the exponential growth of computation time prohibits solving large tasks. This paper describes two techniques to extend the range of robotic tasks that can be solved using a POMDP. Our first technique reduces the motion constraints of a robot and, then, uses state-of-the-art robotic motion planning techniques to respect the true motion constraints at runtime. We then propose a novel task decomposition that can be applied to some indoor robotic tasks. This decomposition transforms a long time horizon task into a set of shorter tasks. We empirically demonstrate the performance gain provided by these two techniques through simulated execution in a variety of environments. Comparing a direct formulation of a POMDP to solving our proposed reductions, we conclude that the techniques proposed in this paper can provide significant enhancement to current POMDP solution techniques, extending the POMDP instances that can be solved to include large continuous-state robotic tasks.</P>",
          "doc_source": "Extending the Applicability of POMDP Solutions to Robotic Tasks Extending the Applicability of POMDP Solutions to Robotic Tasks Extending the Applicability of POMDP Solutions to Robotic Tasks <P>Partially observable Markov decision processes (POMDPs) are used in many robotic task classes from soccer to household chores. Determining an approximately optimal action policy for POMDPs is PSPACE-complete, and the exponential growth of computation time prohibits solving large tasks. This paper describes two techniques to extend the range of robotic tasks that can be solved using a POMDP. Our first technique reduces the motion constraints of a robot and, then, uses state-of-the-art robotic motion planning techniques to respect the true motion constraints at runtime. We then propose a novel task decomposition that can be applied to some indoor robotic tasks. This decomposition transforms a long time horizon task into a set of shorter tasks. We empirically demonstrate the performance gain provided by these two techniques through simulated execution in a variety of environments. Comparing a direct formulation of a POMDP to solving our proposed reductions, we conclude that the techniques proposed in this paper can provide significant enhancement to current POMDP solution techniques, extending the POMDP instances that can be solved to include large continuous-state robotic tasks.</P>",
          "author": "Grady, Devin K.;Moll, Mark;Kavraki, Lydia E.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 18,
          "score": 0.6553840637207031,
          "doc_id": "NART73604379",
          "title": "Conducting behavioral research on Amazon’s Mechanical Turk",
          "abstract": "<P>Amazon&#039;s Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.</P>",
          "doc_source": "Conducting behavioral research on Amazon’s Mechanical Turk Conducting behavioral research on Amazon’s Mechanical Turk Conducting behavioral research on Amazon’s Mechanical Turk <P>Amazon&#039;s Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.</P>",
          "author": "Mason, Winter;Suri, Siddharth;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 19,
          "score": 0.6549183130264282,
          "doc_id": "NART87817032",
          "title": "Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative",
          "abstract": "<P>In this paper we evaluate how to effectively use the crowdsourcing service, Amazon's Mechanical Turk (MTurk), to content analyze textual data for use in psychological research. MTurk is a marketplace for discrete tasks completed by workers, typically for small amounts of money. MTurk has been used to aid psychological research in general, and content analysis in particular. In the current study, MTurk workers content analyzed personally-written textual data using coding categories previously developed and validated in psychological research. These codes were evaluated for reliability, accuracy, completion time, and cost. Results indicate that MTurk workers categorized textual data with comparable reliability and accuracy to both previously published studies and expert raters. Further, the coding tasks were performed quickly and cheaply. These data suggest that crowdsourced content analysis can help advance psychological research.</P>",
          "doc_source": "Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative <P>In this paper we evaluate how to effectively use the crowdsourcing service, Amazon's Mechanical Turk (MTurk), to content analyze textual data for use in psychological research. MTurk is a marketplace for discrete tasks completed by workers, typically for small amounts of money. MTurk has been used to aid psychological research in general, and content analysis in particular. In the current study, MTurk workers content analyzed personally-written textual data using coding categories previously developed and validated in psychological research. These codes were evaluated for reliability, accuracy, completion time, and cost. Results indicate that MTurk workers categorized textual data with comparable reliability and accuracy to both previously published studies and expert raters. Further, the coding tasks were performed quickly and cheaply. These data suggest that crowdsourced content analysis can help advance psychological research.</P>",
          "author": "Tosti-Kharas, Jennifer;Conley, Caryn;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 20,
          "score": 0.6476551294326782,
          "doc_id": "NART105659807",
          "title": "Analyzing the Amazon Mechanical Turk marketplace",
          "abstract": "<P>An associate professor at New York Universitys Stern School of Business uncovers answers about who are the employers in paid crowdsourcing, what tasks they post, and how much they pay.</P>",
          "doc_source": "Analyzing the Amazon Mechanical Turk marketplace Analyzing the Amazon Mechanical Turk marketplace Analyzing the Amazon Mechanical Turk marketplace <P>An associate professor at New York Universitys Stern School of Business uncovers answers about who are the employers in paid crowdsourcing, what tasks they post, and how much they pay.</P>",
          "author": "Ipeirotis, Panagiotis G.",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 21,
          "score": 0.6440168023109436,
          "doc_id": "NART73267731",
          "title": "The (Non) Religion of Mechanical Turk Workers",
          "abstract": "<P>Social science researchers have increasingly come to utilize Amazon's Mechanical Turk (MTurk) to obtain adult, opt&#8208;in samples for use with experiments. Based on the demographic characteristics of MTurk samples, studies have provided some support for the representativeness of MTurk. Others have warranted caution based on demographic characteristics and comparisons of reliability. Yet, what is missing is an examination of the most glaring demographic difference in MTurk&mdash;religion. We compare five MTurk samples with a student convenience sample and the 2012 General Social Survey, finding that MTurk samples have a consistent bias toward nonreligion. MTurk surveys significantly overrepresent seculars and underrepresent Catholics and evangelical Protestants. We then compare the religiosity of religious identifiers across samples as well as relationships between religiosity and partisanship, finding many similarities and a few important differences from the general population.</P>",
          "doc_source": "The (Non) Religion of Mechanical Turk Workers The (Non) Religion of Mechanical Turk Workers The (Non) Religion of Mechanical Turk Workers <P>Social science researchers have increasingly come to utilize Amazon's Mechanical Turk (MTurk) to obtain adult, opt&#8208;in samples for use with experiments. Based on the demographic characteristics of MTurk samples, studies have provided some support for the representativeness of MTurk. Others have warranted caution based on demographic characteristics and comparisons of reliability. Yet, what is missing is an examination of the most glaring demographic difference in MTurk&mdash;religion. We compare five MTurk samples with a student convenience sample and the 2012 General Social Survey, finding that MTurk samples have a consistent bias toward nonreligion. MTurk surveys significantly overrepresent seculars and underrepresent Catholics and evangelical Protestants. We then compare the religiosity of religious identifiers across samples as well as relationships between religiosity and partisanship, finding many similarities and a few important differences from the general population.</P>",
          "author": "Lewis, Andrew R.;Djupe, Paul A.;Mockabee, Stephen T.;Su&#8208;Ya Wu, Joshua;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 22,
          "score": 0.6436327695846558,
          "doc_id": "NPAP13833015",
          "title": "Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk",
          "abstract": "nan",
          "doc_source": "Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk ",
          "author": "Uzor, Stephen;Jacques, Jason T.;Dudley, John J;Kristensson, Per Ola;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 23,
          "score": 0.6430321931838989,
          "doc_id": "NART109637313",
          "title": "Annotator Rationales for Labeling Tasks in Crowdsourcing",
          "abstract": "<P>When collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. To address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. We evaluate this approach on an information retrieval task in which human judges rate the relevance of Web pages for different search topics. Cost-benefit analysis over 10,000 judgments collected on Amazon&rsquo;s Mechanical Turk suggests a win-win. Firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. Secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. Consequently, we can realize the above benefits with minimal additional cost.</P>",
          "doc_source": "Annotator Rationales for Labeling Tasks in Crowdsourcing Annotator Rationales for Labeling Tasks in Crowdsourcing Annotator Rationales for Labeling Tasks in Crowdsourcing <P>When collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. To address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. We evaluate this approach on an information retrieval task in which human judges rate the relevance of Web pages for different search topics. Cost-benefit analysis over 10,000 judgments collected on Amazon&rsquo;s Mechanical Turk suggests a win-win. Firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. Secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. Consequently, we can realize the above benefits with minimal additional cost.</P>",
          "author": "Kutlu, Mucahid;McDonnell, Tyler;Lease, Matthew;Elsayed, Tamer;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 24,
          "score": 0.6416828632354736,
          "doc_id": "NPAP12463036",
          "title": "Exploring Crowd Consistency in a Mechanical Turk Survey",
          "abstract": "<P>Crowdsourcing can provide a platform for evaluating software engineering research. In this paper, we aim to explore characteristics of the worker population on Amazon's Mechanical Turk, a popular micro task crowdsourcing environment, and measure the percentage of workers who are potentially qualified to perform software- or computer science- related tasks. Through a baseline survey and two replications, we measure workers' answer consistency as well as the consistency of sample characteristics. In the end, we deployed 1,200 total surveys that were completed by 1,064 unique workers. Our results show that 24% of the study participants have a computer science or IT background and most people are payment driven when choosing tasks. The sample characteristics can vary significantly, even on large samples with 300 participants. Additionally, we often observed inconsistency in workers' answers for those who completed two surveys; approximately 30% answered at least one question inconsistently between the two survey submissions. This implies a need for replication and quality controls in crowdsourced experiments.</P>",
          "doc_source": "Exploring Crowd Consistency in a Mechanical Turk Survey Exploring Crowd Consistency in a Mechanical Turk Survey Exploring Crowd Consistency in a Mechanical Turk Survey <P>Crowdsourcing can provide a platform for evaluating software engineering research. In this paper, we aim to explore characteristics of the worker population on Amazon's Mechanical Turk, a popular micro task crowdsourcing environment, and measure the percentage of workers who are potentially qualified to perform software- or computer science- related tasks. Through a baseline survey and two replications, we measure workers' answer consistency as well as the consistency of sample characteristics. In the end, we deployed 1,200 total surveys that were completed by 1,064 unique workers. Our results show that 24% of the study participants have a computer science or IT background and most people are payment driven when choosing tasks. The sample characteristics can vary significantly, even on large samples with 300 participants. Additionally, we often observed inconsistency in workers' answers for those who completed two surveys; approximately 30% answered at least one question inconsistently between the two survey submissions. This implies a need for replication and quality controls in crowdsourced experiments.</P>",
          "author": "Sun, Peng;Stolee, Kathryn T.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 25,
          "score": 0.6381716132164001,
          "doc_id": "NPAP13914175",
          "title": "Quality management on Amazon Mechanical Turk",
          "abstract": "nan",
          "doc_source": "Quality management on Amazon Mechanical Turk Quality management on Amazon Mechanical Turk Quality management on Amazon Mechanical Turk ",
          "author": "Ipeirotis, Panagiotis G.;Provost, Foster;Wang, Jing;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 26,
          "score": 0.6280820369720459,
          "doc_id": "NART70960968",
          "title": "A reliability analysis of Mechanical Turk data",
          "abstract": "Amazon's Mechanical Turk (MTurk) provides researchers with access to a diverse set of people who can serve as research participants, making the process of data collection a streamlined and cost-effective one. While a small number of studies are often cited to support the use of this methodology, there remains a need for additional analyses of the quality of the research data. In the present study, MTurk-based responses for a personality scale were found to be significantly less reliable than scores previously reported for a community sample. While score reliability was not affected by the length of the survey or the payment rates, the presence of an item asking respondents to affirm that they were attentive and honest was associated with more reliable responses. Best practices for MTurk-based research and continuing research needs are addressed.",
          "doc_source": "A reliability analysis of Mechanical Turk data A reliability analysis of Mechanical Turk data A reliability analysis of Mechanical Turk data Amazon's Mechanical Turk (MTurk) provides researchers with access to a diverse set of people who can serve as research participants, making the process of data collection a streamlined and cost-effective one. While a small number of studies are often cited to support the use of this methodology, there remains a need for additional analyses of the quality of the research data. In the present study, MTurk-based responses for a personality scale were found to be significantly less reliable than scores previously reported for a community sample. While score reliability was not affected by the length of the survey or the payment rates, the presence of an item asking respondents to affirm that they were attentive and honest was associated with more reliable responses. Best practices for MTurk-based research and continuing research needs are addressed.",
          "author": "Rouse, S.V.",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 27,
          "score": 0.6215888261795044,
          "doc_id": "NART69743055",
          "title": "Crowdsourcing content analysis for managerial research",
          "abstract": "<P><B>Purpose</B></P> <P> &ndash; The purpose of this paper is to evaluate the effectiveness of a novel method for performing content analysis in managerial research &ndash; crowdsourcing, a system where geographically distributed workers complete small, discrete tasks via the internet for a small amount of money. </P> <P><B>Design/methodology/approach</B></P> <P> &ndash; The authors examined whether workers from one popular crowdsourcing marketplace, Amazon's Mechanical Turk, could perform subjective content analytic tasks involving the application of inductively generated codes to unstructured, personally written textual passages. </P> <P><B>Findings</B></P> <P> &ndash; The findings suggest that anonymous, self-selected, non-expert crowdsourced workers were applied content codes efficiently and at low cost, and that their reliability and accuracy compared to that of trained researchers. </P> <P><B>Research limitations/implications</B></P> <P> &ndash; The authors provide recommendations for management researchers interested in using crowdsourcing most effectively for content analysis, including a discussion of the limitations and ethical issues involved in using this method. Future research could extend the findings by considering alternative data sources and coding schemes of interest to management researchers. </P> <P><B>Originality/value</B></P> <P> &ndash; Scholars have begun to explore whether crowdsourcing can assist in academic research; however, this is the first study to examine how crowdsourcing might facilitate content analysis. Crowdsourcing offers several advantages over existing content analytic approaches by combining the efficiency of computer-aided text analysis with the interpretive ability of traditional human coding.</P>",
          "doc_source": "Crowdsourcing content analysis for managerial research Crowdsourcing content analysis for managerial research Crowdsourcing content analysis for managerial research <P><B>Purpose</B></P> <P> &ndash; The purpose of this paper is to evaluate the effectiveness of a novel method for performing content analysis in managerial research &ndash; crowdsourcing, a system where geographically distributed workers complete small, discrete tasks via the internet for a small amount of money. </P> <P><B>Design/methodology/approach</B></P> <P> &ndash; The authors examined whether workers from one popular crowdsourcing marketplace, Amazon's Mechanical Turk, could perform subjective content analytic tasks involving the application of inductively generated codes to unstructured, personally written textual passages. </P> <P><B>Findings</B></P> <P> &ndash; The findings suggest that anonymous, self-selected, non-expert crowdsourced workers were applied content codes efficiently and at low cost, and that their reliability and accuracy compared to that of trained researchers. </P> <P><B>Research limitations/implications</B></P> <P> &ndash; The authors provide recommendations for management researchers interested in using crowdsourcing most effectively for content analysis, including a discussion of the limitations and ethical issues involved in using this method. Future research could extend the findings by considering alternative data sources and coding schemes of interest to management researchers. </P> <P><B>Originality/value</B></P> <P> &ndash; Scholars have begun to explore whether crowdsourcing can assist in academic research; however, this is the first study to examine how crowdsourcing might facilitate content analysis. Crowdsourcing offers several advantages over existing content analytic approaches by combining the efficiency of computer-aided text analysis with the interpretive ability of traditional human coding.</P>",
          "author": "Conley, Caryn;Tosti-Kharas, Jennifer;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 28,
          "score": 0.6160407066345215,
          "doc_id": "NART120020213",
          "title": "Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces",
          "abstract": "<P>Controlling the quality of tasks is a major challenge in crowdsourcing marketplaces. Most of the existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in the marketplaces have to monitor the tasks continuously to find such improper tasks; however, it is too expensive to manually investigate each task. In this paper, we present the reports of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We perform experiments using real task data from a commercial crowdsourcing marketplace and show that the classifier trained by the operator judgments achieves high accuracy in detecting improper tasks. In addition, to reduce the annotation costs of the operator and improve the classification accuracy, we consider the use of crowdsourcing for task annotation. We hire a group of crowdsourcing (non-expert) workers to monitor posted tasks, and incorporate their judgments into the training data of the classifier. By applying quality control techniques to handle the variability in worker reliability, our results show that the use of non-expert judgments by crowdsourcing workers in combination with expert judgments improves the accuracy of detecting improper crowdsourcing tasks.</P>",
          "doc_source": "Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces <P>Controlling the quality of tasks is a major challenge in crowdsourcing marketplaces. Most of the existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in the marketplaces have to monitor the tasks continuously to find such improper tasks; however, it is too expensive to manually investigate each task. In this paper, we present the reports of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We perform experiments using real task data from a commercial crowdsourcing marketplace and show that the classifier trained by the operator judgments achieves high accuracy in detecting improper tasks. In addition, to reduce the annotation costs of the operator and improve the classification accuracy, we consider the use of crowdsourcing for task annotation. We hire a group of crowdsourcing (non-expert) workers to monitor posted tasks, and incorporate their judgments into the training data of the classifier. By applying quality control techniques to handle the variability in worker reliability, our results show that the use of non-expert judgments by crowdsourcing workers in combination with expert judgments improves the accuracy of detecting improper crowdsourcing tasks.</P>",
          "author": "Baba, Yukino;Kashima, Hisashi;Kinoshita, Kei;Yamaguchi, Goushi;Akiyoshi, Yosuke;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 29,
          "score": 0.615523099899292,
          "doc_id": "NART78755911",
          "title": "Using Mechanical Turk for research on cancer survivors",
          "abstract": "<P><B>Abstract</B></P><P><B>Objective</B></P><P>The successful recruitment and study of cancer survivors within psycho&#8208;oncology research can be challenging, time&#8208;consuming, and expensive, particularly for key subgroups such as young adult cancer survivors. Online crowdsourcing platforms offer a potential solution that has not yet been investigated with regard to cancer populations. The current study assessed the presence of cancer survivors on Amazon's Mechanical Turk (MTurk) and the feasibility of using MTurk as an efficient, cost&#8208;effective, and reliable psycho&#8208;oncology recruitment and research platform.</P><P><B>Methods</B></P><P>During a <4&#8208;month period, cancer survivors living in the United States were recruited on MTurk to complete two assessments, spaced 1 week apart, relating to psychosocial and cancer&#8208;related functioning. The reliability and validity of responses were investigated.</P><P><B>Results</B></P><P>Within a <4&#8208;month period, 464 self&#8208;identified cancer survivors on MTurk consented to and completed an online assessment. The vast majority (79.09%) provided reliable and valid study data according to multiple indices. The sample was highly diverse in terms of U.S. geography, socioeconomic status, and cancer type, and reflected a particularly strong presence of distressed and young adult cancer survivors (median age = 36 years). A majority of participants (58.19%) responded to a second survey sent one week later.</P><P><B>Conclusions</B></P><P>Online crowdsourcing represents a feasible, efficient, and cost&#8208;effective recruitment and research platform for cancer survivors, particularly for young adult cancer survivors and those with significant distress. We discuss remaining challenges and future recommendations. Copyright &copy; 2016 John Wiley &amp; Sons, Ltd.</P>",
          "doc_source": "Using Mechanical Turk for research on cancer survivors Using Mechanical Turk for research on cancer survivors Using Mechanical Turk for research on cancer survivors <P><B>Abstract</B></P><P><B>Objective</B></P><P>The successful recruitment and study of cancer survivors within psycho&#8208;oncology research can be challenging, time&#8208;consuming, and expensive, particularly for key subgroups such as young adult cancer survivors. Online crowdsourcing platforms offer a potential solution that has not yet been investigated with regard to cancer populations. The current study assessed the presence of cancer survivors on Amazon's Mechanical Turk (MTurk) and the feasibility of using MTurk as an efficient, cost&#8208;effective, and reliable psycho&#8208;oncology recruitment and research platform.</P><P><B>Methods</B></P><P>During a <4&#8208;month period, cancer survivors living in the United States were recruited on MTurk to complete two assessments, spaced 1 week apart, relating to psychosocial and cancer&#8208;related functioning. The reliability and validity of responses were investigated.</P><P><B>Results</B></P><P>Within a <4&#8208;month period, 464 self&#8208;identified cancer survivors on MTurk consented to and completed an online assessment. The vast majority (79.09%) provided reliable and valid study data according to multiple indices. The sample was highly diverse in terms of U.S. geography, socioeconomic status, and cancer type, and reflected a particularly strong presence of distressed and young adult cancer survivors (median age = 36 years). A majority of participants (58.19%) responded to a second survey sent one week later.</P><P><B>Conclusions</B></P><P>Online crowdsourcing represents a feasible, efficient, and cost&#8208;effective recruitment and research platform for cancer survivors, particularly for young adult cancer survivors and those with significant distress. We discuss remaining challenges and future recommendations. Copyright &copy; 2016 John Wiley &amp; Sons, Ltd.</P>",
          "author": "Arch, Joanna J.;Carr, Alaina L.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 30,
          "score": 0.6090068817138672,
          "doc_id": "NART121336460",
          "title": "Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples",
          "abstract": "<P> Two of the most popular populations for convenience sampling used in the psychological sciences are college students and Mechanical Turk (MTurk) workers. College students represent a traditional type of convenience sample, whereas MTurk workers provide a more modern source of data. However, little research has examined how these populations differ from each other in salient characteristics. Additionally, no research to date has investigated how MTurk college students (a traditional sample collected using modern methods) compare to either population. The current study examined 1,248 participants comprising three samples: MTurk noncollege workers ( n = 533), MTurk college students ( n = 385), and traditional college students ( n = 330). We compared the samples on demographic characteristics, study completion time, attention, and individual difference variables (i.e., personality, social desirability, need for cognition, personal values, and social attitudes). We examined the individual difference variables in terms of mean responses, internal consistency estimates, and subscale intercorrelations. Results indicated the samples were distinct from each other in terms of all variables assessed; in addition, adding demographic characteristics as covariates to the analyses of individual difference variables did not effectively account for sample differences. We conclude that research using convenience samples should take these differences into account. </P>",
          "doc_source": "Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples <P> Two of the most popular populations for convenience sampling used in the psychological sciences are college students and Mechanical Turk (MTurk) workers. College students represent a traditional type of convenience sample, whereas MTurk workers provide a more modern source of data. However, little research has examined how these populations differ from each other in salient characteristics. Additionally, no research to date has investigated how MTurk college students (a traditional sample collected using modern methods) compare to either population. The current study examined 1,248 participants comprising three samples: MTurk noncollege workers ( n = 533), MTurk college students ( n = 385), and traditional college students ( n = 330). We compared the samples on demographic characteristics, study completion time, attention, and individual difference variables (i.e., personality, social desirability, need for cognition, personal values, and social attitudes). We examined the individual difference variables in terms of mean responses, internal consistency estimates, and subscale intercorrelations. Results indicated the samples were distinct from each other in terms of all variables assessed; in addition, adding demographic characteristics as covariates to the analyses of individual difference variables did not effectively account for sample differences. We conclude that research using convenience samples should take these differences into account. </P>",
          "author": "Weigold, Arne;Weigold, Ingrid K.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 31,
          "score": 0.6054846048355103,
          "doc_id": "NART73866218",
          "title": "The Southern Dative Presentative Meets Mechanical Turk",
          "abstract": "<P>This article introduces the southern dative presentative, an understudied construction that varies across speakers of American English. The authors discuss similarities and differences between this construction and the better-studied personal dative construction and compare the Southern dative presentative with similar constructions cross-linguistically. They then present the results of a nationwide acceptability judgment survey administered on Amazon Mechanical Turk. The results show that Southern dative presentatives are alive and well in Southern dialects of American English. In the process, they also illustrate the usefulness of Amazon Mechanical Turk (and similar crowdsourcing platforms) for the study of dialect variation in the domain of syntax.</P>",
          "doc_source": "The Southern Dative Presentative Meets Mechanical Turk The Southern Dative Presentative Meets Mechanical Turk The Southern Dative Presentative Meets Mechanical Turk <P>This article introduces the southern dative presentative, an understudied construction that varies across speakers of American English. The authors discuss similarities and differences between this construction and the better-studied personal dative construction and compare the Southern dative presentative with similar constructions cross-linguistically. They then present the results of a nationwide acceptability judgment survey administered on Amazon Mechanical Turk. The results show that Southern dative presentatives are alive and well in Southern dialects of American English. In the process, they also illustrate the usefulness of Amazon Mechanical Turk (and similar crowdsourcing platforms) for the study of dialect variation in the domain of syntax.</P>",
          "author": "Wood, Jim;Horn, Laurence;Zanuttini, Raffaella;Lindemann, Luke;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 32,
          "score": 0.6032065153121948,
          "doc_id": "NART111084731",
          "title": "Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction",
          "abstract": "nan",
          "doc_source": "Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction ",
          "author": "Nam, Sangha;Lee, Minho;Heo, Cheolhoon;Choi, Key-Sun;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 33,
          "score": 0.6028676629066467,
          "doc_id": "NPAP13897603",
          "title": "TurKit : tools for iterative tasks on mechanical Turk",
          "abstract": "nan",
          "doc_source": "TurKit : tools for iterative tasks on mechanical Turk TurKit : tools for iterative tasks on mechanical Turk TurKit : tools for iterative tasks on mechanical Turk ",
          "author": "Little, Greg;Chilton, Lydia B.;Goldman, Max;Miller, Robert C.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 34,
          "score": 0.5996807813644409,
          "doc_id": "NART77189173",
          "title": "Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks",
          "abstract": "<P>We present the design and evaluation of a method for estimating gaze locations during the analysis of static visualizations using crowdsourcing. Understanding gaze patterns is helpful for evaluating visualizations and user behaviors, but traditional eye-tracking studies require specialized hardware and local users. To avoid these constraints, we developed a method called Fauxvea, which crowdsources visualization tasks on the Web and estimates gaze fixations through cursor interactions without eye-tracking hardware. We ran experiments to evaluate how gaze estimates from our method compare with eye-tracking data. First, we evaluated crowdsourced estimates for three common types of information visualizations and basic visualization tasks using Amazon Mechanical Turk (MTurk). In another, we reproduced findings from a previous eye-tracking study on tree layouts using our method on MTurk. Results from these experiments show that fixation estimates using Fauxvea are qualitatively and quantitatively similar to eye tracking on the same stimulus-task pairs. These findings suggest that crowdsourcing visual analysis tasks with static information visualizations could be a viable alternative to traditional eye-tracking studies for visualization research and design.</P>",
          "doc_source": "Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks <P>We present the design and evaluation of a method for estimating gaze locations during the analysis of static visualizations using crowdsourcing. Understanding gaze patterns is helpful for evaluating visualizations and user behaviors, but traditional eye-tracking studies require specialized hardware and local users. To avoid these constraints, we developed a method called Fauxvea, which crowdsources visualization tasks on the Web and estimates gaze fixations through cursor interactions without eye-tracking hardware. We ran experiments to evaluate how gaze estimates from our method compare with eye-tracking data. First, we evaluated crowdsourced estimates for three common types of information visualizations and basic visualization tasks using Amazon Mechanical Turk (MTurk). In another, we reproduced findings from a previous eye-tracking study on tree layouts using our method on MTurk. Results from these experiments show that fixation estimates using Fauxvea are qualitatively and quantitatively similar to eye tracking on the same stimulus-task pairs. These findings suggest that crowdsourcing visual analysis tasks with static information visualizations could be a viable alternative to traditional eye-tracking studies for visualization research and design.</P>",
          "author": "Gomez, Steven R.;Jianu, Radu;Cabeen, Ryan;Guo, Hua;Laidlaw, David H.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 35,
          "score": 0.5912877917289734,
          "doc_id": "NART75765228",
          "title": "Using Amazon Mechanical Turk and other compensated crowdsourcing sites",
          "abstract": "Crowdsourcing is becoming recognized as a powerful tool that organizations can use in order to get work done, this by freelancers and non-employees. We conceptualize crowdsourcing as a subcategory of outsourcing, with compensated crowdsourcing representing situations in which individuals performing the work receive some sort of payment for accomplishing the organization's tasks. Herein, we discuss how sites that create a crowd, such as Amazon Mechanical Turk, can be powerful tools for business purposes. We highlight the general features of crowdsourcing sites, offering examples drawn from current crowdsourcing sites. We then examine the wide range of tasks that can be accomplished through crowdsourcing sites. Large online worker community websites and forums have been created around such crowdsourcing sites, and we describe the functions they generally play for crowdsourced workers. We also describe how these functions offer opportunities and challenges for organizations. We close by discussing major considerations organizations need to take into account when trying to harness the power of the crowd through compensated crowdsourcing sites.",
          "doc_source": "Using Amazon Mechanical Turk and other compensated crowdsourcing sites Using Amazon Mechanical Turk and other compensated crowdsourcing sites Using Amazon Mechanical Turk and other compensated crowdsourcing sites Crowdsourcing is becoming recognized as a powerful tool that organizations can use in order to get work done, this by freelancers and non-employees. We conceptualize crowdsourcing as a subcategory of outsourcing, with compensated crowdsourcing representing situations in which individuals performing the work receive some sort of payment for accomplishing the organization's tasks. Herein, we discuss how sites that create a crowd, such as Amazon Mechanical Turk, can be powerful tools for business purposes. We highlight the general features of crowdsourcing sites, offering examples drawn from current crowdsourcing sites. We then examine the wide range of tasks that can be accomplished through crowdsourcing sites. Large online worker community websites and forums have been created around such crowdsourcing sites, and we describe the functions they generally play for crowdsourced workers. We also describe how these functions offer opportunities and challenges for organizations. We close by discussing major considerations organizations need to take into account when trying to harness the power of the crowd through compensated crowdsourcing sites.",
          "author": "Schmidt, G.B.;Jettinghoff, W.M.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 36,
          "score": 0.5846113562583923,
          "doc_id": "NPAP13663736",
          "title": "모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정",
          "abstract": "최근 모바일 기술이 실생활에 널리 활용하면서 점점 모바일 크라우드소싱 활용이 크게 기대되고 있다. 그래서 배달 인력이 아닌 일반인도 어플리케이션을 모바일 기기에 설치하면 배달 인력이 되어 작업을 수행할 수 있다. 본 연구에서는 일반인도 참여할 수 있는 모바일 크라우드소싱 기반 배달에서 딥러닝을 이용한 작업자 선정 기법을 소개한다. 그리고 실험을 통하여 합성곱 신경망(Convolutional Neural Network)을 적용한 본 기법이 효과적이라는 것을 보인다.",
          "doc_source": "모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정 모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정 모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정 최근 모바일 기술이 실생활에 널리 활용하면서 점점 모바일 크라우드소싱 활용이 크게 기대되고 있다. 그래서 배달 인력이 아닌 일반인도 어플리케이션을 모바일 기기에 설치하면 배달 인력이 되어 작업을 수행할 수 있다. 본 연구에서는 일반인도 참여할 수 있는 모바일 크라우드소싱 기반 배달에서 딥러닝을 이용한 작업자 선정 기법을 소개한다. 그리고 실험을 통하여 합성곱 신경망(Convolutional Neural Network)을 적용한 본 기법이 효과적이라는 것을 보인다.",
          "author": "이윤열;김응모;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 37,
          "score": 0.5834912061691284,
          "doc_id": "DIKO0016395992",
          "title": "모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당",
          "abstract": "nan",
          "doc_source": "모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당 모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당 모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당 ",
          "author": "이윤열",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 38,
          "score": 0.567690372467041,
          "doc_id": "NPAP13686745",
          "title": "Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk",
          "abstract": "nan",
          "doc_source": "Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk ",
          "author": "Calvo, Roc&iacute;o;Kane, Shaun K.;Hurst, Amy;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 39,
          "score": 0.5674216151237488,
          "doc_id": "JAKO201416760764337",
          "title": "인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가",
          "abstract": "본 논문에서는 TBM 터널의 세그먼트 라이닝 설계 자동화 기술 개발의 일환으로 인공신경망기법을 이용한 세그먼트 라이닝 부재력 산정기법 개발에 관한 내용을 다루었다. 부재력 평가가 가능한 인공신경망을 개발하기 위해 먼저 다양한 설계조건을 도출하고 이에 대해 2-Ring Beam 모델을 이용한 유한요소해석을 수행하여 인공신경망 학습에 필요한 설계조건별 부재력에 관한 DB를 구축하였다. 구축된 DB를 활용하여 인공신경망의 최적화 과정을 통해 최대 부재력 및 분포도를 예측할 수 있는 인공신경망을 구축하였다. 검토 결과 구축된 인공신경망은 유한요소해석과 동일한 정밀도의 부재력 산정 기능을 확보하는 것으로 검토되었으며 따라서 TBM 세그먼트 라이닝 설계시 필요한 부재력 평가를 위한 효율적인 수단으로 활용될 수 있는 것으로 판단된다.",
          "doc_source": "인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가 인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가 인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가 본 논문에서는 TBM 터널의 세그먼트 라이닝 설계 자동화 기술 개발의 일환으로 인공신경망기법을 이용한 세그먼트 라이닝 부재력 산정기법 개발에 관한 내용을 다루었다. 부재력 평가가 가능한 인공신경망을 개발하기 위해 먼저 다양한 설계조건을 도출하고 이에 대해 2-Ring Beam 모델을 이용한 유한요소해석을 수행하여 인공신경망 학습에 필요한 설계조건별 부재력에 관한 DB를 구축하였다. 구축된 DB를 활용하여 인공신경망의 최적화 과정을 통해 최대 부재력 및 분포도를 예측할 수 있는 인공신경망을 구축하였다. 검토 결과 구축된 인공신경망은 유한요소해석과 동일한 정밀도의 부재력 산정 기능을 확보하는 것으로 검토되었으며 따라서 TBM 세그먼트 라이닝 설계시 필요한 부재력 평가를 위한 효율적인 수단으로 활용될 수 있는 것으로 판단된다.",
          "author": "유충식;최정혁;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 40,
          "score": 0.5633382797241211,
          "doc_id": "JAKO202113759910728",
          "title": "FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현",
          "abstract": "In this paper, we present the FlappyBird Competition System (FCS) implementation, a competition-based automated assessment system used in an entry-level artificial intelligence (AI) course at a university. The proposed system provides an evaluation method suitable for AI courses while taking advantage of automated assessment methods. Students are to design a neural network structure, train the weights, and tune hyperparameters using the given reinforcement learning code to improve the overall performance of game AI. Students participate using the resulting trained model during the competition, and the system automatically calculates the final score based on the ranking. The user evaluation conducted after the semester ends shows that our competition-based automated assessment system promotes active participation and inspires students to be interested and motivated to learn AI. Using FCS, the instructor significantly reduces the amount of time required for assessment.",
          "doc_source": "FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현 FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현 FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현 In this paper, we present the FlappyBird Competition System (FCS) implementation, a competition-based automated assessment system used in an entry-level artificial intelligence (AI) course at a university. The proposed system provides an evaluation method suitable for AI courses while taking advantage of automated assessment methods. Students are to design a neural network structure, train the weights, and tune hyperparameters using the given reinforcement learning code to improve the overall performance of game AI. Students participate using the resulting trained model during the competition, and the system automatically calculates the final score based on the ranking. The user evaluation conducted after the semester ends shows that our competition-based automated assessment system promotes active participation and inspires students to be interested and motivated to learn AI. Using FCS, the instructor significantly reduces the amount of time required for assessment.",
          "author": "손의성;김재경;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 41,
          "score": 0.5604743361473083,
          "doc_id": "DIKO0010672988",
          "title": "인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN)",
          "abstract": "Abstract Li Chenghua Department of Information and Communication Chebuk National University Text categorization is an important application of machine learning to the field of document information retrieval. This thesis described two kinds of neural networks for text categorization, multi-output perceptron learning (MOPL) and back propagation neural network (BPNN). BPNN has been widely used in classification and pattern recognition. However it has some generally acknowledged defects, usually these defects evolve from some morbidity neurons In this thesis I proposed a novel adaptive learning approach for text categorization using improved back propagation neural network. This algorithm can overcome some shortcomings in traditional back propagation neural network such as slow training speed and easy to get into local minimum. We compared the training time and performance and test the three methods on the standard Reuter-21578. The results show that the proposed algorithm is able to achieve high categorization effectiveness as measured by precision, recall and F-measure. 요약 문서분류는 정보검색에서 기계학습을 응용하는 중요한 분야이다. 본 논문에서는 다중출력 퍼셉트론 학습(Multi-Output Perceptron Learning:MOPL)과 백 프로퍼게이션 신경망(Back Propagation Neural Network:BPNN) 두 가지의 신경망 이론을 문서분류에 적용하였다. BPNN은 분류와 패턴인식에 많이 사용되고 있지만, 치명적인 신경을 포함하는 몇 가지 결점이 있다. 본 논문에서는 향상된 백 프로퍼게이션 신경망이론을 사용한 새로운 학습법을 제안할 것이다. 이 알고리즘은 기존의 백 프로퍼게이션 신경망의 느린 학습 속도와 쉽게 국소적인 제한치로 빠지는 문제를 개선할 수 있다. 로이터 자료(Reuter-21578)을 이용하여 세 가지 방법을 테스트하고, 학습시간과 성능을 비교하였다. 정확율, 재현율, 그리고 F-mesure를 통하여 본 논문에서 제안한 문서분류 알고리즘의 높은 성능을 확인할 수 있다.",
          "doc_source": "인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN) 인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN) 인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN) Abstract Li Chenghua Department of Information and Communication Chebuk National University Text categorization is an important application of machine learning to the field of document information retrieval. This thesis described two kinds of neural networks for text categorization, multi-output perceptron learning (MOPL) and back propagation neural network (BPNN). BPNN has been widely used in classification and pattern recognition. However it has some generally acknowledged defects, usually these defects evolve from some morbidity neurons In this thesis I proposed a novel adaptive learning approach for text categorization using improved back propagation neural network. This algorithm can overcome some shortcomings in traditional back propagation neural network such as slow training speed and easy to get into local minimum. We compared the training time and performance and test the three methods on the standard Reuter-21578. The results show that the proposed algorithm is able to achieve high categorization effectiveness as measured by precision, recall and F-measure. 요약 문서분류는 정보검색에서 기계학습을 응용하는 중요한 분야이다. 본 논문에서는 다중출력 퍼셉트론 학습(Multi-Output Perceptron Learning:MOPL)과 백 프로퍼게이션 신경망(Back Propagation Neural Network:BPNN) 두 가지의 신경망 이론을 문서분류에 적용하였다. BPNN은 분류와 패턴인식에 많이 사용되고 있지만, 치명적인 신경을 포함하는 몇 가지 결점이 있다. 본 논문에서는 향상된 백 프로퍼게이션 신경망이론을 사용한 새로운 학습법을 제안할 것이다. 이 알고리즘은 기존의 백 프로퍼게이션 신경망의 느린 학습 속도와 쉽게 국소적인 제한치로 빠지는 문제를 개선할 수 있다. 로이터 자료(Reuter-21578)을 이용하여 세 가지 방법을 테스트하고, 학습시간과 성능을 비교하였다. 정확율, 재현율, 그리고 F-mesure를 통하여 본 논문에서 제안한 문서분류 알고리즘의 높은 성능을 확인할 수 있다.",
          "author": "리청화",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 42,
          "score": 0.5601099729537964,
          "doc_id": "DIKO0015787734",
          "title": "검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현",
          "abstract": "본 논문은 검사 공정 작업자 이미지 반복 처리 작업을 자동화 시스템으로 대체하고자 하는 연구를 진행하였다. 검사 장비의 자동화에 따른 작업 스피드가 향상되었지만, 그 이면에 자동화 적용이 어려워 작업자가 직접 해야 하는 이미지 반복 확인 작업의 과제가 남아 있다. 이를 해결하기 위해 딥러닝을 활용한 이미지 분류 자동화 시스템을 개발하는 것이 본고의 목적이다.&amp;#xD; &amp;#xD; 검사 공정 작업자 이미지 반복 작업의 문제점은 단순 반복 행위로 인한 작업자의 건강상 역기능, 즉 시력 저하와 손목 통증이 대표적이다. 또한 작업자의 개인적인 판단과 오류로 인한 품질적인 측면인 재현성과 반복성도 문제점 중 하나이다. 뿐만 아니라 검사기에 이미지가 증가되면 작업자의 업무 부하로 인한 실시간 처리도 지연된다.&amp;#xD; &amp;#xD; 작업자의 이미지 확인 작업의 주요 이유는 이물성 불량을 양품으로 전환하여 수율을 향상시키는 것과 불량의 세부 Trend를 파악하고자 하기 위함이다. 이런 문제들을 해소하고자 딥러닝에서 CNN(Convolution Neural Network) 합성곱 신경망을 활용하여 이미지의 특징을 추출하고 학습을 진행 후 파라미터를 컴퓨터 신경망이 기억하였다가 분류 이미지가 들어오면 기억된 신경망이 이미지를 14가지 유형으로 자동 분류 예측하는 시스템을 개발하였다. 이미지 분류에는 전처리 과정과 이미지 분류 그리고 후처리 과정을 거치게 된다. 개발된 자동화 시스템에 평가 검증의 비교 대상은 작업자의 결과물과 비교하게 된다. 평가품을 선정 후 작업자가 선 작업을 하고 이 후 원본 이미지를 자동화 시스템이 분류하여 작업자 대비 자동화 시스템의 성능인 정합도를 비교하여 취합하였다. 이런 과정을 반복하면서 Confusion Matrix 오차 행렬을 이용하여 세부적 분석을 통해 정합도 이상 유형을 찾고 추가적인 학습에 파라미터를 수정하여 최적화에 점점 다가가게 된다.&amp;#xD; &amp;#xD; 14가지 항목의 이미지 15,000개를 학습시킨 후 1일 1,900,000개의 이미지를 분류하는 자동화 시스템의 정합도는 작업자 대비 93.26%의 정합도를 확인하였고 유형별 정합도의 최대 100%에서 최소 80.87%의 유형별 정합도를 확인하였다. 치명적 오류 681 ppm 그리고 치명적이지 않은 오류 6.7%에 성능을 확인하였다. 작업자 대비 이미지 처리 속도는 약 10배 이상의 개선 효과를 확인하였으며 이로 인하여 작업자의 건강상의 역기능 부담을 감소시켰고 작업자의 이미지 분류 확인 작업의 일관성을 시스템화하였다.&amp;#xD; &amp;#xD; 또한 이미지 처리 속도 향상으로 실시간 처리에 좀 더 가까워졌다. 현재 자동화 시스템의 정합도 93.26%에 유형별 정합도 100% 가까운 유형의 항목은 자동화 시스템의 이미지 분류 후 후처리 과정에서 분류 결과를 확인하여 강제로 작업자에게 전송시키지 않는 상태이며 이로 인한 작업자는 기존 대비 약 80%의 업무 과중을 줄일 수 있다.&amp;#xD; 업무에 과중을 최소화 함으로써 작업자 인력 소인화에 도달하였다. &amp;#xD; &amp;#xD; 추가적으로 향후 분리 되어 있는 검사기와 자동화 시스템을 검사기 장비 내에 결합하여 불필요한 전송 및 Loss를 최소화 하여 실시간 처리 및 인력 무인화에 도움이 될 것이라 기대된다. 또한 GPU를 추가적으로 멀티로 사용하여 자동화 시스템의 처리 속도 또한 현재보다 더 향상될 것으로 기대된다.",
          "doc_source": "검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현 검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현 검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현 본 논문은 검사 공정 작업자 이미지 반복 처리 작업을 자동화 시스템으로 대체하고자 하는 연구를 진행하였다. 검사 장비의 자동화에 따른 작업 스피드가 향상되었지만, 그 이면에 자동화 적용이 어려워 작업자가 직접 해야 하는 이미지 반복 확인 작업의 과제가 남아 있다. 이를 해결하기 위해 딥러닝을 활용한 이미지 분류 자동화 시스템을 개발하는 것이 본고의 목적이다.&amp;#xD; &amp;#xD; 검사 공정 작업자 이미지 반복 작업의 문제점은 단순 반복 행위로 인한 작업자의 건강상 역기능, 즉 시력 저하와 손목 통증이 대표적이다. 또한 작업자의 개인적인 판단과 오류로 인한 품질적인 측면인 재현성과 반복성도 문제점 중 하나이다. 뿐만 아니라 검사기에 이미지가 증가되면 작업자의 업무 부하로 인한 실시간 처리도 지연된다.&amp;#xD; &amp;#xD; 작업자의 이미지 확인 작업의 주요 이유는 이물성 불량을 양품으로 전환하여 수율을 향상시키는 것과 불량의 세부 Trend를 파악하고자 하기 위함이다. 이런 문제들을 해소하고자 딥러닝에서 CNN(Convolution Neural Network) 합성곱 신경망을 활용하여 이미지의 특징을 추출하고 학습을 진행 후 파라미터를 컴퓨터 신경망이 기억하였다가 분류 이미지가 들어오면 기억된 신경망이 이미지를 14가지 유형으로 자동 분류 예측하는 시스템을 개발하였다. 이미지 분류에는 전처리 과정과 이미지 분류 그리고 후처리 과정을 거치게 된다. 개발된 자동화 시스템에 평가 검증의 비교 대상은 작업자의 결과물과 비교하게 된다. 평가품을 선정 후 작업자가 선 작업을 하고 이 후 원본 이미지를 자동화 시스템이 분류하여 작업자 대비 자동화 시스템의 성능인 정합도를 비교하여 취합하였다. 이런 과정을 반복하면서 Confusion Matrix 오차 행렬을 이용하여 세부적 분석을 통해 정합도 이상 유형을 찾고 추가적인 학습에 파라미터를 수정하여 최적화에 점점 다가가게 된다.&amp;#xD; &amp;#xD; 14가지 항목의 이미지 15,000개를 학습시킨 후 1일 1,900,000개의 이미지를 분류하는 자동화 시스템의 정합도는 작업자 대비 93.26%의 정합도를 확인하였고 유형별 정합도의 최대 100%에서 최소 80.87%의 유형별 정합도를 확인하였다. 치명적 오류 681 ppm 그리고 치명적이지 않은 오류 6.7%에 성능을 확인하였다. 작업자 대비 이미지 처리 속도는 약 10배 이상의 개선 효과를 확인하였으며 이로 인하여 작업자의 건강상의 역기능 부담을 감소시켰고 작업자의 이미지 분류 확인 작업의 일관성을 시스템화하였다.&amp;#xD; &amp;#xD; 또한 이미지 처리 속도 향상으로 실시간 처리에 좀 더 가까워졌다. 현재 자동화 시스템의 정합도 93.26%에 유형별 정합도 100% 가까운 유형의 항목은 자동화 시스템의 이미지 분류 후 후처리 과정에서 분류 결과를 확인하여 강제로 작업자에게 전송시키지 않는 상태이며 이로 인한 작업자는 기존 대비 약 80%의 업무 과중을 줄일 수 있다.&amp;#xD; 업무에 과중을 최소화 함으로써 작업자 인력 소인화에 도달하였다. &amp;#xD; &amp;#xD; 추가적으로 향후 분리 되어 있는 검사기와 자동화 시스템을 검사기 장비 내에 결합하여 불필요한 전송 및 Loss를 최소화 하여 실시간 처리 및 인력 무인화에 도움이 될 것이라 기대된다. 또한 GPU를 추가적으로 멀티로 사용하여 자동화 시스템의 처리 속도 또한 현재보다 더 향상될 것으로 기대된다.",
          "author": "김영규",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 43,
          "score": 0.5599267482757568,
          "doc_id": "ATN0027086510",
          "title": "인공신경망과 강건설계를 이용한 기계적 RMR 분류",
          "abstract": "Rock mass rating (RMR) is a relatively simple method for classifying rock mass with the naked eye; however, it becomes inconvenient when the number of survey sections is large. In this study, we developed a learning model and a prediction model using an artificial neural network (ANN) to classify RMR mechanically. Using robust design, 3125 big data were optimized into 25 learning data. The test results after learning through the two methods were exactly the same. Through robust design, the learning data obtained by reducing the total number of cases to less than 1% had the same learning effect as the whole data, which means that the effort and cost of acquiring the learning data can be greatly reduced. For the perfect prediction of the RMR classification system, we tuned the primary predictions within a given range of rating levels. As a result, we implemented an RMR classification ANN system that perfectly predicts the RMR of 3125 big data using 25 learning data through robust design.",
          "doc_source": "인공신경망과 강건설계를 이용한 기계적 RMR 분류 인공신경망과 강건설계를 이용한 기계적 RMR 분류 인공신경망과 강건설계를 이용한 기계적 RMR 분류 Rock mass rating (RMR) is a relatively simple method for classifying rock mass with the naked eye; however, it becomes inconvenient when the number of survey sections is large. In this study, we developed a learning model and a prediction model using an artificial neural network (ANN) to classify RMR mechanically. Using robust design, 3125 big data were optimized into 25 learning data. The test results after learning through the two methods were exactly the same. Through robust design, the learning data obtained by reducing the total number of cases to less than 1% had the same learning effect as the whole data, which means that the effort and cost of acquiring the learning data can be greatly reduced. For the perfect prediction of the RMR classification system, we tuned the primary predictions within a given range of rating levels. As a result, we implemented an RMR classification ANN system that perfectly predicts the RMR of 3125 big data using 25 learning data through robust design.",
          "author": "장명환;하태욱;최기훈;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 44,
          "score": 0.5597909688949585,
          "doc_id": "NPAP13430839",
          "title": "AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion",
          "abstract": "<P>Recent advances of AI applications in various of industries have led to remarkable performance and efficiency. Driven by the great success of datasets and experience sharing, people are exploring more precious datasets with diverse features and longer time range. The promising reasoning information of well-curated student grade datasets is expected to assist young students to find the best of themselves and then improve their learning outcome and study experience. Through data and experience sharing, young students can have a better understanding of their learning condition and possible learning outcomes. Existing course selection systems in Taiwan which offer limited basic enrolling functions fail to provide performance prediction and course arrangement guidance based on their own learning condition. Students now selecting courses with unawareness of their expecting performance. A personalized guide for students on course selection is crucial for how they structure professional knowledge and arrange study schedule. In this paper, we first analyzed what factors can be used on defining learning curve, and discovered the difference between students with different properties and background. Second, we developed a recommendation system based on great amount of grade datasets of past students, and the system can give students suggestions on how to assign their credits based on their own learning curve and students that had similar learning curve. The result of our research demonstrates the feasibility of a new approach on applying big data and AI technology on learning analysis and course selection.</P>",
          "doc_source": "AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion <P>Recent advances of AI applications in various of industries have led to remarkable performance and efficiency. Driven by the great success of datasets and experience sharing, people are exploring more precious datasets with diverse features and longer time range. The promising reasoning information of well-curated student grade datasets is expected to assist young students to find the best of themselves and then improve their learning outcome and study experience. Through data and experience sharing, young students can have a better understanding of their learning condition and possible learning outcomes. Existing course selection systems in Taiwan which offer limited basic enrolling functions fail to provide performance prediction and course arrangement guidance based on their own learning condition. Students now selecting courses with unawareness of their expecting performance. A personalized guide for students on course selection is crucial for how they structure professional knowledge and arrange study schedule. In this paper, we first analyzed what factors can be used on defining learning curve, and discovered the difference between students with different properties and background. Second, we developed a recommendation system based on great amount of grade datasets of past students, and the system can give students suggestions on how to assign their credits based on their own learning curve and students that had similar learning curve. The result of our research demonstrates the feasibility of a new approach on applying big data and AI technology on learning analysis and course selection.</P>",
          "author": "Wu, Yu Hsuan;Wu, Eric Hsiaokuang;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 45,
          "score": 0.5597437620162964,
          "doc_id": "DIKO0016959108",
          "title": "AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구",
          "abstract": "본 연구는 교육부의 AI 디지털교과서 개발 방향 설정에 대한 초등 교사의 인식을 알아보기 위한 목적으로 실시되었다. 이를 위해 교육부의 AI 디지털교과서 개발 정책과 한국교육학술정보원의 AI 디지털교과서 개발 가이드라인을 바탕으로 약 40개의 문항이 담긴 설문지를 제작하여 배포하였다. 회수된 초등 교사 106명의 응답을 대응표본 t-검정, Borich 요구도, The Locus for Focus 모델을 통해 통계 처리하였으며 분석을 통해 얻은 결과는 다음과 같다.&amp;#xD; 첫째, 현재 서비스 되고 있는 디지털교과서와 발행사별 교수학습지원사이트의 이용 경험은 발행사별 교수학습지원사이트 이용 경험에 비해 굉장히 낮았으며 그 원인으로는 교수학습지원사이트에 비해 멀티미디어 콘텐츠의 부족, 사용법의 어려움과 기능의 불편함, 필요한 기능의 부재 등을 꼽았다.&amp;#xD; 둘째, AI 디지털교과서의 기본 기능인 통합 인증 기능, 통합 대시보드, 디지털교과서 책장, 학습데이터 허브 기능의 필요성을 묻는 질문에 대해 약 80% 이상의 교사가 필요하다고 응답하였다.&amp;#xD; 셋째, AI 기반 맞춤형 학습 지원 기능인 학습 진단 기능, 맞춤형 콘텐츠 제공 기능, 대시보드 기능, AI 튜터 기능, AI 보조교사 기능, 교사 재구성 기능의 필요성을 묻는 질문에 대한 응답을 전체 집단과 최종 학력 기준 집단으로 분석을 실시하였다. 대응표본 t-검정 결과는 모든 집단의 응답 결과에 대해 통계적으로 유의미한 차이(&amp;amp;lt; .001)가 있는 것으로 나타났다. Borich 요구도 및 The Locus for Focus 모델에 의한 분석 결과는 전체 집단의 경우 AI 보조교사 AI 튜터 기능에 대한 요구 수준이 가장 높았으며 The Locus for Focus 모델을 통한 분석 결과도 위의 두 기능 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력을 기준으로 집단을 나누어 분석한 결과는 최종 학력이 학사인 경우와 컴퓨터 관련 석사 과정 재학 중 또는 석사 학위 소지인 경우 AI 보조교사 기능과 AI 튜터 기능에 대한 요구도가 가장 높았으며 The Locus for Focus 모델 분석 결과 또한 두 기능이 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력이 그 외 석사 과정 재학 중 또는 석사 학위 소지인 경우 위의 두 집단과 마찬가지로 AI 튜터 기능과 AI 보조교사 기능이 요구도가 가장 높았으나 The Locus for Focus 모델 분석 결과의 경우 1사분면에 위치한 기능이 존재하지 않아 개발 우선 순위에 대한 논의가 필요한 것으로 나타났다.&amp;#xD; 넷째, 여섯 가지 AI 기반 맞춤형 학습 지원 기능에 대한 추가 수요 분석 결과를 실시하였다. 학습 진단 기능에 대한 추가 수요는 학생 참여 정도 진단 서비스, 협업 정도 진단 서비스, 정서 분석 서비스 등으로 나타났다. 맞춤형 콘텐츠 제공 기능에 대한 추가 수요는 이전 학년 과정 추천 기능, 맞춤형 콘텐츠 교사 제시 기능, 문항 난이도 조절 기능, 학습 경로 설정 기능 등으로 나타났다. 대시보드 기능에 대한 추가 수요는 오답 노트 작성 기능, 학습전략 추천 기능, 토론 및 채팅 기능 등으로 나타났다. AI 튜터 기능에 대한 추가 수요는 추가 학습 자료 제공 기능, 힌트 제공 기능 등으로 나타났다. AI 보조교사 기능에 대한 추가 수요는 문항 자동 채점 기능 및 결과 제공 기능, 교과학습발달상황 및 행동발상황 작성 지원 기능, 학생 수준별 문항 자동 구성 및 출제 기능, 수행평가 결과 NEIS 전송 기능 등으로 나타났다. 교사 재구성 기능에 대한 추가 수요는 프로젝트 학습을 위한 교과서 간 내용 통합 및 순서 조정 기능, 발행사별 교과서 내용 비교 및 끌어오기 기능 등으로 나타났다. 그 밖의 기능에 대한 수요는 에듀테크 사이트 연결 기능, 게임을 통한 성취도 확인 기능, 과제에 따른 보상 기능, 학교 간 학습 내용 공유 기능, 자료 저장소 기능, 출결 확인 기능, 커뮤니티 기능 등으로 나타났다. &amp;#xD; 본 연구는 AI 디지털교과서 개발과 관련한 교육부 정책의 방향을 확인하고 한국교육학술정보원이 발간한 AI 디지털교과서 개발 가이드라인의 의미를 해석하는 데 활용될 수 있다. 또한 AI 디지털교과서 개발 방향 설정에 대한 교사의 인식 및 수요를 확인하는 자료로 사용될 수 있으며 이를 바탕으로 AI 디지털교과서 개발 관련자들이 교수학습지원시스템으로서의 AI 디지털교과서를 개발하는 데 도움이 될 것으로 기대한다.",
          "doc_source": "AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구 AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구 AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구 본 연구는 교육부의 AI 디지털교과서 개발 방향 설정에 대한 초등 교사의 인식을 알아보기 위한 목적으로 실시되었다. 이를 위해 교육부의 AI 디지털교과서 개발 정책과 한국교육학술정보원의 AI 디지털교과서 개발 가이드라인을 바탕으로 약 40개의 문항이 담긴 설문지를 제작하여 배포하였다. 회수된 초등 교사 106명의 응답을 대응표본 t-검정, Borich 요구도, The Locus for Focus 모델을 통해 통계 처리하였으며 분석을 통해 얻은 결과는 다음과 같다.&amp;#xD; 첫째, 현재 서비스 되고 있는 디지털교과서와 발행사별 교수학습지원사이트의 이용 경험은 발행사별 교수학습지원사이트 이용 경험에 비해 굉장히 낮았으며 그 원인으로는 교수학습지원사이트에 비해 멀티미디어 콘텐츠의 부족, 사용법의 어려움과 기능의 불편함, 필요한 기능의 부재 등을 꼽았다.&amp;#xD; 둘째, AI 디지털교과서의 기본 기능인 통합 인증 기능, 통합 대시보드, 디지털교과서 책장, 학습데이터 허브 기능의 필요성을 묻는 질문에 대해 약 80% 이상의 교사가 필요하다고 응답하였다.&amp;#xD; 셋째, AI 기반 맞춤형 학습 지원 기능인 학습 진단 기능, 맞춤형 콘텐츠 제공 기능, 대시보드 기능, AI 튜터 기능, AI 보조교사 기능, 교사 재구성 기능의 필요성을 묻는 질문에 대한 응답을 전체 집단과 최종 학력 기준 집단으로 분석을 실시하였다. 대응표본 t-검정 결과는 모든 집단의 응답 결과에 대해 통계적으로 유의미한 차이(&amp;amp;lt; .001)가 있는 것으로 나타났다. Borich 요구도 및 The Locus for Focus 모델에 의한 분석 결과는 전체 집단의 경우 AI 보조교사 AI 튜터 기능에 대한 요구 수준이 가장 높았으며 The Locus for Focus 모델을 통한 분석 결과도 위의 두 기능 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력을 기준으로 집단을 나누어 분석한 결과는 최종 학력이 학사인 경우와 컴퓨터 관련 석사 과정 재학 중 또는 석사 학위 소지인 경우 AI 보조교사 기능과 AI 튜터 기능에 대한 요구도가 가장 높았으며 The Locus for Focus 모델 분석 결과 또한 두 기능이 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력이 그 외 석사 과정 재학 중 또는 석사 학위 소지인 경우 위의 두 집단과 마찬가지로 AI 튜터 기능과 AI 보조교사 기능이 요구도가 가장 높았으나 The Locus for Focus 모델 분석 결과의 경우 1사분면에 위치한 기능이 존재하지 않아 개발 우선 순위에 대한 논의가 필요한 것으로 나타났다.&amp;#xD; 넷째, 여섯 가지 AI 기반 맞춤형 학습 지원 기능에 대한 추가 수요 분석 결과를 실시하였다. 학습 진단 기능에 대한 추가 수요는 학생 참여 정도 진단 서비스, 협업 정도 진단 서비스, 정서 분석 서비스 등으로 나타났다. 맞춤형 콘텐츠 제공 기능에 대한 추가 수요는 이전 학년 과정 추천 기능, 맞춤형 콘텐츠 교사 제시 기능, 문항 난이도 조절 기능, 학습 경로 설정 기능 등으로 나타났다. 대시보드 기능에 대한 추가 수요는 오답 노트 작성 기능, 학습전략 추천 기능, 토론 및 채팅 기능 등으로 나타났다. AI 튜터 기능에 대한 추가 수요는 추가 학습 자료 제공 기능, 힌트 제공 기능 등으로 나타났다. AI 보조교사 기능에 대한 추가 수요는 문항 자동 채점 기능 및 결과 제공 기능, 교과학습발달상황 및 행동발상황 작성 지원 기능, 학생 수준별 문항 자동 구성 및 출제 기능, 수행평가 결과 NEIS 전송 기능 등으로 나타났다. 교사 재구성 기능에 대한 추가 수요는 프로젝트 학습을 위한 교과서 간 내용 통합 및 순서 조정 기능, 발행사별 교과서 내용 비교 및 끌어오기 기능 등으로 나타났다. 그 밖의 기능에 대한 수요는 에듀테크 사이트 연결 기능, 게임을 통한 성취도 확인 기능, 과제에 따른 보상 기능, 학교 간 학습 내용 공유 기능, 자료 저장소 기능, 출결 확인 기능, 커뮤니티 기능 등으로 나타났다. &amp;#xD; 본 연구는 AI 디지털교과서 개발과 관련한 교육부 정책의 방향을 확인하고 한국교육학술정보원이 발간한 AI 디지털교과서 개발 가이드라인의 의미를 해석하는 데 활용될 수 있다. 또한 AI 디지털교과서 개발 방향 설정에 대한 교사의 인식 및 수요를 확인하는 자료로 사용될 수 있으며 이를 바탕으로 AI 디지털교과서 개발 관련자들이 교수학습지원시스템으로서의 AI 디지털교과서를 개발하는 데 도움이 될 것으로 기대한다.",
          "author": "이승현",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 46,
          "score": 0.5574133396148682,
          "doc_id": "JAKO201922663514847",
          "title": "인공신경망 기반 VRF 시스템 제어",
          "abstract": "This study aimed at developing control algorithms for operating a variable refrigerant flow (VRF) heating and cooling system with optimal system parameter set-points. Two artificial neural network (ANN) models, which were respectively designed to predict the heating energy cost and cooling energy amount for upcoming next control cycle, was developed and embedded into the control algorithms. Performance of the algorithms were tested using the computer simulation programs - EnergyPlus, BCVTB, MATLAB in an incorporative manner. The results revealed that the proposed control algorithms remarkably saved the heating energy cost by as much as 7.93% and cooling energy consumption by as much as 28.44%, compared to a conventional control strategy. These findings support that the ANN-based predictive control algorithms showed potential for cost- and energy-effectiveness of VRF heating and cooling systems.",
          "doc_source": "인공신경망 기반 VRF 시스템 제어 인공신경망 기반 VRF 시스템 제어 인공신경망 기반 VRF 시스템 제어 This study aimed at developing control algorithms for operating a variable refrigerant flow (VRF) heating and cooling system with optimal system parameter set-points. Two artificial neural network (ANN) models, which were respectively designed to predict the heating energy cost and cooling energy amount for upcoming next control cycle, was developed and embedded into the control algorithms. Performance of the algorithms were tested using the computer simulation programs - EnergyPlus, BCVTB, MATLAB in an incorporative manner. The results revealed that the proposed control algorithms remarkably saved the heating energy cost by as much as 7.93% and cooling energy consumption by as much as 28.44%, compared to a conventional control strategy. These findings support that the ANN-based predictive control algorithms showed potential for cost- and energy-effectiveness of VRF heating and cooling systems.",
          "author": "문진우",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 47,
          "score": 0.5543884038925171,
          "doc_id": "JAKO202329643231923",
          "title": "국내 AI 수학 학습 플랫폼의 적응형 학습에 대한 분석",
          "abstract": "최근 인공지능(AI) 기술의 발전과 더불어, AI를 활용한 맞춤형 교육의 필요성이 강조되고 있다. 특히, AI 디지털 교과서의 개발과 실행을 앞두고, 개별화 맞춤형 교육에 대한 연구자, 정책 입안자, 개발자, 실행자 및 사용자 간의 공유된 이해 차이가 존재하며, 이는 개발의 효율성과 실행의 효과성에 영향을 미칠 수 있다. 본 연구에서는 체계적 문헌 검토를 통해 AI 수학 디지털교과서에 필요한 필수적인 적응적 기능을 도출하고, 현재 국내 AI 수학 학습 플랫폼의 적응형 학습 제공의 형태를 분석하였다. 분석 결과, 국내 AI 수학 학습 플랫폼에서는 정서와 동기 적응성이나 메타인지 적응성에 비해 지식 적응성에 크게 집중하고 있었다. 또한 적응 방법과 관련해서는 설계와 과제 루프 측면 보다는 단계 루프 측면이 많이 반영이 되었다. 즉, 학습 플랫폼의 설계와 업데이트, 학습 데이터를 바탕으로 한 맞춤형 과제 제공, 사전 진단 및 실시간 모니터링을 통한 문제 난이도 조절 등과 같은 설계적 적응 방법이나 과제적 적응 방법이 잘 나타나지 않았다. 본 연구의 결과는 AI 수학 디지털교과서를 개발하고 있는 이 시점에서 연구자와 정책 입안자, 개발자 들이 협력적으로 학생들의 전략, 오류, 학습 스타일, 메타인지, 협력적 학습과 같은 다양한 적응 대상과 방법을 고려하여 학생들에게 더 풍부한 개별화 맞춤형 학습 경험을 제공할 수 있도록 더 깊이 있게 연구하고, 이에 대한 국가적 담론과 정책 방향을 구성해야 함을 시사한다.",
          "doc_source": "국내 AI 수학 학습 플랫폼의 적응형 학습에 대한 분석 국내 AI 수학 학습 플랫폼의 적응형 학습에 대한 분석 국내 AI 수학 학습 플랫폼의 적응형 학습에 대한 분석 최근 인공지능(AI) 기술의 발전과 더불어, AI를 활용한 맞춤형 교육의 필요성이 강조되고 있다. 특히, AI 디지털 교과서의 개발과 실행을 앞두고, 개별화 맞춤형 교육에 대한 연구자, 정책 입안자, 개발자, 실행자 및 사용자 간의 공유된 이해 차이가 존재하며, 이는 개발의 효율성과 실행의 효과성에 영향을 미칠 수 있다. 본 연구에서는 체계적 문헌 검토를 통해 AI 수학 디지털교과서에 필요한 필수적인 적응적 기능을 도출하고, 현재 국내 AI 수학 학습 플랫폼의 적응형 학습 제공의 형태를 분석하였다. 분석 결과, 국내 AI 수학 학습 플랫폼에서는 정서와 동기 적응성이나 메타인지 적응성에 비해 지식 적응성에 크게 집중하고 있었다. 또한 적응 방법과 관련해서는 설계와 과제 루프 측면 보다는 단계 루프 측면이 많이 반영이 되었다. 즉, 학습 플랫폼의 설계와 업데이트, 학습 데이터를 바탕으로 한 맞춤형 과제 제공, 사전 진단 및 실시간 모니터링을 통한 문제 난이도 조절 등과 같은 설계적 적응 방법이나 과제적 적응 방법이 잘 나타나지 않았다. 본 연구의 결과는 AI 수학 디지털교과서를 개발하고 있는 이 시점에서 연구자와 정책 입안자, 개발자 들이 협력적으로 학생들의 전략, 오류, 학습 스타일, 메타인지, 협력적 학습과 같은 다양한 적응 대상과 방법을 고려하여 학생들에게 더 풍부한 개별화 맞춤형 학습 경험을 제공할 수 있도록 더 깊이 있게 연구하고, 이에 대한 국가적 담론과 정책 방향을 구성해야 함을 시사한다.",
          "author": "이기마;이유정;김희정;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 48,
          "score": 0.5501500368118286,
          "doc_id": "JAKO201017337344335",
          "title": "인공신경망모델을 이용한 교량의 상태평가",
          "abstract": "대부분의 선진국에서 교량의 유지보수 및 보강(Maintenance Repair & Rehabilitation-MR&R)으로 인한 비용은 해마다 증가하고 있다. 전산화된 교량유지관리 및 의사결정시스템(Bridge Management System-BMS)은 가능한 최저의 생애주기비용(Life Cycle Cost - LCC)에 최적의 안정성를 확보하기 위해 개발되었다. 본 논문에서는 제한된 현존하는 교량진단기록을 이용하여 현존하지 않는 과거의 교량상태등급 데이타를 생성하기 위해 Backward Prediction Model(BPM)이라 불리는 인공신경망(Artificial Neural Network-ANN)에 기초한 예측모델을 제시한다. 제안된 BPM은 한정된 교량 정기점검기록으로부터 현존하는 교량진단기록과 연관성을 확립하기 위해 교통량과 인구, 그리고 기후 등과 같은 비구조적 요소를 이용하며, 제한된 교량진단기록과 비구조적 요소 사이에 맺어진 연관성을 통해 현존하지 않는 과거의 교량상태등급 데이타를 생성할 수 있다. BPM의 신뢰도를 측정하기 위하여 Maryland DOT로 부터 얻어진 National Bridge Inventory(NBI)와 BMS 교량진단자료를 이용하였다. 이중 NBI자료를 이용한 Backward comparison 에 있어서 실제 NBI기록과 BPM으로 생성된 교량상태등급과의 차이(상판: 6.68%, 상부구조부: 6.61%, 하부구조부: 7.52%)는 BPM으로 생성된 결과의 높은 신뢰도를 보여준다. 이 연구의 결과는 제한된 정기점검 기록으로 야기되는 BMS의 장기 교량손상 예측에 관련된 사용상의 문제를 최소화하고 전반적인 BMS 결과의 신뢰도를 높이는데 기여 할 수 있다.",
          "doc_source": "인공신경망모델을 이용한 교량의 상태평가 인공신경망모델을 이용한 교량의 상태평가 인공신경망모델을 이용한 교량의 상태평가 대부분의 선진국에서 교량의 유지보수 및 보강(Maintenance Repair & Rehabilitation-MR&R)으로 인한 비용은 해마다 증가하고 있다. 전산화된 교량유지관리 및 의사결정시스템(Bridge Management System-BMS)은 가능한 최저의 생애주기비용(Life Cycle Cost - LCC)에 최적의 안정성를 확보하기 위해 개발되었다. 본 논문에서는 제한된 현존하는 교량진단기록을 이용하여 현존하지 않는 과거의 교량상태등급 데이타를 생성하기 위해 Backward Prediction Model(BPM)이라 불리는 인공신경망(Artificial Neural Network-ANN)에 기초한 예측모델을 제시한다. 제안된 BPM은 한정된 교량 정기점검기록으로부터 현존하는 교량진단기록과 연관성을 확립하기 위해 교통량과 인구, 그리고 기후 등과 같은 비구조적 요소를 이용하며, 제한된 교량진단기록과 비구조적 요소 사이에 맺어진 연관성을 통해 현존하지 않는 과거의 교량상태등급 데이타를 생성할 수 있다. BPM의 신뢰도를 측정하기 위하여 Maryland DOT로 부터 얻어진 National Bridge Inventory(NBI)와 BMS 교량진단자료를 이용하였다. 이중 NBI자료를 이용한 Backward comparison 에 있어서 실제 NBI기록과 BPM으로 생성된 교량상태등급과의 차이(상판: 6.68%, 상부구조부: 6.61%, 하부구조부: 7.52%)는 BPM으로 생성된 결과의 높은 신뢰도를 보여준다. 이 연구의 결과는 제한된 정기점검 기록으로 야기되는 BMS의 장기 교량손상 예측에 관련된 사용상의 문제를 최소화하고 전반적인 BMS 결과의 신뢰도를 높이는데 기여 할 수 있다.",
          "author": "오순택;이동준;이재호;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 49,
          "score": 0.5500335693359375,
          "doc_id": "NART126358912",
          "title": "Analyzing the Alignment between AI Curriculum and AI Textbooks through Text Mining",
          "abstract": "<P>The field of artificial intelligence (AI) is permeating education worldwide, reflecting societal changes driven by advancements in computing technology and the data revolution. Herein, we analyze the alignment between core AI educational curricula and textbooks to provide guidance on structuring AI knowledge. Text mining techniques using Python 3.10.3 and frame-based content analysis tailored to the computing field are employed to examine a substantial amount of text data within educational curriculum textbooks. We comprehensively examine the frequency of knowledge incorporated in AI curricula, topic structure, and practical tool utilization. The degree to which keywords are reflected in curriculum textbooks and in the textbook characteristics are determined using Term Frequency (TF) and Term Frequency-Inverse Document Frequency (TF-IDF) analysis, respectively. The topic structure distribution is derived by Latent Dirichlet Allocation (LDA) topic modeling and the trained model is visualized using PyLDAvis. Furthermore, the variation in vertical content range or level is investigated by content analysis, considering the tools used to teach similar AI knowledge. Lastly, the implications for AI curriculum structure are discussed in terms of curriculum composition, knowledge construction, practical application, and curriculum utilization. This study provides practical guidance for structuring curricula that effectively foster AI competency based on a systematic research methodology.</P>",
          "doc_source": "Analyzing the Alignment between AI Curriculum and AI Textbooks through Text Mining Analyzing the Alignment between AI Curriculum and AI Textbooks through Text Mining Analyzing the Alignment between AI Curriculum and AI Textbooks through Text Mining <P>The field of artificial intelligence (AI) is permeating education worldwide, reflecting societal changes driven by advancements in computing technology and the data revolution. Herein, we analyze the alignment between core AI educational curricula and textbooks to provide guidance on structuring AI knowledge. Text mining techniques using Python 3.10.3 and frame-based content analysis tailored to the computing field are employed to examine a substantial amount of text data within educational curriculum textbooks. We comprehensively examine the frequency of knowledge incorporated in AI curricula, topic structure, and practical tool utilization. The degree to which keywords are reflected in curriculum textbooks and in the textbook characteristics are determined using Term Frequency (TF) and Term Frequency-Inverse Document Frequency (TF-IDF) analysis, respectively. The topic structure distribution is derived by Latent Dirichlet Allocation (LDA) topic modeling and the trained model is visualized using PyLDAvis. Furthermore, the variation in vertical content range or level is investigated by content analysis, considering the tools used to teach similar AI knowledge. Lastly, the implications for AI curriculum structure are discussed in terms of curriculum composition, knowledge construction, practical application, and curriculum utilization. This study provides practical guidance for structuring curricula that effectively foster AI competency based on a systematic research methodology.</P>",
          "author": "Yang, Hyeji;Kim, Jamee;Lee, Wongyu;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 50,
          "score": 0.5496907234191895,
          "doc_id": "JAKO202510643203882",
          "title": "AI 디지털교과서 활용 역량 강화를 위한 교사 연수 모델 개발",
          "abstract": "The advancement of artificial intelligence (AI) in education has led to the emergence of AI Digital Textbooks (AIDT), transforming teacher-student interactions by supporting personalized, self-directed, and collaborative learning. To effectively utilize AIDT, teachers require enhanced AI and digital literacy competencies. However, existing training programs lack a structured approach to integrating AI-based pedagogical strategies. This study develops a teacher training model that strengthens AIDT utilization by incorporating the TPACK framework, Cognitive Load Theory, and Learning Analytics. Additionally, the Forschendes Lernen (FL) approach is applied to foster inquiry-based learning, enabling teachers to explore and implement AI-driven teaching methods. The proposed model emphasizes hands-on AI teaching strategies, adaptive lesson design, and AI-assisted feedback mechanisms, overcoming the limitations of traditional training programs. This research provides a structured framework for AIDT-based teacher professional development and offers insights into AI integration in education. Future research should focus on empirical validation of the model's effectiveness and continuous refinement to align with emerging AI technologies.",
          "doc_source": "AI 디지털교과서 활용 역량 강화를 위한 교사 연수 모델 개발 AI 디지털교과서 활용 역량 강화를 위한 교사 연수 모델 개발 AI 디지털교과서 활용 역량 강화를 위한 교사 연수 모델 개발 The advancement of artificial intelligence (AI) in education has led to the emergence of AI Digital Textbooks (AIDT), transforming teacher-student interactions by supporting personalized, self-directed, and collaborative learning. To effectively utilize AIDT, teachers require enhanced AI and digital literacy competencies. However, existing training programs lack a structured approach to integrating AI-based pedagogical strategies. This study develops a teacher training model that strengthens AIDT utilization by incorporating the TPACK framework, Cognitive Load Theory, and Learning Analytics. Additionally, the Forschendes Lernen (FL) approach is applied to foster inquiry-based learning, enabling teachers to explore and implement AI-driven teaching methods. The proposed model emphasizes hands-on AI teaching strategies, adaptive lesson design, and AI-assisted feedback mechanisms, overcoming the limitations of traditional training programs. This research provides a structured framework for AIDT-based teacher professional development and offers insights into AI integration in education. Future research should focus on empirical validation of the model's effectiveness and continuous refinement to align with emerging AI technologies.",
          "author": "고호경;안서현;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        }
      ]
    },
    {
      "query": "What results characterize the system that learns TurKontrol’s POMDP parameters from Mechanical Turk data to optimize iterative crowdsourced tasks?",
      "query_meta": {
        "type": "single_hop",
        "index": 1
      },
      "top_k": 50,
      "hits": [
        {
          "rank": 1,
          "score": 0.8061032295227051,
          "doc_id": "NART66567138",
          "title": "POMDP-based control of workflows for crowdsourcing",
          "abstract": "Crowdsourcing, outsourcing of tasks to a crowd of unknown people (''workers'') in an open call, is rapidly rising in popularity. It is already being heavily used by numerous employers (''requesters'') for solving a wide variety of tasks, such as audio transcription, content screening, and labeling training data for machine learning. However, quality control of such tasks continues to be a key challenge because of the high variability in worker quality. In this paper we show the value of decision-theoretic techniques for the problem of optimizing workflows used in crowdsourcing. In particular, we design AI agents that use Bayesian network learning and inference in combination with Partially-Observable Markov Decision Processes (POMDPs) for obtaining excellent cost-quality tradeoffs. We use these techniques for three distinct crowdsourcing scenarios: (1) control of voting to answer a binary-choice question, (2) control of an iterative improvement workflow, and (3) control of switching between alternate workflows for a task. In each scenario, we design a Bayes net model that relates worker competency, task difficulty and worker response quality. We also design a POMDP for each task, whose solution provides the dynamic control policy. We demonstrate the usefulness of our models and agents in live experiments on Amazon Mechanical Turk. We consistently achieve superior quality results than non-adaptive controllers, while incurring equal or less cost.",
          "doc_source": "POMDP-based control of workflows for crowdsourcing POMDP-based control of workflows for crowdsourcing POMDP-based control of workflows for crowdsourcing Crowdsourcing, outsourcing of tasks to a crowd of unknown people (''workers'') in an open call, is rapidly rising in popularity. It is already being heavily used by numerous employers (''requesters'') for solving a wide variety of tasks, such as audio transcription, content screening, and labeling training data for machine learning. However, quality control of such tasks continues to be a key challenge because of the high variability in worker quality. In this paper we show the value of decision-theoretic techniques for the problem of optimizing workflows used in crowdsourcing. In particular, we design AI agents that use Bayesian network learning and inference in combination with Partially-Observable Markov Decision Processes (POMDPs) for obtaining excellent cost-quality tradeoffs. We use these techniques for three distinct crowdsourcing scenarios: (1) control of voting to answer a binary-choice question, (2) control of an iterative improvement workflow, and (3) control of switching between alternate workflows for a task. In each scenario, we design a Bayes net model that relates worker competency, task difficulty and worker response quality. We also design a POMDP for each task, whose solution provides the dynamic control policy. We demonstrate the usefulness of our models and agents in live experiments on Amazon Mechanical Turk. We consistently achieve superior quality results than non-adaptive controllers, while incurring equal or less cost.",
          "author": "Dai, P.;Lin, C.H.;Mausam;Weld, D.S.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 2,
          "score": 0.7522146701812744,
          "doc_id": "NART69625850",
          "title": "Inside the Turk : Understanding Mechanical Turk as a Participant Pool",
          "abstract": "<P>Mechanical Turk (MTurk), an online labor market created by Amazon, has recently become popular among social scientists as a source of survey and experimental data. The workers who populate this market have been assessed on dimensions that are universally relevant to understanding whether, why, and when they should be recruited as research participants. We discuss the characteristics of MTurk as a participant pool for psychology and other social sciences, highlighting the traits of the MTurk samples, why people become MTurk workers and research participants, and how data quality on MTurk compares to that from other pools and depends on controllable and uncontrollable factors.</P>",
          "doc_source": "Inside the Turk : Understanding Mechanical Turk as a Participant Pool Inside the Turk : Understanding Mechanical Turk as a Participant Pool Inside the Turk : Understanding Mechanical Turk as a Participant Pool <P>Mechanical Turk (MTurk), an online labor market created by Amazon, has recently become popular among social scientists as a source of survey and experimental data. The workers who populate this market have been assessed on dimensions that are universally relevant to understanding whether, why, and when they should be recruited as research participants. We discuss the characteristics of MTurk as a participant pool for psychology and other social sciences, highlighting the traits of the MTurk samples, why people become MTurk workers and research participants, and how data quality on MTurk compares to that from other pools and depends on controllable and uncontrollable factors.</P>",
          "author": "Paolacci, Gabriele;Chandler, Jesse;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 3,
          "score": 0.7312042713165283,
          "doc_id": "NART127620540",
          "title": "Mechanical Turk Versus Student Samples: Comparisons and Recommendations",
          "abstract": "<P>Mechanical Turk and other online crowdsourcing markets (OCMs) have become a go-to data source across scientific disciplines. In 2014 Steelman and colleagues investigated how Mechanical Turk data compared with student samples and consumer panels. They found the data to be comparable and reliable for academic research. In the nearly 10 years since its publication, the use of Mechanical Turk in research has grown substantially. To understand whether their results still hold, we conducted a partial replication to determine how Mechanical Turk workers continue to compare with students using UTAUT 2 as our theoretical model and virtual-reality headsets as the focal IT artifact. Our findings generally align with Steelman et al. (2014) and confirm that Mechanical Turk continues to offer a suitable alternative to student samples. This study reveals consistent results between the student and OCM samples, indicating the potential for interchangeability. The OCM samples are primarily male, while the student sample is majority female, following current US academic trends. All samples are significantly different in age, and only the US OCM and non-US OCM samples are similar in education. The path coefficients from the non-US OCM sample differ significantly from those from other OCM samples; the path coefficients derived from the student sample do not differ significantly from any OCM sample. While sample differences exist, as expected, many are addressable post hoc if anticipated and designed for during data collection. From our findings and the extant literature, we summarize recommendations for researchers and review teams.</P>",
          "doc_source": "Mechanical Turk Versus Student Samples: Comparisons and Recommendations Mechanical Turk Versus Student Samples: Comparisons and Recommendations Mechanical Turk Versus Student Samples: Comparisons and Recommendations <P>Mechanical Turk and other online crowdsourcing markets (OCMs) have become a go-to data source across scientific disciplines. In 2014 Steelman and colleagues investigated how Mechanical Turk data compared with student samples and consumer panels. They found the data to be comparable and reliable for academic research. In the nearly 10 years since its publication, the use of Mechanical Turk in research has grown substantially. To understand whether their results still hold, we conducted a partial replication to determine how Mechanical Turk workers continue to compare with students using UTAUT 2 as our theoretical model and virtual-reality headsets as the focal IT artifact. Our findings generally align with Steelman et al. (2014) and confirm that Mechanical Turk continues to offer a suitable alternative to student samples. This study reveals consistent results between the student and OCM samples, indicating the potential for interchangeability. The OCM samples are primarily male, while the student sample is majority female, following current US academic trends. All samples are significantly different in age, and only the US OCM and non-US OCM samples are similar in education. The path coefficients from the non-US OCM sample differ significantly from those from other OCM samples; the path coefficients derived from the student sample do not differ significantly from any OCM sample. While sample differences exist, as expected, many are addressable post hoc if anticipated and designed for during data collection. From our findings and the extant literature, we summarize recommendations for researchers and review teams.</P>",
          "author": "De Lurgio II, Stephen A.;Young, Amber;Steelman, Zachary R.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 4,
          "score": 0.7305762767791748,
          "doc_id": "NART77197564",
          "title": "Online recruitment and testing of infants with Mechanical Turk",
          "abstract": "Testing infants in the laboratory is expensive in time and money; consequently, many studies are underpowered, reducing their reproducibility. We investigated whether the online platform, Amazon Mechanical Turk (MTurk), could be used as a resource to more easily recruit and measure the behavior of infant populations. Using a looking time paradigm, with users' webcams we recorded how long infants aged 5 to 8months attended while viewing children's television programs. We found that infants (N=57) were more reliably engaged by some movies than by others and that the most engaging movies could maintain attention for approximately 70% of a 10- to 13-min period. We then identified the cinematic features within the movies. Faces, singing-and-rhyming, and camera zooms were found to increase infant attention. Together, we established that MTurk can be used as a rapid tool for effectively recruiting and testing infants.",
          "doc_source": "Online recruitment and testing of infants with Mechanical Turk Online recruitment and testing of infants with Mechanical Turk Online recruitment and testing of infants with Mechanical Turk Testing infants in the laboratory is expensive in time and money; consequently, many studies are underpowered, reducing their reproducibility. We investigated whether the online platform, Amazon Mechanical Turk (MTurk), could be used as a resource to more easily recruit and measure the behavior of infant populations. Using a looking time paradigm, with users' webcams we recorded how long infants aged 5 to 8months attended while viewing children's television programs. We found that infants (N=57) were more reliably engaged by some movies than by others and that the most engaging movies could maintain attention for approximately 70% of a 10- to 13-min period. We then identified the cinematic features within the movies. Faces, singing-and-rhyming, and camera zooms were found to increase infant attention. Together, we established that MTurk can be used as a rapid tool for effectively recruiting and testing infants.",
          "author": "Tran, M.;Cabral, L.;Patel, R.;Cusack, R.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 5,
          "score": 0.72977614402771,
          "doc_id": "NART75736850",
          "title": "Mechanical Turk upends social sciences",
          "abstract": "<P>In May, 23,000 people voluntarily took part in thousands of social science experiments without ever visiting a lab. All they did was log on to Amazon Mechanical Turk (MTurk), an online crowdsourcing service run by the Seattle, Washington&#x2013;based company better known for its massive internet-based retail business. Those research subjects completed 230,000 tasks on their computers in 3.3 million minutes&#x2014;more than 6 years of effort in total. The prodigious output demonstrates the popularity of an online platform that scientists had only begun to exploit 5 years ago. But the growing use of MTurk has raised concerns, as researchers discussed at the Association for Psychological Science meeting in Chicago, Illinois, last month. Some worry that they are becoming too dependent on a commercial platform. Others question whether the research volunteers are paid fairly and treated ethically. And looming over it all are questions about who these anonymous volunteers actually are, and concerns that they are less numerous and diverse than researchers hope.</P>",
          "doc_source": "Mechanical Turk upends social sciences Mechanical Turk upends social sciences Mechanical Turk upends social sciences <P>In May, 23,000 people voluntarily took part in thousands of social science experiments without ever visiting a lab. All they did was log on to Amazon Mechanical Turk (MTurk), an online crowdsourcing service run by the Seattle, Washington&#x2013;based company better known for its massive internet-based retail business. Those research subjects completed 230,000 tasks on their computers in 3.3 million minutes&#x2014;more than 6 years of effort in total. The prodigious output demonstrates the popularity of an online platform that scientists had only begun to exploit 5 years ago. But the growing use of MTurk has raised concerns, as researchers discussed at the Association for Psychological Science meeting in Chicago, Illinois, last month. Some worry that they are becoming too dependent on a commercial platform. Others question whether the research volunteers are paid fairly and treated ethically. And looming over it all are questions about who these anonymous volunteers actually are, and concerns that they are less numerous and diverse than researchers hope.</P>",
          "author": "Bohannon, John",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 6,
          "score": 0.7211211919784546,
          "doc_id": "NART117776930",
          "title": "Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk",
          "abstract": "<P><B>Background</B></P><P>Crowdsourcing services, such as Amazon Mechanical Turk (AMT), allow researchers to use the collective intelligence of a wide range of web users for labor-intensive tasks. As the manual verification of the quality of the collected results is difficult because of the large volume of data and the quick turnaround time of the process, many questions remain to be explored regarding the reliability of these resources for developing digital public health systems.</P><P><B>Objective</B></P><P>This study aims to explore and evaluate the application of crowdsourcing, generally, and AMT, specifically, for developing digital public health surveillance systems.</P><P><B>Methods</B></P><P>We collected 296,166 crowd-generated labels for 98,722 tweets, labeled by 610 AMT workers, to develop machine learning (ML) models for detecting behaviors related to physical activity, sedentary behavior, and sleep quality among Twitter users. To infer the ground truth labels and explore the quality of these labels, we studied 4 statistical consensus methods that are agnostic of task features and only focus on worker labeling behavior. Moreover, to model the meta-information associated with each labeling task and leverage the potential of context-sensitive data in the truth inference process, we developed 7 ML models, including traditional classifiers (offline and active), a deep learning&#x2013;based classification model, and a hybrid convolutional neural network model.</P><P><B>Results</B></P><P>Although most crowdsourcing-based studies in public health have often equated majority vote with quality, the results of our study using a truth set of 9000 manually labeled tweets showed that consensus-based inference models mask underlying uncertainty in data and overlook the importance of task meta-information. Our evaluations across 3 physical activity, sedentary behavior, and sleep quality data sets showed that truth inference is a context-sensitive process, and none of the methods studied in this paper were consistently superior to others in predicting the truth label. We also found that the performance of the ML models trained on crowd-labeled data was sensitive to the quality of these labels, and poor-quality labels led to incorrect assessment of these models. Finally, we have provided a set of practical recommendations to improve the quality and reliability of crowdsourced data.</P><P><B>Conclusions</B></P><P>Our findings indicate the importance of the quality of crowd-generated labels in developing ML models designed for decision-making purposes, such as public health surveillance decisions. A combination of inference models outlined and analyzed in this study could be used to quantitatively measure and improve the quality of crowd-generated labels for training ML models.</P>",
          "doc_source": "Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk Crowdsourcing for Machine Learning in Public Health Surveillance: Lessons Learned From Amazon Mechanical Turk <P><B>Background</B></P><P>Crowdsourcing services, such as Amazon Mechanical Turk (AMT), allow researchers to use the collective intelligence of a wide range of web users for labor-intensive tasks. As the manual verification of the quality of the collected results is difficult because of the large volume of data and the quick turnaround time of the process, many questions remain to be explored regarding the reliability of these resources for developing digital public health systems.</P><P><B>Objective</B></P><P>This study aims to explore and evaluate the application of crowdsourcing, generally, and AMT, specifically, for developing digital public health surveillance systems.</P><P><B>Methods</B></P><P>We collected 296,166 crowd-generated labels for 98,722 tweets, labeled by 610 AMT workers, to develop machine learning (ML) models for detecting behaviors related to physical activity, sedentary behavior, and sleep quality among Twitter users. To infer the ground truth labels and explore the quality of these labels, we studied 4 statistical consensus methods that are agnostic of task features and only focus on worker labeling behavior. Moreover, to model the meta-information associated with each labeling task and leverage the potential of context-sensitive data in the truth inference process, we developed 7 ML models, including traditional classifiers (offline and active), a deep learning&#x2013;based classification model, and a hybrid convolutional neural network model.</P><P><B>Results</B></P><P>Although most crowdsourcing-based studies in public health have often equated majority vote with quality, the results of our study using a truth set of 9000 manually labeled tweets showed that consensus-based inference models mask underlying uncertainty in data and overlook the importance of task meta-information. Our evaluations across 3 physical activity, sedentary behavior, and sleep quality data sets showed that truth inference is a context-sensitive process, and none of the methods studied in this paper were consistently superior to others in predicting the truth label. We also found that the performance of the ML models trained on crowd-labeled data was sensitive to the quality of these labels, and poor-quality labels led to incorrect assessment of these models. Finally, we have provided a set of practical recommendations to improve the quality and reliability of crowdsourced data.</P><P><B>Conclusions</B></P><P>Our findings indicate the importance of the quality of crowd-generated labels in developing ML models designed for decision-making purposes, such as public health surveillance decisions. A combination of inference models outlined and analyzed in this study could be used to quantitatively measure and improve the quality of crowd-generated labels for training ML models.</P>",
          "author": "Shakeri Hossein Abad, Zahra;Butler, Gregory P;Thompson, Wendy;Lee, Joon;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 7,
          "score": 0.7128536701202393,
          "doc_id": "NART92832666",
          "title": "Using Amazon Mechanical Turk for linguistic research",
          "abstract": "<P>Amazon?s Mechanical Turk service makes linguistic experimentation quick, easy, and inexpensive. However, researchers have not been certain about its reliability. In a series of experiments, this paper compares data collected via Mechanical Turk to those obtained using more traditional methods One set of experiments measured the predictability of words in sentences using the Cloze sentence completion task (Taylor, 1953). The correlation between traditional and Turk Cloze scores is high (rho=0.823) and both data sets perform similarly against alternative measures of contextual predictability. Five other experiments on the semantic relatedness of verbs and phrasal verbs (how much is ?lift? part of ?lift up?) manipulate the presence of the sentence context and the composition of the experimental list. The results indicate that Turk data correlate well between experiments and with data from traditional methods (rho up to 0.9), and they show high inter-rater consistency and agreement. We conclude that Mechanical Turk is a reliable source of data for complex linguistic tasks in heavy use by psycholinguists. The paper provides suggestions for best practices in data collection and scrubbing.</P>",
          "doc_source": "Using Amazon Mechanical Turk for linguistic research Using Amazon Mechanical Turk for linguistic research Using Amazon Mechanical Turk for linguistic research <P>Amazon?s Mechanical Turk service makes linguistic experimentation quick, easy, and inexpensive. However, researchers have not been certain about its reliability. In a series of experiments, this paper compares data collected via Mechanical Turk to those obtained using more traditional methods One set of experiments measured the predictability of words in sentences using the Cloze sentence completion task (Taylor, 1953). The correlation between traditional and Turk Cloze scores is high (rho=0.823) and both data sets perform similarly against alternative measures of contextual predictability. Five other experiments on the semantic relatedness of verbs and phrasal verbs (how much is ?lift? part of ?lift up?) manipulate the presence of the sentence context and the composition of the experimental list. The results indicate that Turk data correlate well between experiments and with data from traditional methods (rho up to 0.9), and they show high inter-rater consistency and agreement. We conclude that Mechanical Turk is a reliable source of data for complex linguistic tasks in heavy use by psycholinguists. The paper provides suggestions for best practices in data collection and scrubbing.</P>",
          "author": "Schnoebelen, Tyler;Kuperman, Victor;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 8,
          "score": 0.7049505114555359,
          "doc_id": "JAKO202411139606539",
          "title": "Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이",
          "abstract": "온라인 크라우드소싱 플랫폼인 Amazon Mechanical Turk(MTurk)은 뛰어난 과제 수행 기록을 가진 참가자들에게 마스터 등급을 부여한다. 그러나 MTurk의 마스터 참가자와 일반 참가자를 비교한 선행 연구들은 두 집단이 실제로 수행의 차이를 보이는가에 대해 일관되지 않은 결과를 보고했다. 또한 선행 연구들은 대부분 설문 조사 방식을 사용했으며 MTurk의 마스터와 일반 참가자의 인지 과제 수행 능력을 비교한 연구는 부족한 상황이다. 본 연구는 시각 기억 재인 과제를 사용하여 MTurk 마스터 및 일반 참가자와 오프라인에서 모집한 대학생 참가자 집단의 수행을 비교했다. 연구 결과, MTurk 마스터 참가자와 오프라인 참가자는 동일한 수준의 기억 수행을 보였다. 그러나 MTurk 일반 참가자의 기억 과제 수행은 마스터와 오프라인 참가자 집단의 결과와 차이를 보였다. 각 집단에서 기억 과제 정확률이 낮은 참가자를 제외한 후에도 동일한 결과가 나타났다. 이러한 결과는 온라인에서 참가자 집단을 적절히 선발하면 기존의 오프라인 실험 결과를 잘 재현할 수 있음을 보여준다. 동시에 본 연구의 결과는 온라인 크라우드소싱 플랫폼의 참가자 집단이 균일하지 않으며, 집단 선정 방식에 따라 연구의 결과가 다르게 나타날 수 있음을 시사한다.",
          "doc_source": "Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이 Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이 Amazon Mechanical Turk 마스터, 일반 참가자, 오프라인 참가자 집단의 기억 수행 차이 온라인 크라우드소싱 플랫폼인 Amazon Mechanical Turk(MTurk)은 뛰어난 과제 수행 기록을 가진 참가자들에게 마스터 등급을 부여한다. 그러나 MTurk의 마스터 참가자와 일반 참가자를 비교한 선행 연구들은 두 집단이 실제로 수행의 차이를 보이는가에 대해 일관되지 않은 결과를 보고했다. 또한 선행 연구들은 대부분 설문 조사 방식을 사용했으며 MTurk의 마스터와 일반 참가자의 인지 과제 수행 능력을 비교한 연구는 부족한 상황이다. 본 연구는 시각 기억 재인 과제를 사용하여 MTurk 마스터 및 일반 참가자와 오프라인에서 모집한 대학생 참가자 집단의 수행을 비교했다. 연구 결과, MTurk 마스터 참가자와 오프라인 참가자는 동일한 수준의 기억 수행을 보였다. 그러나 MTurk 일반 참가자의 기억 과제 수행은 마스터와 오프라인 참가자 집단의 결과와 차이를 보였다. 각 집단에서 기억 과제 정확률이 낮은 참가자를 제외한 후에도 동일한 결과가 나타났다. 이러한 결과는 온라인에서 참가자 집단을 적절히 선발하면 기존의 오프라인 실험 결과를 잘 재현할 수 있음을 보여준다. 동시에 본 연구의 결과는 온라인 크라우드소싱 플랫폼의 참가자 집단이 균일하지 않으며, 집단 선정 방식에 따라 연구의 결과가 다르게 나타날 수 있음을 시사한다.",
          "author": "정수근",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 9,
          "score": 0.7007761001586914,
          "doc_id": "NART106764981",
          "title": "Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data",
          "abstract": "<P>Within the field of Linguistics, Amazon&rsquo;s Mechanical Turk, a crowdsourcing marketplace specializes in computer-based Human Intelligence Tasks, has been praised as a cost efficient source of data for English and other major languages. Spanish is a good candidate due to its presence within the US and beyond. Still, detailed information concerning the linguistic and demographic profile of Spanish-speaking &lsquo;Turkers&rsquo; is missing, thus making it difficult for researchers to evaluate whether the Mechanical Turk provides the right environment for their tasks. This paper addresses this gap in our knowledge by developing the first detailed study of the presence of Spanish-speaking workers, focusing on factors relevant for research planning, namely, (socio)linguistically relevant variables and information concerning work habits. The results show that this platform provides access to a fairly active participant pool of both L1 and L2Spanish speakers as well as bilinguals. A brief introduction to how Amazon&rsquo;s Mechanical Turk works and overview of Hispanic Linguistics projects that have so far used the Mechanical Turk successfully is included.</P>",
          "doc_source": "Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data Crowdsourcing for Hispanic Linguistics: Amazon’s Mechanical Turk as a source of Spanish data <P>Within the field of Linguistics, Amazon&rsquo;s Mechanical Turk, a crowdsourcing marketplace specializes in computer-based Human Intelligence Tasks, has been praised as a cost efficient source of data for English and other major languages. Spanish is a good candidate due to its presence within the US and beyond. Still, detailed information concerning the linguistic and demographic profile of Spanish-speaking &lsquo;Turkers&rsquo; is missing, thus making it difficult for researchers to evaluate whether the Mechanical Turk provides the right environment for their tasks. This paper addresses this gap in our knowledge by developing the first detailed study of the presence of Spanish-speaking workers, focusing on factors relevant for research planning, namely, (socio)linguistically relevant variables and information concerning work habits. The results show that this platform provides access to a fairly active participant pool of both L1 and L2Spanish speakers as well as bilinguals. A brief introduction to how Amazon&rsquo;s Mechanical Turk works and overview of Hispanic Linguistics projects that have so far used the Mechanical Turk successfully is included.</P>",
          "author": "Ortega-Santos, Iv&aacute;n;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 10,
          "score": 0.7000492215156555,
          "doc_id": "NART123803221",
          "title": "Running experiments on Amazon Mechanical Turk",
          "abstract": "<P><B>Abstract</B><P>Although Mechanical Turk has recently become popular among social scientists as a source of experimental data, doubts may linger about the quality of data provided by subjects recruited from online labor markets. We address these potential concerns by presenting new demographic data about the Mechanical Turk subject population, reviewing the strengths of Mechanical Turk relative to other online and offline methods of recruiting subjects, and comparing the magnitude of effects obtained using Mechanical Turk and traditional subject pools. We further discuss some additional benefits such as the possibility of longitudinal, cross cultural and prescreening designs, and offer some advice on how to best manage a common subject pool.</P></P>",
          "doc_source": "Running experiments on Amazon Mechanical Turk Running experiments on Amazon Mechanical Turk Running experiments on Amazon Mechanical Turk <P><B>Abstract</B><P>Although Mechanical Turk has recently become popular among social scientists as a source of experimental data, doubts may linger about the quality of data provided by subjects recruited from online labor markets. We address these potential concerns by presenting new demographic data about the Mechanical Turk subject population, reviewing the strengths of Mechanical Turk relative to other online and offline methods of recruiting subjects, and comparing the magnitude of effects obtained using Mechanical Turk and traditional subject pools. We further discuss some additional benefits such as the possibility of longitudinal, cross cultural and prescreening designs, and offer some advice on how to best manage a common subject pool.</P></P>",
          "author": "Paolacci, Gabriele;Chandler, Jesse;Ipeirotis, Panagiotis G.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 11,
          "score": 0.6968144774436951,
          "doc_id": "NART74131061",
          "title": "Lessons Learned from Crowdsourcing Complex Engineering Tasks",
          "abstract": "<P><B>Crowdsourcing</B></P><P>Crowdsourcing is the practice of obtaining needed ideas, services, or content by requesting contributions from a large group of people. Amazon Mechanical Turk is a web marketplace for crowdsourcing microtasks, such as answering surveys and image tagging. We explored the limits of crowdsourcing by using Mechanical Turk for a more complicated task: analysis and creation of wind simulations.</P><P><B>Harnessing Crowdworkers for Engineering</B></P><P>Our investigation examined the feasibility of using crowdsourcing for complex, highly technical tasks. This was done to determine if the benefits of crowdsourcing could be harnessed to accurately and effectively contribute to solving complex real world engineering problems. Of course, untrained crowds cannot be used as a mere substitute for trained expertise. Rather, we sought to understand how crowd workers can be used as a large pool of labor for a preliminary analysis of complex data.</P><P><B>Virtual Wind Tunnel</B></P><P>We compared the skill of the anonymous crowd workers from Amazon Mechanical Turk with that of civil engineering graduate students, making a first pass at analyzing wind simulation data. For the first phase, we posted analysis questions to Amazon crowd workers and to two groups of civil engineering graduate students. A second phase of our experiment instructed crowd workers and students to create simulations on our Virtual Wind Tunnel website to solve a more complex task.</P><P><B>Conclusions</B></P><P>With a sufficiently comprehensive tutorial and compensation similar to typical crowd-sourcing wages, we were able to enlist crowd workers to effectively complete longer, more complex tasks with competence comparable to that of graduate students with more comprehensive, expert-level knowledge. Furthermore, more complex tasks require increased communication with the workers. As tasks become more complex, the employment relationship begins to become more akin to outsourcing than crowdsourcing. Through this investigation, we were able to stretch and explore the limits of crowdsourcing as a tool for solving complex problems.</P>",
          "doc_source": "Lessons Learned from Crowdsourcing Complex Engineering Tasks Lessons Learned from Crowdsourcing Complex Engineering Tasks Lessons Learned from Crowdsourcing Complex Engineering Tasks <P><B>Crowdsourcing</B></P><P>Crowdsourcing is the practice of obtaining needed ideas, services, or content by requesting contributions from a large group of people. Amazon Mechanical Turk is a web marketplace for crowdsourcing microtasks, such as answering surveys and image tagging. We explored the limits of crowdsourcing by using Mechanical Turk for a more complicated task: analysis and creation of wind simulations.</P><P><B>Harnessing Crowdworkers for Engineering</B></P><P>Our investigation examined the feasibility of using crowdsourcing for complex, highly technical tasks. This was done to determine if the benefits of crowdsourcing could be harnessed to accurately and effectively contribute to solving complex real world engineering problems. Of course, untrained crowds cannot be used as a mere substitute for trained expertise. Rather, we sought to understand how crowd workers can be used as a large pool of labor for a preliminary analysis of complex data.</P><P><B>Virtual Wind Tunnel</B></P><P>We compared the skill of the anonymous crowd workers from Amazon Mechanical Turk with that of civil engineering graduate students, making a first pass at analyzing wind simulation data. For the first phase, we posted analysis questions to Amazon crowd workers and to two groups of civil engineering graduate students. A second phase of our experiment instructed crowd workers and students to create simulations on our Virtual Wind Tunnel website to solve a more complex task.</P><P><B>Conclusions</B></P><P>With a sufficiently comprehensive tutorial and compensation similar to typical crowd-sourcing wages, we were able to enlist crowd workers to effectively complete longer, more complex tasks with competence comparable to that of graduate students with more comprehensive, expert-level knowledge. Furthermore, more complex tasks require increased communication with the workers. As tasks become more complex, the employment relationship begins to become more akin to outsourcing than crowdsourcing. Through this investigation, we were able to stretch and explore the limits of crowdsourcing as a tool for solving complex problems.</P>",
          "author": "Staffelbach, Matthew;Sempolinski, Peter;Kijewski-Correa, Tracy;Thain, Douglas;Wei, Daniel;Kareem, Ahsan;Madey, Gregory;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 12,
          "score": 0.6935259103775024,
          "doc_id": "NART103944733",
          "title": "The Language Demographics of Amazon Mechanical Turk",
          "abstract": "<P> We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers&rsquo; self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems. </P>",
          "doc_source": "The Language Demographics of Amazon Mechanical Turk The Language Demographics of Amazon Mechanical Turk The Language Demographics of Amazon Mechanical Turk <P> We present a large scale study of the languages spoken by bilingual workers on Mechanical Turk (MTurk). We establish a methodology for determining the language skills of anonymous crowd workers that is more robust than simple surveying. We validate workers&rsquo; self-reported language skill claims by measuring their ability to correctly translate words, and by geolocating workers to see if they reside in countries where the languages are likely to be spoken. Rather than posting a one-off survey, we posted paid tasks consisting of 1,000 assignments to translate a total of 10,000 words in each of 100 languages. Our study ran for several months, and was highly visible on the MTurk crowdsourcing platform, increasing the chances that bilingual workers would complete it. Our study was useful both to create bilingual dictionaries and to act as census of the bilingual speakers on MTurk. We use this data to recommend languages with the largest speaker populations as good candidates for other researchers who want to develop crowdsourced, multilingual technologies. To further demonstrate the value of creating data via crowdsourcing, we hire workers to create bilingual parallel corpora in six Indian languages, and use them to train statistical machine translation systems. </P>",
          "author": "Pavlick, Ellie;Post, Matt;Irvine, Ann;Kachaev, Dmitry;Callison-Burch, Chris;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 13,
          "score": 0.6923535466194153,
          "doc_id": "NART70754548",
          "title": "Amazon Mechanical Turk and the commodification of labour",
          "abstract": "<P>Crowd employment platforms enable firms to source labour and expertise by leveraging Internet technology. Rather than offshoring jobs to low&#8208;cost geographies, functions once performed by internal employees can be outsourced to an undefined pool of digital labour using a virtual network. This enables firms to shift costs and offload risk as they access a flexible, scalable workforce that sits outside the traditional boundaries of labour laws and regulations. The micro&#8208;tasks of &lsquo;clickwork&rsquo; are tedious, repetitive and poorly paid, with remuneration often well below minimum wage. This article will present an analysis of one of the most popular crowdsourcing sites&mdash;Mechanical Turk&mdash;to illuminate how Amazon's platform enables an array of companies to access digital labour at low cost and without any of the associated social protection or moral obligation.</P>",
          "doc_source": "Amazon Mechanical Turk and the commodification of labour Amazon Mechanical Turk and the commodification of labour Amazon Mechanical Turk and the commodification of labour <P>Crowd employment platforms enable firms to source labour and expertise by leveraging Internet technology. Rather than offshoring jobs to low&#8208;cost geographies, functions once performed by internal employees can be outsourced to an undefined pool of digital labour using a virtual network. This enables firms to shift costs and offload risk as they access a flexible, scalable workforce that sits outside the traditional boundaries of labour laws and regulations. The micro&#8208;tasks of &lsquo;clickwork&rsquo; are tedious, repetitive and poorly paid, with remuneration often well below minimum wage. This article will present an analysis of one of the most popular crowdsourcing sites&mdash;Mechanical Turk&mdash;to illuminate how Amazon's platform enables an array of companies to access digital labour at low cost and without any of the associated social protection or moral obligation.</P>",
          "author": "Bergvall&#8208;K&aring;reborn, Birgitta;Howcroft, Debra;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 14,
          "score": 0.6906248331069946,
          "doc_id": "NART88129314",
          "title": "Using Mechanical Turk to Study Clinical Populations",
          "abstract": "<P> Although participants with psychiatric symptoms, specific risk factors, or rare demographic characteristics can be difficult to identify and recruit for participation in research, participants with these characteristics are crucial for research in the social, behavioral, and clinical sciences. Online research in general and crowdsourcing software in particular may offer a solution. However, no research to date has examined the utility of crowdsourcing software for conducting research on psychopathology. In the current study, we examined the prevalence of several psychiatric disorders and related problems, as well as the reliability and validity of participant reports on these domains, among users of Amazon&rsquo;s Mechanical Turk. Findings suggest that crowdsourcing software offers several advantages for clinical research while providing insight into potential problems, such as misrepresentation, that researchers should address when collecting data online. </P>",
          "doc_source": "Using Mechanical Turk to Study Clinical Populations Using Mechanical Turk to Study Clinical Populations Using Mechanical Turk to Study Clinical Populations <P> Although participants with psychiatric symptoms, specific risk factors, or rare demographic characteristics can be difficult to identify and recruit for participation in research, participants with these characteristics are crucial for research in the social, behavioral, and clinical sciences. Online research in general and crowdsourcing software in particular may offer a solution. However, no research to date has examined the utility of crowdsourcing software for conducting research on psychopathology. In the current study, we examined the prevalence of several psychiatric disorders and related problems, as well as the reliability and validity of participant reports on these domains, among users of Amazon&rsquo;s Mechanical Turk. Findings suggest that crowdsourcing software offers several advantages for clinical research while providing insight into potential problems, such as misrepresentation, that researchers should address when collecting data online. </P>",
          "author": "Shapiro, Danielle N.;Chandler, Jesse;Mueller, Pam A.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 15,
          "score": 0.68595951795578,
          "doc_id": "NPAP12463036",
          "title": "Exploring Crowd Consistency in a Mechanical Turk Survey",
          "abstract": "<P>Crowdsourcing can provide a platform for evaluating software engineering research. In this paper, we aim to explore characteristics of the worker population on Amazon's Mechanical Turk, a popular micro task crowdsourcing environment, and measure the percentage of workers who are potentially qualified to perform software- or computer science- related tasks. Through a baseline survey and two replications, we measure workers' answer consistency as well as the consistency of sample characteristics. In the end, we deployed 1,200 total surveys that were completed by 1,064 unique workers. Our results show that 24% of the study participants have a computer science or IT background and most people are payment driven when choosing tasks. The sample characteristics can vary significantly, even on large samples with 300 participants. Additionally, we often observed inconsistency in workers' answers for those who completed two surveys; approximately 30% answered at least one question inconsistently between the two survey submissions. This implies a need for replication and quality controls in crowdsourced experiments.</P>",
          "doc_source": "Exploring Crowd Consistency in a Mechanical Turk Survey Exploring Crowd Consistency in a Mechanical Turk Survey Exploring Crowd Consistency in a Mechanical Turk Survey <P>Crowdsourcing can provide a platform for evaluating software engineering research. In this paper, we aim to explore characteristics of the worker population on Amazon's Mechanical Turk, a popular micro task crowdsourcing environment, and measure the percentage of workers who are potentially qualified to perform software- or computer science- related tasks. Through a baseline survey and two replications, we measure workers' answer consistency as well as the consistency of sample characteristics. In the end, we deployed 1,200 total surveys that were completed by 1,064 unique workers. Our results show that 24% of the study participants have a computer science or IT background and most people are payment driven when choosing tasks. The sample characteristics can vary significantly, even on large samples with 300 participants. Additionally, we often observed inconsistency in workers' answers for those who completed two surveys; approximately 30% answered at least one question inconsistently between the two survey submissions. This implies a need for replication and quality controls in crowdsourced experiments.</P>",
          "author": "Sun, Peng;Stolee, Kathryn T.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 16,
          "score": 0.6829960346221924,
          "doc_id": "ATN0036732137",
          "title": "How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk",
          "abstract": "<jats:p>Since its very first appearance the concept of crowdsourcing has undergone major variations, coming to include highly heterogeneous phenomena such as Google’s data mining, exchanges on sharing economy platforms (e.g. Airbnb or eBay), contents production within creative communities online (e.g. Wikipedia) and much more. If one assumes a very broad perspective, it is eventually possible to extend the category of crowdsourcing to cover whatsoever phenomena involving the participation of the crowd online, as in fact has been done. On the contrary, I will argue that crowdsourcing – and in particular its microwork branch – represents the specific practice of extending outsourcing processes to a large, low-cost, scalable and flexible workforce, in order to generate greater added value for a supply chain. To develop this analysis, I will especially focus on the case of Amazon Mechanical Turk, and on how the operations carried out on this platform are primarily intended to manage the huge flow of information which spans across a supply chain. The practice of subcontracting to the crowd tasks previously carried out by employees or third-party suppliers highlights how crowdsourcing involves a reshaping of the supply chain, further extending it to a large network of individuals. Through crowdsourcing processes, companies are either able to replace or train AI, integrating human computation skills in algorithmic structures through simple, and oftentimes tedious, microtasks. In this context, processes of gamification are capable to put further downward pressure on already small piece-wages, as long as crowdworkers are rather willing to earn an even lower economic compensation, if it’s associated to challenging tasks; thus, to make a task more enjoyable through gamification could be an effective way to further reduce a supply chain’s expenditures in crowdsourcing, pushing forward labor exploitation practices structurally embedded in this phenomenon.</jats:p>",
          "doc_source": "How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk How Gamification Affects Crowdsourcing: The Case of Amazon Mechanical Turk <jats:p>Since its very first appearance the concept of crowdsourcing has undergone major variations, coming to include highly heterogeneous phenomena such as Google’s data mining, exchanges on sharing economy platforms (e.g. Airbnb or eBay), contents production within creative communities online (e.g. Wikipedia) and much more. If one assumes a very broad perspective, it is eventually possible to extend the category of crowdsourcing to cover whatsoever phenomena involving the participation of the crowd online, as in fact has been done. On the contrary, I will argue that crowdsourcing – and in particular its microwork branch – represents the specific practice of extending outsourcing processes to a large, low-cost, scalable and flexible workforce, in order to generate greater added value for a supply chain. To develop this analysis, I will especially focus on the case of Amazon Mechanical Turk, and on how the operations carried out on this platform are primarily intended to manage the huge flow of information which spans across a supply chain. The practice of subcontracting to the crowd tasks previously carried out by employees or third-party suppliers highlights how crowdsourcing involves a reshaping of the supply chain, further extending it to a large network of individuals. Through crowdsourcing processes, companies are either able to replace or train AI, integrating human computation skills in algorithmic structures through simple, and oftentimes tedious, microtasks. In this context, processes of gamification are capable to put further downward pressure on already small piece-wages, as long as crowdworkers are rather willing to earn an even lower economic compensation, if it’s associated to challenging tasks; thus, to make a task more enjoyable through gamification could be an effective way to further reduce a supply chain’s expenditures in crowdsourcing, pushing forward labor exploitation practices structurally embedded in this phenomenon.</jats:p>",
          "author": "De Lellis Lorenzo",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 17,
          "score": 0.6811766624450684,
          "doc_id": "NART134452383",
          "title": "&raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk",
          "abstract": "<P>This article centers Amazon Mechanical Turk (MTurk) workers to examine their alienation, as they complete monotonous and repetitive microtasks from behind their screens. Confronted with various &raquo;virtual assembly lines&laquo; that produce data across the globe, their labor can be further used for machine learning specifically and Artificial Intelligence more generally. Engaging with these workers and their labor is central to general contemporary and future technological developments bound to bring their own repercussions with them - including the growing and central role of algorithms in managing the world of work.</P>",
          "doc_source": "&raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk &raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk &raquo;K&uuml;nstliche K&uuml;nstliche Intelligenz&laquo; : Gigging auf Amazons Plattform Mechanical Turk <P>This article centers Amazon Mechanical Turk (MTurk) workers to examine their alienation, as they complete monotonous and repetitive microtasks from behind their screens. Confronted with various &raquo;virtual assembly lines&laquo; that produce data across the globe, their labor can be further used for machine learning specifically and Artificial Intelligence more generally. Engaging with these workers and their labor is central to general contemporary and future technological developments bound to bring their own repercussions with them - including the growing and central role of algorithms in managing the world of work.</P>",
          "author": "Kassem, Sarrah;Wilpert (&Uuml;bersetzung), Chris W.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 18,
          "score": 0.678891658782959,
          "doc_id": "NART87817032",
          "title": "Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative",
          "abstract": "<P>In this paper we evaluate how to effectively use the crowdsourcing service, Amazon's Mechanical Turk (MTurk), to content analyze textual data for use in psychological research. MTurk is a marketplace for discrete tasks completed by workers, typically for small amounts of money. MTurk has been used to aid psychological research in general, and content analysis in particular. In the current study, MTurk workers content analyzed personally-written textual data using coding categories previously developed and validated in psychological research. These codes were evaluated for reliability, accuracy, completion time, and cost. Results indicate that MTurk workers categorized textual data with comparable reliability and accuracy to both previously published studies and expert raters. Further, the coding tasks were performed quickly and cheaply. These data suggest that crowdsourced content analysis can help advance psychological research.</P>",
          "doc_source": "Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative Coding Psychological Constructs in Text Using Mechanical Turk: A Reliable, Accurate, and Efficient Alternative <P>In this paper we evaluate how to effectively use the crowdsourcing service, Amazon's Mechanical Turk (MTurk), to content analyze textual data for use in psychological research. MTurk is a marketplace for discrete tasks completed by workers, typically for small amounts of money. MTurk has been used to aid psychological research in general, and content analysis in particular. In the current study, MTurk workers content analyzed personally-written textual data using coding categories previously developed and validated in psychological research. These codes were evaluated for reliability, accuracy, completion time, and cost. Results indicate that MTurk workers categorized textual data with comparable reliability and accuracy to both previously published studies and expert raters. Further, the coding tasks were performed quickly and cheaply. These data suggest that crowdsourced content analysis can help advance psychological research.</P>",
          "author": "Tosti-Kharas, Jennifer;Conley, Caryn;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 19,
          "score": 0.6681106090545654,
          "doc_id": "NART73267731",
          "title": "The (Non) Religion of Mechanical Turk Workers",
          "abstract": "<P>Social science researchers have increasingly come to utilize Amazon's Mechanical Turk (MTurk) to obtain adult, opt&#8208;in samples for use with experiments. Based on the demographic characteristics of MTurk samples, studies have provided some support for the representativeness of MTurk. Others have warranted caution based on demographic characteristics and comparisons of reliability. Yet, what is missing is an examination of the most glaring demographic difference in MTurk&mdash;religion. We compare five MTurk samples with a student convenience sample and the 2012 General Social Survey, finding that MTurk samples have a consistent bias toward nonreligion. MTurk surveys significantly overrepresent seculars and underrepresent Catholics and evangelical Protestants. We then compare the religiosity of religious identifiers across samples as well as relationships between religiosity and partisanship, finding many similarities and a few important differences from the general population.</P>",
          "doc_source": "The (Non) Religion of Mechanical Turk Workers The (Non) Religion of Mechanical Turk Workers The (Non) Religion of Mechanical Turk Workers <P>Social science researchers have increasingly come to utilize Amazon's Mechanical Turk (MTurk) to obtain adult, opt&#8208;in samples for use with experiments. Based on the demographic characteristics of MTurk samples, studies have provided some support for the representativeness of MTurk. Others have warranted caution based on demographic characteristics and comparisons of reliability. Yet, what is missing is an examination of the most glaring demographic difference in MTurk&mdash;religion. We compare five MTurk samples with a student convenience sample and the 2012 General Social Survey, finding that MTurk samples have a consistent bias toward nonreligion. MTurk surveys significantly overrepresent seculars and underrepresent Catholics and evangelical Protestants. We then compare the religiosity of religious identifiers across samples as well as relationships between religiosity and partisanship, finding many similarities and a few important differences from the general population.</P>",
          "author": "Lewis, Andrew R.;Djupe, Paul A.;Mockabee, Stephen T.;Su&#8208;Ya Wu, Joshua;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 20,
          "score": 0.667040228843689,
          "doc_id": "NART109637313",
          "title": "Annotator Rationales for Labeling Tasks in Crowdsourcing",
          "abstract": "<P>When collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. To address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. We evaluate this approach on an information retrieval task in which human judges rate the relevance of Web pages for different search topics. Cost-benefit analysis over 10,000 judgments collected on Amazon&rsquo;s Mechanical Turk suggests a win-win. Firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. Secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. Consequently, we can realize the above benefits with minimal additional cost.</P>",
          "doc_source": "Annotator Rationales for Labeling Tasks in Crowdsourcing Annotator Rationales for Labeling Tasks in Crowdsourcing Annotator Rationales for Labeling Tasks in Crowdsourcing <P>When collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. To address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. We evaluate this approach on an information retrieval task in which human judges rate the relevance of Web pages for different search topics. Cost-benefit analysis over 10,000 judgments collected on Amazon&rsquo;s Mechanical Turk suggests a win-win. Firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. Secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. Consequently, we can realize the above benefits with minimal additional cost.</P>",
          "author": "Kutlu, Mucahid;McDonnell, Tyler;Lease, Matthew;Elsayed, Tamer;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 21,
          "score": 0.6617283821105957,
          "doc_id": "NART73517650",
          "title": "Extending the Applicability of POMDP Solutions to Robotic Tasks",
          "abstract": "<P>Partially observable Markov decision processes (POMDPs) are used in many robotic task classes from soccer to household chores. Determining an approximately optimal action policy for POMDPs is PSPACE-complete, and the exponential growth of computation time prohibits solving large tasks. This paper describes two techniques to extend the range of robotic tasks that can be solved using a POMDP. Our first technique reduces the motion constraints of a robot and, then, uses state-of-the-art robotic motion planning techniques to respect the true motion constraints at runtime. We then propose a novel task decomposition that can be applied to some indoor robotic tasks. This decomposition transforms a long time horizon task into a set of shorter tasks. We empirically demonstrate the performance gain provided by these two techniques through simulated execution in a variety of environments. Comparing a direct formulation of a POMDP to solving our proposed reductions, we conclude that the techniques proposed in this paper can provide significant enhancement to current POMDP solution techniques, extending the POMDP instances that can be solved to include large continuous-state robotic tasks.</P>",
          "doc_source": "Extending the Applicability of POMDP Solutions to Robotic Tasks Extending the Applicability of POMDP Solutions to Robotic Tasks Extending the Applicability of POMDP Solutions to Robotic Tasks <P>Partially observable Markov decision processes (POMDPs) are used in many robotic task classes from soccer to household chores. Determining an approximately optimal action policy for POMDPs is PSPACE-complete, and the exponential growth of computation time prohibits solving large tasks. This paper describes two techniques to extend the range of robotic tasks that can be solved using a POMDP. Our first technique reduces the motion constraints of a robot and, then, uses state-of-the-art robotic motion planning techniques to respect the true motion constraints at runtime. We then propose a novel task decomposition that can be applied to some indoor robotic tasks. This decomposition transforms a long time horizon task into a set of shorter tasks. We empirically demonstrate the performance gain provided by these two techniques through simulated execution in a variety of environments. Comparing a direct formulation of a POMDP to solving our proposed reductions, we conclude that the techniques proposed in this paper can provide significant enhancement to current POMDP solution techniques, extending the POMDP instances that can be solved to include large continuous-state robotic tasks.</P>",
          "author": "Grady, Devin K.;Moll, Mark;Kavraki, Lydia E.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 22,
          "score": 0.6532520055770874,
          "doc_id": "NPAP13833015",
          "title": "Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk",
          "abstract": "nan",
          "doc_source": "Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk Investigating the Accessibility of Crowdwork Tasks on Mechanical Turk ",
          "author": "Uzor, Stephen;Jacques, Jason T.;Dudley, John J;Kristensson, Per Ola;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 23,
          "score": 0.6527199149131775,
          "doc_id": "NART73604379",
          "title": "Conducting behavioral research on Amazon’s Mechanical Turk",
          "abstract": "<P>Amazon&#039;s Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.</P>",
          "doc_source": "Conducting behavioral research on Amazon’s Mechanical Turk Conducting behavioral research on Amazon’s Mechanical Turk Conducting behavioral research on Amazon’s Mechanical Turk <P>Amazon&#039;s Mechanical Turk is an online labor market where requesters post jobs and workers choose which jobs to do for pay. The central purpose of this article is to demonstrate how to use this Web site for conducting behavioral research and to lower the barrier to entry for researchers who could benefit from this platform. We describe general techniques that apply to a variety of types of research and experiments across disciplines. We begin by discussing some of the advantages of doing experiments on Mechanical Turk, such as easy access to a large, stable, and diverse subject pool, the low cost of doing experiments, and faster iteration between developing theory and executing experiments. While other methods of conducting behavioral research may be comparable to or even better than Mechanical Turk on one or more of the axes outlined above, we will show that when taken as a whole Mechanical Turk can be a useful tool for many researchers. We will discuss how the behavior of workers compares with that of experts and laboratory subjects. Then we will illustrate the mechanics of putting a task on Mechanical Turk, including recruiting subjects, executing the task, and reviewing the work that was submitted. We also provide solutions to common problems that a researcher might face when executing their research on this platform, including techniques for conducting synchronous experiments, methods for ensuring high-quality work, how to keep data private, and how to maintain code security.</P>",
          "author": "Mason, Winter;Suri, Siddharth;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 24,
          "score": 0.6520376205444336,
          "doc_id": "NART78755911",
          "title": "Using Mechanical Turk for research on cancer survivors",
          "abstract": "<P><B>Abstract</B></P><P><B>Objective</B></P><P>The successful recruitment and study of cancer survivors within psycho&#8208;oncology research can be challenging, time&#8208;consuming, and expensive, particularly for key subgroups such as young adult cancer survivors. Online crowdsourcing platforms offer a potential solution that has not yet been investigated with regard to cancer populations. The current study assessed the presence of cancer survivors on Amazon's Mechanical Turk (MTurk) and the feasibility of using MTurk as an efficient, cost&#8208;effective, and reliable psycho&#8208;oncology recruitment and research platform.</P><P><B>Methods</B></P><P>During a <4&#8208;month period, cancer survivors living in the United States were recruited on MTurk to complete two assessments, spaced 1 week apart, relating to psychosocial and cancer&#8208;related functioning. The reliability and validity of responses were investigated.</P><P><B>Results</B></P><P>Within a <4&#8208;month period, 464 self&#8208;identified cancer survivors on MTurk consented to and completed an online assessment. The vast majority (79.09%) provided reliable and valid study data according to multiple indices. The sample was highly diverse in terms of U.S. geography, socioeconomic status, and cancer type, and reflected a particularly strong presence of distressed and young adult cancer survivors (median age = 36 years). A majority of participants (58.19%) responded to a second survey sent one week later.</P><P><B>Conclusions</B></P><P>Online crowdsourcing represents a feasible, efficient, and cost&#8208;effective recruitment and research platform for cancer survivors, particularly for young adult cancer survivors and those with significant distress. We discuss remaining challenges and future recommendations. Copyright &copy; 2016 John Wiley &amp; Sons, Ltd.</P>",
          "doc_source": "Using Mechanical Turk for research on cancer survivors Using Mechanical Turk for research on cancer survivors Using Mechanical Turk for research on cancer survivors <P><B>Abstract</B></P><P><B>Objective</B></P><P>The successful recruitment and study of cancer survivors within psycho&#8208;oncology research can be challenging, time&#8208;consuming, and expensive, particularly for key subgroups such as young adult cancer survivors. Online crowdsourcing platforms offer a potential solution that has not yet been investigated with regard to cancer populations. The current study assessed the presence of cancer survivors on Amazon's Mechanical Turk (MTurk) and the feasibility of using MTurk as an efficient, cost&#8208;effective, and reliable psycho&#8208;oncology recruitment and research platform.</P><P><B>Methods</B></P><P>During a <4&#8208;month period, cancer survivors living in the United States were recruited on MTurk to complete two assessments, spaced 1 week apart, relating to psychosocial and cancer&#8208;related functioning. The reliability and validity of responses were investigated.</P><P><B>Results</B></P><P>Within a <4&#8208;month period, 464 self&#8208;identified cancer survivors on MTurk consented to and completed an online assessment. The vast majority (79.09%) provided reliable and valid study data according to multiple indices. The sample was highly diverse in terms of U.S. geography, socioeconomic status, and cancer type, and reflected a particularly strong presence of distressed and young adult cancer survivors (median age = 36 years). A majority of participants (58.19%) responded to a second survey sent one week later.</P><P><B>Conclusions</B></P><P>Online crowdsourcing represents a feasible, efficient, and cost&#8208;effective recruitment and research platform for cancer survivors, particularly for young adult cancer survivors and those with significant distress. We discuss remaining challenges and future recommendations. Copyright &copy; 2016 John Wiley &amp; Sons, Ltd.</P>",
          "author": "Arch, Joanna J.;Carr, Alaina L.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 25,
          "score": 0.6504065990447998,
          "doc_id": "NART70960968",
          "title": "A reliability analysis of Mechanical Turk data",
          "abstract": "Amazon's Mechanical Turk (MTurk) provides researchers with access to a diverse set of people who can serve as research participants, making the process of data collection a streamlined and cost-effective one. While a small number of studies are often cited to support the use of this methodology, there remains a need for additional analyses of the quality of the research data. In the present study, MTurk-based responses for a personality scale were found to be significantly less reliable than scores previously reported for a community sample. While score reliability was not affected by the length of the survey or the payment rates, the presence of an item asking respondents to affirm that they were attentive and honest was associated with more reliable responses. Best practices for MTurk-based research and continuing research needs are addressed.",
          "doc_source": "A reliability analysis of Mechanical Turk data A reliability analysis of Mechanical Turk data A reliability analysis of Mechanical Turk data Amazon's Mechanical Turk (MTurk) provides researchers with access to a diverse set of people who can serve as research participants, making the process of data collection a streamlined and cost-effective one. While a small number of studies are often cited to support the use of this methodology, there remains a need for additional analyses of the quality of the research data. In the present study, MTurk-based responses for a personality scale were found to be significantly less reliable than scores previously reported for a community sample. While score reliability was not affected by the length of the survey or the payment rates, the presence of an item asking respondents to affirm that they were attentive and honest was associated with more reliable responses. Best practices for MTurk-based research and continuing research needs are addressed.",
          "author": "Rouse, S.V.",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 26,
          "score": 0.6501059532165527,
          "doc_id": "NART105659807",
          "title": "Analyzing the Amazon Mechanical Turk marketplace",
          "abstract": "<P>An associate professor at New York Universitys Stern School of Business uncovers answers about who are the employers in paid crowdsourcing, what tasks they post, and how much they pay.</P>",
          "doc_source": "Analyzing the Amazon Mechanical Turk marketplace Analyzing the Amazon Mechanical Turk marketplace Analyzing the Amazon Mechanical Turk marketplace <P>An associate professor at New York Universitys Stern School of Business uncovers answers about who are the employers in paid crowdsourcing, what tasks they post, and how much they pay.</P>",
          "author": "Ipeirotis, Panagiotis G.",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 27,
          "score": 0.6432468891143799,
          "doc_id": "NPAP13914175",
          "title": "Quality management on Amazon Mechanical Turk",
          "abstract": "nan",
          "doc_source": "Quality management on Amazon Mechanical Turk Quality management on Amazon Mechanical Turk Quality management on Amazon Mechanical Turk ",
          "author": "Ipeirotis, Panagiotis G.;Provost, Foster;Wang, Jing;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 28,
          "score": 0.6341761350631714,
          "doc_id": "NART120020213",
          "title": "Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces",
          "abstract": "<P>Controlling the quality of tasks is a major challenge in crowdsourcing marketplaces. Most of the existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in the marketplaces have to monitor the tasks continuously to find such improper tasks; however, it is too expensive to manually investigate each task. In this paper, we present the reports of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We perform experiments using real task data from a commercial crowdsourcing marketplace and show that the classifier trained by the operator judgments achieves high accuracy in detecting improper tasks. In addition, to reduce the annotation costs of the operator and improve the classification accuracy, we consider the use of crowdsourcing for task annotation. We hire a group of crowdsourcing (non-expert) workers to monitor posted tasks, and incorporate their judgments into the training data of the classifier. By applying quality control techniques to handle the variability in worker reliability, our results show that the use of non-expert judgments by crowdsourcing workers in combination with expert judgments improves the accuracy of detecting improper crowdsourcing tasks.</P>",
          "doc_source": "Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces Leveraging Crowdsourcing to Detect Improper Tasks in Crowdsourcing Marketplaces <P>Controlling the quality of tasks is a major challenge in crowdsourcing marketplaces. Most of the existing crowdsourcing services prohibit requesters from posting illegal or objectionable tasks. Operators in the marketplaces have to monitor the tasks continuously to find such improper tasks; however, it is too expensive to manually investigate each task. In this paper, we present the reports of our trial study on automatic detection of improper tasks to support the monitoring of activities by marketplace operators. We perform experiments using real task data from a commercial crowdsourcing marketplace and show that the classifier trained by the operator judgments achieves high accuracy in detecting improper tasks. In addition, to reduce the annotation costs of the operator and improve the classification accuracy, we consider the use of crowdsourcing for task annotation. We hire a group of crowdsourcing (non-expert) workers to monitor posted tasks, and incorporate their judgments into the training data of the classifier. By applying quality control techniques to handle the variability in worker reliability, our results show that the use of non-expert judgments by crowdsourcing workers in combination with expert judgments improves the accuracy of detecting improper crowdsourcing tasks.</P>",
          "author": "Baba, Yukino;Kashima, Hisashi;Kinoshita, Kei;Yamaguchi, Goushi;Akiyoshi, Yosuke;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 29,
          "score": 0.6328755617141724,
          "doc_id": "NART69743055",
          "title": "Crowdsourcing content analysis for managerial research",
          "abstract": "<P><B>Purpose</B></P> <P> &ndash; The purpose of this paper is to evaluate the effectiveness of a novel method for performing content analysis in managerial research &ndash; crowdsourcing, a system where geographically distributed workers complete small, discrete tasks via the internet for a small amount of money. </P> <P><B>Design/methodology/approach</B></P> <P> &ndash; The authors examined whether workers from one popular crowdsourcing marketplace, Amazon's Mechanical Turk, could perform subjective content analytic tasks involving the application of inductively generated codes to unstructured, personally written textual passages. </P> <P><B>Findings</B></P> <P> &ndash; The findings suggest that anonymous, self-selected, non-expert crowdsourced workers were applied content codes efficiently and at low cost, and that their reliability and accuracy compared to that of trained researchers. </P> <P><B>Research limitations/implications</B></P> <P> &ndash; The authors provide recommendations for management researchers interested in using crowdsourcing most effectively for content analysis, including a discussion of the limitations and ethical issues involved in using this method. Future research could extend the findings by considering alternative data sources and coding schemes of interest to management researchers. </P> <P><B>Originality/value</B></P> <P> &ndash; Scholars have begun to explore whether crowdsourcing can assist in academic research; however, this is the first study to examine how crowdsourcing might facilitate content analysis. Crowdsourcing offers several advantages over existing content analytic approaches by combining the efficiency of computer-aided text analysis with the interpretive ability of traditional human coding.</P>",
          "doc_source": "Crowdsourcing content analysis for managerial research Crowdsourcing content analysis for managerial research Crowdsourcing content analysis for managerial research <P><B>Purpose</B></P> <P> &ndash; The purpose of this paper is to evaluate the effectiveness of a novel method for performing content analysis in managerial research &ndash; crowdsourcing, a system where geographically distributed workers complete small, discrete tasks via the internet for a small amount of money. </P> <P><B>Design/methodology/approach</B></P> <P> &ndash; The authors examined whether workers from one popular crowdsourcing marketplace, Amazon's Mechanical Turk, could perform subjective content analytic tasks involving the application of inductively generated codes to unstructured, personally written textual passages. </P> <P><B>Findings</B></P> <P> &ndash; The findings suggest that anonymous, self-selected, non-expert crowdsourced workers were applied content codes efficiently and at low cost, and that their reliability and accuracy compared to that of trained researchers. </P> <P><B>Research limitations/implications</B></P> <P> &ndash; The authors provide recommendations for management researchers interested in using crowdsourcing most effectively for content analysis, including a discussion of the limitations and ethical issues involved in using this method. Future research could extend the findings by considering alternative data sources and coding schemes of interest to management researchers. </P> <P><B>Originality/value</B></P> <P> &ndash; Scholars have begun to explore whether crowdsourcing can assist in academic research; however, this is the first study to examine how crowdsourcing might facilitate content analysis. Crowdsourcing offers several advantages over existing content analytic approaches by combining the efficiency of computer-aided text analysis with the interpretive ability of traditional human coding.</P>",
          "author": "Conley, Caryn;Tosti-Kharas, Jennifer;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 30,
          "score": 0.632236897945404,
          "doc_id": "NART121336460",
          "title": "Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples",
          "abstract": "<P> Two of the most popular populations for convenience sampling used in the psychological sciences are college students and Mechanical Turk (MTurk) workers. College students represent a traditional type of convenience sample, whereas MTurk workers provide a more modern source of data. However, little research has examined how these populations differ from each other in salient characteristics. Additionally, no research to date has investigated how MTurk college students (a traditional sample collected using modern methods) compare to either population. The current study examined 1,248 participants comprising three samples: MTurk noncollege workers ( n = 533), MTurk college students ( n = 385), and traditional college students ( n = 330). We compared the samples on demographic characteristics, study completion time, attention, and individual difference variables (i.e., personality, social desirability, need for cognition, personal values, and social attitudes). We examined the individual difference variables in terms of mean responses, internal consistency estimates, and subscale intercorrelations. Results indicated the samples were distinct from each other in terms of all variables assessed; in addition, adding demographic characteristics as covariates to the analyses of individual difference variables did not effectively account for sample differences. We conclude that research using convenience samples should take these differences into account. </P>",
          "doc_source": "Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples Traditional and Modern Convenience Samples: An Investigation of College Student, Mechanical Turk, and Mechanical Turk College Student Samples <P> Two of the most popular populations for convenience sampling used in the psychological sciences are college students and Mechanical Turk (MTurk) workers. College students represent a traditional type of convenience sample, whereas MTurk workers provide a more modern source of data. However, little research has examined how these populations differ from each other in salient characteristics. Additionally, no research to date has investigated how MTurk college students (a traditional sample collected using modern methods) compare to either population. The current study examined 1,248 participants comprising three samples: MTurk noncollege workers ( n = 533), MTurk college students ( n = 385), and traditional college students ( n = 330). We compared the samples on demographic characteristics, study completion time, attention, and individual difference variables (i.e., personality, social desirability, need for cognition, personal values, and social attitudes). We examined the individual difference variables in terms of mean responses, internal consistency estimates, and subscale intercorrelations. Results indicated the samples were distinct from each other in terms of all variables assessed; in addition, adding demographic characteristics as covariates to the analyses of individual difference variables did not effectively account for sample differences. We conclude that research using convenience samples should take these differences into account. </P>",
          "author": "Weigold, Arne;Weigold, Ingrid K.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 31,
          "score": 0.618955135345459,
          "doc_id": "NART77189173",
          "title": "Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks",
          "abstract": "<P>We present the design and evaluation of a method for estimating gaze locations during the analysis of static visualizations using crowdsourcing. Understanding gaze patterns is helpful for evaluating visualizations and user behaviors, but traditional eye-tracking studies require specialized hardware and local users. To avoid these constraints, we developed a method called Fauxvea, which crowdsources visualization tasks on the Web and estimates gaze fixations through cursor interactions without eye-tracking hardware. We ran experiments to evaluate how gaze estimates from our method compare with eye-tracking data. First, we evaluated crowdsourced estimates for three common types of information visualizations and basic visualization tasks using Amazon Mechanical Turk (MTurk). In another, we reproduced findings from a previous eye-tracking study on tree layouts using our method on MTurk. Results from these experiments show that fixation estimates using Fauxvea are qualitatively and quantitatively similar to eye tracking on the same stimulus-task pairs. These findings suggest that crowdsourcing visual analysis tasks with static information visualizations could be a viable alternative to traditional eye-tracking studies for visualization research and design.</P>",
          "doc_source": "Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks <P>We present the design and evaluation of a method for estimating gaze locations during the analysis of static visualizations using crowdsourcing. Understanding gaze patterns is helpful for evaluating visualizations and user behaviors, but traditional eye-tracking studies require specialized hardware and local users. To avoid these constraints, we developed a method called Fauxvea, which crowdsources visualization tasks on the Web and estimates gaze fixations through cursor interactions without eye-tracking hardware. We ran experiments to evaluate how gaze estimates from our method compare with eye-tracking data. First, we evaluated crowdsourced estimates for three common types of information visualizations and basic visualization tasks using Amazon Mechanical Turk (MTurk). In another, we reproduced findings from a previous eye-tracking study on tree layouts using our method on MTurk. Results from these experiments show that fixation estimates using Fauxvea are qualitatively and quantitatively similar to eye tracking on the same stimulus-task pairs. These findings suggest that crowdsourcing visual analysis tasks with static information visualizations could be a viable alternative to traditional eye-tracking studies for visualization research and design.</P>",
          "author": "Gomez, Steven R.;Jianu, Radu;Cabeen, Ryan;Guo, Hua;Laidlaw, David H.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 32,
          "score": 0.6166528463363647,
          "doc_id": "NART73866218",
          "title": "The Southern Dative Presentative Meets Mechanical Turk",
          "abstract": "<P>This article introduces the southern dative presentative, an understudied construction that varies across speakers of American English. The authors discuss similarities and differences between this construction and the better-studied personal dative construction and compare the Southern dative presentative with similar constructions cross-linguistically. They then present the results of a nationwide acceptability judgment survey administered on Amazon Mechanical Turk. The results show that Southern dative presentatives are alive and well in Southern dialects of American English. In the process, they also illustrate the usefulness of Amazon Mechanical Turk (and similar crowdsourcing platforms) for the study of dialect variation in the domain of syntax.</P>",
          "doc_source": "The Southern Dative Presentative Meets Mechanical Turk The Southern Dative Presentative Meets Mechanical Turk The Southern Dative Presentative Meets Mechanical Turk <P>This article introduces the southern dative presentative, an understudied construction that varies across speakers of American English. The authors discuss similarities and differences between this construction and the better-studied personal dative construction and compare the Southern dative presentative with similar constructions cross-linguistically. They then present the results of a nationwide acceptability judgment survey administered on Amazon Mechanical Turk. The results show that Southern dative presentatives are alive and well in Southern dialects of American English. In the process, they also illustrate the usefulness of Amazon Mechanical Turk (and similar crowdsourcing platforms) for the study of dialect variation in the domain of syntax.</P>",
          "author": "Wood, Jim;Horn, Laurence;Zanuttini, Raffaella;Lindemann, Luke;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 33,
          "score": 0.6073062419891357,
          "doc_id": "NPAP13897603",
          "title": "TurKit : tools for iterative tasks on mechanical Turk",
          "abstract": "nan",
          "doc_source": "TurKit : tools for iterative tasks on mechanical Turk TurKit : tools for iterative tasks on mechanical Turk TurKit : tools for iterative tasks on mechanical Turk ",
          "author": "Little, Greg;Chilton, Lydia B.;Goldman, Max;Miller, Robert C.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 34,
          "score": 0.6051305532455444,
          "doc_id": "NART111084731",
          "title": "Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction",
          "abstract": "nan",
          "doc_source": "Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction Cost-effective Multi-task Crowdsourcing Method for Knowledge Extraction ",
          "author": "Nam, Sangha;Lee, Minho;Heo, Cheolhoon;Choi, Key-Sun;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 35,
          "score": 0.5974745750427246,
          "doc_id": "NPAP13663736",
          "title": "모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정",
          "abstract": "최근 모바일 기술이 실생활에 널리 활용하면서 점점 모바일 크라우드소싱 활용이 크게 기대되고 있다. 그래서 배달 인력이 아닌 일반인도 어플리케이션을 모바일 기기에 설치하면 배달 인력이 되어 작업을 수행할 수 있다. 본 연구에서는 일반인도 참여할 수 있는 모바일 크라우드소싱 기반 배달에서 딥러닝을 이용한 작업자 선정 기법을 소개한다. 그리고 실험을 통하여 합성곱 신경망(Convolutional Neural Network)을 적용한 본 기법이 효과적이라는 것을 보인다.",
          "doc_source": "모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정 모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정 모바일 크라우드소싱 기반 음식 배달에서 딥러닝을 이용한 작업자 선정 최근 모바일 기술이 실생활에 널리 활용하면서 점점 모바일 크라우드소싱 활용이 크게 기대되고 있다. 그래서 배달 인력이 아닌 일반인도 어플리케이션을 모바일 기기에 설치하면 배달 인력이 되어 작업을 수행할 수 있다. 본 연구에서는 일반인도 참여할 수 있는 모바일 크라우드소싱 기반 배달에서 딥러닝을 이용한 작업자 선정 기법을 소개한다. 그리고 실험을 통하여 합성곱 신경망(Convolutional Neural Network)을 적용한 본 기법이 효과적이라는 것을 보인다.",
          "author": "이윤열;김응모;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 36,
          "score": 0.5952051281929016,
          "doc_id": "DIKO0016395992",
          "title": "모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당",
          "abstract": "nan",
          "doc_source": "모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당 모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당 모바일 크라우드소싱에서 딥러닝 기반 신뢰성 인지 작업 할당 ",
          "author": "이윤열",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 37,
          "score": 0.5934004783630371,
          "doc_id": "NART75765228",
          "title": "Using Amazon Mechanical Turk and other compensated crowdsourcing sites",
          "abstract": "Crowdsourcing is becoming recognized as a powerful tool that organizations can use in order to get work done, this by freelancers and non-employees. We conceptualize crowdsourcing as a subcategory of outsourcing, with compensated crowdsourcing representing situations in which individuals performing the work receive some sort of payment for accomplishing the organization's tasks. Herein, we discuss how sites that create a crowd, such as Amazon Mechanical Turk, can be powerful tools for business purposes. We highlight the general features of crowdsourcing sites, offering examples drawn from current crowdsourcing sites. We then examine the wide range of tasks that can be accomplished through crowdsourcing sites. Large online worker community websites and forums have been created around such crowdsourcing sites, and we describe the functions they generally play for crowdsourced workers. We also describe how these functions offer opportunities and challenges for organizations. We close by discussing major considerations organizations need to take into account when trying to harness the power of the crowd through compensated crowdsourcing sites.",
          "doc_source": "Using Amazon Mechanical Turk and other compensated crowdsourcing sites Using Amazon Mechanical Turk and other compensated crowdsourcing sites Using Amazon Mechanical Turk and other compensated crowdsourcing sites Crowdsourcing is becoming recognized as a powerful tool that organizations can use in order to get work done, this by freelancers and non-employees. We conceptualize crowdsourcing as a subcategory of outsourcing, with compensated crowdsourcing representing situations in which individuals performing the work receive some sort of payment for accomplishing the organization's tasks. Herein, we discuss how sites that create a crowd, such as Amazon Mechanical Turk, can be powerful tools for business purposes. We highlight the general features of crowdsourcing sites, offering examples drawn from current crowdsourcing sites. We then examine the wide range of tasks that can be accomplished through crowdsourcing sites. Large online worker community websites and forums have been created around such crowdsourcing sites, and we describe the functions they generally play for crowdsourced workers. We also describe how these functions offer opportunities and challenges for organizations. We close by discussing major considerations organizations need to take into account when trying to harness the power of the crowd through compensated crowdsourcing sites.",
          "author": "Schmidt, G.B.;Jettinghoff, W.M.;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 38,
          "score": 0.5797721147537231,
          "doc_id": "NPAP13686745",
          "title": "Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk",
          "abstract": "nan",
          "doc_source": "Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk Evaluating the accessibility of crowdsourcing tasks on Amazon's mechanical turk ",
          "author": "Calvo, Roc&iacute;o;Kane, Shaun K.;Hurst, Amy;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 39,
          "score": 0.5753893852233887,
          "doc_id": "JAKO201922663514847",
          "title": "인공신경망 기반 VRF 시스템 제어",
          "abstract": "This study aimed at developing control algorithms for operating a variable refrigerant flow (VRF) heating and cooling system with optimal system parameter set-points. Two artificial neural network (ANN) models, which were respectively designed to predict the heating energy cost and cooling energy amount for upcoming next control cycle, was developed and embedded into the control algorithms. Performance of the algorithms were tested using the computer simulation programs - EnergyPlus, BCVTB, MATLAB in an incorporative manner. The results revealed that the proposed control algorithms remarkably saved the heating energy cost by as much as 7.93% and cooling energy consumption by as much as 28.44%, compared to a conventional control strategy. These findings support that the ANN-based predictive control algorithms showed potential for cost- and energy-effectiveness of VRF heating and cooling systems.",
          "doc_source": "인공신경망 기반 VRF 시스템 제어 인공신경망 기반 VRF 시스템 제어 인공신경망 기반 VRF 시스템 제어 This study aimed at developing control algorithms for operating a variable refrigerant flow (VRF) heating and cooling system with optimal system parameter set-points. Two artificial neural network (ANN) models, which were respectively designed to predict the heating energy cost and cooling energy amount for upcoming next control cycle, was developed and embedded into the control algorithms. Performance of the algorithms were tested using the computer simulation programs - EnergyPlus, BCVTB, MATLAB in an incorporative manner. The results revealed that the proposed control algorithms remarkably saved the heating energy cost by as much as 7.93% and cooling energy consumption by as much as 28.44%, compared to a conventional control strategy. These findings support that the ANN-based predictive control algorithms showed potential for cost- and energy-effectiveness of VRF heating and cooling systems.",
          "author": "문진우",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 40,
          "score": 0.5744521021842957,
          "doc_id": "JAKO202113759910728",
          "title": "FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현",
          "abstract": "In this paper, we present the FlappyBird Competition System (FCS) implementation, a competition-based automated assessment system used in an entry-level artificial intelligence (AI) course at a university. The proposed system provides an evaluation method suitable for AI courses while taking advantage of automated assessment methods. Students are to design a neural network structure, train the weights, and tune hyperparameters using the given reinforcement learning code to improve the overall performance of game AI. Students participate using the resulting trained model during the competition, and the system automatically calculates the final score based on the ranking. The user evaluation conducted after the semester ends shows that our competition-based automated assessment system promotes active participation and inspires students to be interested and motivated to learn AI. Using FCS, the instructor significantly reduces the amount of time required for assessment.",
          "doc_source": "FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현 FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현 FlappyBird Competition System: 인공지능 수업의 경쟁 기반 평가 시스템의 구현 In this paper, we present the FlappyBird Competition System (FCS) implementation, a competition-based automated assessment system used in an entry-level artificial intelligence (AI) course at a university. The proposed system provides an evaluation method suitable for AI courses while taking advantage of automated assessment methods. Students are to design a neural network structure, train the weights, and tune hyperparameters using the given reinforcement learning code to improve the overall performance of game AI. Students participate using the resulting trained model during the competition, and the system automatically calculates the final score based on the ranking. The user evaluation conducted after the semester ends shows that our competition-based automated assessment system promotes active participation and inspires students to be interested and motivated to learn AI. Using FCS, the instructor significantly reduces the amount of time required for assessment.",
          "author": "손의성;김재경;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 41,
          "score": 0.5721744298934937,
          "doc_id": "ATN0047358973",
          "title": "딥러닝 언어 모델과 인공신경망 기계 번역을 활용한 담화 조응 현상과 한정 명사구 연구",
          "abstract": "In this preliminary study, we investigate the phenomena of discourse anaphora and definite descriptions within the framework of the so-called “donkey sentence.” Unlike English, Korean allows for the expression of donkey anaphora using either the pronoun kukes ‘it’ or definite noun phrases (bare NP or ku+NP). Employing neural machine translations and deep learning models, we examine the appropriateness of these two types of donkey sentences in Korean through the following procedure: Firstly, utilizing ChatGPT, we generate 60 sentences with donkey structures containing both pronouns and definite noun phrases. Secondly, we employ Google Translation and Papago to translate these sentences. Thirdly, we use KR-BERT to evaluate the acceptability of the translations. Finally, we conduct a statistical analysis based on the obtained acceptability scores. The results reveal that definite noun phrases are a more natural expression than pronouns in Korean donkey sentences. This novel finding suggests that the E-type approach would provide a better theoretical account than DRT (Discourse Representation Theory).",
          "doc_source": "딥러닝 언어 모델과 인공신경망 기계 번역을 활용한 담화 조응 현상과 한정 명사구 연구 딥러닝 언어 모델과 인공신경망 기계 번역을 활용한 담화 조응 현상과 한정 명사구 연구 딥러닝 언어 모델과 인공신경망 기계 번역을 활용한 담화 조응 현상과 한정 명사구 연구 In this preliminary study, we investigate the phenomena of discourse anaphora and definite descriptions within the framework of the so-called “donkey sentence.” Unlike English, Korean allows for the expression of donkey anaphora using either the pronoun kukes ‘it’ or definite noun phrases (bare NP or ku+NP). Employing neural machine translations and deep learning models, we examine the appropriateness of these two types of donkey sentences in Korean through the following procedure: Firstly, utilizing ChatGPT, we generate 60 sentences with donkey structures containing both pronouns and definite noun phrases. Secondly, we employ Google Translation and Papago to translate these sentences. Thirdly, we use KR-BERT to evaluate the acceptability of the translations. Finally, we conduct a statistical analysis based on the obtained acceptability scores. The results reveal that definite noun phrases are a more natural expression than pronouns in Korean donkey sentences. This novel finding suggests that the E-type approach would provide a better theoretical account than DRT (Discourse Representation Theory).",
          "author": "강아름;이용훈;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 42,
          "score": 0.566983699798584,
          "doc_id": "DIKO0016959108",
          "title": "AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구",
          "abstract": "본 연구는 교육부의 AI 디지털교과서 개발 방향 설정에 대한 초등 교사의 인식을 알아보기 위한 목적으로 실시되었다. 이를 위해 교육부의 AI 디지털교과서 개발 정책과 한국교육학술정보원의 AI 디지털교과서 개발 가이드라인을 바탕으로 약 40개의 문항이 담긴 설문지를 제작하여 배포하였다. 회수된 초등 교사 106명의 응답을 대응표본 t-검정, Borich 요구도, The Locus for Focus 모델을 통해 통계 처리하였으며 분석을 통해 얻은 결과는 다음과 같다.&amp;#xD; 첫째, 현재 서비스 되고 있는 디지털교과서와 발행사별 교수학습지원사이트의 이용 경험은 발행사별 교수학습지원사이트 이용 경험에 비해 굉장히 낮았으며 그 원인으로는 교수학습지원사이트에 비해 멀티미디어 콘텐츠의 부족, 사용법의 어려움과 기능의 불편함, 필요한 기능의 부재 등을 꼽았다.&amp;#xD; 둘째, AI 디지털교과서의 기본 기능인 통합 인증 기능, 통합 대시보드, 디지털교과서 책장, 학습데이터 허브 기능의 필요성을 묻는 질문에 대해 약 80% 이상의 교사가 필요하다고 응답하였다.&amp;#xD; 셋째, AI 기반 맞춤형 학습 지원 기능인 학습 진단 기능, 맞춤형 콘텐츠 제공 기능, 대시보드 기능, AI 튜터 기능, AI 보조교사 기능, 교사 재구성 기능의 필요성을 묻는 질문에 대한 응답을 전체 집단과 최종 학력 기준 집단으로 분석을 실시하였다. 대응표본 t-검정 결과는 모든 집단의 응답 결과에 대해 통계적으로 유의미한 차이(&amp;amp;lt; .001)가 있는 것으로 나타났다. Borich 요구도 및 The Locus for Focus 모델에 의한 분석 결과는 전체 집단의 경우 AI 보조교사 AI 튜터 기능에 대한 요구 수준이 가장 높았으며 The Locus for Focus 모델을 통한 분석 결과도 위의 두 기능 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력을 기준으로 집단을 나누어 분석한 결과는 최종 학력이 학사인 경우와 컴퓨터 관련 석사 과정 재학 중 또는 석사 학위 소지인 경우 AI 보조교사 기능과 AI 튜터 기능에 대한 요구도가 가장 높았으며 The Locus for Focus 모델 분석 결과 또한 두 기능이 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력이 그 외 석사 과정 재학 중 또는 석사 학위 소지인 경우 위의 두 집단과 마찬가지로 AI 튜터 기능과 AI 보조교사 기능이 요구도가 가장 높았으나 The Locus for Focus 모델 분석 결과의 경우 1사분면에 위치한 기능이 존재하지 않아 개발 우선 순위에 대한 논의가 필요한 것으로 나타났다.&amp;#xD; 넷째, 여섯 가지 AI 기반 맞춤형 학습 지원 기능에 대한 추가 수요 분석 결과를 실시하였다. 학습 진단 기능에 대한 추가 수요는 학생 참여 정도 진단 서비스, 협업 정도 진단 서비스, 정서 분석 서비스 등으로 나타났다. 맞춤형 콘텐츠 제공 기능에 대한 추가 수요는 이전 학년 과정 추천 기능, 맞춤형 콘텐츠 교사 제시 기능, 문항 난이도 조절 기능, 학습 경로 설정 기능 등으로 나타났다. 대시보드 기능에 대한 추가 수요는 오답 노트 작성 기능, 학습전략 추천 기능, 토론 및 채팅 기능 등으로 나타났다. AI 튜터 기능에 대한 추가 수요는 추가 학습 자료 제공 기능, 힌트 제공 기능 등으로 나타났다. AI 보조교사 기능에 대한 추가 수요는 문항 자동 채점 기능 및 결과 제공 기능, 교과학습발달상황 및 행동발상황 작성 지원 기능, 학생 수준별 문항 자동 구성 및 출제 기능, 수행평가 결과 NEIS 전송 기능 등으로 나타났다. 교사 재구성 기능에 대한 추가 수요는 프로젝트 학습을 위한 교과서 간 내용 통합 및 순서 조정 기능, 발행사별 교과서 내용 비교 및 끌어오기 기능 등으로 나타났다. 그 밖의 기능에 대한 수요는 에듀테크 사이트 연결 기능, 게임을 통한 성취도 확인 기능, 과제에 따른 보상 기능, 학교 간 학습 내용 공유 기능, 자료 저장소 기능, 출결 확인 기능, 커뮤니티 기능 등으로 나타났다. &amp;#xD; 본 연구는 AI 디지털교과서 개발과 관련한 교육부 정책의 방향을 확인하고 한국교육학술정보원이 발간한 AI 디지털교과서 개발 가이드라인의 의미를 해석하는 데 활용될 수 있다. 또한 AI 디지털교과서 개발 방향 설정에 대한 교사의 인식 및 수요를 확인하는 자료로 사용될 수 있으며 이를 바탕으로 AI 디지털교과서 개발 관련자들이 교수학습지원시스템으로서의 AI 디지털교과서를 개발하는 데 도움이 될 것으로 기대한다.",
          "doc_source": "AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구 AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구 AI 디지털교과서 개발 방향 설정에 대한 초등 교사 인식 연구 본 연구는 교육부의 AI 디지털교과서 개발 방향 설정에 대한 초등 교사의 인식을 알아보기 위한 목적으로 실시되었다. 이를 위해 교육부의 AI 디지털교과서 개발 정책과 한국교육학술정보원의 AI 디지털교과서 개발 가이드라인을 바탕으로 약 40개의 문항이 담긴 설문지를 제작하여 배포하였다. 회수된 초등 교사 106명의 응답을 대응표본 t-검정, Borich 요구도, The Locus for Focus 모델을 통해 통계 처리하였으며 분석을 통해 얻은 결과는 다음과 같다.&amp;#xD; 첫째, 현재 서비스 되고 있는 디지털교과서와 발행사별 교수학습지원사이트의 이용 경험은 발행사별 교수학습지원사이트 이용 경험에 비해 굉장히 낮았으며 그 원인으로는 교수학습지원사이트에 비해 멀티미디어 콘텐츠의 부족, 사용법의 어려움과 기능의 불편함, 필요한 기능의 부재 등을 꼽았다.&amp;#xD; 둘째, AI 디지털교과서의 기본 기능인 통합 인증 기능, 통합 대시보드, 디지털교과서 책장, 학습데이터 허브 기능의 필요성을 묻는 질문에 대해 약 80% 이상의 교사가 필요하다고 응답하였다.&amp;#xD; 셋째, AI 기반 맞춤형 학습 지원 기능인 학습 진단 기능, 맞춤형 콘텐츠 제공 기능, 대시보드 기능, AI 튜터 기능, AI 보조교사 기능, 교사 재구성 기능의 필요성을 묻는 질문에 대한 응답을 전체 집단과 최종 학력 기준 집단으로 분석을 실시하였다. 대응표본 t-검정 결과는 모든 집단의 응답 결과에 대해 통계적으로 유의미한 차이(&amp;amp;lt; .001)가 있는 것으로 나타났다. Borich 요구도 및 The Locus for Focus 모델에 의한 분석 결과는 전체 집단의 경우 AI 보조교사 AI 튜터 기능에 대한 요구 수준이 가장 높았으며 The Locus for Focus 모델을 통한 분석 결과도 위의 두 기능 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력을 기준으로 집단을 나누어 분석한 결과는 최종 학력이 학사인 경우와 컴퓨터 관련 석사 과정 재학 중 또는 석사 학위 소지인 경우 AI 보조교사 기능과 AI 튜터 기능에 대한 요구도가 가장 높았으며 The Locus for Focus 모델 분석 결과 또한 두 기능이 모두 1사분면(HH)에 위치하여 우선순위가 가장 높은 것으로 나타났다. 최종 학력이 그 외 석사 과정 재학 중 또는 석사 학위 소지인 경우 위의 두 집단과 마찬가지로 AI 튜터 기능과 AI 보조교사 기능이 요구도가 가장 높았으나 The Locus for Focus 모델 분석 결과의 경우 1사분면에 위치한 기능이 존재하지 않아 개발 우선 순위에 대한 논의가 필요한 것으로 나타났다.&amp;#xD; 넷째, 여섯 가지 AI 기반 맞춤형 학습 지원 기능에 대한 추가 수요 분석 결과를 실시하였다. 학습 진단 기능에 대한 추가 수요는 학생 참여 정도 진단 서비스, 협업 정도 진단 서비스, 정서 분석 서비스 등으로 나타났다. 맞춤형 콘텐츠 제공 기능에 대한 추가 수요는 이전 학년 과정 추천 기능, 맞춤형 콘텐츠 교사 제시 기능, 문항 난이도 조절 기능, 학습 경로 설정 기능 등으로 나타났다. 대시보드 기능에 대한 추가 수요는 오답 노트 작성 기능, 학습전략 추천 기능, 토론 및 채팅 기능 등으로 나타났다. AI 튜터 기능에 대한 추가 수요는 추가 학습 자료 제공 기능, 힌트 제공 기능 등으로 나타났다. AI 보조교사 기능에 대한 추가 수요는 문항 자동 채점 기능 및 결과 제공 기능, 교과학습발달상황 및 행동발상황 작성 지원 기능, 학생 수준별 문항 자동 구성 및 출제 기능, 수행평가 결과 NEIS 전송 기능 등으로 나타났다. 교사 재구성 기능에 대한 추가 수요는 프로젝트 학습을 위한 교과서 간 내용 통합 및 순서 조정 기능, 발행사별 교과서 내용 비교 및 끌어오기 기능 등으로 나타났다. 그 밖의 기능에 대한 수요는 에듀테크 사이트 연결 기능, 게임을 통한 성취도 확인 기능, 과제에 따른 보상 기능, 학교 간 학습 내용 공유 기능, 자료 저장소 기능, 출결 확인 기능, 커뮤니티 기능 등으로 나타났다. &amp;#xD; 본 연구는 AI 디지털교과서 개발과 관련한 교육부 정책의 방향을 확인하고 한국교육학술정보원이 발간한 AI 디지털교과서 개발 가이드라인의 의미를 해석하는 데 활용될 수 있다. 또한 AI 디지털교과서 개발 방향 설정에 대한 교사의 인식 및 수요를 확인하는 자료로 사용될 수 있으며 이를 바탕으로 AI 디지털교과서 개발 관련자들이 교수학습지원시스템으로서의 AI 디지털교과서를 개발하는 데 도움이 될 것으로 기대한다.",
          "author": "이승현",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 43,
          "score": 0.566933274269104,
          "doc_id": "DIKO0015787734",
          "title": "검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현",
          "abstract": "본 논문은 검사 공정 작업자 이미지 반복 처리 작업을 자동화 시스템으로 대체하고자 하는 연구를 진행하였다. 검사 장비의 자동화에 따른 작업 스피드가 향상되었지만, 그 이면에 자동화 적용이 어려워 작업자가 직접 해야 하는 이미지 반복 확인 작업의 과제가 남아 있다. 이를 해결하기 위해 딥러닝을 활용한 이미지 분류 자동화 시스템을 개발하는 것이 본고의 목적이다.&amp;#xD; &amp;#xD; 검사 공정 작업자 이미지 반복 작업의 문제점은 단순 반복 행위로 인한 작업자의 건강상 역기능, 즉 시력 저하와 손목 통증이 대표적이다. 또한 작업자의 개인적인 판단과 오류로 인한 품질적인 측면인 재현성과 반복성도 문제점 중 하나이다. 뿐만 아니라 검사기에 이미지가 증가되면 작업자의 업무 부하로 인한 실시간 처리도 지연된다.&amp;#xD; &amp;#xD; 작업자의 이미지 확인 작업의 주요 이유는 이물성 불량을 양품으로 전환하여 수율을 향상시키는 것과 불량의 세부 Trend를 파악하고자 하기 위함이다. 이런 문제들을 해소하고자 딥러닝에서 CNN(Convolution Neural Network) 합성곱 신경망을 활용하여 이미지의 특징을 추출하고 학습을 진행 후 파라미터를 컴퓨터 신경망이 기억하였다가 분류 이미지가 들어오면 기억된 신경망이 이미지를 14가지 유형으로 자동 분류 예측하는 시스템을 개발하였다. 이미지 분류에는 전처리 과정과 이미지 분류 그리고 후처리 과정을 거치게 된다. 개발된 자동화 시스템에 평가 검증의 비교 대상은 작업자의 결과물과 비교하게 된다. 평가품을 선정 후 작업자가 선 작업을 하고 이 후 원본 이미지를 자동화 시스템이 분류하여 작업자 대비 자동화 시스템의 성능인 정합도를 비교하여 취합하였다. 이런 과정을 반복하면서 Confusion Matrix 오차 행렬을 이용하여 세부적 분석을 통해 정합도 이상 유형을 찾고 추가적인 학습에 파라미터를 수정하여 최적화에 점점 다가가게 된다.&amp;#xD; &amp;#xD; 14가지 항목의 이미지 15,000개를 학습시킨 후 1일 1,900,000개의 이미지를 분류하는 자동화 시스템의 정합도는 작업자 대비 93.26%의 정합도를 확인하였고 유형별 정합도의 최대 100%에서 최소 80.87%의 유형별 정합도를 확인하였다. 치명적 오류 681 ppm 그리고 치명적이지 않은 오류 6.7%에 성능을 확인하였다. 작업자 대비 이미지 처리 속도는 약 10배 이상의 개선 효과를 확인하였으며 이로 인하여 작업자의 건강상의 역기능 부담을 감소시켰고 작업자의 이미지 분류 확인 작업의 일관성을 시스템화하였다.&amp;#xD; &amp;#xD; 또한 이미지 처리 속도 향상으로 실시간 처리에 좀 더 가까워졌다. 현재 자동화 시스템의 정합도 93.26%에 유형별 정합도 100% 가까운 유형의 항목은 자동화 시스템의 이미지 분류 후 후처리 과정에서 분류 결과를 확인하여 강제로 작업자에게 전송시키지 않는 상태이며 이로 인한 작업자는 기존 대비 약 80%의 업무 과중을 줄일 수 있다.&amp;#xD; 업무에 과중을 최소화 함으로써 작업자 인력 소인화에 도달하였다. &amp;#xD; &amp;#xD; 추가적으로 향후 분리 되어 있는 검사기와 자동화 시스템을 검사기 장비 내에 결합하여 불필요한 전송 및 Loss를 최소화 하여 실시간 처리 및 인력 무인화에 도움이 될 것이라 기대된다. 또한 GPU를 추가적으로 멀티로 사용하여 자동화 시스템의 처리 속도 또한 현재보다 더 향상될 것으로 기대된다.",
          "doc_source": "검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현 검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현 검사 공정 작업자 이미지 반복 처리 작업의 자동화 구현 본 논문은 검사 공정 작업자 이미지 반복 처리 작업을 자동화 시스템으로 대체하고자 하는 연구를 진행하였다. 검사 장비의 자동화에 따른 작업 스피드가 향상되었지만, 그 이면에 자동화 적용이 어려워 작업자가 직접 해야 하는 이미지 반복 확인 작업의 과제가 남아 있다. 이를 해결하기 위해 딥러닝을 활용한 이미지 분류 자동화 시스템을 개발하는 것이 본고의 목적이다.&amp;#xD; &amp;#xD; 검사 공정 작업자 이미지 반복 작업의 문제점은 단순 반복 행위로 인한 작업자의 건강상 역기능, 즉 시력 저하와 손목 통증이 대표적이다. 또한 작업자의 개인적인 판단과 오류로 인한 품질적인 측면인 재현성과 반복성도 문제점 중 하나이다. 뿐만 아니라 검사기에 이미지가 증가되면 작업자의 업무 부하로 인한 실시간 처리도 지연된다.&amp;#xD; &amp;#xD; 작업자의 이미지 확인 작업의 주요 이유는 이물성 불량을 양품으로 전환하여 수율을 향상시키는 것과 불량의 세부 Trend를 파악하고자 하기 위함이다. 이런 문제들을 해소하고자 딥러닝에서 CNN(Convolution Neural Network) 합성곱 신경망을 활용하여 이미지의 특징을 추출하고 학습을 진행 후 파라미터를 컴퓨터 신경망이 기억하였다가 분류 이미지가 들어오면 기억된 신경망이 이미지를 14가지 유형으로 자동 분류 예측하는 시스템을 개발하였다. 이미지 분류에는 전처리 과정과 이미지 분류 그리고 후처리 과정을 거치게 된다. 개발된 자동화 시스템에 평가 검증의 비교 대상은 작업자의 결과물과 비교하게 된다. 평가품을 선정 후 작업자가 선 작업을 하고 이 후 원본 이미지를 자동화 시스템이 분류하여 작업자 대비 자동화 시스템의 성능인 정합도를 비교하여 취합하였다. 이런 과정을 반복하면서 Confusion Matrix 오차 행렬을 이용하여 세부적 분석을 통해 정합도 이상 유형을 찾고 추가적인 학습에 파라미터를 수정하여 최적화에 점점 다가가게 된다.&amp;#xD; &amp;#xD; 14가지 항목의 이미지 15,000개를 학습시킨 후 1일 1,900,000개의 이미지를 분류하는 자동화 시스템의 정합도는 작업자 대비 93.26%의 정합도를 확인하였고 유형별 정합도의 최대 100%에서 최소 80.87%의 유형별 정합도를 확인하였다. 치명적 오류 681 ppm 그리고 치명적이지 않은 오류 6.7%에 성능을 확인하였다. 작업자 대비 이미지 처리 속도는 약 10배 이상의 개선 효과를 확인하였으며 이로 인하여 작업자의 건강상의 역기능 부담을 감소시켰고 작업자의 이미지 분류 확인 작업의 일관성을 시스템화하였다.&amp;#xD; &amp;#xD; 또한 이미지 처리 속도 향상으로 실시간 처리에 좀 더 가까워졌다. 현재 자동화 시스템의 정합도 93.26%에 유형별 정합도 100% 가까운 유형의 항목은 자동화 시스템의 이미지 분류 후 후처리 과정에서 분류 결과를 확인하여 강제로 작업자에게 전송시키지 않는 상태이며 이로 인한 작업자는 기존 대비 약 80%의 업무 과중을 줄일 수 있다.&amp;#xD; 업무에 과중을 최소화 함으로써 작업자 인력 소인화에 도달하였다. &amp;#xD; &amp;#xD; 추가적으로 향후 분리 되어 있는 검사기와 자동화 시스템을 검사기 장비 내에 결합하여 불필요한 전송 및 Loss를 최소화 하여 실시간 처리 및 인력 무인화에 도움이 될 것이라 기대된다. 또한 GPU를 추가적으로 멀티로 사용하여 자동화 시스템의 처리 속도 또한 현재보다 더 향상될 것으로 기대된다.",
          "author": "김영규",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 44,
          "score": 0.5669165849685669,
          "doc_id": "ATN0027086510",
          "title": "인공신경망과 강건설계를 이용한 기계적 RMR 분류",
          "abstract": "Rock mass rating (RMR) is a relatively simple method for classifying rock mass with the naked eye; however, it becomes inconvenient when the number of survey sections is large. In this study, we developed a learning model and a prediction model using an artificial neural network (ANN) to classify RMR mechanically. Using robust design, 3125 big data were optimized into 25 learning data. The test results after learning through the two methods were exactly the same. Through robust design, the learning data obtained by reducing the total number of cases to less than 1% had the same learning effect as the whole data, which means that the effort and cost of acquiring the learning data can be greatly reduced. For the perfect prediction of the RMR classification system, we tuned the primary predictions within a given range of rating levels. As a result, we implemented an RMR classification ANN system that perfectly predicts the RMR of 3125 big data using 25 learning data through robust design.",
          "doc_source": "인공신경망과 강건설계를 이용한 기계적 RMR 분류 인공신경망과 강건설계를 이용한 기계적 RMR 분류 인공신경망과 강건설계를 이용한 기계적 RMR 분류 Rock mass rating (RMR) is a relatively simple method for classifying rock mass with the naked eye; however, it becomes inconvenient when the number of survey sections is large. In this study, we developed a learning model and a prediction model using an artificial neural network (ANN) to classify RMR mechanically. Using robust design, 3125 big data were optimized into 25 learning data. The test results after learning through the two methods were exactly the same. Through robust design, the learning data obtained by reducing the total number of cases to less than 1% had the same learning effect as the whole data, which means that the effort and cost of acquiring the learning data can be greatly reduced. For the perfect prediction of the RMR classification system, we tuned the primary predictions within a given range of rating levels. As a result, we implemented an RMR classification ANN system that perfectly predicts the RMR of 3125 big data using 25 learning data through robust design.",
          "author": "장명환;하태욱;최기훈;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 45,
          "score": 0.5667533874511719,
          "doc_id": "DIKO0010672988",
          "title": "인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN)",
          "abstract": "Abstract Li Chenghua Department of Information and Communication Chebuk National University Text categorization is an important application of machine learning to the field of document information retrieval. This thesis described two kinds of neural networks for text categorization, multi-output perceptron learning (MOPL) and back propagation neural network (BPNN). BPNN has been widely used in classification and pattern recognition. However it has some generally acknowledged defects, usually these defects evolve from some morbidity neurons In this thesis I proposed a novel adaptive learning approach for text categorization using improved back propagation neural network. This algorithm can overcome some shortcomings in traditional back propagation neural network such as slow training speed and easy to get into local minimum. We compared the training time and performance and test the three methods on the standard Reuter-21578. The results show that the proposed algorithm is able to achieve high categorization effectiveness as measured by precision, recall and F-measure. 요약 문서분류는 정보검색에서 기계학습을 응용하는 중요한 분야이다. 본 논문에서는 다중출력 퍼셉트론 학습(Multi-Output Perceptron Learning:MOPL)과 백 프로퍼게이션 신경망(Back Propagation Neural Network:BPNN) 두 가지의 신경망 이론을 문서분류에 적용하였다. BPNN은 분류와 패턴인식에 많이 사용되고 있지만, 치명적인 신경을 포함하는 몇 가지 결점이 있다. 본 논문에서는 향상된 백 프로퍼게이션 신경망이론을 사용한 새로운 학습법을 제안할 것이다. 이 알고리즘은 기존의 백 프로퍼게이션 신경망의 느린 학습 속도와 쉽게 국소적인 제한치로 빠지는 문제를 개선할 수 있다. 로이터 자료(Reuter-21578)을 이용하여 세 가지 방법을 테스트하고, 학습시간과 성능을 비교하였다. 정확율, 재현율, 그리고 F-mesure를 통하여 본 논문에서 제안한 문서분류 알고리즘의 높은 성능을 확인할 수 있다.",
          "doc_source": "인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN) 인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN) 인공신경망을 이용한 문서 분류 : Text Categorization based on Artificial Neural Networks (ANN) Abstract Li Chenghua Department of Information and Communication Chebuk National University Text categorization is an important application of machine learning to the field of document information retrieval. This thesis described two kinds of neural networks for text categorization, multi-output perceptron learning (MOPL) and back propagation neural network (BPNN). BPNN has been widely used in classification and pattern recognition. However it has some generally acknowledged defects, usually these defects evolve from some morbidity neurons In this thesis I proposed a novel adaptive learning approach for text categorization using improved back propagation neural network. This algorithm can overcome some shortcomings in traditional back propagation neural network such as slow training speed and easy to get into local minimum. We compared the training time and performance and test the three methods on the standard Reuter-21578. The results show that the proposed algorithm is able to achieve high categorization effectiveness as measured by precision, recall and F-measure. 요약 문서분류는 정보검색에서 기계학습을 응용하는 중요한 분야이다. 본 논문에서는 다중출력 퍼셉트론 학습(Multi-Output Perceptron Learning:MOPL)과 백 프로퍼게이션 신경망(Back Propagation Neural Network:BPNN) 두 가지의 신경망 이론을 문서분류에 적용하였다. BPNN은 분류와 패턴인식에 많이 사용되고 있지만, 치명적인 신경을 포함하는 몇 가지 결점이 있다. 본 논문에서는 향상된 백 프로퍼게이션 신경망이론을 사용한 새로운 학습법을 제안할 것이다. 이 알고리즘은 기존의 백 프로퍼게이션 신경망의 느린 학습 속도와 쉽게 국소적인 제한치로 빠지는 문제를 개선할 수 있다. 로이터 자료(Reuter-21578)을 이용하여 세 가지 방법을 테스트하고, 학습시간과 성능을 비교하였다. 정확율, 재현율, 그리고 F-mesure를 통하여 본 논문에서 제안한 문서분류 알고리즘의 높은 성능을 확인할 수 있다.",
          "author": "리청화",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 46,
          "score": 0.5656005144119263,
          "doc_id": "JAKO201416760764337",
          "title": "인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가",
          "abstract": "본 논문에서는 TBM 터널의 세그먼트 라이닝 설계 자동화 기술 개발의 일환으로 인공신경망기법을 이용한 세그먼트 라이닝 부재력 산정기법 개발에 관한 내용을 다루었다. 부재력 평가가 가능한 인공신경망을 개발하기 위해 먼저 다양한 설계조건을 도출하고 이에 대해 2-Ring Beam 모델을 이용한 유한요소해석을 수행하여 인공신경망 학습에 필요한 설계조건별 부재력에 관한 DB를 구축하였다. 구축된 DB를 활용하여 인공신경망의 최적화 과정을 통해 최대 부재력 및 분포도를 예측할 수 있는 인공신경망을 구축하였다. 검토 결과 구축된 인공신경망은 유한요소해석과 동일한 정밀도의 부재력 산정 기능을 확보하는 것으로 검토되었으며 따라서 TBM 세그먼트 라이닝 설계시 필요한 부재력 평가를 위한 효율적인 수단으로 활용될 수 있는 것으로 판단된다.",
          "doc_source": "인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가 인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가 인공신경망 기반의 TBM 터널 세그먼트 라이닝 부재력 평가 본 논문에서는 TBM 터널의 세그먼트 라이닝 설계 자동화 기술 개발의 일환으로 인공신경망기법을 이용한 세그먼트 라이닝 부재력 산정기법 개발에 관한 내용을 다루었다. 부재력 평가가 가능한 인공신경망을 개발하기 위해 먼저 다양한 설계조건을 도출하고 이에 대해 2-Ring Beam 모델을 이용한 유한요소해석을 수행하여 인공신경망 학습에 필요한 설계조건별 부재력에 관한 DB를 구축하였다. 구축된 DB를 활용하여 인공신경망의 최적화 과정을 통해 최대 부재력 및 분포도를 예측할 수 있는 인공신경망을 구축하였다. 검토 결과 구축된 인공신경망은 유한요소해석과 동일한 정밀도의 부재력 산정 기능을 확보하는 것으로 검토되었으며 따라서 TBM 세그먼트 라이닝 설계시 필요한 부재력 평가를 위한 효율적인 수단으로 활용될 수 있는 것으로 판단된다.",
          "author": "유충식;최정혁;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 47,
          "score": 0.564154326915741,
          "doc_id": "JAKO201017337344335",
          "title": "인공신경망모델을 이용한 교량의 상태평가",
          "abstract": "대부분의 선진국에서 교량의 유지보수 및 보강(Maintenance Repair & Rehabilitation-MR&R)으로 인한 비용은 해마다 증가하고 있다. 전산화된 교량유지관리 및 의사결정시스템(Bridge Management System-BMS)은 가능한 최저의 생애주기비용(Life Cycle Cost - LCC)에 최적의 안정성를 확보하기 위해 개발되었다. 본 논문에서는 제한된 현존하는 교량진단기록을 이용하여 현존하지 않는 과거의 교량상태등급 데이타를 생성하기 위해 Backward Prediction Model(BPM)이라 불리는 인공신경망(Artificial Neural Network-ANN)에 기초한 예측모델을 제시한다. 제안된 BPM은 한정된 교량 정기점검기록으로부터 현존하는 교량진단기록과 연관성을 확립하기 위해 교통량과 인구, 그리고 기후 등과 같은 비구조적 요소를 이용하며, 제한된 교량진단기록과 비구조적 요소 사이에 맺어진 연관성을 통해 현존하지 않는 과거의 교량상태등급 데이타를 생성할 수 있다. BPM의 신뢰도를 측정하기 위하여 Maryland DOT로 부터 얻어진 National Bridge Inventory(NBI)와 BMS 교량진단자료를 이용하였다. 이중 NBI자료를 이용한 Backward comparison 에 있어서 실제 NBI기록과 BPM으로 생성된 교량상태등급과의 차이(상판: 6.68%, 상부구조부: 6.61%, 하부구조부: 7.52%)는 BPM으로 생성된 결과의 높은 신뢰도를 보여준다. 이 연구의 결과는 제한된 정기점검 기록으로 야기되는 BMS의 장기 교량손상 예측에 관련된 사용상의 문제를 최소화하고 전반적인 BMS 결과의 신뢰도를 높이는데 기여 할 수 있다.",
          "doc_source": "인공신경망모델을 이용한 교량의 상태평가 인공신경망모델을 이용한 교량의 상태평가 인공신경망모델을 이용한 교량의 상태평가 대부분의 선진국에서 교량의 유지보수 및 보강(Maintenance Repair & Rehabilitation-MR&R)으로 인한 비용은 해마다 증가하고 있다. 전산화된 교량유지관리 및 의사결정시스템(Bridge Management System-BMS)은 가능한 최저의 생애주기비용(Life Cycle Cost - LCC)에 최적의 안정성를 확보하기 위해 개발되었다. 본 논문에서는 제한된 현존하는 교량진단기록을 이용하여 현존하지 않는 과거의 교량상태등급 데이타를 생성하기 위해 Backward Prediction Model(BPM)이라 불리는 인공신경망(Artificial Neural Network-ANN)에 기초한 예측모델을 제시한다. 제안된 BPM은 한정된 교량 정기점검기록으로부터 현존하는 교량진단기록과 연관성을 확립하기 위해 교통량과 인구, 그리고 기후 등과 같은 비구조적 요소를 이용하며, 제한된 교량진단기록과 비구조적 요소 사이에 맺어진 연관성을 통해 현존하지 않는 과거의 교량상태등급 데이타를 생성할 수 있다. BPM의 신뢰도를 측정하기 위하여 Maryland DOT로 부터 얻어진 National Bridge Inventory(NBI)와 BMS 교량진단자료를 이용하였다. 이중 NBI자료를 이용한 Backward comparison 에 있어서 실제 NBI기록과 BPM으로 생성된 교량상태등급과의 차이(상판: 6.68%, 상부구조부: 6.61%, 하부구조부: 7.52%)는 BPM으로 생성된 결과의 높은 신뢰도를 보여준다. 이 연구의 결과는 제한된 정기점검 기록으로 야기되는 BMS의 장기 교량손상 예측에 관련된 사용상의 문제를 최소화하고 전반적인 BMS 결과의 신뢰도를 높이는데 기여 할 수 있다.",
          "author": "오순택;이동준;이재호;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 48,
          "score": 0.5625017285346985,
          "doc_id": "NPAP13430839",
          "title": "AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion",
          "abstract": "<P>Recent advances of AI applications in various of industries have led to remarkable performance and efficiency. Driven by the great success of datasets and experience sharing, people are exploring more precious datasets with diverse features and longer time range. The promising reasoning information of well-curated student grade datasets is expected to assist young students to find the best of themselves and then improve their learning outcome and study experience. Through data and experience sharing, young students can have a better understanding of their learning condition and possible learning outcomes. Existing course selection systems in Taiwan which offer limited basic enrolling functions fail to provide performance prediction and course arrangement guidance based on their own learning condition. Students now selecting courses with unawareness of their expecting performance. A personalized guide for students on course selection is crucial for how they structure professional knowledge and arrange study schedule. In this paper, we first analyzed what factors can be used on defining learning curve, and discovered the difference between students with different properties and background. Second, we developed a recommendation system based on great amount of grade datasets of past students, and the system can give students suggestions on how to assign their credits based on their own learning curve and students that had similar learning curve. The result of our research demonstrates the feasibility of a new approach on applying big data and AI technology on learning analysis and course selection.</P>",
          "doc_source": "AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion AI-based College Course Selection Recommendation System: Performance Prediction and Curriculum Suggestion <P>Recent advances of AI applications in various of industries have led to remarkable performance and efficiency. Driven by the great success of datasets and experience sharing, people are exploring more precious datasets with diverse features and longer time range. The promising reasoning information of well-curated student grade datasets is expected to assist young students to find the best of themselves and then improve their learning outcome and study experience. Through data and experience sharing, young students can have a better understanding of their learning condition and possible learning outcomes. Existing course selection systems in Taiwan which offer limited basic enrolling functions fail to provide performance prediction and course arrangement guidance based on their own learning condition. Students now selecting courses with unawareness of their expecting performance. A personalized guide for students on course selection is crucial for how they structure professional knowledge and arrange study schedule. In this paper, we first analyzed what factors can be used on defining learning curve, and discovered the difference between students with different properties and background. Second, we developed a recommendation system based on great amount of grade datasets of past students, and the system can give students suggestions on how to assign their credits based on their own learning curve and students that had similar learning curve. The result of our research demonstrates the feasibility of a new approach on applying big data and AI technology on learning analysis and course selection.</P>",
          "author": "Wu, Yu Hsuan;Wu, Eric Hsiaokuang;",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 49,
          "score": 0.5577548742294312,
          "doc_id": "JAKO202433843342535",
          "title": "초등교사의 수학 교과 AI-TPACK 분석",
          "abstract": "최근 AI 기술의 발달과 더불어 이를 효과적으로 교육에 활용하고자 하는 노력이 활발하다. 이러한 맥락에서 본 연구는 수학수업에 AI를 효과적으로 활용하기 위해 필요한 교사지식을 TPACK을 바탕으로 살펴보고자 하였다. 특히 AI를 수학교육에 효과적으로 활용하기 위해서는 교사의 AI와 관련된 기술 지식뿐만 아니라 교육학적 지식, 더 나아가 윤리적인 측면을 고려할 필요가 있다. 이를 위해 AI 기반 도구의 윤리적 활용을 강조한 Celik(2023)의 Intelligent-TPACK 측정 도구를 수학교육에 특화하여 AI-TPACK 측정 도구로 활용하였으며, AI-TPACK 구성요소 사이의 구조적 관계를 분석하였다. 그 결과, AI-TPACK의 구성 요소 간 계층적 특성과 윤리적 지식(Ethics)의 영향을 확인할 수 있었다. 특히 기술 내용 지식(AI-TCK)과 기술 교육 지식(AI-TPK)이 AI-TPACK에 미치는 영향이 크게 나타났으며, 기술 지식(AI-TK)은 AI-TPACK에 직접적인 영향을 미치지 않았으나 기술 내용 지식(AI-TCK), 기술 교육 지식(AI-TPK), 윤리적 지식(Ethics)을 매개로 하여 AI-TPACK에 간접적으로 유의미한 영향을 미쳤다. 이와 같은 연구 결과를 바탕으로 AI를 교육에 효과적으로 활용하기 위한 교사지식 및 교사 교육에 관한 시사점을 논의하였다.",
          "doc_source": "초등교사의 수학 교과 AI-TPACK 분석 초등교사의 수학 교과 AI-TPACK 분석 초등교사의 수학 교과 AI-TPACK 분석 최근 AI 기술의 발달과 더불어 이를 효과적으로 교육에 활용하고자 하는 노력이 활발하다. 이러한 맥락에서 본 연구는 수학수업에 AI를 효과적으로 활용하기 위해 필요한 교사지식을 TPACK을 바탕으로 살펴보고자 하였다. 특히 AI를 수학교육에 효과적으로 활용하기 위해서는 교사의 AI와 관련된 기술 지식뿐만 아니라 교육학적 지식, 더 나아가 윤리적인 측면을 고려할 필요가 있다. 이를 위해 AI 기반 도구의 윤리적 활용을 강조한 Celik(2023)의 Intelligent-TPACK 측정 도구를 수학교육에 특화하여 AI-TPACK 측정 도구로 활용하였으며, AI-TPACK 구성요소 사이의 구조적 관계를 분석하였다. 그 결과, AI-TPACK의 구성 요소 간 계층적 특성과 윤리적 지식(Ethics)의 영향을 확인할 수 있었다. 특히 기술 내용 지식(AI-TCK)과 기술 교육 지식(AI-TPK)이 AI-TPACK에 미치는 영향이 크게 나타났으며, 기술 지식(AI-TK)은 AI-TPACK에 직접적인 영향을 미치지 않았으나 기술 내용 지식(AI-TCK), 기술 교육 지식(AI-TPK), 윤리적 지식(Ethics)을 매개로 하여 AI-TPACK에 간접적으로 유의미한 영향을 미쳤다. 이와 같은 연구 결과를 바탕으로 AI를 교육에 효과적으로 활용하기 위한 교사지식 및 교사 교육에 관한 시사점을 논의하였다.",
          "author": "이유진",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        },
        {
          "rank": 50,
          "score": 0.5568900108337402,
          "doc_id": "DIKO0013926033",
          "title": "피드백과 목표가 크라우드소싱 결과물의 질에 미치는 영향",
          "abstract": "nan",
          "doc_source": "피드백과 목표가 크라우드소싱 결과물의 질에 미치는 영향 피드백과 목표가 크라우드소싱 결과물의 질에 미치는 영향 피드백과 목표가 크라우드소싱 결과물의 질에 미치는 영향 ",
          "author": "임재은",
          "embedding_mode": "nan",
          "embedding_text": "3*title+abstract"
        }
      ]
    }
  ],
  "meta": {
    "model": "gemini-2.5-flash",
    "temperature": 0.2
  }
}