{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe5318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_path=\"../configs/query_encoder/config_PwC-Embedding_expr.json\"\n",
    "# data_schema=\"/workspace/configs/csv_schema/test_2.json\"\n",
    "# docs_jsonl_path=\"/workspace/results/searched_docs_db/search_documents_test.jsonl\"\n",
    "# auto_data_load=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf753651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from utils.load_json import load_json as load_config\n",
    "from utils.load_jsonl_and_make_text_for_embedding import (\n",
    "    load_jsonl_and_make_text_for_embedding as load_jsonl_docs,\n",
    ")\n",
    "from utils.create_class_from_schema import create_class_from_schema\n",
    "\n",
    "# Import the newly created modules\n",
    "from features.embedding_processor import generate_batch_embeddings\n",
    "from data_handler.for_embedding import prepare_documents, save_results\n",
    "\n",
    "\n",
    "def build_vectordb_search(\n",
    "    config_path=\"../configs/query_encoder/config_PwC-Embedding_expr.json\",\n",
    "    data_schema=\"/workspace/configs/csv_schema/test_2.json\",\n",
    "    docs_jsonl_path=\"/workspace/results/searched_docs_db/search_documents_test.jsonl\",\n",
    "    auto_data_load=True,\n",
    "):\n",
    "    # 1. Load configurations and create dynamic document class\n",
    "    config = load_config(config_path)\n",
    "    try:\n",
    "        DynamicDocument = create_class_from_schema(\"Document\", data_schema)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load schema and create class: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Load source documents\n",
    "    if auto_data_load:\n",
    "        jsonl_path = config.get(\"jsonl_path\", config_path)\n",
    "    else:\n",
    "        jsonl_path = docs_jsonl_path\n",
    "    print(f\"Loading documents from {jsonl_path}: Auto Loading Data\", auto_data_load)\n",
    "    documents_data = load_jsonl_docs(jsonl_path)\n",
    "    print(f\"Loaded {len(documents_data)} documents\")\n",
    "\n",
    "    if not documents_data:\n",
    "        print(\"No documents found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 3. Generate Embeddings (Separated Logic)\n",
    "    # This function is now focused only on the ML model and vector generation.\n",
    "    model_name = config[\"model_name\"]\n",
    "    embeddings = generate_batch_embeddings(documents_data, model_name)\n",
    "\n",
    "    if embeddings is None:\n",
    "        print(\"Embedding generation failed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 4. Prepare Document Objects for Saving (Separated Logic)\n",
    "    # This function handles the data structuring, combining raw data with embeddings.\n",
    "    documents_to_save = prepare_documents(documents_data, embeddings, DynamicDocument)\n",
    "\n",
    "    if not documents_to_save:\n",
    "        print(\"No documents were successfully prepared for saving. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 5. Save Results and Update Config (Separated Logic)\n",
    "    # This function is responsible for all file I/O and finalization.\n",
    "    save_results(\n",
    "        config=config,\n",
    "        documents_to_save=documents_to_save,\n",
    "        embedding_shape=embeddings.shape,\n",
    "        document_class=DynamicDocument,\n",
    "        config_path=config_path,\n",
    "        model_name=model_name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Dense Retrieval Script with Pluggable Retrievers.\n",
    "Refactored for modularity and extensibility.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from . import data_loader, query_encoder, result_saver\n",
    "from .retrievers import get_retriever, base as retriever_base\n",
    "\n",
    "def run_retrieval_for_question(\n",
    "    q_item: data_loader.QuestionItem,\n",
    "    encoder: query_encoder.QueryEncoder,\n",
    "    retriever: retriever_base.Retriever,\n",
    "    vectordb: data_loader.VectorDB,\n",
    "    top_k: int,\n",
    "    query_instruction: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Orchestrates the retrieval process for a single question item.\"\"\"\n",
    "    all_queries = [q_item.original_question] + q_item.single_hop_questions\n",
    "    query_metas = [{\"type\": \"original\"}] + [\n",
    "        {\"type\": \"single_hop\", \"index\": i} for i in range(len(q_item.single_hop_questions))\n",
    "    ]\n",
    "\n",
    "    query_vecs = encoder.encode(all_queries, instruction=query_instruction)\n",
    "    scores, indices = retriever.search(query_vecs, top_k=top_k)\n",
    "\n",
    "    results = []\n",
    "    for i, (query_text, query_meta) in enumerate(zip(all_queries, query_metas)):\n",
    "        hits = []\n",
    "        for rank in range(scores.shape[1]):\n",
    "            doc_idx = indices[i, rank]\n",
    "            hit = {\n",
    "                \"rank\": rank + 1,\n",
    "                \"score\": float(scores[i, rank]),\n",
    "                \"doc_id\": vectordb.doc_ids[doc_idx],\n",
    "                **vectordb.metadata[doc_idx],  # Unpack all metadata\n",
    "            }\n",
    "            hits.append(hit)\n",
    "        results.append({\"query\": query_text, \"query_meta\": query_meta, \"hits\": hits})\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser(description=\"Dense Retrieval with a configured model.\")\n",
    "    p.add_argument(\"--vectordb_csv\", type=str, required=True, help=\"Path to the vector DB CSV file.\")\n",
    "    p.add_argument(\"--config_json\", type=str, required=True, help=\"Path to the model configuration JSON.\")\n",
    "    p.add_argument(\"--questions_jsonl\", type=str, required=True, help=\"Path to the questions JSONL file.\")\n",
    "    p.add_argument(\"--ids\", type=str, help=\"Comma-separated question IDs to process.\")\n",
    "    p.add_argument(\"--range\", dest=\"range_spec\", type=str, help=\"1-based inclusive range like '1-10'.\")\n",
    "    p.add_argument(\"--top_k\", type=int, default=10, help=\"Number of documents to retrieve.\")\n",
    "    p.add_argument(\"--device\", type=str, default=\"auto\", help=\"PyTorch device ('auto', 'cpu', 'cuda:0').\")\n",
    "    p.add_argument(\"--output_root\", type=str, default=\"/workspace/results/retrieval_docs\", help=\"Root directory for outputs.\")\n",
    "    p.add_argument(\"--query_instruction\", type=str, default=None, help=\"Instruction to prepend to queries. Overrides model default.\")\n",
    "    p.add_argument(\"--force_numpy\", action=\"store_true\", help=\"Force using NumPy retriever even if FAISS is available.\")\n",
    "    args = p.parse_args()\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    print(\"[INFO] Loading data...\", file=sys.stderr)\n",
    "    with open(args.config_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = json.load(f)\n",
    "    model_name = config.get(\"model_name\")\n",
    "    if not model_name:\n",
    "        raise ValueError(\"'model_name' not found in config JSON.\")\n",
    "        \n",
    "    vectordb = data_loader.load_vectordb_from_csv(args.vectordb_csv)\n",
    "    all_questions = data_loader.load_questions_jsonl(args.questions_jsonl)\n",
    "    \n",
    "    selected_questions = data_loader.select_questions(\n",
    "        all_questions,\n",
    "        ids=args.ids.split(\",\") if args.ids else None,\n",
    "        range_spec=args.range_spec\n",
    "    )\n",
    "    print(f\"[INFO] Loaded {len(vectordb.doc_ids)} documents.\", file=sys.stderr)\n",
    "    print(f\"[INFO] Selected {len(selected_questions)} questions to process.\", file=sys.stderr)\n",
    "\n",
    "    # --- 2. Initialize Components ---\n",
    "    encoder = query_encoder.QueryEncoder(model_name=model_name, device=args.device)\n",
    "    retriever = get_retriever(vectordb.embeddings, force_numpy=args.force_numpy)\n",
    "    saver = result_saver.ResultSaver(args.output_root)\n",
    "\n",
    "    # --- 3. Run Retrieval Loop ---\n",
    "    saved_paths = []\n",
    "    for q_item in selected_questions:\n",
    "        retrieved_data = run_retrieval_for_question(\n",
    "            q_item, encoder, retriever, vectordb, args.top_k, args.query_instruction\n",
    "        )\n",
    "        path = saver.save(q_item.qid, model_name, retrieved_data, q_item.meta)\n",
    "        saved_paths.append(path)\n",
    "        print(f\"[OK] Saved result for QID {q_item.qid} to {path}\", file=sys.stderr)\n",
    "        \n",
    "    # --- 4. Final Output ---\n",
    "    print(json.dumps({\"saved_files\": saved_paths, \"output_folder\": saver.session_folder}, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
